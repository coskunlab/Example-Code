{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections import defaultdict\n",
    "from tqdm.auto import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import networkx as nx\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric.utils\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_dir = (Path().cwd().parents[0]).absolute()\n",
    "\n",
    "module_path = str(p_dir / \"src\")\n",
    "\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = (Path().cwd().parents[0] / 'data').absolute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\torchaudio\\backend\\utils.py:74: UserWarning: No audio backend is available.\n",
      "  warnings.warn(\"No audio backend is available.\")\n"
     ]
    }
   ],
   "source": [
    "import lightning.pytorch as pl\n",
    "import PPIGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "# Define condition mapping\n",
    "condition_mapping = {'HCC827Ctrl': 0, 'HCC827Osim': 1}\n",
    "\n",
    "# Load graph dataset and process if neede\n",
    "graph_path = data_dir / '9PPI' / 'graphs' \n",
    "\n",
    "dataset = PPIGraph.GraphDataset(graph_path, 'raw', 'pt', condition_mapping=condition_mapping, n_c=2)\n",
    "train_set, val_set, test_set = PPIGraph.train_test_val_split(dataset)\n",
    "\n",
    "# Create Dataloader\n",
    "train_loader = DataLoader(train_set, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Print some information on the dataset\n",
    "# print(f'Dataset: {dataset}:')\n",
    "# print('======================')\n",
    "# print(f'Number of graphs: {len(dataset)}')\n",
    "# print(f'Number of features: {dataset.num_features}')\n",
    "# print(f'Number of classes: {dataset.num_classes}')\n",
    "\n",
    "# print(f'Train set: {len(train_set)}, test set: {len(test_set)}, val set: {len(val_set)}')\n",
    "# for step, data in enumerate(train_loader):\n",
    "\n",
    "#     print(f'Step {step + 1}:')\n",
    "#     print('=======')\n",
    "#     print(f'Number of graphs in the current batch: {data.num_graphs}')\n",
    "#     print(data)\n",
    "#     print()\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-fold on filter dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from lightning.pytorch.accelerators import find_usable_cuda_devices\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from torch.utils.data import SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out by maximum number of counts per cell\n",
    "min_count = 100\n",
    "max_count = 400\n",
    "\n",
    "graph_path = data_dir / '9PPI' / 'graphs' \n",
    "dataset = PPIGraph.GraphDataset(graph_path, 'raw', 'pt', condition_mapping=condition_mapping, n_c=2)\n",
    "\n",
    "# Create Dataloader\n",
    "loader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# Get Indices\n",
    "indices = []\n",
    "for step, data in enumerate(loader):\n",
    "    if len(data.x) <= min_count:\n",
    "        continue \n",
    "    \n",
    "    if (data.x.sum(axis=0) >= max_count).any():\n",
    "        continue\n",
    "    indices.append(step)\n",
    "    \n",
    "# Get subset dataset\n",
    "dataset_filtered = dataset.index_select(indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1368"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = '9PPI_v3'\n",
    "condition = 'Kfold'\n",
    "checkpoint_folder = (Path().cwd().parents[0]).absolute() / 'data' / '9PPI' /\"saved_models\" / dataset_name / f\"Graph_GNNs_{condition}\" \n",
    "project_name = f'PLA_10152023_{dataset_name}_{condition}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    }
   ],
   "source": [
    "AVAIL_GPUS = [0]\n",
    "\n",
    "# Setting the seed\n",
    "pl.seed_everything(42)\n",
    "\n",
    "NUM_LAYERS = 2\n",
    "HIDDEN_CHANNELS = 16\n",
    "pools = ['mean', 'max', 'sum', 'attention', 'attention2']\n",
    "num_layers = [2,3,4]\n",
    "hiddens = [16, 32, 64]\n",
    "\n",
    "\n",
    "epochs = 80\n",
    "model = 'MLP'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision('medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mthoomas\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20231230_142742-hqvhlpln</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/hqvhlpln' target=\"_blank\">MLP_2_16_onehot_0</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/hqvhlpln' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/hqvhlpln</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_2_16_onehot_0\\mean\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 432   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "586       Trainable params\n",
      "0         Non-trainable params\n",
      "586       Total params\n",
      "0.002     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\utilities\\data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 2. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▄▄▄▄▄▄▄▄▄▅▅▇▇▇▇▇▇█▇▇██▇▇▇▇▇█▇█▇█████▇█▇</td></tr><tr><td>train_auc</td><td>▁▁▂▁▁▁▂▂▂▃▄▅▇▇▇▇▇▇█▇▇▇▇▇█▇█▇█▇█▇█████▇██</td></tr><tr><td>train_f1</td><td>▇▂▁▁▁▁▁▁▁▁▃▄▆▇▇█▇▇█▇▇█▇▇██▇▇█▇█▇█████▇█▇</td></tr><tr><td>train_loss_epoch</td><td>█▆▆▆▆▆▆▆▅▅▅▄▂▃▂▃▃▂▂▂▂▂▁▂▂▁▁▁▂▁▁▂▁▁▂▂▁▂▁▂</td></tr><tr><td>train_loss_step</td><td>▇▆█▇▇▇▇▇▆▆█▆▃▅▇▄▄▅▅▄▆▆▅▄▁▄█▃█▃▅▄▄▂▂▄▄▄▆▄</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▂▃▆▇▇▇▇▆█▆▆▇▇▆▆▇▇▆▇█▇█▇▇█▇█▇▇█▆</td></tr><tr><td>val_auc</td><td>▁▅▆▇▇██▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▂▃▅▆▇▆▆▇▇▇█▆▇▇▇▇▇▇▇█▆▇▇▇█▇▇▇▇▇▆</td></tr><tr><td>val_loss_epoch</td><td>▇██▆▆▆▇▇▆▆▆▆▅▃▄▃▅▇▆▅█▆▄▇▁▂▇▅▅▂▂▁▄▅▃▅▂▅▄▃</td></tr><tr><td>val_loss_step</td><td>▆▇▇▅▅▅▆▆▅▅▆▆▅▃▄▃▅▇▆▅█▆▄▇▁▂▇▆▅▂▂▁▅▆▃▅▂▅▄▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.6819</td></tr><tr><td>train_auc</td><td>0.73768</td></tr><tr><td>train_f1</td><td>0.48368</td></tr><tr><td>train_loss_epoch</td><td>0.59923</td></tr><tr><td>train_loss_step</td><td>0.57701</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.64964</td></tr><tr><td>val_auc</td><td>0.6735</td></tr><tr><td>val_f1</td><td>0.41463</td></tr><tr><td>val_loss_epoch</td><td>0.5864</td></tr><tr><td>val_loss_step</td><td>0.53132</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_16_onehot_0</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/hqvhlpln' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/hqvhlpln</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231230_142742-hqvhlpln\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20231230_144656-qxnf2cn3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/qxnf2cn3' target=\"_blank\">MLP_2_16_onehot_0</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/qxnf2cn3' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/qxnf2cn3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_2_16_onehot_0\\max\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 432   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "586       Trainable params\n",
      "0         Non-trainable params\n",
      "586       Total params\n",
      "0.002     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\torch_geometric\\warnings.py:17: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▅▆▆▆▆▆▆▆▆▆▆▆▆▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇▇▇▇▇▇</td></tr><tr><td>train_auc</td><td>▃▂▄▂▃▁▅▂▂▄▄▄▂▃▁▃▅▅▄▃▅▄▅▄▆▆▆▃▇▆▆██▇▆█▅█▆█</td></tr><tr><td>train_f1</td><td>█▃▂▂▁▁▁▁▁▁▁▁▁▁▃▃▄▄▄▅▄▅▅▅▄▅▄▅▄▆▅▅▅▆▅▅▅▄▅▅</td></tr><tr><td>train_loss_epoch</td><td>█▄▃▃▃▃▃▃▂▂▂▃▂▃▂▂▂▂▂▂▂▂▁▂▂▁▁▁▂▂▁▁▁▂▂▂▁▂▂▁</td></tr><tr><td>train_loss_step</td><td>▃▄▆▆▅▅▅▄▃▆▇▆▃▂▄▃▂▂█▃▄▂▄▄▃▃▇▃▆▂▁▄▂▃▃▅▆▁▃▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁███████████▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>▄▄▇▅▃▂▂▂▁▂▂▄▂▂▂▂▂▂▂▄▄▄██▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█</td></tr><tr><td>val_f1</td><td>█▁▁▁▁▁▁▁▁▁▁▁████████████████████████████</td></tr><tr><td>val_loss_epoch</td><td>▂▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▃▃▄▄▄▅▂▄▄▄▆▆█▇▇▅▅█▆█▇</td></tr><tr><td>val_loss_step</td><td>▂▂▂▁▁▁▂▂▁▁▂▂▂▂▂▃▃▂▁▃▃▄▅▃▆▁▄▃▃▆▅█▆▇▄▄█▅█▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.60969</td></tr><tr><td>train_auc</td><td>0.55675</td></tr><tr><td>train_f1</td><td>0.29187</td></tr><tr><td>train_loss_epoch</td><td>0.65343</td></tr><tr><td>train_loss_step</td><td>0.63648</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.40876</td></tr><tr><td>val_auc</td><td>0.67419</td></tr><tr><td>val_f1</td><td>0.58031</td></tr><tr><td>val_loss_epoch</td><td>0.91555</td></tr><tr><td>val_loss_step</td><td>0.89952</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_16_onehot_0</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/qxnf2cn3' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/qxnf2cn3</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231230_144656-qxnf2cn3\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20231230_151935-r941xpc1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/r941xpc1' target=\"_blank\">MLP_2_16_onehot_0</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/r941xpc1' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/r941xpc1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_2_16_onehot_0\\sum\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 432   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "586       Trainable params\n",
      "0         Non-trainable params\n",
      "586       Total params\n",
      "0.002     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▃▃▂▃▄▅▄▅▅▄▅▆▆▅▅▆▆▆▅▇▇▆▇▆▇█▆▇▇█▇▇▇█▇██▇▇</td></tr><tr><td>train_auc</td><td>▅▅█▃▇▄▇▅▆▆▇▅▆▅▄▅▅▆▅▅▅▆▅▄▄▃▃▄▃▃▃▄▃▃▃▂▃▄▁▁</td></tr><tr><td>train_f1</td><td>▇▁▄▆▅▅▄▅▆▄▅▄▆▄▅▅▅▅▆▃▆▇▄▅▆▅▆▆▄▇▆▆▆▆▇▆█▆▆▅</td></tr><tr><td>train_loss_epoch</td><td>█▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▂█▃▅▃▅▃▃▄▆▅▅▆▆▆▆▆▇▆▇▇▆▆▆▆▆▆▇▆▇▅▆▅▇▇▆▅▆</td></tr><tr><td>val_auc</td><td>▁▁█▂█▂▇▄▃▄▇▃▆▂▁▃▂▆▄▁▂▂▂▁▁▁▂▁▁▂▁▂▁▁▁▂▂▂▂▁</td></tr><tr><td>val_f1</td><td>▁▁▂█▃▆▃▆▃▄▅▆▆▄▅▆▆▅▆▇▅▇▇▆▇▇▆▇▇▇▇▆▇█▇▇█▆▇▆</td></tr><tr><td>val_loss_epoch</td><td>█▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_step</td><td>█▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.63985</td></tr><tr><td>train_auc</td><td>0.40005</td></tr><tr><td>train_f1</td><td>0.44972</td></tr><tr><td>train_loss_epoch</td><td>0.62222</td></tr><tr><td>train_loss_step</td><td>0.61641</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.64599</td></tr><tr><td>val_auc</td><td>0.30655</td></tr><tr><td>val_f1</td><td>0.35762</td></tr><tr><td>val_loss_epoch</td><td>0.58406</td></tr><tr><td>val_loss_step</td><td>0.55087</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_16_onehot_0</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/r941xpc1' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/r941xpc1</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231230_151935-r941xpc1\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20231230_153958-ynynbg80</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/ynynbg80' target=\"_blank\">MLP_2_16_onehot_0</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/ynynbg80' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/ynynbg80</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\torch_geometric\\deprecation.py:22: UserWarning: 'nn.glob.GlobalAttention' is deprecated, use 'nn.aggr.AttentionalAggregation' instead\n",
      "  warnings.warn(out)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_2_16_onehot_0\\attention\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 432   \n",
      "2  | head        | Sequential       | 154   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 17    \n",
      "--------------------------------------------------\n",
      "603       Trainable params\n",
      "0         Non-trainable params\n",
      "603       Total params\n",
      "0.002     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▄▄▄▄▄▄▄▅▆▇▇▇▇▇█▇█▇▇█▇██▇▇████████████▇█</td></tr><tr><td>train_auc</td><td>▂▂▁▁▂▁▂▂▂▄▅▇▇▆▆▇▇▇▇▇█▇█▇█▇▇██████▇█▇██▇█</td></tr><tr><td>train_f1</td><td>▇▂▁▁▁▁▁▁▂▆▆▆▇▅▇█▇▇▇▇▇█▇▇▇▇██▇▇▇█▇██▇████</td></tr><tr><td>train_loss_epoch</td><td>█▇▇▇▆▆▇▆▆▄▃▃▂▃▄▂▂▂▂▂▁▂▁▃▂▃▂▂▁▁▂▁▂▂▁▂▁▁▃▂</td></tr><tr><td>train_loss_step</td><td>▇▆▆▆▇▆▆▆▅▄▅▄▄▅▅▄▄▅▃▅▅▄▂▅▅▅▅▆▄▁▆▂▆▄▃▂▅█▆▄</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▃▃▃▆▆▄▃▅▆▆▇▅▆▅▇▇▇▆▆▇▇▇▇▇▇▄▇█▆█▇█</td></tr><tr><td>val_auc</td><td>▁▆▇███▇▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▂▃▄▇▇▅▄▅▇▇█▆▆▆███▆▆█▇▇▇██▅██▇███</td></tr><tr><td>val_loss_epoch</td><td>▆▅▆▅▅▅▆▅▅▄▄▃▄▇▆▆▁▄▃▅▅▄▄▄▄▄▁█▄▄▇▆▅▆▇▅▃▂▄▄</td></tr><tr><td>val_loss_step</td><td>▅▄▆▅▅▅▅▅▅▃▃▃▄▇▆▆▁▄▃▅▅▄▄▄▄▄▁█▄▄▇▆▅▅▇▄▃▂▄▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.69104</td></tr><tr><td>train_auc</td><td>0.72287</td></tr><tr><td>train_f1</td><td>0.56997</td></tr><tr><td>train_loss_epoch</td><td>0.60343</td></tr><tr><td>train_loss_step</td><td>0.59181</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.66423</td></tr><tr><td>val_auc</td><td>0.68623</td></tr><tr><td>val_f1</td><td>0.4026</td></tr><tr><td>val_loss_epoch</td><td>0.63041</td></tr><tr><td>val_loss_step</td><td>0.63073</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_16_onehot_0</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/ynynbg80' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/ynynbg80</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231230_153958-ynynbg80\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20231230_160003-mg7gvnjb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/mg7gvnjb' target=\"_blank\">MLP_2_16_onehot_0</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/mg7gvnjb' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/mg7gvnjb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_2_16_onehot_0\\attention2\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 432   \n",
      "2  | head        | Sequential       | 154   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 281   \n",
      "--------------------------------------------------\n",
      "867       Trainable params\n",
      "0         Non-trainable params\n",
      "867       Total params\n",
      "0.003     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▄▄▄▄▄▄▄▄▅▆▇▇▆▇▇▇█▇▇▇▇▇▇▇█▇▇▇██▇▇▇▇▇▇██▇</td></tr><tr><td>train_auc</td><td>▁▂▂▁▂▂▃▁▂▃▅▇▇▆▇▇▇▇▇▇▇▇▇█▇█▇▇▇▇▇▇▇█▇▇▇██▇</td></tr><tr><td>train_f1</td><td>▇▃▂▂▁▁▁▁▁▂▅▆▆▅█▇▇▇▇▇▇▇▇▇▇▇██▇▇▇▇▇▇▇▇▇██▇</td></tr><tr><td>train_loss_epoch</td><td>█▆▆▇▇▆▆▆▆▅▄▃▃▅▃▃▃▃▂▃▂▃▂▂▂▂▂▃▃▁▃▃▂▂▂▂▂▂▂▃</td></tr><tr><td>train_loss_step</td><td>▇█▇▇▆▇▇▇▆▆▆▆▄▇▄▅▅▅▇▄▃▄▃▁▃▃▆▅▄▄▄▄▆▆▄▅▃▆▆▆</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▂▃▄██▅▅█▆▆▇█▇▇▇██▇█▇█▇███████▇█▇</td></tr><tr><td>val_auc</td><td>▁▆███▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▂▂▄▇▇▅▅█▆▆▇█▇▇▇▇▇▇▇█▇▇██▇████▇█▇</td></tr><tr><td>val_loss_epoch</td><td>▆▆▆▆▆▆▇▆▆▅▄▅▃▅▅▅▅▄█▅▅▆▃▃▁▇▅▄▅▅▃▄▇▃▄▆█▃▅▆</td></tr><tr><td>val_loss_step</td><td>▆▆▆▆▅▆▆▆▆▅▄▅▃▅▅▅▅▄█▅▅▆▃▃▁▇▅▄▅▅▃▄▇▃▄▆█▃▅▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.69013</td></tr><tr><td>train_auc</td><td>0.72518</td></tr><tr><td>train_f1</td><td>0.56031</td></tr><tr><td>train_loss_epoch</td><td>0.60374</td></tr><tr><td>train_loss_step</td><td>0.64441</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.65328</td></tr><tr><td>val_auc</td><td>0.66231</td></tr><tr><td>val_f1</td><td>0.48087</td></tr><tr><td>val_loss_epoch</td><td>0.66783</td></tr><tr><td>val_loss_step</td><td>0.70212</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_16_onehot_0</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/mg7gvnjb' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/mg7gvnjb</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231230_160003-mg7gvnjb\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20231230_162040-d0cwiqrl</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/d0cwiqrl' target=\"_blank\">MLP_2_32_onehot_0</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/d0cwiqrl' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/d0cwiqrl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_2_32_onehot_0\\mean\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 1.4 K \n",
      "2 | head        | Sequential       | 562   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.008     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▁▁▂▁▄▅▆▆▇▇▇▇▇▇▇██▆▆▇▇█▇█▇▇▇▆▆▇█▇▇▇▇▆▆█</td></tr><tr><td>train_auc</td><td>▁▁▁▁▂▃▅▆▇▇▇▇██▇▇▇███▇█████▇▇█████████▇▇█</td></tr><tr><td>train_f1</td><td>▁▁▁▁▂▁▅▅▇▇▇▇▆█▇▇▇██▇▇█▇▇▇█▇▇█▇▇██▇▇█▇▆▇█</td></tr><tr><td>train_loss_epoch</td><td>██▇▇▇▇▆▄▄▄▃▃▂▂▃▂▂▂▂▂▂▂▂▂▂▁▃▂▂▃▃▂▂▂▁▃▂▃▂▃</td></tr><tr><td>train_loss_step</td><td>▅▆▆▆▄▅▄▄▂▃▃▃▁▃▁▂▁▃▃▁▃▄▃▃▃▃▅▂▅▄▃▃▄▃▃█▃▄▃▄</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▂▂▃▅▅▆▇▆▆▆▇▇▇▆█▆▆▇▇▇▇██▇██▆█▇▇▇▇▅█▇█</td></tr><tr><td>val_auc</td><td>▁▅▆▆▇▇▇▇▇███████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▂▂▃▇▇▇▇█▆▇▇▇▇▇▇▇▇█▇▇▇██▇▇█▆██▇▇██▇▇▇</td></tr><tr><td>val_loss_epoch</td><td>▇▅▇▇▆▆▇▅▆▃█▃▅█▆▄▅▄▆▄▅▇▃▆█▃▅▁▅▁▄▂▂▃▇▃▄▃▇█</td></tr><tr><td>val_loss_step</td><td>▆▄▆▆▅▆▇▅▆▃█▃▅▇▆▄▅▄▆▄▅▇▃▆█▄▅▁▅▁▄▂▂▃▇▃▄▃▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.70658</td></tr><tr><td>train_auc</td><td>0.74831</td></tr><tr><td>train_f1</td><td>0.63146</td></tr><tr><td>train_loss_epoch</td><td>0.59998</td></tr><tr><td>train_loss_step</td><td>0.63551</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.67153</td></tr><tr><td>val_auc</td><td>0.68155</td></tr><tr><td>val_f1</td><td>0.49438</td></tr><tr><td>val_loss_epoch</td><td>0.70404</td></tr><tr><td>val_loss_step</td><td>0.79225</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_32_onehot_0</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/d0cwiqrl' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/d0cwiqrl</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231230_162040-d0cwiqrl\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20231230_164036-06k1mri2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/06k1mri2' target=\"_blank\">MLP_2_32_onehot_0</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/06k1mri2' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/06k1mri2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_2_32_onehot_0\\max\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 1.4 K \n",
      "2 | head        | Sequential       | 562   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.008     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\torch_geometric\\warnings.py:17: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▃▃▃▃▄▄▅▄▄▅▅▆▆▆▆▆▅▆▆▆▆▅▆▇█▆▇▆▆▇▇▇▆█▇▇█▇▇</td></tr><tr><td>train_auc</td><td>▁▂▁▁▂▂▂▃▅▅▅▅▆▅▆▅▆▇▇▆▆▆▆▇▇▇▆▇▇▇▇▇▇██▇▇▇▇▇</td></tr><tr><td>train_f1</td><td>▅▂▂▁▁▂▄▅▅▄▅▅▆▅▆▆▇▆▆▇▆▇▆▆▇▇▇▇▇▆▇█▇▆█▇▇▇██</td></tr><tr><td>train_loss_epoch</td><td>█▇▆▆▅▅▅▅▆▅▄▄▃▃▃▃▄▂▃▃▄▃▄▃▂▁▄▃▃▂▂▃▂▂▂▃▂▂▁▃</td></tr><tr><td>train_loss_step</td><td>▇▇▇█▅▇▅▄▅▆▄▆▃▄▁▅▃▄▆▅▇▆▆▅▄▂▅▄▃▁▆▆▆▅▅▅▅▆▂▆</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▇▇▇▇▇▇▇▇██▇█▇▇▇▆▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>▂▁▄▆▇███████████████████████████▇▇▇▇█▇█▇</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▆▆█▇████▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>val_loss_epoch</td><td>▂▁▂▂▁▂▂▂▂▂▂▂▂▂▂▂▃▃▃▄▄▄▅▄▄▆█▅▆▇▇█▆▄▅▇█▇▅▅</td></tr><tr><td>val_loss_step</td><td>▂▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▃▃▃▄▄▄▅▃▃▆█▄▆▇▆▇▆▃▄▆█▇▄▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.65448</td></tr><tr><td>train_auc</td><td>0.67486</td></tr><tr><td>train_f1</td><td>0.54126</td></tr><tr><td>train_loss_epoch</td><td>0.64134</td></tr><tr><td>train_loss_step</td><td>0.68005</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.40876</td></tr><tr><td>val_auc</td><td>0.6409</td></tr><tr><td>val_f1</td><td>0.58031</td></tr><tr><td>val_loss_epoch</td><td>0.88463</td></tr><tr><td>val_loss_step</td><td>0.83628</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_32_onehot_0</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/06k1mri2' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/06k1mri2</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231230_164036-06k1mri2\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20231230_170111-uai9k1ej</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/uai9k1ej' target=\"_blank\">MLP_2_32_onehot_0</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/uai9k1ej' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/uai9k1ej</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_2_32_onehot_0\\sum\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 1.4 K \n",
      "2 | head        | Sequential       | 562   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.008     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▂▃▃▃▅▅▄▅▅▅▆▆▆▆▆▆▆▇▆▇▇▇▇▇▆▇▇█▇█▇▇█▇▆▇▇▇</td></tr><tr><td>train_auc</td><td>▆▅▅▆▆▆▅▆█▆▆▆▇▆▅▆▆▆▆▄▄▄▅▅▅▂▃▃▄▂▃▁▂▃▂▁▂▁▂▂</td></tr><tr><td>train_f1</td><td>▂▃▂▃▄▁▃▃▄▃▄▄▅▆▆▅▅▆▆▆▆▇▇▆▆▇▆▆▇█▆█▇▇███▆▆▇</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▅▃▃▂▂▂▂▁▂▂▁▂▁▁▁▁▁▂▁▁▂▁▁▁▁▂▁▁▁▂▁▂▁▁▂▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▄▁▄▄▄▅▄▄▆▅▆▆▆▇▆▇▇▇▇▇▇▇▇██▇▇▇▇▇▇▇▇▇▇▇▇▆█▇</td></tr><tr><td>val_auc</td><td>▁█▂▃▆█▂▇▆▅▇▃▅█▃▃▇▄▃▁▁▁▃▃▁▁▂▃▂▂▁▁▂▁▁▁▁▂▁▁</td></tr><tr><td>val_f1</td><td>▁▅▆▁▂▃▁▂▃▃▄▄▄▄▆▇▇▆▇▇▆▇▇▇▆▆▇▇▇▇▇▇▇▇▇▇██▇▇</td></tr><tr><td>val_loss_epoch</td><td>█▇█▅▅▆▇▅▆▄▇▃▅▆▅▅▅▄▆▄▄▆▃▅▇▄▄▁▃▁▄▂▃▃▇▄▄▅▇▆</td></tr><tr><td>val_loss_step</td><td>▅▆▇▅▅▆▇▅▆▄▇▃▆▆▆▅▅▄▆▄▅▇▄▆█▅▅▁▄▁▅▂▃▄█▄▄▅▇▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.68921</td></tr><tr><td>train_auc</td><td>0.41053</td></tr><tr><td>train_f1</td><td>0.59036</td></tr><tr><td>train_loss_epoch</td><td>0.60662</td></tr><tr><td>train_loss_step</td><td>0.59777</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.66058</td></tr><tr><td>val_auc</td><td>0.28946</td></tr><tr><td>val_f1</td><td>0.50794</td></tr><tr><td>val_loss_epoch</td><td>0.64073</td></tr><tr><td>val_loss_step</td><td>0.68635</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_32_onehot_0</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/uai9k1ej' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/uai9k1ej</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231230_170111-uai9k1ej\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20231230_172143-1n538l1j</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/1n538l1j' target=\"_blank\">MLP_2_32_onehot_0</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/1n538l1j' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/1n538l1j</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\torch_geometric\\deprecation.py:22: UserWarning: 'nn.glob.GlobalAttention' is deprecated, use 'nn.aggr.AttentionalAggregation' instead\n",
      "  warnings.warn(out)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_2_32_onehot_0\\attention\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 1.4 K \n",
      "2  | head        | Sequential       | 562   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 33    \n",
      "--------------------------------------------------\n",
      "2.0 K     Trainable params\n",
      "0         Non-trainable params\n",
      "2.0 K     Total params\n",
      "0.008     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▁▁▁▄▆▆▆▇▇▇▇▆█▆▆▇▆▇██▇█▆▆▇█▆▆█▇▇▆███▇▆█</td></tr><tr><td>train_auc</td><td>▁▂▂▂▄▅▇▇▇▇▇▇█▇▇█████████▇█████████████▇█</td></tr><tr><td>train_f1</td><td>▁▁▁▁▁▃▆▆▇▇▇▇▇▇▇▆▆█▇▇▇▇██▆▆▇▇▇▇█▇▇▆▇▇█▇██</td></tr><tr><td>train_loss_epoch</td><td>██▇█▇▅▃▄▃▃▃▃▂▂▂▃▂▂▂▂▃▂▃▃▂▂▂▂▂▂▂▂▃▂▁▁▃▂▄▁</td></tr><tr><td>train_loss_step</td><td>▅▇▆▅▅▃▅▅▆▂▃▄▁▁▃▂▃▅▄▅▆▃▄▅▃▃▂▂▄▃▃▃▂▄▃▆▃█▄▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▂▄▄▇▇▆▆▆▇▆▆▇▅▅▆▇▇▆▅█▇▄▆▆▆▆▇█▇█▇▇▇▇█▇</td></tr><tr><td>val_auc</td><td>▁▆▆▇▇███████████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▂▃▄▇▆▆▆▆▇▆▇▇▅▆▆▇▇▆▅█▇▅▆▆▆▆▇█▇▇▆▇▇▆█▇</td></tr><tr><td>val_loss_epoch</td><td>▄▅▄▅▄▁▃▁▅▄▂▅▄█▃▃▄▄▆▆▁▂▆▅▇▄▆▅▅▆▅▄▁▅▂▃▁▄▄▆</td></tr><tr><td>val_loss_step</td><td>▄▅▄▄▄▁▃▂▅▅▃▅▄█▃▃▃▄▆▆▁▂▆▅▇▄▅▅▅▆▅▄▂▅▁▃▁▄▄▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.70475</td></tr><tr><td>train_auc</td><td>0.75517</td></tr><tr><td>train_f1</td><td>0.62572</td></tr><tr><td>train_loss_epoch</td><td>0.58731</td></tr><tr><td>train_loss_step</td><td>0.56342</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.65328</td></tr><tr><td>val_auc</td><td>0.67653</td></tr><tr><td>val_f1</td><td>0.37086</td></tr><tr><td>val_loss_epoch</td><td>0.72218</td></tr><tr><td>val_loss_step</td><td>0.80277</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_32_onehot_0</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/1n538l1j' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/1n538l1j</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231230_172143-1n538l1j\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20231230_174309-cfhu2xgb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/cfhu2xgb' target=\"_blank\">MLP_2_32_onehot_0</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/cfhu2xgb' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/cfhu2xgb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_2_32_onehot_0\\attention2\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 1.4 K \n",
      "2  | head        | Sequential       | 562   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 1.1 K \n",
      "--------------------------------------------------\n",
      "3.0 K     Trainable params\n",
      "0         Non-trainable params\n",
      "3.0 K     Total params\n",
      "0.012     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▁▁▁▂▃▅▆▆▆▆▇▇▇▇▆▆██▇▆▇▇▆▇▇██▇██▇▆▇█▇▇▇▇</td></tr><tr><td>train_auc</td><td>▁▁▂▁▃▄▆▇▇▇▇███▇██████████████████▇██████</td></tr><tr><td>train_f1</td><td>▁▁▁▁▁▂▄▇▇▇▇▆███▇▆▇███▇██▇▇▇██▇█▇▇▇▇█▇██▇</td></tr><tr><td>train_loss_epoch</td><td>█▇█▇▇▇▆▄▄▃▂▃▂▃▃▂▃▁▂▂▂▃▂▂▄▂▁▂▂▁▂▂▂▃▂▁▂▂▂▃</td></tr><tr><td>train_loss_step</td><td>▆▇▅▇▇▆▅▄▄▃▅▄▃▂▃▂▅▅▃▆▄▆▂▂▄▂▃█▃▆▄▁▃▅▅▃▂▃▁▇</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▃▄▅▇██▇▇▆███▇▇▇█▇▇▇▇▇▇▇▇▇▇▇█▇█▇▇▇▇▇</td></tr><tr><td>val_auc</td><td>▁▅▆▇▇▇▇█████████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▂▄▅▇█▇▇▆▆▇█▇▆▇▇▇▇▇▇▇▇▆▇▇▆▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>val_loss_epoch</td><td>▇▇▇▆▅▇▅▅▆▆▄▅▅▃▆▂▄█▅▆▄▇▆▆▆▆█▄▁▂▅▄▅▇▇▇▆▆▅▃</td></tr><tr><td>val_loss_step</td><td>▆▆▆▆▄▆▅▅▆▆▄▅▅▃▆▂▄█▅▆▄▇▆▇▆▆█▄▁▂▅▄▅▇▇▇▆▆▅▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.6883</td></tr><tr><td>train_auc</td><td>0.74534</td></tr><tr><td>train_f1</td><td>0.57107</td></tr><tr><td>train_loss_epoch</td><td>0.61112</td></tr><tr><td>train_loss_step</td><td>0.70661</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.65328</td></tr><tr><td>val_auc</td><td>0.67196</td></tr><tr><td>val_f1</td><td>0.46328</td></tr><tr><td>val_loss_epoch</td><td>0.58056</td></tr><tr><td>val_loss_step</td><td>0.5157</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_32_onehot_0</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/cfhu2xgb' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/cfhu2xgb</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231230_174309-cfhu2xgb\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20231230_180606-qpgl80dn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/qpgl80dn' target=\"_blank\">MLP_2_64_onehot_0</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/qpgl80dn' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/qpgl80dn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_2_64_onehot_0\\mean\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 4.8 K \n",
      "2 | head        | Sequential       | 2.1 K \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "6.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "6.9 K     Total params\n",
      "0.028     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▃▃▃▅▆▆▇▇▇█▇█▇█▇▇▇▇█▇█▇███▇███▇▇██▇▇▇▇▇▇</td></tr><tr><td>train_auc</td><td>▁▂▂▃▅▆▇█▇██▇█▇██▇▇▇████████████████▇████</td></tr><tr><td>train_f1</td><td>▅▄▂▁▅▅▇▇▇▇█▇█▇▇█▇▇▇█▇███▇▇▇▇▇██▇▇█▇▇█▇▇▇</td></tr><tr><td>train_loss_epoch</td><td>█▇▇▆▆▄▃▂▂▂▂▃▂▂▂▄▂▃▃▂▂▂▂▂▁▂▁▁▃▁▂▂▁▂▁▃▂▁▂▂</td></tr><tr><td>train_loss_step</td><td>▇▇▆▆▅▅▅▆▄▂▃▇▅▂▄█▃▄▅▂▃▄▅▄▃▃▅▅▄▆▄▃▄▅▅▇▄▁▅▅</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▂▂▃▆▆▇▇▇▇▇█▆▆▆█▅▆▆▇█▇▇▇▇▇▇█▇▇▇▆▇█▇█▇▆▇█</td></tr><tr><td>val_auc</td><td>▁▂▁▂▄▅▆▆▆▆▆▆▆▆▇▇▆▇▇▇▇██▇▇██████▇████████</td></tr><tr><td>val_f1</td><td>▁▂▂▂▆█▇▇▇▇▇▇▆▇▇██▇▇██▇▇██▇▇█▇▇██▇█▇▇▇█▇█</td></tr><tr><td>val_loss_epoch</td><td>▆▅▅▅▄▅▄▄▃▅▆▃▃▃▇▄▄▅▄▃▄▂▆▃▃▄▄▂▂▃▅▆▁▄▄█▁▃▅▃</td></tr><tr><td>val_loss_step</td><td>▅▄▄▄▄▅▄▄▃▄▆▃▂▃▇▄▄▅▄▃▄▂▅▃▃▄▄▂▂▃▅▆▁▄▄█▁▃▅▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.6883</td></tr><tr><td>train_auc</td><td>0.74348</td></tr><tr><td>train_f1</td><td>0.5689</td></tr><tr><td>train_loss_epoch</td><td>0.5895</td></tr><tr><td>train_loss_step</td><td>0.61807</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.67153</td></tr><tr><td>val_auc</td><td>0.68028</td></tr><tr><td>val_f1</td><td>0.53608</td></tr><tr><td>val_loss_epoch</td><td>0.56958</td></tr><tr><td>val_loss_step</td><td>0.51023</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_64_onehot_0</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/qpgl80dn' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/qpgl80dn</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231230_180606-qpgl80dn\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20231230_182642-jhcmxv2t</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/jhcmxv2t' target=\"_blank\">MLP_2_64_onehot_0</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/jhcmxv2t' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/jhcmxv2t</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_2_64_onehot_0\\max\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 4.8 K \n",
      "2 | head        | Sequential       | 2.1 K \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "6.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "6.9 K     Total params\n",
      "0.028     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\torch_geometric\\warnings.py:17: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▃▂▄▅▅▆▆▆▆▇▇▆▆▇▇▆▆▆▇▆▇█▇▇▇▇▇▇█▇▇▇▇▇▆▇▇▇▇</td></tr><tr><td>train_auc</td><td>▁▂▂▃▅▅▅▅▆▆▆▆▆▆▇▇▆▇▆▇▇▇▇▇▆▇▇▇▇█▇▇▇▇▆▇▇▇▇▇</td></tr><tr><td>train_f1</td><td>▄▄▂▁▅▄▆▆▅▇▇▇▇▆▆▇▆▆▇▇▆▇█▇▆▇▇▇▇█▇▇▆▇▇▆▇▇▆▇</td></tr><tr><td>train_loss_epoch</td><td>█▆▅▅▅▄▄▄▄▃▃▄▃▄▃▃▃▃▃▃▃▁▃▃▃▃▂▂▂▁▂▃▃▃▃▃▂▂▂▂</td></tr><tr><td>train_loss_step</td><td>█▇▅▆▅▆▇▆▆▃▃█▅▅▆█▅▂█▃▄▃▅▆▃▁▄▃▄▄▅▆▃▆▄▇▂▃▅▅</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇████▇▅▅▅▇▆▅▅▅▆▄▁▁▄▂▁▂▂▄▄</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_epoch</td><td>▁▁▁▁▁▁▁▂▂▂▂▄▃▃▃▃▄▄▆▅▆█▇█▆▆▇█▆▇▇▆▇▇▄▅▆▅▄▅</td></tr><tr><td>val_loss_step</td><td>▁▁▁▁▁▁▁▂▂▂▂▄▃▃▂▂▄▄▆▄▅█▆▇▅▄▆▇▅▇▅▅▆▆▂▃▄▄▂▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.67093</td></tr><tr><td>train_auc</td><td>0.70569</td></tr><tr><td>train_f1</td><td>0.54887</td></tr><tr><td>train_loss_epoch</td><td>0.6078</td></tr><tr><td>train_loss_step</td><td>0.63828</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.40876</td></tr><tr><td>val_auc</td><td>0.62698</td></tr><tr><td>val_f1</td><td>0.58031</td></tr><tr><td>val_loss_epoch</td><td>1.25377</td></tr><tr><td>val_loss_step</td><td>1.11188</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_64_onehot_0</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/jhcmxv2t' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/jhcmxv2t</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231230_182642-jhcmxv2t\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20231230_184736-4w3gnc4l</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/4w3gnc4l' target=\"_blank\">MLP_2_64_onehot_0</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/4w3gnc4l' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/4w3gnc4l</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_2_64_onehot_0\\sum\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 4.8 K \n",
      "2 | head        | Sequential       | 2.1 K \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "6.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "6.9 K     Total params\n",
      "0.028     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▄▃▃▄▅▆▇▇▇▆▇▆▇▇▇▇▇▇▆▇▇▇█▇█▇███▆▇▇▇██▇███</td></tr><tr><td>train_auc</td><td>▂▁▁▁▁▃▂▄▄▄▃▄▅▅▅▅▄▅▆▅▅▅▅▆▇▆▇▇▆▅▆█▇▆▇▇▇▇█▇</td></tr><tr><td>train_f1</td><td>▁▁▂▃▃▄▇▄▆▆▅▅█▇▆▇█▇▇▇▄▇▇█▇█▇█▇███▇█▆▆▆█▇█</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▄▄▂▁▂▂▂▁▁▂▂▁▁▁▂▁▂▂▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▆▁▇█▇▇██▇███▇██▇▇▇█▇▇▇█▇▇█▇▇▇██▇▇▇▇▇██▇▇</td></tr><tr><td>val_auc</td><td>▇▁██████████████▇▇██▇▇▇█████▇▇▇████▇█▇█▇</td></tr><tr><td>val_f1</td><td>▁█▇▇▆▄█▇▅▇█▇▄▇▇▇▇█▇▇█▇▇▇▇▇▇▇▇▇▇▇▇█▇█▇▇▇▇</td></tr><tr><td>val_loss_epoch</td><td>█▄▂▂▁▂▂▁▁▁▂▁▁▁▁▁▁▂▁▁▁▁▂▁▁▁▁▁▁▁▂▂▁▂▁▂▁▁▁▁</td></tr><tr><td>val_loss_step</td><td>█▅▂▂▂▃▂▂▂▂▂▂▁▂▂▂▂▂▂▂▂▁▂▁▂▂▂▁▂▂▂▃▁▂▂▃▁▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.7011</td></tr><tr><td>train_auc</td><td>0.68316</td></tr><tr><td>train_f1</td><td>0.62883</td></tr><tr><td>train_loss_epoch</td><td>0.60445</td></tr><tr><td>train_loss_step</td><td>0.65541</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.66058</td></tr><tr><td>val_auc</td><td>0.70905</td></tr><tr><td>val_f1</td><td>0.52308</td></tr><tr><td>val_loss_epoch</td><td>0.55587</td></tr><tr><td>val_loss_step</td><td>0.51089</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_64_onehot_0</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/4w3gnc4l' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/4w3gnc4l</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231230_184736-4w3gnc4l\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20231230_190632-epp0r0yj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/epp0r0yj' target=\"_blank\">MLP_2_64_onehot_0</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/epp0r0yj' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/epp0r0yj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\torch_geometric\\deprecation.py:22: UserWarning: 'nn.glob.GlobalAttention' is deprecated, use 'nn.aggr.AttentionalAggregation' instead\n",
      "  warnings.warn(out)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_2_64_onehot_0\\attention\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 4.8 K \n",
      "2  | head        | Sequential       | 2.1 K \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 65    \n",
      "--------------------------------------------------\n",
      "7.0 K     Trainable params\n",
      "0         Non-trainable params\n",
      "7.0 K     Total params\n",
      "0.028     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▃▃▅▆▇▇▇▇▇▇█▇▇▇▇█▇▇▇█▇▆▇▇█▇█▇█▇▇█▇▇▇██▇▇</td></tr><tr><td>train_auc</td><td>▁▂▂▅▆▇▇▇▇▇██▇▇▇▇█▇█▇██▇▇▇███▇████▇██████</td></tr><tr><td>train_f1</td><td>▄▁▂▄▆██▇▆▆▇▇█▆▇██▇▇▇▇▇▇▆▆▇▇█▇█▆▇██▇██▇▇▇</td></tr><tr><td>train_loss_epoch</td><td>█▇▇▆▅▂▃▃▃▃▃▁▂▃▂▂▃▃▂▂▂▃▄▂▂▂▁▁▂▂▂▂▂▃▁▂▃▂▂▂</td></tr><tr><td>train_loss_step</td><td>▆▇▇▄▅▄▄▂▃▄▅▃█▃▁▅▅▃▆▂▂▇▂▆▂▂▅▃▁▂▆▃▅▅▃▂▁▃▄▅</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▂▆▃▃▃▃▅▆▆▄▅▇▅▄▄▆▆▆▆▆▆▇▇▇▇▇▆▇▆▆▇▇▇█▆██▇</td></tr><tr><td>val_auc</td><td>▁▁▃▆▇▆▆▆▆▅▅▅▅▆▆▇▇▆▆▇▇▇▇████▇▇▇███▇▇████▇</td></tr><tr><td>val_f1</td><td>▁▁▂▆▅▃▄▅▇▇▇▆▅▇▆▅▅▆▇▇▇▇▇▇▇▇▇▇▆▇▇▇▇▇██▇██▇</td></tr><tr><td>val_loss_epoch</td><td>▇▇▇▅▆▇▇▇▆▆▇▆▇▇▅▅▅▅▅▇▃▃▄█▄▁▅▅▄▅▄▆▄▄▂▄▄▄▇▇</td></tr><tr><td>val_loss_step</td><td>▆▆▆▅▆▆▆▆▆▆▆▆▅▇▅▄▄▅▅▇▃▂▄█▄▁▄▅▃▅▄▆▃▄▂▄▄▄▇▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.69561</td></tr><tr><td>train_auc</td><td>0.76669</td></tr><tr><td>train_f1</td><td>0.59241</td></tr><tr><td>train_loss_epoch</td><td>0.57803</td></tr><tr><td>train_loss_step</td><td>0.61161</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.67518</td></tr><tr><td>val_auc</td><td>0.70486</td></tr><tr><td>val_f1</td><td>0.51366</td></tr><tr><td>val_loss_epoch</td><td>0.6721</td></tr><tr><td>val_loss_step</td><td>0.72838</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_64_onehot_0</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/epp0r0yj' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/epp0r0yj</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231230_190632-epp0r0yj\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20231230_192649-0sqoiv24</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/0sqoiv24' target=\"_blank\">MLP_2_64_onehot_0</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/0sqoiv24' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/0sqoiv24</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_2_64_onehot_0\\attention2\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 4.8 K \n",
      "2  | head        | Sequential       | 2.1 K \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 4.2 K \n",
      "--------------------------------------------------\n",
      "11.1 K    Trainable params\n",
      "0         Non-trainable params\n",
      "11.1 K    Total params\n",
      "0.045     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▃▃▆▆▆▇██▇▇▇▇▇▇▇▇▇▇▇▇█▇▇██▇█▇▇▇▇▇▇▇▇▇▇█</td></tr><tr><td>train_auc</td><td>▁▁▁▃▅▆▆▇██▇▇██▇█████▇▇█▇████████████████</td></tr><tr><td>train_f1</td><td>▅▄▁▂▅▆▅▇▇▇▇▇█▇▇█▇▇▆▆▆▇█▇▇▇▇▇▇▇▇█▇▇▆▆▇█▇▇</td></tr><tr><td>train_loss_epoch</td><td>███▇▅▅▅▃▃▂▂▃▂▃▃▁▂▂▂▂▄▃▂▃▂▃▂▂▁▁▁▃▃▂▃▂▂▂▂▂</td></tr><tr><td>train_loss_step</td><td>▆▆▆▅▄▄▆▃█▄▄█▅▅▃▃▂▄▄▄▄▄▃▄▆▅▁▁▅▄▄▂▃▂▂▃▄▂▂▅</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▂▂▄▅▆▃▆▆▅▇▆▅▆▇▅▅██▆▅▇▆▇▇█▇▇▇▅▆▇▆▅▇▇▇▆▆</td></tr><tr><td>val_auc</td><td>▁▁▂▃██▆▆▆▅▄▅▄▄▄▄▄▄▄▅▄▄▄▄▄▄▄▄▄▄▄▅▅▅▅▆▆▆▆▅</td></tr><tr><td>val_f1</td><td>▁▁▂▂▄█▆▄▆▆▆▇▆▅▆▇▇▇▇▇▆▅▇▇▇▇▇▇▇▇▆▆▇▇▇▇▇▆▇▇</td></tr><tr><td>val_loss_epoch</td><td>▅▄▅▄▅▃▅▄▆▅▅▄▂▃▆▁▃▃▃▃▄▃▄▅▂▅▃▄▄▃█▄▅▆▃▇▁▂▅▂</td></tr><tr><td>val_loss_step</td><td>▅▄▅▄▅▃▆▄▆▅▆▄▂▃▆▁▃▃▃▃▄▃▄▅▂▅▃▄▄▃█▄▅▆▃▇▂▂▆▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.71846</td></tr><tr><td>train_auc</td><td>0.75834</td></tr><tr><td>train_f1</td><td>0.60914</td></tr><tr><td>train_loss_epoch</td><td>0.58843</td></tr><tr><td>train_loss_step</td><td>0.63033</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.65693</td></tr><tr><td>val_auc</td><td>0.67526</td></tr><tr><td>val_f1</td><td>0.53465</td></tr><tr><td>val_loss_epoch</td><td>0.58764</td></tr><tr><td>val_loss_step</td><td>0.52534</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_64_onehot_0</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/0sqoiv24' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/0sqoiv24</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231230_192649-0sqoiv24\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20231230_194645-htnxqh6b</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/htnxqh6b' target=\"_blank\">MLP_3_16_onehot_0</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/htnxqh6b' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/htnxqh6b</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_3_16_onehot_0\\mean\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 704   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "858       Trainable params\n",
      "0         Non-trainable params\n",
      "858       Total params\n",
      "0.003     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▂▂▂▂▂▂▂▂▄▅▆▆▇▆▆▇▇▇▇▆▇▇▇▆▇▇▆█▇▇▇▇▇▇█▇▇▇</td></tr><tr><td>train_auc</td><td>▁▂▂▂▂▃▂▂▂▃▅▇▇▇█▇█████▇████████▇█▇████▇██</td></tr><tr><td>train_f1</td><td>▃▂▁▁▁▁▁▁▁▁▄▅▆▆▇▇▇█▇█▇▇▇█▇▇▇█▆█▇▇█▇▇█▇▇██</td></tr><tr><td>train_loss_epoch</td><td>█▇▇▇▇▆▇▇▇▆▅▄▃▄▂▂▂▃▂▃▂▄▂▂▂▂▁▃▃▂▂▃▂▂▂▁▁▃▂▂</td></tr><tr><td>train_loss_step</td><td>▇█▅▇▇█▆▆▇▇▄▄▄▇▄▃▂▄▂▄▄▆▆▅▄▃▇▇▄▃▅▄▂▅▆▁▄▅▂▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▂▃▇▇▇▆█▆▇▆▇▇▇▇▆▇▇▇█▇▆▆▇▇▇▇▇▇█▇▆</td></tr><tr><td>val_auc</td><td>▁▃▅▆▇▇▇▇████████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▂▃▇█▇▆▇▇▇▇▇▇▇▆█▇▇▇█▇▇▇▇██▆█▇▇▇▇</td></tr><tr><td>val_loss_epoch</td><td>▇█▇▆▆▆▇▅▇▆▆▅▃▂▁▅▅▃▆▂▅▄▃▂▂▇▃▃▆▇▆▇▂▅▅▇▆▄▁▃</td></tr><tr><td>val_loss_step</td><td>▇█▆▆▆▅▆▄▇▆▇▆▃▂▁▅▆▄▆▂▆▄▂▂▂█▃▃▇█▇█▂▆▆▇▆▄▁▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.69196</td></tr><tr><td>train_auc</td><td>0.73159</td></tr><tr><td>train_f1</td><td>0.5707</td></tr><tr><td>train_loss_epoch</td><td>0.59777</td></tr><tr><td>train_loss_step</td><td>0.5824</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.64234</td></tr><tr><td>val_auc</td><td>0.66601</td></tr><tr><td>val_f1</td><td>0.46154</td></tr><tr><td>val_loss_epoch</td><td>0.59471</td></tr><tr><td>val_loss_step</td><td>0.55425</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_16_onehot_0</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/htnxqh6b' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/htnxqh6b</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231230_194645-htnxqh6b\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20231230_200745-qy3f55kn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/qy3f55kn' target=\"_blank\">MLP_3_16_onehot_0</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/qy3f55kn' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/qy3f55kn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_3_16_onehot_0\\max\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 704   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "858       Trainable params\n",
      "0         Non-trainable params\n",
      "858       Total params\n",
      "0.003     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\torch_geometric\\warnings.py:17: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▄▅▅▅▅▅▆▆▆▆▆▆▆▇▆▆▆▆▆▇▇▇▇▇▇▆▇█▇▇▆█▆▇▇▆██</td></tr><tr><td>train_auc</td><td>▁▂▃▁▂▄▂▃▂▃▃▃▅▄▅▆▅▆▅▆▇▇▆▅▇▇▆▇██▆█▇▇▇██▇▇█</td></tr><tr><td>train_f1</td><td>▇▆▂▂▁▁▁▂▄▃▃▅▄▅▅▆▅▅▄▅▆▆▇▅▆▆▆▅▇▇▇▅▇▇▇▇█▅▇█</td></tr><tr><td>train_loss_epoch</td><td>█▅▄▄▃▃▄▃▄▃▂▃▃▃▂▃▃▃▃▃▂▃▂▃▂▂▂▂▂▂▂▂▂▂▂▁▂▃▁▂</td></tr><tr><td>train_loss_step</td><td>▇█▃▅▅▇▄▄▅▆▅▃▃▄▂▁▁▃▂▅▂▅▄▆▆▅▆▄█▂▅█▂▁▃▁▂▄▁▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>▇▅▅▂▁▅▇▇▆▇▇▇▇▇▇▇▇▇▇▆▇▇██▇▇▇███▇██▇████▇▇</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_epoch</td><td>▆█▅▄▄▃▅▁▆▄▆▆▂▃▁▃▁▄▅▁▄▁▂▅▂▇▃▃▃▄█▄▃▄▃▅▃▃▃▄</td></tr><tr><td>val_loss_step</td><td>▆▇▄▄▄▃▄▁▆▄▆▆▂▃▂▃▁▅▅▁▄▂▃▅▂▇▃▃▄▄█▄▃▄▃▆▄▃▃▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.62157</td></tr><tr><td>train_auc</td><td>0.62861</td></tr><tr><td>train_f1</td><td>0.39826</td></tr><tr><td>train_loss_epoch</td><td>0.65622</td></tr><tr><td>train_loss_step</td><td>0.65193</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.59124</td></tr><tr><td>val_auc</td><td>0.63881</td></tr><tr><td>val_f1</td><td>0.0</td></tr><tr><td>val_loss_epoch</td><td>0.66218</td></tr><tr><td>val_loss_step</td><td>0.6526</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_16_onehot_0</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/qy3f55kn' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/qy3f55kn</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231230_200745-qy3f55kn\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20231230_202837-aeecbach</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/aeecbach' target=\"_blank\">MLP_3_16_onehot_0</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/aeecbach' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/aeecbach</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_3_16_onehot_0\\sum\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 704   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "858       Trainable params\n",
      "0         Non-trainable params\n",
      "858       Total params\n",
      "0.003     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▃▂▂▂▃▃▃▃▄▃▄▄▄▃▄▄▅▄▅▆▅▆▆▆▆▅▆▅▅▅▆▆▆▇▇█▇██</td></tr><tr><td>train_auc</td><td>▄▆█▆▅▄▆▅▃▅▄▆▅▃▃▄▄▅▆▃▄▄▄▄▄▄▄▄▂▅▃▃▃▄▁▂▂▂▁▁</td></tr><tr><td>train_f1</td><td>▄▁▆▆▅▆▅▅▅▆▄▄▄▂▅▄▂▄▃▁▃▃▃▅▅▃▅▄▁▅▃▆▅▄▇██▃██</td></tr><tr><td>train_loss_epoch</td><td>█▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▂▂▃▂▂▂▁▁▁▁▁▁▁▃▃▁▃▃▃▃▄▃▅▃▅▅▅▅▄▅▅▅▅▆█▅▅▅</td></tr><tr><td>val_auc</td><td>███▁▂▁▁▁▂▂▂▁▁▂▂▁▂▂▁▁▁▁▁▁▂▂▂▁▂▂▂▂▂▂▂▂▂▂▂▂</td></tr><tr><td>val_f1</td><td>▁▁▂▂▃▂▂▂▁▁▁▁▁▁▁▃▃▁▃▃▃▃▄▃▅▃▅▅▅▅▄▅▅▅▅▆█▅▅▅</td></tr><tr><td>val_loss_epoch</td><td>▅█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_step</td><td>▄█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.64351</td></tr><tr><td>train_auc</td><td>0.40597</td></tr><tr><td>train_f1</td><td>0.48549</td></tr><tr><td>train_loss_epoch</td><td>0.63424</td></tr><tr><td>train_loss_step</td><td>0.61709</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.60949</td></tr><tr><td>val_auc</td><td>0.28423</td></tr><tr><td>val_f1</td><td>0.08547</td></tr><tr><td>val_loss_epoch</td><td>0.61644</td></tr><tr><td>val_loss_step</td><td>0.57466</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_16_onehot_0</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/aeecbach' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/aeecbach</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231230_202837-aeecbach\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20231230_204916-dyb46nw6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/dyb46nw6' target=\"_blank\">MLP_3_16_onehot_0</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/dyb46nw6' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/dyb46nw6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\torch_geometric\\deprecation.py:22: UserWarning: 'nn.glob.GlobalAttention' is deprecated, use 'nn.aggr.AttentionalAggregation' instead\n",
      "  warnings.warn(out)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_3_16_onehot_0\\attention\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 704   \n",
      "2  | head        | Sequential       | 154   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 17    \n",
      "--------------------------------------------------\n",
      "875       Trainable params\n",
      "0         Non-trainable params\n",
      "875       Total params\n",
      "0.003     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▁▁▁▁▁▁▁▄▄▄▆▆▆▆▇▇▆▆▇▇▇▆▆▇▇▇▇█▇▇▇█▆▇▇█▇▇</td></tr><tr><td>train_auc</td><td>▁▂▂▂▂▂▁▂▃▅▆▇▇▇▇▇▇▇▇▇▇▇▇█▇▇██████▇██▇██▇█</td></tr><tr><td>train_f1</td><td>▃▁▁▁▁▁▁▁▂▅▆▆▆▆▆▆██▆▇▇█▇▇▆▇▇▇▇███▇▇▆▇▇█▇▇</td></tr><tr><td>train_loss_epoch</td><td>██▇▇▇▇▇▇▇▆▅▄▃▄▃▄▃▃▃▂▂▄▂▃▃▂▂▃▂▂▂▂▁▁▂▃▁▂▃▃</td></tr><tr><td>train_loss_step</td><td>▇▆▇█▇▇▆▆█▇▇█▂█▃▆▃▄▄▅▅▆▄▁▂▅▂▃▆▃▃▂▃▅▄▄▃▂▅█</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▂▃▃▃▃▃▅▄▃▆▆▇▆▆▆▇▇▆█▇▆██▇█▆▆▇▇▆▆▆</td></tr><tr><td>val_auc</td><td>▁▄▅▅▆▆▇▇▇███████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▂▂▂▂▃▃▅▄▃▆▆▆▅▅▆▆▆▆▇▇▇▇▇▇▇█▇▆▇▇▇▇</td></tr><tr><td>val_loss_epoch</td><td>▅▅▆▆▅▆▆▅▆▄█▆▅▇▆▆▆▇▄▄▂▅▁▂▃▄▁▄▃▄▄▂▃▅▂▅▄▄▄▄</td></tr><tr><td>val_loss_step</td><td>▅▅▆▆▅▅▆▅▅▄█▆▅▇▆▆▆▇▅▄▂▅▁▂▃▅▁▅▄▅▄▂▄▅▃▆▅▅▄▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.67459</td></tr><tr><td>train_auc</td><td>0.73603</td></tr><tr><td>train_f1</td><td>0.55611</td></tr><tr><td>train_loss_epoch</td><td>0.62243</td></tr><tr><td>train_loss_step</td><td>0.71032</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.65328</td></tr><tr><td>val_auc</td><td>0.67389</td></tr><tr><td>val_f1</td><td>0.45087</td></tr><tr><td>val_loss_epoch</td><td>0.60647</td></tr><tr><td>val_loss_step</td><td>0.58513</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_16_onehot_0</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/dyb46nw6' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/dyb46nw6</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231230_204916-dyb46nw6\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20231230_211036-28dmftr1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/28dmftr1' target=\"_blank\">MLP_3_16_onehot_0</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/28dmftr1' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/28dmftr1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_3_16_onehot_0\\attention2\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 704   \n",
      "2  | head        | Sequential       | 154   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 281   \n",
      "--------------------------------------------------\n",
      "1.1 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.1 K     Total params\n",
      "0.005     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▁▁▁▁▁▁▁▂▃▄▆▆▇▇▇▇▆▇█▆▇█▇▇▇█▇▇▇█▇▇▇▇█▇▇█</td></tr><tr><td>train_auc</td><td>▂▂▂▂▁▂▂▂▂▃▄▅▇▇▇▇█▇▇▇█▇▇▇▇█▇█▇████▇▇▇██▇█</td></tr><tr><td>train_f1</td><td>▂▂▁▁▁▁▁▁▁▁▃▅▆▇▇▇▇▇▇▇▇▇██▇▇▇▇█▇██▇▇▇▇▇▇██</td></tr><tr><td>train_loss_epoch</td><td>█▇█▇▇█▇▇▇▇▇▆▃▅▃▂▂▁▂▂▂▃▄▂▂▂▂▁▃▂▁▂▃▂▃▁▂▁▃▁</td></tr><tr><td>train_loss_step</td><td>▇▇█▆▅▅▆▆▇▇▇▅▅▃▆▆▄▅▃▅█▄▇▃▇▃▂█▂▁▅▄▅▃▇▃▅▄▅▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▂▂▅▆▇▇▇▇▆▆▆█▇▇▇▆▇▆▆▇▆▇▆▇▆▇▆▇█▇▇</td></tr><tr><td>val_auc</td><td>▁▄▆▆▇▇▇▇▇▇██████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▂▂▄▆▇▇▇▇▇▇██▇▆█▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▆</td></tr><tr><td>val_loss_epoch</td><td>▆▇▇▆▇▆▅▅▆▆▆▅▄▃▄▆▂▃▆█▂▅▃█▂▁▁▅▄▆▅▅▄▆▂▂▆▃▄▃</td></tr><tr><td>val_loss_step</td><td>▅▆▆▅▆▅▄▄▅▆▅▅▄▃▄▆▂▃▆█▂▅▃█▂▁▁▅▄▆▅▅▄▆▂▂▆▃▄▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.69013</td></tr><tr><td>train_auc</td><td>0.73618</td></tr><tr><td>train_f1</td><td>0.57251</td></tr><tr><td>train_loss_epoch</td><td>0.58358</td></tr><tr><td>train_loss_step</td><td>0.52974</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.65328</td></tr><tr><td>val_auc</td><td>0.66226</td></tr><tr><td>val_f1</td><td>0.3949</td></tr><tr><td>val_loss_epoch</td><td>0.60401</td></tr><tr><td>val_loss_step</td><td>0.55384</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_16_onehot_0</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/28dmftr1' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/28dmftr1</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231230_211036-28dmftr1\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20231230_213133-xwgipivp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/xwgipivp' target=\"_blank\">MLP_3_32_onehot_0</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/xwgipivp' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/xwgipivp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_3_32_onehot_0\\mean\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 2.4 K \n",
      "2 | head        | Sequential       | 562   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "3.0 K     Trainable params\n",
      "0         Non-trainable params\n",
      "3.0 K     Total params\n",
      "0.012     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▂▂▂▂▂▂▂▃▆▇▆▇▇▇▇▇▅▇▇▇▇▇▇▇▇▇▇█▇▇▇▇▇▇▇█▇▇</td></tr><tr><td>train_auc</td><td>▂▂▁▂▂▂▂▃▄▅▇▇▇▇▇███▇▇██████████▇█████████</td></tr><tr><td>train_f1</td><td>▃▁▁▁▁▁▁▁▁▃▇▇▆▇▇▇███▇▇▇████▇▇▇█▇▇▇█▇▇▇███</td></tr><tr><td>train_loss_epoch</td><td>█▇███▇██▇▆▄▄▄▂▄▂▂▃▄▄▃▁▃▃▂▂▃▃▁▂▃▂▂▃▂▄▃▂▂▃</td></tr><tr><td>train_loss_step</td><td>▆▆▆▅▇▆▆▅▆▄▆▃▂▄▄▂▂▄▃▄▂▅█▄▃▃▂▃▅▅▆▅▅▁▅▃▁▁▂▆</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▂▄▆▅▆▆▆▇▆▆▆▆▇▇▆▇█▇█▇▇███▇▇█▇▇▇▇█</td></tr><tr><td>val_auc</td><td>▂▁▄▅▆▅▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇██▇▇███████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▂▄▇█▇▇▇▇▆▇▆█▇▇▇▇▇█▇▇▆▇▇▇▇█▇█▇▇▇▇</td></tr><tr><td>val_loss_epoch</td><td>▄▄▅▄▄▄▅▅▄▂▆▄▃▃▃▃▆▄█▃▃▃▁▆▅▂▆▅▂▆▄▅▃▁▄▂▄▂▄▅</td></tr><tr><td>val_loss_step</td><td>▃▃▄▄▃▄▅▄▄▂▅▄▃▃▃▃▆▄█▃▃▃▁▆▅▂▆▅▂▆▄▅▃▁▅▂▄▂▄▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.69835</td></tr><tr><td>train_auc</td><td>0.74164</td></tr><tr><td>train_f1</td><td>0.60337</td></tr><tr><td>train_loss_epoch</td><td>0.61175</td></tr><tr><td>train_loss_step</td><td>0.70365</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.66788</td></tr><tr><td>val_auc</td><td>0.67532</td></tr><tr><td>val_f1</td><td>0.51337</td></tr><tr><td>val_loss_epoch</td><td>0.68919</td></tr><tr><td>val_loss_step</td><td>0.76202</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_32_onehot_0</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/xwgipivp' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/xwgipivp</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231230_213133-xwgipivp\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20231230_215352-dee9pw2e</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/dee9pw2e' target=\"_blank\">MLP_3_32_onehot_0</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/dee9pw2e' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/dee9pw2e</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_3_32_onehot_0\\max\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 2.4 K \n",
      "2 | head        | Sequential       | 562   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "3.0 K     Trainable params\n",
      "0         Non-trainable params\n",
      "3.0 K     Total params\n",
      "0.012     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\torch_geometric\\warnings.py:17: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▄▄▄▄▄▄▄▄▄▄▅▅▅▅▆▆▆▆▄▄▅▅▆▇▇▆▆▅▅▅▅▆▆▆▇█▆▇▆</td></tr><tr><td>train_auc</td><td>▂▂▁▄▃▃▂▄▄▃▄▅▅▅▃▆▆▅▆▆▅▆▅▇▆▆▆▆▆▇▆▆▆▇▇██▆▇▇</td></tr><tr><td>train_f1</td><td>▅▁▁▁▁▂▁▁▁▁▁▂▃▂▃▃▆██▂▇▃▆▆▆▆▅▆▅▆▅█▅▇▆██▆▆█</td></tr><tr><td>train_loss_epoch</td><td>█▅▆▆▆▆▆▆▇▅▄▆▃▂▅▃▃▅▆▃▅▄▅▅▄▄▅▄▃▃▅▅▂▂▂▃▃▅▁▄</td></tr><tr><td>train_loss_step</td><td>▄▃▄▁▆▄▄▂▃▄▃▃▃▄▂▃▃█▂▅▂▂▄▄▄▃▂▁▅▁▄▃▄▃▄▆▂▃▃▄</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>█████▁███▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>▅▂▃▃▃▇█▇█▆██▆▁▃▂▃▃▃▅▅▅▄▃▃▅▆▇▇█▅▅▅▂▃▅▅▅▇▅</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁█▁▁▁███████████████████████████████</td></tr><tr><td>val_loss_epoch</td><td>▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▃▃▃▂▃▄▄▄▃▄▄▃▄▄▄▄▅▅▄▆▆█▇▇▆</td></tr><tr><td>val_loss_step</td><td>▁▁▁▁▁▁▁▁▁▂▂▁▂▂▂▃▃▂▁▂▃▄▄▂▃▄▂▃▃▃▄▅▅▂▅▄█▇█▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.60055</td></tr><tr><td>train_auc</td><td>0.60566</td></tr><tr><td>train_f1</td><td>0.44046</td></tr><tr><td>train_loss_epoch</td><td>0.66829</td></tr><tr><td>train_loss_step</td><td>0.68336</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.40876</td></tr><tr><td>val_auc</td><td>0.48553</td></tr><tr><td>val_f1</td><td>0.58031</td></tr><tr><td>val_loss_epoch</td><td>0.97668</td></tr><tr><td>val_loss_step</td><td>1.03151</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_32_onehot_0</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/dee9pw2e' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/dee9pw2e</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231230_215352-dee9pw2e\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20231230_221410-4ndtc2de</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/4ndtc2de' target=\"_blank\">MLP_3_32_onehot_0</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/4ndtc2de' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/4ndtc2de</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_3_32_onehot_0\\sum\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 2.4 K \n",
      "2 | head        | Sequential       | 562   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "3.0 K     Trainable params\n",
      "0         Non-trainable params\n",
      "3.0 K     Total params\n",
      "0.012     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▂▂▁▂▃▃▃▄▄▃▅▄▃▄▄▄▅▆▄▇▆▇▇█▇▇▇▇██▆▇▇▇▇▇▇▇▇▇</td></tr><tr><td>train_auc</td><td>██▆▅▅▅▇▆▇▆▆▅▅▄▆▆▄▃▅▄▃▄▃▂▃▃▂▂▃▂▂▃▂▂▂▂▃▂▁▂</td></tr><tr><td>train_f1</td><td>▄▄▅▃▄▄▁▄▂▄▃▂▅▄▄▄▅▇▇▇▆▆▇██▇▇▅▇█▇▇██▇▇▆▆▇█</td></tr><tr><td>train_loss_epoch</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▃▂▂▂▂▂▂▂▂▂▂▁▂▂▂▂▂▂▂▁▂▂▂▂▁▁▂▂▂▂▂▂▁▁▂▁▁▁▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▂▁▁▂▂▆▂▄▃▆▆█▅▅█▅▆▂▆▄▂▅▆▆▅▅▄▅▆▅▆▆▆▄▅▅▅▆▅▅</td></tr><tr><td>val_auc</td><td>█▁▄▁▁▂▅▄▄▂▁▁▁▁▂▁▁▁▂▃▂▂▁▂▂▂▂▂▂▂▂▂▂▂▂▃▂▂▂▂</td></tr><tr><td>val_f1</td><td>▁▁▁▂▂▇▂▄▃█▅▇▄▄▇▄▇███████▇▇█▇▆▇▇█▆▇▇▇▇▇▇█</td></tr><tr><td>val_loss_epoch</td><td>▄▃█▇▆▇█▇▆▇▅▇▅▅▆▄▆▅▆▆▆▅▃▆▅▁▇▅▁▄▄▅▁▃▆▁▅▂▃▆</td></tr><tr><td>val_loss_step</td><td>▃▁█▆▅▆█▆▆▆▅▇▅▅▆▅▆▄▆▅▅▅▂▇▆▂▇▅▂▅▅▆▂▄▇▁▆▃▄▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.67002</td></tr><tr><td>train_auc</td><td>0.35551</td></tr><tr><td>train_f1</td><td>0.61224</td></tr><tr><td>train_loss_epoch</td><td>0.63617</td></tr><tr><td>train_loss_step</td><td>0.70131</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.64234</td></tr><tr><td>val_auc</td><td>0.29652</td></tr><tr><td>val_f1</td><td>0.59836</td></tr><tr><td>val_loss_epoch</td><td>0.66153</td></tr><tr><td>val_loss_step</td><td>0.70877</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_32_onehot_0</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/4ndtc2de' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/4ndtc2de</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231230_221410-4ndtc2de\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20231230_223544-64yoj6xn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/64yoj6xn' target=\"_blank\">MLP_3_32_onehot_0</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/64yoj6xn' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/64yoj6xn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\torch_geometric\\deprecation.py:22: UserWarning: 'nn.glob.GlobalAttention' is deprecated, use 'nn.aggr.AttentionalAggregation' instead\n",
      "  warnings.warn(out)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_3_32_onehot_0\\attention\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 2.4 K \n",
      "2  | head        | Sequential       | 562   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 33    \n",
      "--------------------------------------------------\n",
      "3.0 K     Trainable params\n",
      "0         Non-trainable params\n",
      "3.0 K     Total params\n",
      "0.012     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▂▂▂▂▂▂▂▄▇▆▇▇▇▇█▇▆███▇▇▇█▇██▇▇▇▇▇██▇▇▇█</td></tr><tr><td>train_auc</td><td>▂▂▂▁▁▁▂▁▂▅▇▇▇▇▇▇█▇▇████▇██▇██▇█▇████▇▇██</td></tr><tr><td>train_f1</td><td>▃▁▁▁▁▁▁▁▁▄▇▆▇▇▇▇▇▇▇▇▇▇█▇▆▇▇▇█▇█▇▇██▇▇█▇█</td></tr><tr><td>train_loss_epoch</td><td>████▇██▇▇▅▄▃▃▃▄▃▁▃▃▂▁▁▃▃▂▂▃▂▂▃▃▃▂▂▂▂▂▄▁▂</td></tr><tr><td>train_loss_step</td><td>▆█▇▇▆▆▆█▆▄▄▃▄▃▃▂▂▁▇▃▄▄▄▅█▃▅▇▁▆▄▆▃▅▄▃▃▄▁▄</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▂▃▃▃▃▃▄▃▄▆▅▅▇▆▅▇▆▅▅▇▅▅▅▇▆▆▇▇▇▇▇█</td></tr><tr><td>val_auc</td><td>▆▁▂▆▇▇▇▇▇███████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▂▂▂▃▃▃▄▃▄▆▅▅█▇▅▆▆▅▅▆▅▅▅▆▇▆▇▇▆▇█▇</td></tr><tr><td>val_loss_epoch</td><td>▄▃▄▄▄▄▄▃▄▄▇▅▆█▆▅▅▅▂▃▃▄▅▃▃▃▆▄▆▆▅▅▅▄▃▁█▇▄▃</td></tr><tr><td>val_loss_step</td><td>▅▃▄▄▄▄▄▄▄▄▇▄▇█▆▅▅▅▂▃▄▄▅▃▃▃▆▄▆▆▅▆▅▄▃▁█▇▅▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.6947</td></tr><tr><td>train_auc</td><td>0.74807</td></tr><tr><td>train_f1</td><td>0.60981</td></tr><tr><td>train_loss_epoch</td><td>0.59015</td></tr><tr><td>train_loss_step</td><td>0.59174</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.67153</td></tr><tr><td>val_auc</td><td>0.67389</td></tr><tr><td>val_f1</td><td>0.41558</td></tr><tr><td>val_loss_epoch</td><td>0.64808</td></tr><tr><td>val_loss_step</td><td>0.58184</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_32_onehot_0</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/64yoj6xn' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/64yoj6xn</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231230_223544-64yoj6xn\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20231230_225609-t8z74w3o</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/t8z74w3o' target=\"_blank\">MLP_3_32_onehot_0</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/t8z74w3o' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/t8z74w3o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_3_32_onehot_0\\attention2\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 2.4 K \n",
      "2  | head        | Sequential       | 562   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 1.1 K \n",
      "--------------------------------------------------\n",
      "4.1 K     Trainable params\n",
      "0         Non-trainable params\n",
      "4.1 K     Total params\n",
      "0.016     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▁▁▁▁▁▁▂▂▃▆▆▅▆▆▆▆▆▆▇▆▆▇█▆▆▇▇▆▇▆▆▇▇▇▆▆▆▆</td></tr><tr><td>train_auc</td><td>▂▁▁▂▁▂▂▂▃▄▆▇▇▇▇▇▇█▇███▇███▇▇█▇█▇▇████▇▇█</td></tr><tr><td>train_f1</td><td>▃▁▁▁▁▁▁▁▁▁▃▇▇▇▇▇▇▇▇▇█▇███▇▇▇█▇██▇▇▇▇▇▆▇▇</td></tr><tr><td>train_loss_epoch</td><td>██████▇█▇▇▅▄▃▄▄▂▃▃▃▃▃▃▂▂▂▃▃▃▁▃▂▃▃▃▃▁▂▃▃▂</td></tr><tr><td>train_loss_step</td><td>██▇▇█▇▇█▇▆▄▇▇▆▅▁▇▆▄▆█▆▃▅▅▄▅▄▃█▅▃▅▂▃▄▅▆▇▄</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▂▆▅▆▆▆▆▇▇█▆▆▆▆▆▇▆▆▇▇▇▆▇▇█▇▆█▆▇▇█</td></tr><tr><td>val_auc</td><td>▅▁▃▅▆▆▆▇▇▇▇█████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▂▆▄▆▇▇█▇███▇▇▇▇▇▇▇▇▇▇▇▇▇█▇███▇▇█</td></tr><tr><td>val_loss_epoch</td><td>▆▅▅▆▃▅▅▅▅▃▅▃▃▄▆▅▇▆▁▂█▁▅▅█▅▇▄▄▇▃▄▅▄▆▃▂▃▃▁</td></tr><tr><td>val_loss_step</td><td>▅▄▄▅▃▄▅▄▄▃▅▃▃▄▆▅▇▆▁▂█▁▅▅█▅▇▄▄▇▃▄▅▄▆▃▂▃▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.68739</td></tr><tr><td>train_auc</td><td>0.74368</td></tr><tr><td>train_f1</td><td>0.57143</td></tr><tr><td>train_loss_epoch</td><td>0.58313</td></tr><tr><td>train_loss_step</td><td>0.55371</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.66423</td></tr><tr><td>val_auc</td><td>0.67119</td></tr><tr><td>val_f1</td><td>0.51579</td></tr><tr><td>val_loss_epoch</td><td>0.55439</td></tr><tr><td>val_loss_step</td><td>0.46533</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_32_onehot_0</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/t8z74w3o' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/t8z74w3o</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231230_225609-t8z74w3o\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20231230_231655-2nqf3e5m</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/2nqf3e5m' target=\"_blank\">MLP_3_64_onehot_0</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/2nqf3e5m' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/2nqf3e5m</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_3_64_onehot_0\\mean\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 9.0 K \n",
      "2 | head        | Sequential       | 2.1 K \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "11.1 K    Trainable params\n",
      "0         Non-trainable params\n",
      "11.1 K    Total params\n",
      "0.044     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▁▁▃▅▆▇▇▆▆▇▆▇█▇▇▇▇▆▆▇▇▇▇▇▇█▇▇█▇▇▇█▇▇▇█▆</td></tr><tr><td>train_auc</td><td>▁▁▁▃▅▆▇▇▇█▇▇▇▇██████▇▇██▇███▇███████▇███</td></tr><tr><td>train_f1</td><td>▃▁▁▁▅▇▆████▇▇▇██▇▇▇▇▇▇██▇▇██▇▇███▇█▇▇▇█▇</td></tr><tr><td>train_loss_epoch</td><td>███▇▆▅▃▄▂▂▄▃▂▃▁▂▂▁▂▂▃▂▁▂▂▂▁▁▂▂▂▂▂▁▂▂▂▂▁▂</td></tr><tr><td>train_loss_step</td><td>▆▆▆▅▄█▅▆▇▄▃▄▄▅▆▃▃▅▅▄▃▄▁▃▆▃▅▅▄▅▄▁▅▅▂▃▂▆▅▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▂▄▇▆▆▆▆▆▅▅▅▅▆▆▆▇▆▆▆▇▇█▇▇█▇▇▇▇▇▇▇▇▇█▇▇</td></tr><tr><td>val_auc</td><td>▁▁▂▃▅▆▆▆▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇█▇███████████</td></tr><tr><td>val_f1</td><td>▁▁▁▂▃▆▅▇▇▆▇█▇▇▇▇▇▇█▇▇▆▇██▇▇██▇▇▇▇██▇▇▇▇▇</td></tr><tr><td>val_loss_epoch</td><td>▅▅▅▅▆▁▅▅▂▆▃▄▁▂▅▂▄▆▁▃▅▃▂▇▁▃▃▆▅▃▃▄█▃▁▃▄▁▂▂</td></tr><tr><td>val_loss_step</td><td>▄▄▄▅▆▁▅▅▂▅▃▄▁▂▄▂▄▆▁▃▅▃▂▇▁▃▃▆▅▃▃▄█▄▁▃▄▁▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.68373</td></tr><tr><td>train_auc</td><td>0.74309</td></tr><tr><td>train_f1</td><td>0.5043</td></tr><tr><td>train_loss_epoch</td><td>0.58757</td></tr><tr><td>train_loss_step</td><td>0.55588</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.66423</td></tr><tr><td>val_auc</td><td>0.67708</td></tr><tr><td>val_f1</td><td>0.48889</td></tr><tr><td>val_loss_epoch</td><td>0.58577</td></tr><tr><td>val_loss_step</td><td>0.54132</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_64_onehot_0</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/2nqf3e5m' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/2nqf3e5m</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231230_231655-2nqf3e5m\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20231230_233713-ygwu5e0j</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/ygwu5e0j' target=\"_blank\">MLP_3_64_onehot_0</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/ygwu5e0j' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/ygwu5e0j</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_3_64_onehot_0\\max\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 9.0 K \n",
      "2 | head        | Sequential       | 2.1 K \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "11.1 K    Trainable params\n",
      "0         Non-trainable params\n",
      "11.1 K    Total params\n",
      "0.044     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\torch_geometric\\warnings.py:17: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▂▂▂▃▂▃▃▄▃▅▅▆▆▆▆▆▅▅▅▅▆▆▇▆▆▇▆▆█▆▇▆▇▇▆▇▇▆</td></tr><tr><td>train_auc</td><td>▁▂▁▂▂▂▂▃▃▅▄▅▅▇▇▇▇▇▇▆▆▆▆▇▇▇▇▇▆▇█▇▇█▇▇▇██▇</td></tr><tr><td>train_f1</td><td>▃▁▂▁▁▃▁▃▂▆▆▅▅▆▇▇▆▇▅▆▅▅▆▇▇▆▆▇▆▇██▇▆▇▆▇▇▇▆</td></tr><tr><td>train_loss_epoch</td><td>█▇▇▇▆▇▇▇▆▆▆▅▅▄▄▄▂▃▄▄▄▄▄▃▂▃▃▂▃▃▁▃▃▂▄▃▁▃▁▂</td></tr><tr><td>train_loss_step</td><td>▆▆▅▅▇▅▆▅▆▇▄▄▅▅▄▅▄▃▃▄▄▅▄▆▅▃▁▅▆▅▅▃▄█▅▄▂▅▇▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>▁▅▆▅▅▇▅▂▆███████████▇███████▆▃▁▆▇▆▆▁▄▄▇▂</td></tr><tr><td>val_f1</td><td>█▁██████████████████████████████████████</td></tr><tr><td>val_loss_epoch</td><td>▁▁▁▁▁▂▁▁▂▂▃▃▄▄▄▅▅▇▆▇▅▅▅▅▇▆█▆▇█▆▅▄▇▇▆▇██▇</td></tr><tr><td>val_loss_step</td><td>▁▁▁▁▁▂▂▁▂▁▃▃▄▅▄▅▄█▆▇▄▄▅▄▇▅█▅▆▇▄▂▂▇▆▅▆▆▆▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.66362</td></tr><tr><td>train_auc</td><td>0.69585</td></tr><tr><td>train_f1</td><td>0.48603</td></tr><tr><td>train_loss_epoch</td><td>0.61323</td></tr><tr><td>train_loss_step</td><td>0.56381</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.40876</td></tr><tr><td>val_auc</td><td>0.39542</td></tr><tr><td>val_f1</td><td>0.58031</td></tr><tr><td>val_loss_epoch</td><td>1.23646</td></tr><tr><td>val_loss_step</td><td>1.26278</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_64_onehot_0</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/ygwu5e0j' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/ygwu5e0j</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231230_233713-ygwu5e0j\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20231230_235738-tjcbx5aj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/tjcbx5aj' target=\"_blank\">MLP_3_64_onehot_0</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/tjcbx5aj' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/tjcbx5aj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_3_64_onehot_0\\sum\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 9.0 K \n",
      "2 | head        | Sequential       | 2.1 K \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "11.1 K    Trainable params\n",
      "0         Non-trainable params\n",
      "11.1 K    Total params\n",
      "0.044     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▃▃▃▄▄▅▅▅▆▆▆▇▇▇▇▇▇▇▇▇███▇▇██▇▇▇▇▇██▇▇▇█▇</td></tr><tr><td>train_auc</td><td>▁▁▂▂▂▄▄▃▄▅▃▄▇▇▅▆▇▇▇▇▇██▆▄▆▇█▅▇▆█▅▅▆▅▆▃▆▅</td></tr><tr><td>train_f1</td><td>▄▄▅▅▃▃▁▂▂▄▆▆▆▆▇▆▅▆▅▅▆▇▇█▇▆▇▆▆▆▇██▇█▆▇▇█▅</td></tr><tr><td>train_loss_epoch</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▂▂▁▂▂▂▂▂▂▁▁▁▂▂▁▁▁▂▁▁▁▁▁▂▁▁▂▁▂▁▁▁▁▁▁▁▂▂▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▂▂▃▃▃▃▃▃▃▃▅▅▅▆▄▄▄▃▅▅▅▄▇▇▃▄▄▃▄▅▆▅▄▅█</td></tr><tr><td>val_auc</td><td>▁▁▁▁▄▇▇▇██▇▇▆▆▇▄▅▅▄▅▅▅▅▁▁▁▄▄▃▆▆▄▄▅▆▆▅▆▇▆</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▂▂▂▂▂▂▃▄▄▃▆▅▆▇▅▄▅▄▅▅▅▆▇▇▅▄▆▄▅▆▇▆▆▆█</td></tr><tr><td>val_loss_epoch</td><td>█▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_step</td><td>█▂▂▂▂▁▂▂▁▂▁▁▁▁▂▁▂▁▁▁▂▂▁▂▁▂▂▂▂▁▂▂▃▁▁▂▂▁▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.67824</td></tr><tr><td>train_auc</td><td>0.60606</td></tr><tr><td>train_f1</td><td>0.50838</td></tr><tr><td>train_loss_epoch</td><td>0.60615</td></tr><tr><td>train_loss_step</td><td>0.5574</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.66423</td></tr><tr><td>val_auc</td><td>0.70056</td></tr><tr><td>val_f1</td><td>0.425</td></tr><tr><td>val_loss_epoch</td><td>0.56594</td></tr><tr><td>val_loss_step</td><td>0.50867</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_64_onehot_0</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/tjcbx5aj' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/tjcbx5aj</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231230_235738-tjcbx5aj\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20231231_001746-vx72nwjk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/vx72nwjk' target=\"_blank\">MLP_3_64_onehot_0</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/vx72nwjk' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/vx72nwjk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\torch_geometric\\deprecation.py:22: UserWarning: 'nn.glob.GlobalAttention' is deprecated, use 'nn.aggr.AttentionalAggregation' instead\n",
      "  warnings.warn(out)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_3_64_onehot_0\\attention\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 9.0 K \n",
      "2  | head        | Sequential       | 2.1 K \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 65    \n",
      "--------------------------------------------------\n",
      "11.2 K    Trainable params\n",
      "0         Non-trainable params\n",
      "11.2 K    Total params\n",
      "0.045     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▂▂▄▇▆▇▇▇█▇██▇▇██▆▄▇██▇▇█████▇██▇▇▆▆▇▇▇</td></tr><tr><td>train_auc</td><td>▁▁▂▃▅▇▆▇▇▇█▇███▇██▇▇█████████▇████▇█▇███</td></tr><tr><td>train_f1</td><td>▃▁▁▁▄▇▇█▇▇▇▇▇█▆▇▇██▄▇▇▇▇▇█▇▇██▆▇▇█▇▆▆▇▇▇</td></tr><tr><td>train_loss_epoch</td><td>██▇▇▅▃▄▄▃▃▂▂▂▁▃▃▂▂▅▅▃▂▁▂▁▂▂▁▁▂▂▂▂▃▃▃▂▂▁▂</td></tr><tr><td>train_loss_step</td><td>▆▆▇▆▄▂▆▄▇▁▆▃▆▄▅▆▅▅▆▅▅▇▂▄▄▃▆█▂▂▄▄▆▃▅▅▃▁▃▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▂▄▇▄▄▇▆▆▇▅▅▄▆▇▅▇▆▅▇▇▆▆▆▆▆▇▇▇▇█▇█▇▇▇██</td></tr><tr><td>val_auc</td><td>▁▂▃▄▇▇▇▆▇▇▇▆▆▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇████████</td></tr><tr><td>val_f1</td><td>▁▁▁▂▄▇▄▄▇▆▆▆▅▄█▇▇▄▆▆▄▇▆▆▆▆▆▇▅▆▆▆▆▆▆▇▇▇▇▇</td></tr><tr><td>val_loss_epoch</td><td>▄▆▅▆▄▃▅▃▄▁▆▂▅▄▅▅▂▆▃▅▅▆▄▂█▅▇▃▃▆▂▄▆▅▃▄▂▂▄▅</td></tr><tr><td>val_loss_step</td><td>▄▆▅▅▄▃▅▃▄▁▆▂▄▃▅▅▂▆▃▅▆▇▄▂█▄▇▄▃▆▂▄▆▅▃▄▃▂▄▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.68464</td></tr><tr><td>train_auc</td><td>0.73817</td></tr><tr><td>train_f1</td><td>0.50785</td></tr><tr><td>train_loss_epoch</td><td>0.5907</td></tr><tr><td>train_loss_step</td><td>0.57158</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.66788</td></tr><tr><td>val_auc</td><td>0.67758</td></tr><tr><td>val_f1</td><td>0.51337</td></tr><tr><td>val_loss_epoch</td><td>0.65771</td></tr><tr><td>val_loss_step</td><td>0.69891</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_64_onehot_0</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/vx72nwjk' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/vx72nwjk</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231231_001746-vx72nwjk\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20231231_003621-fx7cos1v</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/fx7cos1v' target=\"_blank\">MLP_3_64_onehot_0</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/fx7cos1v' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/fx7cos1v</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_3_64_onehot_0\\attention2\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 9.0 K \n",
      "2  | head        | Sequential       | 2.1 K \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 4.2 K \n",
      "--------------------------------------------------\n",
      "15.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "15.3 K    Total params\n",
      "0.061     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▂▂▂▅▇▇█▇███▇▇▇█▇███▆██▇█▇▇█▇██▇███▇▇██</td></tr><tr><td>train_auc</td><td>▁▁▂▂▄▆▇▇▇▇███▇███▇▇██▇██▇███████████▇███</td></tr><tr><td>train_f1</td><td>▃▁▁▁▁▅█▆█▇█▇█▇▇▇█▇▇█▇▆▇▇▇▇▇█▇▇▇████▇▆▇▇█</td></tr><tr><td>train_loss_epoch</td><td>███▇▇▅▄▂▃▃▃▃▃▄▂▂▁▃▃▃▂▃▃▂▃▁▁▃▂▂▂▂▃▃▂▂▃▂▂▂</td></tr><tr><td>train_loss_step</td><td>██▇▆▇▄▃▄▂▆▂▁▃▅█▄▄▅▃▄▂▃▄▁▂▃▄▅▃▅▃▃▆▃▂▅▅▂▂▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▃▃▆▆▆▆▇▇▆▆▅▆▅▆▆▆▅▇▆▆▆▇▇▅▆▇▇▇▇▇▇▇█▇▆▇</td></tr><tr><td>val_auc</td><td>▁▂▃▃▄▆▆▇▇▇▇▇▇▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇███████████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▂█▅▇▆▆▆▆▆█▇▇▆█▇▇██▇▇▇▇▇▇▇▇▇▆▇▆▇▇▇▇▇▇</td></tr><tr><td>val_loss_epoch</td><td>▆▅▆▄▆▆▇▅▄▃▇▇▅▃▃▄▅▇▄▃▄▃▂▁▂▂▂▂▂▄▄▂▂▂▄█▄▂▃▆</td></tr><tr><td>val_loss_step</td><td>▅▄▅▄▅▅▆▅▄▃▆▆▅▃▃▄▅▇▄▃▄▃▂▁▂▂▂▂▃▄▄▂▃▂▄█▄▃▃▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.69561</td></tr><tr><td>train_auc</td><td>0.74863</td></tr><tr><td>train_f1</td><td>0.63366</td></tr><tr><td>train_loss_epoch</td><td>0.58593</td></tr><tr><td>train_loss_step</td><td>0.56453</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.66423</td></tr><tr><td>val_auc</td><td>0.67477</td></tr><tr><td>val_f1</td><td>0.46512</td></tr><tr><td>val_loss_epoch</td><td>0.70173</td></tr><tr><td>val_loss_step</td><td>0.77925</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_64_onehot_0</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/fx7cos1v' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/fx7cos1v</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231231_003621-fx7cos1v\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20231231_005557-2uwh8syc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/2uwh8syc' target=\"_blank\">MLP_4_16_onehot_0</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/2uwh8syc' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/2uwh8syc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_4_16_onehot_0\\mean\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 976   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "1.1 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.1 K     Total params\n",
      "0.005     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_auc</td><td>▆▅▃▄▅▆▆▄▂▇▄▁▆▄▅▃▆▃▃▅▅▁▆▇▃▃▇▄▃█▂█▄▄▄▃▄▁▂▄</td></tr><tr><td>train_f1</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_epoch</td><td>▄▅▅▆█▄▄▆▇▁▄▆▆▄▅▇▆▂▅▅▆▅▆▆▁▆▄▁▇▄▅▃▂▄▅▄▄▂▃▇</td></tr><tr><td>train_loss_step</td><td>▂▄▄▄▄▄▅▅▄▆▄█▃▅█▃▃▄▄▁▅▄▃▄▄▅▄▁▄▃▆▅▄▄▄▆▄▅▅▇</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>▅▆▇███▇▄▇█▇▇▇▆▇▆▆▇▆▆▆▅▄▄▄▃▃▁▃▇▅▃▃▃▃▃▃▃▃▃</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_epoch</td><td>▅▅▃▅▃▄▅▃▇▄▄▅▅█▅▄▅▄▆▅▄▂█▆▅▅▅▅▃▄▃▁▅▅▆▄▆▄▄█</td></tr><tr><td>val_loss_step</td><td>▅▅▃▅▃▄▅▃▇▄▄▅▅█▅▄▅▄▆▅▄▂█▆▅▅▅▅▃▄▃▁▅▅▆▄▆▄▄█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.58135</td></tr><tr><td>train_auc</td><td>0.49418</td></tr><tr><td>train_f1</td><td>0.0</td></tr><tr><td>train_loss_epoch</td><td>0.68626</td></tr><tr><td>train_loss_step</td><td>0.7194</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.59124</td></tr><tr><td>val_auc</td><td>0.5</td></tr><tr><td>val_f1</td><td>0.0</td></tr><tr><td>val_loss_epoch</td><td>0.71093</td></tr><tr><td>val_loss_step</td><td>0.7499</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_16_onehot_0</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/2uwh8syc' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/2uwh8syc</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231231_005557-2uwh8syc\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20231231_011511-ldbjoazs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/ldbjoazs' target=\"_blank\">MLP_4_16_onehot_0</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/ldbjoazs' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/ldbjoazs</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_4_16_onehot_0\\max\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 976   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "1.1 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.1 K     Total params\n",
      "0.005     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\torch_geometric\\warnings.py:17: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁█▇█████████████████████████████████████</td></tr><tr><td>train_auc</td><td>▄▄▁▃▄▃▅▃▁▅▄▁▄▃▅▃▇▃▂▂▄▃▃▅▅▂▆▃▂█▂▆▃▄▆▃▄▄▂▆</td></tr><tr><td>train_f1</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_epoch</td><td>█▃▃▃▄▃▃▃▃▁▂▃▃▃▃▄▃▂▃▃▃▃▃▃▁▃▂▁▄▂▃▂▂▃▃▃▂▁▂▃</td></tr><tr><td>train_loss_step</td><td>▃▄▄▅▄▄▅▅▄▆▅█▃▅█▃▃▄▄▂▆▅▄▄▄▅▄▁▄▃▆▅▅▄▅▆▄▆▅▇</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>▇▄▂▁▃▂▃▄▇▇▅▆▆▆█▇▇▇▅▅▅▅▃▅▅▅▅▃▃▃▃▃▃▃▃▃▃▃▃▅</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_epoch</td><td>▅▅▃▅▃▄▅▃▇▄▄▅▅█▅▄▅▄▆▅▄▂█▆▅▅▅▅▃▄▃▁▅▅▆▄▆▄▄▇</td></tr><tr><td>val_loss_step</td><td>▅▅▃▅▃▄▅▃▇▄▄▅▅█▅▄▅▄▆▅▄▂█▆▅▅▅▅▃▄▃▁▅▅▆▄▆▄▄▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.58135</td></tr><tr><td>train_auc</td><td>0.51864</td></tr><tr><td>train_f1</td><td>0.0</td></tr><tr><td>train_loss_epoch</td><td>0.68346</td></tr><tr><td>train_loss_step</td><td>0.70969</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.59124</td></tr><tr><td>val_auc</td><td>0.57019</td></tr><tr><td>val_f1</td><td>0.0</td></tr><tr><td>val_loss_epoch</td><td>0.69723</td></tr><tr><td>val_loss_step</td><td>0.71384</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_16_onehot_0</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/ldbjoazs' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/ldbjoazs</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231231_011511-ldbjoazs\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20231231_013441-gozmbexk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/gozmbexk' target=\"_blank\">MLP_4_16_onehot_0</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/gozmbexk' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/gozmbexk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_4_16_onehot_0\\sum\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 976   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "1.1 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.1 K     Total params\n",
      "0.005     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▂▁▁▁▂▂▂▃▁▃▃▄▃▄▃▄▃▅▄▅▆▅▅▇▆▅▆▆▇▇▇▇▇▆▇██▇█▇</td></tr><tr><td>train_auc</td><td>▇▅▆▄▅▇▇█▃▆▇▇▃▇▅▅▅▄▅▅▄▅▅▅▄▄▃▄▆▅▅▅▄▅▃▄▅▁▇▅</td></tr><tr><td>train_f1</td><td>▄▃▄▄▂▃▄▄▂▃▃▁▃▂▄▄▃▅▂▃▆▅▅▄▅▆▇▆▇▄▄█▇▃██▇▆▇█</td></tr><tr><td>train_loss_epoch</td><td>█▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▄▂▂▂▂▂▁▁▂▂▂▂▂▂▂▁▂▂▁▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▂▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▂▂▁▂▂▁▂▂▁▂▃▂▃▃▃▅▂▂▄▄▇▆▄▄█▅▅▅▅▆▅▆</td></tr><tr><td>val_auc</td><td>▇▇▇▇▇▇▇███▇▅▃█▅▄▇▃▇▂▂▅▇▄▂▁▁▂▆▆▂▃▃▂▁▆▅▅▅▄</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▂▂▁▂▂▁▂▂▁▂▂▂▂▂▂▄▂▂▃▃▆▅▃▃█▅▆▆▄▅▆▆</td></tr><tr><td>val_loss_epoch</td><td>█▂▂▂▂▁▂▁▂▁▂▂▂▂▂▁▁▁▂▂▁▁▂▂▂▂▂▂▁▁▁▁▁▁▂▁▂▁▂▂</td></tr><tr><td>val_loss_step</td><td>█▂▂▂▂▂▂▂▂▂▂▂▂▃▂▂▂▂▂▂▂▁▃▂▂▂▂▂▂▂▂▁▂▂▂▂▂▁▂▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.64899</td></tr><tr><td>train_auc</td><td>0.49753</td></tr><tr><td>train_f1</td><td>0.56854</td></tr><tr><td>train_loss_epoch</td><td>0.62698</td></tr><tr><td>train_loss_step</td><td>0.57189</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.63869</td></tr><tr><td>val_auc</td><td>0.48115</td></tr><tr><td>val_f1</td><td>0.31724</td></tr><tr><td>val_loss_epoch</td><td>0.73389</td></tr><tr><td>val_loss_step</td><td>0.86307</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_16_onehot_0</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/gozmbexk' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/gozmbexk</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231231_013441-gozmbexk\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20231231_015419-agi6es8x</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/agi6es8x' target=\"_blank\">MLP_4_16_onehot_0</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/agi6es8x' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/agi6es8x</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\torch_geometric\\deprecation.py:22: UserWarning: 'nn.glob.GlobalAttention' is deprecated, use 'nn.aggr.AttentionalAggregation' instead\n",
      "  warnings.warn(out)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_4_16_onehot_0\\attention\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 976   \n",
      "2  | head        | Sequential       | 154   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 17    \n",
      "--------------------------------------------------\n",
      "1.1 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.1 K     Total params\n",
      "0.005     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▄██████████████████████████████████████</td></tr><tr><td>train_auc</td><td>▅▄▆▅▅▇▄▄▄▆▄▄▁▄▄▅▁▁▅█▂▄▃▄▃▄▅▃▃▆▆▇▅▇▆▂▃▄▅▅</td></tr><tr><td>train_f1</td><td>██▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_epoch</td><td>▅▄▃▂▃▁▃▃▃▃▅▃▄▂▃▃█▃▄▂▄▃▄▁▃▃▂▅▃▄▂▂▄▃▄▅▄▄▅▄</td></tr><tr><td>train_loss_step</td><td>▃▃▆▆▄▂▃▂▅▅▄▆▅▃▁▃▃▃▂▅▃▃█▃▇▃▁▃▅▃▆▄▅▄▆▆▅▃▄▅</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>█▇▆▅▅▅▄▄▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▄▅▅▄▃▄▂▂▁▁▁▁▂▁▁</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_epoch</td><td>█▇▅▇▆▁▂▃▆▂▅▆▄▆▆▇▇▅▆▄▇▅▆▅▆▅▇▅▇▆▇▃▅▆▅▇▅█▅▆</td></tr><tr><td>val_loss_step</td><td>█▆▅▇▆▁▂▃▆▂▅▆▄▆▆▇▇▅▆▄▇▅▆▅▆▅▇▅▇▆▇▃▅▆▅▇▅█▅▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.58135</td></tr><tr><td>train_auc</td><td>0.50285</td></tr><tr><td>train_f1</td><td>0.0</td></tr><tr><td>train_loss_epoch</td><td>0.68269</td></tr><tr><td>train_loss_step</td><td>0.69857</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.59124</td></tr><tr><td>val_auc</td><td>0.5</td></tr><tr><td>val_f1</td><td>0.0</td></tr><tr><td>val_loss_epoch</td><td>0.67221</td></tr><tr><td>val_loss_step</td><td>0.66682</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_16_onehot_0</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/agi6es8x' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/agi6es8x</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231231_015419-agi6es8x\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20231231_021317-y4ofhr4q</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/y4ofhr4q' target=\"_blank\">MLP_4_16_onehot_0</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/y4ofhr4q' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/y4ofhr4q</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_4_16_onehot_0\\attention2\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 976   \n",
      "2  | head        | Sequential       | 154   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 281   \n",
      "--------------------------------------------------\n",
      "1.4 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.4 K     Total params\n",
      "0.006     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁██▇▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅</td></tr><tr><td>train_auc</td><td>▇▄▇▄██▆▆▃▆█▅▇▅▇▁▄▆▁▆▄▇▇▅▄▇▅▃▄▆▅▄█▆▃▆▅▄▂▅</td></tr><tr><td>train_f1</td><td>█▄▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_epoch</td><td>▂▆▆▆▃▄▄▅█▅▄▇▅▄▂▆▂▂▄▇▃▄▂▄▅▁▆▅▁▇▃▄▄▄▂▁▂▅█▅</td></tr><tr><td>train_loss_step</td><td>▁▄▆▆▄▆▆█▄▅▅▅▅▇▄▃▇█▅▂█▇▃▆▅▂▃▅▁▄▂▃▇▅▃▃▅▇▆▅</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>▇█▇▆▆▆▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▄▄▄▄▄▄▄▄▄▄▄▅▄▄▁▁</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_epoch</td><td>▂▇▅▃▆▅▆▅▅▃▃▂▅▄▅▅█▅▃▆▄▄▅▅▂▅▃▃▄▇█▅▃▅▃▄▅▁▅▅</td></tr><tr><td>val_loss_step</td><td>▂▇▅▃▆▅▆▅▅▃▃▂▅▄▅▅█▅▃▆▄▄▅▅▂▅▃▃▄▇█▅▃▅▃▄▅▁▅▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.58135</td></tr><tr><td>train_auc</td><td>0.49145</td></tr><tr><td>train_f1</td><td>0.0</td></tr><tr><td>train_loss_epoch</td><td>0.68008</td></tr><tr><td>train_loss_step</td><td>0.67997</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.59124</td></tr><tr><td>val_auc</td><td>0.5</td></tr><tr><td>val_f1</td><td>0.0</td></tr><tr><td>val_loss_epoch</td><td>0.68106</td></tr><tr><td>val_loss_step</td><td>0.68552</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_16_onehot_0</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/y4ofhr4q' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/y4ofhr4q</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231231_021317-y4ofhr4q\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20231231_023323-xskke0bv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/xskke0bv' target=\"_blank\">MLP_4_32_onehot_0</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/xskke0bv' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/xskke0bv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_4_32_onehot_0\\mean\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 3.5 K \n",
      "2 | head        | Sequential       | 562   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "4.0 K     Trainable params\n",
      "0         Non-trainable params\n",
      "4.0 K     Total params\n",
      "0.016     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▄▄▄▄▄▄▄▄▄▄▄▄▄▆▇▇▇█▇▇▇█▇▇▇█▇██▇█▇▇█▇█▇██</td></tr><tr><td>train_auc</td><td>▂▁▂▂▁▂▂▁▁▂▂▁▃▄▆▇▇▇▇█▇▇█▇███▇██▇█████████</td></tr><tr><td>train_f1</td><td>▇▂▃▂▁▁▁▁▁▁▁▁▁▁▃▇▇▇██▇▇▇▆▆▇▇▇▇▇▇█▇▇▇▇▇▇█▇</td></tr><tr><td>train_loss_epoch</td><td>█▇▇▇▇▆▇▇▇▇▇▆▆▆▄▃▂▃▂▃▃▃▂▃▁▁▁▂▂▁▂▁▂▁▂▃▁▂▂▂</td></tr><tr><td>train_loss_step</td><td>█▇▇▆▇▆▇▇▇▆▇▆▅▅▅▄▅▂▆▅▄▃▅▃▃▂▃▄▃▆▄▄▃▄▆▄▂▇▁▆</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▂▅▆▆▆▇▇█▇▆▆█▆▆█▇▇▇▇▇▇▇▇▇██▇</td></tr><tr><td>val_auc</td><td>█▄▆▃▂▁▂▁▁▃▃▃▄▅▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇███████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▂▄▇▆▆▆▆▆▇█▇▇▇▇█▇▆▇▇█▇▇▇▇▆▆▇</td></tr><tr><td>val_loss_epoch</td><td>▆▆▆▆▆▆▆▆▆▆▆▆▅▄▅▅▃▄▆▄▄▄▄▂▁▁▆▂▅▂▁▅▄█▅▄▄▅▂▂</td></tr><tr><td>val_loss_step</td><td>▅▅▅▅▅▅▅▅▅▅▅▅▄▄▅▅▂▄▅▄▄▄▄▂▁▁▆▂▅▂▁▅▄█▅▄▄▅▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.69378</td></tr><tr><td>train_auc</td><td>0.73533</td></tr><tr><td>train_f1</td><td>0.57216</td></tr><tr><td>train_loss_epoch</td><td>0.60103</td></tr><tr><td>train_loss_step</td><td>0.63803</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.64964</td></tr><tr><td>val_auc</td><td>0.66937</td></tr><tr><td>val_f1</td><td>0.46667</td></tr><tr><td>val_loss_epoch</td><td>0.55012</td></tr><tr><td>val_loss_step</td><td>0.46503</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_32_onehot_0</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/xskke0bv' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/xskke0bv</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231231_023323-xskke0bv\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20231231_025206-ru5toaes</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/ru5toaes' target=\"_blank\">MLP_4_32_onehot_0</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/ru5toaes' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/ru5toaes</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_4_32_onehot_0\\max\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 3.5 K \n",
      "2 | head        | Sequential       | 562   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "4.0 K     Trainable params\n",
      "0         Non-trainable params\n",
      "4.0 K     Total params\n",
      "0.016     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\torch_geometric\\warnings.py:17: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▄▄▅▅▄▅▅▄▅▅▄▅▇▇▅▇▆▁▇▆▆▇▆▆▇▆▅▆▆▆▅▆▇▆▇▇██</td></tr><tr><td>train_auc</td><td>▂▁▃▁▂▂▄▂▃▃▃▃▄▄▆▆▅▇▅▄▅▆▇▆█▆▇▅▄▇▄▅▅▆▆▇▆█▇█</td></tr><tr><td>train_f1</td><td>▇▅▃▂▂▃▁▁▇▁▃▃▁▁▅▅▃▄▆█▅▄▆▇▆▇█▆▅█▆▆▄▅▆▆▅▆▇█</td></tr><tr><td>train_loss_epoch</td><td>█▇▇▅▅▄▅█▆▆▅▄▄▅▂▃▃▃▂▆▂▁▂▁▂▂▃▄▄▄▄▃▄▂▄▂▂▃▂▄</td></tr><tr><td>train_loss_step</td><td>▅▆▅▃▆▂▃▅▂▄▅▃▁▄▃▃▁▄▅▃▃▃▄▃▅▅▆▄▆▃▅▃▄▃▅▃▄▃▅█</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>▄▅▅▄▄▅▃▆▇▇▃▁▁▃▁▆▆▇▇▇▃▆▆▃▂▃▆▂▁▂▅▂▂▃▅█▅▁▁▁</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_epoch</td><td>▁▁▁▁▁▁▁▂▂▁▁▂▂▃▂▃▃▄▄▄▄▃▅▅▇▆▄▂▅▇▅▅▄▄▃▅▅▅██</td></tr><tr><td>val_loss_step</td><td>▂▂▂▂▂▂▂▂▂▂▂▂▃▃▂▃▄▄▄▃▄▃▅▄▇▆▃▁▄▇▄▅▄▄▂▅▅▄█▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.63437</td></tr><tr><td>train_auc</td><td>0.6322</td></tr><tr><td>train_f1</td><td>0.49875</td></tr><tr><td>train_loss_epoch</td><td>0.66866</td></tr><tr><td>train_loss_step</td><td>0.74588</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.40876</td></tr><tr><td>val_auc</td><td>0.36257</td></tr><tr><td>val_f1</td><td>0.58031</td></tr><tr><td>val_loss_epoch</td><td>1.05745</td></tr><tr><td>val_loss_step</td><td>1.1882</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_32_onehot_0</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/ru5toaes' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/ru5toaes</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231231_025206-ru5toaes\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20231231_031142-ld4y51u0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/ld4y51u0' target=\"_blank\">MLP_4_32_onehot_0</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/ld4y51u0' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/ld4y51u0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_4_32_onehot_0\\sum\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 3.5 K \n",
      "2 | head        | Sequential       | 562   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "4.0 K     Trainable params\n",
      "0         Non-trainable params\n",
      "4.0 K     Total params\n",
      "0.016     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▂▁▃▄▄▄▄▄▄▄▄▅▅▅▆▆▇▇▇▅▅▆▇▇▇▇▇▇▇▇▇▆▆▇▇▇▇▆█▇</td></tr><tr><td>train_auc</td><td>▅▁█▄▄▅▄▅▄▆▄▄▆▆▆█▇▇█▇▆▇▇▇█▆▆▆▇█▇▇▅▅▆█▇▇▆▆</td></tr><tr><td>train_f1</td><td>▃▃▅▄▃▅▂▄▅▁▄▃▃▂▅▇▆▆▇█▇▅▆▇▆▇▇▇▇▇▇█▄▆▆▆▇▇█▆</td></tr><tr><td>train_loss_epoch</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▃▂▁▁▁▁▂▁▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▁▁▁▁▂▁▁▁▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▃▂▁▄▃▃▃▄▄▅▄▃▅▇▇▇▅▇▆█▆█▆█▆▇▆▅▇▅▅▅▇█▇▆▇</td></tr><tr><td>val_auc</td><td>▃▁▃▂▂▂▂▃▃▄▆███████▇▆▆▇▇▇▇▇▆▅▆▆▆▅▃▂▆▄▄▅▅▄</td></tr><tr><td>val_f1</td><td>▁▁▁▄▄▁▆▄▄▃▅▄▄▄▂▅▆▇▇▄██▇▆▇▆▇█▇▇▇██▇▇▇▇▇▇▇</td></tr><tr><td>val_loss_epoch</td><td>█▇▅▃▄▄▄▄▄▄▃▄▃▃▄▂▁▄▄▃▄▃▂▂▁▂▅▂▅▁▂▄▄▆▃▂▃▄▂▁</td></tr><tr><td>val_loss_step</td><td>█▇▅▃▄▄▄▄▄▄▃▅▄▃▄▃▁▄▅▃▄▃▃▂▁▂▆▂▅▂▂▅▄▇▄▃▃▄▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.67824</td></tr><tr><td>train_auc</td><td>0.56176</td></tr><tr><td>train_f1</td><td>0.54047</td></tr><tr><td>train_loss_epoch</td><td>0.60818</td></tr><tr><td>train_loss_step</td><td>0.66826</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.67153</td></tr><tr><td>val_auc</td><td>0.68056</td></tr><tr><td>val_f1</td><td>0.51087</td></tr><tr><td>val_loss_epoch</td><td>0.53533</td></tr><tr><td>val_loss_step</td><td>0.4575</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_32_onehot_0</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/ld4y51u0' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/ld4y51u0</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231231_031142-ld4y51u0\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20231231_033135-udsk394n</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/udsk394n' target=\"_blank\">MLP_4_32_onehot_0</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/udsk394n' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/udsk394n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\torch_geometric\\deprecation.py:22: UserWarning: 'nn.glob.GlobalAttention' is deprecated, use 'nn.aggr.AttentionalAggregation' instead\n",
      "  warnings.warn(out)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_4_32_onehot_0\\attention\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 3.5 K \n",
      "2  | head        | Sequential       | 562   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 33    \n",
      "--------------------------------------------------\n",
      "4.1 K     Trainable params\n",
      "0         Non-trainable params\n",
      "4.1 K     Total params\n",
      "0.016     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▄▄▄▅▅▅▅▅▅▅▅▅▅▅▇▇▇▇▇█▇▇▇▇▇███████▇█▇▇███</td></tr><tr><td>train_auc</td><td>▂▁▂▁▂▂▁▂▂▂▁▂▃▃▅▇▇▇▇▇▇▇▇▇▇▇████▇▇█▇█▇▇███</td></tr><tr><td>train_f1</td><td>▇▂▂▁▁▁▁▁▁▁▁▁▁▁▃▇▇▆▇▇▇▇▇█▆▆▇▇▇▇▇▇▇▆▆▅▇█▇█</td></tr><tr><td>train_loss_epoch</td><td>█▇▇▇▇▆▆▆▆▆▆▆▆▆▅▄▃▂▂▃▂▃▂▃▃▂▂▂▂▁▂▂▂▂▃▂▃▂▁▁</td></tr><tr><td>train_loss_step</td><td>▇▆██▇█▆▆█▆█▇▇▆▇▆▅▄▅▄▃▄▆▃▄▄▂▆▅▃▃▄▁▁▃▆▆▃▃▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▃▃▄▃▅▅▃▃▆▄▅█▅█▆▅██▆▇▇▇▃▃▇▇▆</td></tr><tr><td>val_auc</td><td>▇█▄▂▂▁▁▁▂▁▃▃▄▅▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▂▃▄▃▅▅▃▃▇▄▅▇▅▇▅▅▇█▇▇▇▇▃▃▆▇▅</td></tr><tr><td>val_loss_epoch</td><td>▄▄▄▄▄▄▄▅▅▄▅▄▄▄▄▃▆▅▃▄▄▃▃▁▁▃▄▅▃▄▃█▄▄▇▃█▅▄▄</td></tr><tr><td>val_loss_step</td><td>▄▄▄▄▃▄▄▅▅▄▅▄▄▄▄▃▆▅▃▃▄▃▃▁▁▃▄▄▂▄▃█▄▄▇▃█▅▄▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.6883</td></tr><tr><td>train_auc</td><td>0.74945</td></tr><tr><td>train_f1</td><td>0.59453</td></tr><tr><td>train_loss_epoch</td><td>0.58487</td></tr><tr><td>train_loss_step</td><td>0.55754</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.64234</td></tr><tr><td>val_auc</td><td>0.66595</td></tr><tr><td>val_f1</td><td>0.27941</td></tr><tr><td>val_loss_epoch</td><td>0.67667</td></tr><tr><td>val_loss_step</td><td>0.56907</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_32_onehot_0</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/udsk394n' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/udsk394n</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231231_033135-udsk394n\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20231231_035130-9h0x4vp6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/9h0x4vp6' target=\"_blank\">MLP_4_32_onehot_0</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/9h0x4vp6' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/9h0x4vp6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_4_32_onehot_0\\attention2\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 3.5 K \n",
      "2  | head        | Sequential       | 562   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 1.1 K \n",
      "--------------------------------------------------\n",
      "5.1 K     Trainable params\n",
      "0         Non-trainable params\n",
      "5.1 K     Total params\n",
      "0.020     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▄▃▄▄▄▄▄▄▄▄▄▄▅▆▇▇▇▇▇▇█▇██▇▇█▇██▇█▇▇▇████</td></tr><tr><td>train_auc</td><td>▂▂▁▂▁▁▁▂▂▁▁▁▃▄▆▇▇▇▇▇▇███▇▇██▇██▇█▇█▇▇█▇█</td></tr><tr><td>train_f1</td><td>▇▃▂▁▁▁▁▁▁▁▁▁▁▂▇▆▆▇▇█▇▇▆▇▇▇▇▇▇▇█▇▇▇▇▆▇▇▇▇</td></tr><tr><td>train_loss_epoch</td><td>█▇▇▆▇▇▇▇▇▇▇▇▆▆▅▃▂▃▂▃▃▂▂▁▁▃▂▂▂▁▃▂▂▁▂▂▂▂▂▃</td></tr><tr><td>train_loss_step</td><td>▇▇▇█▇▇▆▇▇▇▇▇▆▇▅▄▅▆▄█▄▅▅▆▁█▇▃▂▆▃▆▄▄▅▄▅▃▆▇</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▆▅▆▆▆█▇▆▇▆▇▆▆▇▆▇▇▇▇▇▆█▇▆▇▇█</td></tr><tr><td>val_auc</td><td>▆█▁▅▅▅▄▄▄▄▅▅▅▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▅▄██▇▆▆▇▇█▇██▇▆▇▇▇▇▇▇█▇▇▇▇▇</td></tr><tr><td>val_loss_epoch</td><td>▅▅▆▇▅▆▆▅▆▅▆▇▆▅▅▆▆▅▅▅▃█▆▄▂▄▇▆▇▃▅▄▃▄▄▃▅▄▁▆</td></tr><tr><td>val_loss_step</td><td>▄▅▅▆▅▅▅▅▅▄▆▆▅▄▅▆▆▅▅▄▃█▆▄▁▃▇▆▇▃▅▄▃▄▄▄▅▄▁▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.69196</td></tr><tr><td>train_auc</td><td>0.74079</td></tr><tr><td>train_f1</td><td>0.56516</td></tr><tr><td>train_loss_epoch</td><td>0.61054</td></tr><tr><td>train_loss_step</td><td>0.70447</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.66058</td></tr><tr><td>val_auc</td><td>0.66567</td></tr><tr><td>val_f1</td><td>0.51813</td></tr><tr><td>val_loss_epoch</td><td>0.67567</td></tr><tr><td>val_loss_step</td><td>0.72058</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_32_onehot_0</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/9h0x4vp6' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/9h0x4vp6</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231231_035130-9h0x4vp6\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20231231_041030-9cq7af7u</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/9cq7af7u' target=\"_blank\">MLP_4_64_onehot_0</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/9cq7af7u' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/9cq7af7u</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_4_64_onehot_0\\mean\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 13.1 K\n",
      "2 | head        | Sequential       | 2.1 K \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "15.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "15.3 K    Total params\n",
      "0.061     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▃▃▃▃▃▃▃▃▃▅▆▇▇▇▇▇█▇▇▇▇▇▇▇▇▇▇█▇▇▇▇▇▇█▇▇▇█</td></tr><tr><td>train_auc</td><td>▂▂▁▁▁▂▁▁▂▂▅▇██▇▇▇▇▇▇██▇██████▇██▇█████▇█</td></tr><tr><td>train_f1</td><td>▅▂▁▁▁▁▁▁▁▁▃▇█▇▇▇▇▇▇█████▇▇▇██▇▇███▇███▆█</td></tr><tr><td>train_loss_epoch</td><td>█▇▇▇▇▇▇▇▇▇▆▃▃▂▃▄▃▄▄▃▄▃▄▃▁▂▂▂▂▃▃▃▃▃▂▂▃▃▃▂</td></tr><tr><td>train_loss_step</td><td>▆▆▇▇█▆▆▅▆▆▃▅▅▃▄▃▄▄▅▂▄▄▅▂▃▂▄▄▆▅▁▇▄█▅▅▄▅▄▅</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▆▇▇▇▆▆▆▆▆▅▆▇▆█▇▆▇▇▇▇▆▇▇▇▇█▇▇▅█</td></tr><tr><td>val_auc</td><td>▃▁▂▂▂▂▂▂▃▄▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇███████████▇█</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▆▇▆▆▇▇▇▇▇▄▆▆▇█▇▇▇▆▆▆▇▆▆▇▇█▆▇██</td></tr><tr><td>val_loss_epoch</td><td>▅▅▅▅▅▅▅▅▆▅▄▁▅▁▅▃▅▆▇▅▃▆▅▇▄▅▃▆█▄▆▃▆▅▆▆▅▄▄▃</td></tr><tr><td>val_loss_step</td><td>▅▅▅▅▅▅▅▅▅▅▄▁▅▂▅▃▅▆▇▄▃▆▅▇▄▅▃▆█▄▆▃▇▆▆▆▅▅▄▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.71755</td></tr><tr><td>train_auc</td><td>0.75504</td></tr><tr><td>train_f1</td><td>0.63082</td></tr><tr><td>train_loss_epoch</td><td>0.58678</td></tr><tr><td>train_loss_step</td><td>0.63088</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.66423</td></tr><tr><td>val_auc</td><td>0.66937</td></tr><tr><td>val_f1</td><td>0.51064</td></tr><tr><td>val_loss_epoch</td><td>0.59937</td></tr><tr><td>val_loss_step</td><td>0.56628</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_64_onehot_0</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/9cq7af7u' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/9cq7af7u</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231231_041030-9cq7af7u\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20231231_042949-kxtjb33n</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/kxtjb33n' target=\"_blank\">MLP_4_64_onehot_0</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/kxtjb33n' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/kxtjb33n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_4_64_onehot_0\\max\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 13.1 K\n",
      "2 | head        | Sequential       | 2.1 K \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "15.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "15.3 K    Total params\n",
      "0.061     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\torch_geometric\\warnings.py:17: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▄▅▅▅▅▅▅▅▅▆▆▆▇▅▇█▇▆▇▅▅▇▆▆▇▇▆▇▄█▇█▇▇█▆█▇▇</td></tr><tr><td>train_auc</td><td>▁▂▂▁▁▃▃▂▃▄▅▅▅▇▅▆▇▆▅▆▆▅▅▅▅▇▇█▇▇▇▇▇▆▇▇▆█▇█</td></tr><tr><td>train_f1</td><td>▅▃▁▃▁▃▂▂▃▂▄▂▃▅▆▄▅▅▅▇▇▇▅▅▅▇▇█▆█▆▇▆▆▆▇▇█▇▇</td></tr><tr><td>train_loss_epoch</td><td>█▅▅▅▅▅▄▄▄▄▅▄▃▂▃▄▃▄▄▃▄▄▃▄▃▃▂▄▁▄▂▃▂▃▂▃▂▁▃▂</td></tr><tr><td>train_loss_step</td><td>▅▅▆▅█▅▄▃▅▅▃▅▅▅▁▃▅▂▆▅▅▄▆▆▃▄▄▃█▅▆▄▃▆▄▆▂▃▄▅</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>▄▃▁▂▇▆▅▂█▆▆█▆▂▆▅▃▂█▇▆▆▃▆▃▅▃▃▄▇▅▆▅▄▁▂▆▃▇▇</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_epoch</td><td>▁▁▁▁▁▁▁▁▂▂▂▃▂▅▄▆▃▅▄▅▆▄▅▄▄▆▇▆▅▇▄▅▄▅▇▆▇█▄▆</td></tr><tr><td>val_loss_step</td><td>▁▁▁▁▁▁▁▁▁▁▂▄▁▆▄▇▂▅▂▄▆▃▄▃▃▅▇▅▄▆▂▄▂▃▅▄▅█▂▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.61883</td></tr><tr><td>train_auc</td><td>0.65532</td></tr><tr><td>train_f1</td><td>0.45632</td></tr><tr><td>train_loss_epoch</td><td>0.64678</td></tr><tr><td>train_loss_step</td><td>0.66333</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.40876</td></tr><tr><td>val_auc</td><td>0.51973</td></tr><tr><td>val_f1</td><td>0.58031</td></tr><tr><td>val_loss_epoch</td><td>1.1489</td></tr><tr><td>val_loss_step</td><td>1.06066</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_64_onehot_0</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/kxtjb33n' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/kxtjb33n</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231231_042949-kxtjb33n\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20231231_044916-xhij02q5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/xhij02q5' target=\"_blank\">MLP_4_64_onehot_0</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/xhij02q5' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/xhij02q5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_4_64_onehot_0\\sum\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 13.1 K\n",
      "2 | head        | Sequential       | 2.1 K \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "15.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "15.3 K    Total params\n",
      "0.061     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▂▂▃▃▄▄▄▃▄▄▅▆▅▇▆▆▇█▇██▇▇███▇▇▇▇▇▇██▇▇▇▇</td></tr><tr><td>train_auc</td><td>▂▃▄▄▇▄▂▂▆▆▇▇▄▂▄▄▃▁▄▆▆▆▆▅▄▅▅▄▄▅▃▄▃▆▇█▅▃▇▅</td></tr><tr><td>train_f1</td><td>▁▁▁▄▃▁▂▂▂▁▃▁▄▅▆▆▅▆▆▇█▇▇▆▇█▇█▇█▇█▇▇▇▇▇▇▇▇</td></tr><tr><td>train_loss_epoch</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▃▂▂▂▂▂▁▂▂▂▂▂▂▁▁▂▁▁▁▁▂▂▁▁▁▂▁▂▂▂▂▁▂▂▂▁▁▂▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▆▆▆▆▆▆▆▆▆▆▇▆▆▇▇█▇▇▇▇▇█▇██▇▇▇▇▇▇███▇▇██</td></tr><tr><td>val_auc</td><td>▁▇▁▇▇▁▃█▇▇▇█▆▃▇█▄▂██████████████████████</td></tr><tr><td>val_f1</td><td>██▁▁▁▁▁▁▁▁▃▁▃▃▂▄▅▆▄▃▃▄▅▆▅▆▆▅▅▃▅▄▅▇▇▇▅▅▇▇</td></tr><tr><td>val_loss_epoch</td><td>█▂▁▁▁▁▁▂▁▁▁▁▂▁▁▁▁▁▂▁▁▂▁▂▁▁▁▂▂▁▁▁▂▂▁▂▁▁▁▁</td></tr><tr><td>val_loss_step</td><td>█▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▂▂▃▁▂▂▂▂▂▂▁▂▂▂▂▁▂▂▂▂▂▁▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.67185</td></tr><tr><td>train_auc</td><td>0.55258</td></tr><tr><td>train_f1</td><td>0.57615</td></tr><tr><td>train_loss_epoch</td><td>0.61844</td></tr><tr><td>train_loss_step</td><td>0.65785</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.67153</td></tr><tr><td>val_auc</td><td>0.71186</td></tr><tr><td>val_f1</td><td>0.5</td></tr><tr><td>val_loss_epoch</td><td>0.60162</td></tr><tr><td>val_loss_step</td><td>0.5985</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_64_onehot_0</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/xhij02q5' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/xhij02q5</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231231_044916-xhij02q5\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20231231_050812-4njniie6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/4njniie6' target=\"_blank\">MLP_4_64_onehot_0</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/4njniie6' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/4njniie6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\torch_geometric\\deprecation.py:22: UserWarning: 'nn.glob.GlobalAttention' is deprecated, use 'nn.aggr.AttentionalAggregation' instead\n",
      "  warnings.warn(out)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_4_64_onehot_0\\attention\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 13.1 K\n",
      "2  | head        | Sequential       | 2.1 K \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 65    \n",
      "--------------------------------------------------\n",
      "15.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "15.3 K    Total params\n",
      "0.061     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▃▄▄▃▄▄▄▆▆█▅▅▇▇▆▇▇▆▇▇█▇█▇▇▇████▇▇▇▇█▇█▇▇</td></tr><tr><td>train_auc</td><td>▁▂▂▂▁▃▂▂▅▇▇▅▆▇▇▇█▇▇▇█▇██▇▇██████▇█████▇▇</td></tr><tr><td>train_f1</td><td>▅▂▁▁▁▁▁▁▆▅▇▇▃▆██▇▇▅▇██▇▇▇▆▇▇███▇▆▇▆▇▇▇█▆</td></tr><tr><td>train_loss_epoch</td><td>█▇▇▇▇▇▇▇▆▅▂▆▅▃▂▅▃▃▃▂▃▃▃▃▃▃▂▂▂▂▁▂▃▃▃▂▃▁▃▃</td></tr><tr><td>train_loss_step</td><td>█▆▆▅▆▆▆▆▅▄▇▆▄▅▂▄▄▃▃▃▂▂▆▄▃▂▄▃▁▃▃▂▂▁▄▂▃▃▃▆</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▂▃▅▇▂▇▄▃▃▇█▅▃▃▅▅▇▆▆▇▇▆▇█▇▇▇▆▇▇▅▇</td></tr><tr><td>val_auc</td><td>▃▂▁▁▁▁▁▄▆▇█▇▇██████▇▇▇▇██▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▂▃██▂▆▄▃▃▆▇▅▃▃▄▄▆▅▆▆▆▆▆▆▆▆▇▆▆▆▄▆</td></tr><tr><td>val_loss_epoch</td><td>▅▅▄▄▅▄▄▃▆▄▃▄▄▄▇▆▄▅▄▁█▅▄▆▄▃▃▆▄▄▆▇▄▅▂▅▄▇▄▃</td></tr><tr><td>val_loss_step</td><td>▅▅▅▅▅▅▅▄▇▅▄▅▅▅▇▆▅▇▅▁█▅▅▇▅▃▃▇▅▅▆█▅▆▂▆▄█▄▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.67276</td></tr><tr><td>train_auc</td><td>0.72627</td></tr><tr><td>train_f1</td><td>0.46567</td></tr><tr><td>train_loss_epoch</td><td>0.6183</td></tr><tr><td>train_loss_step</td><td>0.66891</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.66058</td></tr><tr><td>val_auc</td><td>0.6643</td></tr><tr><td>val_f1</td><td>0.45614</td></tr><tr><td>val_loss_epoch</td><td>0.59619</td></tr><tr><td>val_loss_step</td><td>0.52895</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_64_onehot_0</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/4njniie6' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/4njniie6</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231231_050812-4njniie6\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20231231_052738-dqmuup2q</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/dqmuup2q' target=\"_blank\">MLP_4_64_onehot_0</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/dqmuup2q' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/dqmuup2q</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_4_64_onehot_0\\attention2\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 13.1 K\n",
      "2  | head        | Sequential       | 2.1 K \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 4.2 K \n",
      "--------------------------------------------------\n",
      "19.5 K    Trainable params\n",
      "0         Non-trainable params\n",
      "19.5 K    Total params\n",
      "0.078     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▃▃▃▃▃▃▃▅▆▇█▇▇▇▇█▇▇▇▆▇█▇█████▇████████▇▇</td></tr><tr><td>train_auc</td><td>▁▁▁▂▁▂▁▃▅▇▇▇▇█▇▇█▇▇█▇█▇▇█████▇█▇███▇███▇</td></tr><tr><td>train_f1</td><td>▆▂▁▁▁▁▁▁▇█▇█▇████▇▇▇▆▇▇▆▇█▇▇█▇▇█▇██▇██▇▆</td></tr><tr><td>train_loss_epoch</td><td>██▇▇▇▇█▇▆▅▃▃▂▂▃▄▂▄▃▃▄▂▃▃▂▃▂▂▂▃▃▂▃▂▁▂▂▂▂▃</td></tr><tr><td>train_loss_step</td><td>▆▇▆▆▇▆▇▄▅▅▆▃█▃▄▂▂▁▅▂▄▃▁▄▃▅▂▃▂▅▃▄▅▄▄▃▃▁▇▅</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▃▃▅▇▆▄▇█▆▆▆▆▇▇▆▅▇▇▇▇▆▇▆▇▆▇▇▇▇▇▆▇█</td></tr><tr><td>val_auc</td><td>▃▁▁▁▂▂▂▄▆▇▇▇▇▇▇▇▇▇▇█████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▂▃▄▇▇█▆▆▆▆▆▆▇▇▇█▇▆▇▇▇▇█▇▇▇▇▆▇▆▇▇▇</td></tr><tr><td>val_loss_epoch</td><td>▆▅▆▅▅▆▆▄▆▅▅▄▃▃▆▆▇▄▄▃▁▆▇▃▃█▄▅▆▅▃▆▅▇▆▅▅▂▆▅</td></tr><tr><td>val_loss_step</td><td>▅▄▅▅▅▅▅▄▅▅▅▄▃▃▆▅▆▄▄▃▁▆▇▃▃█▄▅▆▅▃▆▄▇▆▅▄▂▅▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.66636</td></tr><tr><td>train_auc</td><td>0.73437</td></tr><tr><td>train_f1</td><td>0.44613</td></tr><tr><td>train_loss_epoch</td><td>0.60873</td></tr><tr><td>train_loss_step</td><td>0.64379</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.66423</td></tr><tr><td>val_auc</td><td>0.66452</td></tr><tr><td>val_f1</td><td>0.45238</td></tr><tr><td>val_loss_epoch</td><td>0.65021</td></tr><tr><td>val_loss_step</td><td>0.67355</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_64_onehot_0</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/dqmuup2q' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/dqmuup2q</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231231_052738-dqmuup2q\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20231231_054701-y4nntl0j</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/y4nntl0j' target=\"_blank\">MLP_2_16_onehot_1</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/y4nntl0j' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/y4nntl0j</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_2_16_onehot_1\\mean\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 432   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "586       Trainable params\n",
      "0         Non-trainable params\n",
      "586       Total params\n",
      "0.002     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▄▅▅▅▅▅▅▅▅▆▆▇▇▇▇▇▇▇▇▇▇▇█▇█▇▇███▇▇█▇██▇█▇</td></tr><tr><td>train_auc</td><td>▁▁▂▁▁▂▂▂▂▄▄▅▇▆▇▇▇▇▇▇█▇▇███▇█████████▇▇██</td></tr><tr><td>train_f1</td><td>▇▂▁▁▁▁▁▁▁▁▄▅▆▇▇█▇▇▇▇▇█▇▇▇█▇█▇██▇▇█▇▇█▇▇▆</td></tr><tr><td>train_loss_epoch</td><td>█▆▆▆▆▆▆▆▅▅▄▄▃▃▃▂▃▂▃▂▂▂▁▁▂▁▂▁▂▁▁▂▁▂▁▁▂▂▂▁</td></tr><tr><td>train_loss_step</td><td>▇▅█▇▇▆▆▆▅▆▆▆▁▅▂▂▃▁▇▂▆▄▆▂▅▆▄▄▃▃▄▃▃▃▅▁▂▃▇▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▄▆▆▇▆▇▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇█▇██▇▇▇▇▇</td></tr><tr><td>val_auc</td><td>▁▅▆▆▇███████████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▅▆▇▇▇▇█▇██▇▇██▇█▇▇█▇██▇█████▇▇</td></tr><tr><td>val_loss_epoch</td><td>▇██▆▇▆▇▇▆▆▇▅▅▆▄▃▅▄▅▆▄▆▂▇▁▇▆▃▅▁▇▁▂▇▅▁▄▃▄▄</td></tr><tr><td>val_loss_step</td><td>▆▇▇▅▆▅▆▆▆▅▆▅▅▆▄▃▅▄▆▆▄▆▂█▁▇▇▃▅▁█▁▂▇▅▁▅▃▄▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.66453</td></tr><tr><td>train_auc</td><td>0.72204</td></tr><tr><td>train_f1</td><td>0.44812</td></tr><tr><td>train_loss_epoch</td><td>0.60235</td></tr><tr><td>train_loss_step</td><td>0.59372</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.69708</td></tr><tr><td>val_auc</td><td>0.77629</td></tr><tr><td>val_f1</td><td>0.52023</td></tr><tr><td>val_loss_epoch</td><td>0.5952</td></tr><tr><td>val_loss_step</td><td>0.59647</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_16_onehot_1</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/y4nntl0j' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/y4nntl0j</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231231_054701-y4nntl0j\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20231231_060621-kd3zlfp8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/kd3zlfp8' target=\"_blank\">MLP_2_16_onehot_1</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/kd3zlfp8' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/kd3zlfp8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_2_16_onehot_1\\max\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 432   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "586       Trainable params\n",
      "0         Non-trainable params\n",
      "586       Total params\n",
      "0.002     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\torch_geometric\\warnings.py:17: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▅▆▆▇▆▆▆▆▆▆▆▆▆▆▆▇▇▆▇▇▇▇█▇▇█▇▇▇▇█▇▇█▇██▇▇</td></tr><tr><td>train_auc</td><td>▃▂▅▃▄▁▅▅▃▅▆▄▃▃▃▅▅▆▄▃▃▁▃▃▅▆▅▆▄▃▅▄▇█▇▆█▇▇█</td></tr><tr><td>train_f1</td><td>█▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▃▃▄▄▅▅▅▅▅▃▅▄▅▅▄▅▄▆▅▅▄</td></tr><tr><td>train_loss_epoch</td><td>█▄▃▃▃▃▃▃▂▂▃▃▃▃▂▃▃▃▃▂▂▂▂▂▂▁▂▂▂▂▂▁▁▂▁▂▁▁▃▂</td></tr><tr><td>train_loss_step</td><td>▂▄▆▆▆▅▅▄▃▆▇▆▃▃▅▃▃▃█▄▄▅▅▂▅▅▅▄▅▅▄▂▁▅▂▃▂▆▂▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▅▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>▄▄▅▄▂▂▂▂▂▂▂▂▂▁▁▁▂▃▁▁▂▂▁▃▄▆▃▃▃▃▂▃█▇▃▇████</td></tr><tr><td>val_f1</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▅██████████████████████</td></tr><tr><td>val_loss_epoch</td><td>▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▄▂▄▃▃▄▃▄▅▆▅▅█▆▇▅</td></tr><tr><td>val_loss_step</td><td>▂▂▂▁▁▁▂▁▁▁▁▂▂▂▁▁▁▂▂▂▂▂▂▂▄▁▃▂▂▄▃▅▄▆▄▄█▅▆▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.60603</td></tr><tr><td>train_auc</td><td>0.54665</td></tr><tr><td>train_f1</td><td>0.26825</td></tr><tr><td>train_loss_epoch</td><td>0.66156</td></tr><tr><td>train_loss_step</td><td>0.6333</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.40876</td></tr><tr><td>val_auc</td><td>0.66995</td></tr><tr><td>val_f1</td><td>0.58031</td></tr><tr><td>val_loss_epoch</td><td>0.7953</td></tr><tr><td>val_loss_step</td><td>0.78897</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_16_onehot_1</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/kd3zlfp8' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/kd3zlfp8</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231231_060621-kd3zlfp8\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20231231_062616-j3oxk9r5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/j3oxk9r5' target=\"_blank\">MLP_2_16_onehot_1</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/j3oxk9r5' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/j3oxk9r5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_2_16_onehot_1\\sum\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 432   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "586       Trainable params\n",
      "0         Non-trainable params\n",
      "586       Total params\n",
      "0.002     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▃▄▂▄▄▄▅▅▅▅▅▆▆▇▆▆▇█▇▆▇▇▇█▆▇█▇█▇██████▇▇█</td></tr><tr><td>train_auc</td><td>▅▅▇▄█▆▆▇▅▅▅▅▅▅▃▅▅▅▆▅▅▄▄▂▄▄▃▃▄▃▃▂▂▃▁▂▂▂▂▁</td></tr><tr><td>train_f1</td><td>▆▁▄▅▄▅▃▄▅▄▄▄▄▅▅▅▅▄▇▅▄▆▅▆▆▅▅█▅▇▅▇▆▆▆▅█▆▅▅</td></tr><tr><td>train_loss_epoch</td><td>█▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▃▂▅▂▄▄▃▅▅▄▄▅▆▅▄▆▆▄▆▆▆▆▆▆▇▇▆▆▇▇▆▇▅█▆▆▆</td></tr><tr><td>val_auc</td><td>▂▃█▄█▄▇▆▃▄▆▄▅▃▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_f1</td><td>▁▁▁▅▂▅▂▅▅▃▆▅▅▅▅▆▅▅▇▇▅▇▆▆▆▆▆▇▇▆▇▇▇▇▇▆█▇▇▆</td></tr><tr><td>val_loss_epoch</td><td>█▂▂▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_step</td><td>█▂▂▁▁▁▁▁▂▁▂▁▁▂▁▁▁▁▁▁▁▁▁▂▁▂▁▁▁▁▂▁▁▂▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.64808</td></tr><tr><td>train_auc</td><td>0.37117</td></tr><tr><td>train_f1</td><td>0.48322</td></tr><tr><td>train_loss_epoch</td><td>0.61434</td></tr><tr><td>train_loss_step</td><td>0.58505</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.68613</td></tr><tr><td>val_auc</td><td>0.2365</td></tr><tr><td>val_f1</td><td>0.46914</td></tr><tr><td>val_loss_epoch</td><td>0.57122</td></tr><tr><td>val_loss_step</td><td>0.53015</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_16_onehot_1</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/j3oxk9r5' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/j3oxk9r5</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231231_062616-j3oxk9r5\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20231231_064527-xobhegia</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/xobhegia' target=\"_blank\">MLP_2_16_onehot_1</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/xobhegia' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/xobhegia</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\torch_geometric\\deprecation.py:22: UserWarning: 'nn.glob.GlobalAttention' is deprecated, use 'nn.aggr.AttentionalAggregation' instead\n",
      "  warnings.warn(out)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_2_16_onehot_1\\attention\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 432   \n",
      "2  | head        | Sequential       | 154   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 17    \n",
      "--------------------------------------------------\n",
      "603       Trainable params\n",
      "0         Non-trainable params\n",
      "603       Total params\n",
      "0.002     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▄▄▄▄▄▄▄▄▅▆▇▇▇▇▇▇▇▆▆█▇▇█▇▇██▇▇▇▇▇▇▇▇█▇▇▇</td></tr><tr><td>train_auc</td><td>▂▂▁▁▂▁▂▁▃▃▅▆▆▇▆▇▆▇▇▆▇▇▇▇▇▇▇▇▇▇▇▇███▇████</td></tr><tr><td>train_f1</td><td>▇▂▁▁▁▁▁▁▁▄▄▇▇▆▆▇▇▇▆▆█▆▇▇▇▇██▇▇▇▇▇█▆▆█▆▇▇</td></tr><tr><td>train_loss_epoch</td><td>█▇▇▇▆▆▇▆▆▅▄▄▃▃▃▂▃▂▂▃▃▂▁▂▂▂▂▃▂▂▁▂▁▃▂▁▂▂▂▃</td></tr><tr><td>train_loss_step</td><td>▇▆▆▇█▆▅▆▆▄▅▆▄▄▅▁▃▅▄▃▄▃▄▅▄▄▅▆▃▃█▁▅▄▅▃▄▅▅▇</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▂▂▂▂▂▂▂▂▁▃▅▅▆▇▆▆▆▇█▄▇▇▅▅▇▇▇▇▇█▇▇▇██▆▇█▇▇</td></tr><tr><td>val_auc</td><td>▁▅▇▇████████████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▃▅▅▆▇▆▅▆▇█▄▇▇▆▆▇▇▇▇▇▇▇▇▇██▆▇█▇▇</td></tr><tr><td>val_loss_epoch</td><td>▇▆▇▇▇▇▇▆▆▅▅▅▅▇▄▆▂▅▁█▄▇▅▄▂▆▂▆▅▃▅▅▇▆▅▄▃▅▄▄</td></tr><tr><td>val_loss_step</td><td>▆▆▇▆▆▆▇▆▆▅▅▄▆▇▄▆▂▅▁█▄▇▅▄▂▆▂▇▆▄▅▅▇▆▅▅▃▅▄▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.68007</td></tr><tr><td>train_auc</td><td>0.72508</td></tr><tr><td>train_f1</td><td>0.50284</td></tr><tr><td>train_loss_epoch</td><td>0.62445</td></tr><tr><td>train_loss_step</td><td>0.7051</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.66423</td></tr><tr><td>val_auc</td><td>0.77794</td></tr><tr><td>val_f1</td><td>0.41026</td></tr><tr><td>val_loss_epoch</td><td>0.58218</td></tr><tr><td>val_loss_step</td><td>0.55377</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_16_onehot_1</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/xobhegia' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/xobhegia</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231231_064527-xobhegia\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20231231_070546-cic6325u</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/cic6325u' target=\"_blank\">MLP_2_16_onehot_1</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/cic6325u' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/cic6325u</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_2_16_onehot_1\\attention2\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 432   \n",
      "2  | head        | Sequential       | 154   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 281   \n",
      "--------------------------------------------------\n",
      "867       Trainable params\n",
      "0         Non-trainable params\n",
      "867       Total params\n",
      "0.003     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▄▄▅▅▅▅▅▅▅▅▆▇▇▇▇▇█▇▇▇▇▇▇███▇▇▇█▇█████▇█▇</td></tr><tr><td>train_auc</td><td>▁▂▂▁▁▂▃▁▂▂▃▅▆▆▆▇▇▇▇▇▇▇▇▇█▇▇▇▇██▇████████</td></tr><tr><td>train_f1</td><td>▇▃▂▂▁▁▁▁▁▁▂▄▆▆▇▇▆▇▇▇▇▇▆▇▇▇▇▇▆▇█▇▇▇▇▇█▇█▇</td></tr><tr><td>train_loss_epoch</td><td>█▆▆▆▆▆▆▆▆▅▅▄▃▄▃▃▂▃▂▂▂▂▃▂▂▃▁▃▂▁▂▃▁▃▁▂▂▁▃▂</td></tr><tr><td>train_loss_step</td><td>▆█▆▇▆▆▇▇▆▆▆▆▂▄▂▃▅▄▃▆▁▂▁▃▃▅▅▅▂▆▄▃▁▄▄▆▄▄▃▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▂▃▆▆▃▇▆▆▆▇▆▆▆▆▆▆▇▇▇▆█▆▆▇▇▆█▆▇▇</td></tr><tr><td>val_auc</td><td>▁▅▇█████████████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▂▄▆▆▄▇▆▆▆▇▆▆▆▆▇▇▇▇▇▆█▇▇▇▇▇█▇▇▇</td></tr><tr><td>val_loss_epoch</td><td>▇▇▇▇▆▇█▇▇▇▅▆▅▆▆▇█▄▂▄▅▄▅▂▃▄█▅▄▄▄▃▃▁▂▆▄▅▅▇</td></tr><tr><td>val_loss_step</td><td>▆▆▆▆▅▆▆▆▆▅▅▅▅▅▅▆▇▄▂▄▅▃▅▂▃▄█▅▄▄▄▃▃▁▂▆▄▅▆▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.67002</td></tr><tr><td>train_auc</td><td>0.71265</td></tr><tr><td>train_f1</td><td>0.54477</td></tr><tr><td>train_loss_epoch</td><td>0.6066</td></tr><tr><td>train_loss_step</td><td>0.57582</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.68613</td></tr><tr><td>val_auc</td><td>0.7732</td></tr><tr><td>val_f1</td><td>0.52222</td></tr><tr><td>val_loss_epoch</td><td>0.68794</td></tr><tr><td>val_loss_step</td><td>0.79718</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_16_onehot_1</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/cic6325u' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/cic6325u</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231231_070546-cic6325u\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20231231_072521-s49zpzpn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/s49zpzpn' target=\"_blank\">MLP_2_32_onehot_1</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/s49zpzpn' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/s49zpzpn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_2_32_onehot_1\\mean\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 1.4 K \n",
      "2 | head        | Sequential       | 562   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.008     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▁▁▁▁▄▅▆▇▆▆▆▇▇▆▇▇▇██▇▇▇▇▇▇▆▇▇█▇▇▇█▇▇▇▇█</td></tr><tr><td>train_auc</td><td>▁▂▁▁▁▃▄▆▆▇▇▇▇▇▇██████▇█▇████████████████</td></tr><tr><td>train_f1</td><td>▁▁▁▁▁▁▅▅▇▇▇▇▆▇▇▇▇██▇▇██▇▇▇█▇▇▇▇█▇▇██▇▇██</td></tr><tr><td>train_loss_epoch</td><td>██▇█▇▇▆▄▄▃▄▄▃▃▃▂▃▂▂▁▂▃▃▂▂▂▃▂▂▁▁▃▂▂▃▂▂▂▂▂</td></tr><tr><td>train_loss_step</td><td>▅▅▅▆▄▅▃▄▂▄▂▂▂▃▄▃▃▄▁▂▄▃▁▃▅█▃▃▃▂▂▄▂▁▄▃▃▁▂▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▄█▇▇██▇████▇███▇▇█████████████▇███</td></tr><tr><td>val_auc</td><td>▁▇▇▇▇███████████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▄▇▇▇██▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>val_loss_epoch</td><td>█▆▇█▇▇█▆▄▂▄▅▄▃▇▃▇▄▆▃▁▆▇█▃▄▅▇▃▃▁▃▂▆▇▇▃▅▃▆</td></tr><tr><td>val_loss_step</td><td>▆▅▆▆▅▆▇▅▄▂▃▅▄▃▇▃▇▄▆▃▁▆▇█▃▅▅▇▃▄▁▃▂▆▇█▂▅▃▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.70384</td></tr><tr><td>train_auc</td><td>0.72855</td></tr><tr><td>train_f1</td><td>0.58987</td></tr><tr><td>train_loss_epoch</td><td>0.59264</td></tr><tr><td>train_loss_step</td><td>0.55376</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.70073</td></tr><tr><td>val_auc</td><td>0.77673</td></tr><tr><td>val_f1</td><td>0.53933</td></tr><tr><td>val_loss_epoch</td><td>0.63912</td></tr><tr><td>val_loss_step</td><td>0.6885</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_32_onehot_1</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/s49zpzpn' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/s49zpzpn</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231231_072521-s49zpzpn\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20231231_074445-d67tptd9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/d67tptd9' target=\"_blank\">MLP_2_32_onehot_1</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/d67tptd9' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/d67tptd9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_2_32_onehot_1\\max\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 1.4 K \n",
      "2 | head        | Sequential       | 562   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.008     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\torch_geometric\\warnings.py:17: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▃▃▃▃▃▃▃▄▄▅▆▅▄▄▇▇▆▆▆▆▇▇▆█▇█▆▇▇█▇▇▇██▇▇▇</td></tr><tr><td>train_auc</td><td>▂▂▂▂▂▁▁▃▂▄▄▅▆▅▆▆▇▇▇▇▇▇▇█▇█▇▇▇▇████▇█▇▇▇▇</td></tr><tr><td>train_f1</td><td>▅▂▂▁▁▁▁▁▂▄▄▅▆▅▄▅▇▇▇▆▆▇▇▇▇█▇▇▇▇▇█▇▆██▇▇██</td></tr><tr><td>train_loss_epoch</td><td>█▆▆▆▆▆▆▆▆▅▅▅▃▅▄▃▂▃▃▂▃▂▂▂▂▁▂▂▃▂▂▂▃▁▂▂▁▃▂▂</td></tr><tr><td>train_loss_step</td><td>▇▇▇█▄▇▅▆▆▆▄▄▄▅▄▄▆▆▄▅▇▄▄▄▃▆▃▄▄▆▂▇▅▃▆▄▄▄▁▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>███████████▇▅▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>▁▂▂▂▂▁▁▂▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇█████▇█▇▇▇▇▂▃▄▇▂</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁██████████████████████████████</td></tr><tr><td>val_loss_epoch</td><td>▂▁▂▂▁▂▂▂▂▂▂▂▂▂▂▂▃▄▃▄▄▄▅▄▃▅▆▅▅▆▆▇▆▄▅▆█▆▅▅</td></tr><tr><td>val_loss_step</td><td>▂▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▃▄▃▄▄▃▅▃▂▄▆▄▅▆▅▆▅▃▄▆█▆▄▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.65356</td></tr><tr><td>train_auc</td><td>0.67631</td></tr><tr><td>train_f1</td><td>0.53497</td></tr><tr><td>train_loss_epoch</td><td>0.62872</td></tr><tr><td>train_loss_step</td><td>0.59961</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.40876</td></tr><tr><td>val_auc</td><td>0.42413</td></tr><tr><td>val_f1</td><td>0.58031</td></tr><tr><td>val_loss_epoch</td><td>0.96593</td></tr><tr><td>val_loss_step</td><td>0.90287</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_32_onehot_1</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/d67tptd9' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/d67tptd9</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231231_074445-d67tptd9\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20231231_080436-mra76tnz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/mra76tnz' target=\"_blank\">MLP_2_32_onehot_1</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/mra76tnz' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/mra76tnz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_2_32_onehot_1\\sum\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 1.4 K \n",
      "2 | head        | Sequential       | 562   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.008     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▂▂▂▃▄▄▅▄▅▅▅▆▅▆▆▆▇▇▇█▇▇▇▇▇█▇███▇▇██████</td></tr><tr><td>train_auc</td><td>▆▇█▇▇▆▆▆▆▄▅▄▆▅▄▄▃▄▃▂▄▃▃▂▃▁▃▂▄▅▃▅▅▄▃▃▄▄▄▂</td></tr><tr><td>train_f1</td><td>▄▄▂▂▃▁▄▂▂▂▄▃▃▆▄▄▅▇▇▇▇█▇▅▆▇▇▆▆▇▇██▇███▆▇▇</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▄▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▂▁▁▁▁▁▁▁▁▂▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▂▃▂▂▃▃▃▅▄▅▆▆▄▆▆▆▆▆█▆▆▆▆█▇▇▇▆▆▇▇██▇▆▇</td></tr><tr><td>val_auc</td><td>▂▃██▃▂▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_f1</td><td>▁▁▁▁▂▃▂▂▃▃▄▆▄▅▆▆▅▆▇▆▇▇█▇▇▆▇█▇▇▇▇▇▇▇██▇▇▇</td></tr><tr><td>val_loss_epoch</td><td>█▁▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▂▂▁▁▁▂</td></tr><tr><td>val_loss_step</td><td>█▁▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▂▂▁▁▂▂▂▁▂▂▂▂▁▁▁▁▂▂▂▁▂▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.68099</td></tr><tr><td>train_auc</td><td>0.38579</td></tr><tr><td>train_f1</td><td>0.5686</td></tr><tr><td>train_loss_epoch</td><td>0.58893</td></tr><tr><td>train_loss_step</td><td>0.563</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.70803</td></tr><tr><td>val_auc</td><td>0.22608</td></tr><tr><td>val_f1</td><td>0.56989</td></tr><tr><td>val_loss_epoch</td><td>0.69048</td></tr><tr><td>val_loss_step</td><td>0.76822</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_32_onehot_1</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/mra76tnz' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/mra76tnz</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231231_080436-mra76tnz\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20231231_082332-eu5d3y72</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/eu5d3y72' target=\"_blank\">MLP_2_32_onehot_1</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/eu5d3y72' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/eu5d3y72</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\torch_geometric\\deprecation.py:22: UserWarning: 'nn.glob.GlobalAttention' is deprecated, use 'nn.aggr.AttentionalAggregation' instead\n",
      "  warnings.warn(out)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_2_32_onehot_1\\attention\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 1.4 K \n",
      "2  | head        | Sequential       | 562   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 33    \n",
      "--------------------------------------------------\n",
      "2.0 K     Trainable params\n",
      "0         Non-trainable params\n",
      "2.0 K     Total params\n",
      "0.008     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▁▁▂▂▅▆█▆▆▇▇█▇▆▇█▇▆▇▆█▇▇▇▆▇▆▇█▇▇▇▇██▇▆█</td></tr><tr><td>train_auc</td><td>▁▂▂▂▄▅▇▇▇▇▇▇▇██▇██▇▇▇▇███▇▇▇▇██▇████▇█▇█</td></tr><tr><td>train_f1</td><td>▁▁▁▁▂▃▅▆█▇▇▇▇█▇▆▇██▇▇▆██▇▇▆▆▆█▇▇▇▇▇▇▇▇██</td></tr><tr><td>train_loss_epoch</td><td>█▇▇▇▇▆▃▃▃▄▃▃▂▂▃▄▂▂▃▄▂▃▂▃▂▂▃▃▃▃▂▂▃▃▂▂▂▁▃▂</td></tr><tr><td>train_loss_step</td><td>▅▇▅▅▅▄▄▅▃▄▃▄▂▃▃▄▅▁▃▅▃▄▅▄▇▃▅▄▄▄▂▄▂▂▂▃▁█▄▄</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▃▄▃▃▅▅▅▅▆▆▆▆▆▆▇▇▆▆▇█▇▇▅▆██▇▇▇█▇▇▇▆</td></tr><tr><td>val_auc</td><td>▁▆▇▇▇███████████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▂▄▅▄▄▅▆▆▆▇▇▆▆▇▆▇▇▇▇▇█▇█▅▆██▇█▇██▇█▇</td></tr><tr><td>val_loss_epoch</td><td>▅▆▅▅▅▃▃▃█▇▃▅▆▆▄▄▆▄▇▃▂▁▄▃▃▂▃▂▄▂▃▂▃▃▄▄▅▃▄▅</td></tr><tr><td>val_loss_step</td><td>▄▅▅▅▅▂▃▂█▇▃▅▆▇▄▄▇▄▇▃▂▁▅▄▄▂▃▂▄▃▄▂▄▄▅▄▆▃▄▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.69835</td></tr><tr><td>train_auc</td><td>0.74379</td></tr><tr><td>train_f1</td><td>0.59259</td></tr><tr><td>train_loss_epoch</td><td>0.60199</td></tr><tr><td>train_loss_step</td><td>0.64986</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.67153</td></tr><tr><td>val_auc</td><td>0.77778</td></tr><tr><td>val_f1</td><td>0.4375</td></tr><tr><td>val_loss_epoch</td><td>0.66157</td></tr><tr><td>val_loss_step</td><td>0.72662</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_32_onehot_1</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/eu5d3y72' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/eu5d3y72</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231231_082332-eu5d3y72\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20231231_084353-3p28lyqj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/3p28lyqj' target=\"_blank\">MLP_2_32_onehot_1</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/3p28lyqj' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/3p28lyqj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_2_32_onehot_1\\attention2\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 1.4 K \n",
      "2  | head        | Sequential       | 562   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 1.1 K \n",
      "--------------------------------------------------\n",
      "3.0 K     Trainable params\n",
      "0         Non-trainable params\n",
      "3.0 K     Total params\n",
      "0.012     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▁▁▁▂▄▅▆▇▆▇▆▇▆▆▄▇▇▆▇▆▆▇▇▆▇█▇▇▇▇█▇▇▇▆▆█▇</td></tr><tr><td>train_auc</td><td>▁▂▁▂▃▃▅▆▇▇▇█▇▇▇█▇▇███▇▇██████▇██████▇▇██</td></tr><tr><td>train_f1</td><td>▁▁▁▂▁▃▅▆▇▇▇▆▇▇▇▆▅▇▇▆█▇▇▇▇▇▇█▇▇█▇█▇▇█▇▇█▇</td></tr><tr><td>train_loss_epoch</td><td>█▇█▇▇▇▅▄▃▃▃▁▂▃▄▃▄▂▂▂▂▃▂▂▂▂▁▃▂▁▂▁▂▂▁▂▃▃▂▁</td></tr><tr><td>train_loss_step</td><td>▇▇▆█▇▆▆▄▃▆▃▆▅▁▄▅▄▅▄▅▁▃▄▃▃▆▂▆▄▃▂▄▅▅▄▃█▂▃▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▆▆██▇▇▆▇█▇▇▆▇▇▆█▇█▇█▇▇▇█▇▇█████▇▇█</td></tr><tr><td>val_auc</td><td>▁▇▇▇▇███████████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▆▆███▇▇▇██▆▆█▇▆█▇█▇█▇▇██▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>val_loss_epoch</td><td>█▇▇▇▆▇▅▆▅▅▃▃▅▃▆▆▃█▆▄▇▄▁▄▁█▄▂▄▃▄▃▂▂▆▄▅▄▄▄</td></tr><tr><td>val_loss_step</td><td>▆▆▆▆▄▆▄▆▄▄▃▃▅▃▆▅▃█▅▃▇▄▁▄▁█▄▂▄▃▄▃▂▂▆▄▅▄▄▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.69196</td></tr><tr><td>train_auc</td><td>0.74571</td></tr><tr><td>train_f1</td><td>0.54025</td></tr><tr><td>train_loss_epoch</td><td>0.58627</td></tr><tr><td>train_loss_step</td><td>0.57837</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.70438</td></tr><tr><td>val_auc</td><td>0.77734</td></tr><tr><td>val_f1</td><td>0.56216</td></tr><tr><td>val_loss_epoch</td><td>0.58377</td></tr><tr><td>val_loss_step</td><td>0.5713</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_32_onehot_1</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/3p28lyqj' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/3p28lyqj</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231231_084353-3p28lyqj\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20231231_090444-yiw8dhej</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/yiw8dhej' target=\"_blank\">MLP_2_64_onehot_1</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/yiw8dhej' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/yiw8dhej</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_2_64_onehot_1\\mean\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 4.8 K \n",
      "2 | head        | Sequential       | 2.1 K \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "6.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "6.9 K     Total params\n",
      "0.028     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▃▄▄▅▆▆▆▇▇██▇█▇▇▇▇▇▇▇▇██▇▇█▇██▇█▇█▇▇███▇</td></tr><tr><td>train_auc</td><td>▁▂▄▃▅▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇████▇█████▇█▇▇▇▇██▇▇</td></tr><tr><td>train_f1</td><td>▄▂▁▂▄▄▆▆▇▇█▇██▇▇▇▇▇▇▆▇██▆▇█▇▇▇██▇▇▆▇▇▇▇▇</td></tr><tr><td>train_loss_epoch</td><td>█▇▆▆▅▅▃▃▃▃▃▃▃▃▂▄▂▂▃▃▂▂▂▂▃▂▂▂▂▁▁▂▂▂▂▃▂▂▂▃</td></tr><tr><td>train_loss_step</td><td>▇▇▅▆▄▄▅▂▄▅▅▄▄▅▆█▄▅▄▃▄▅▄▄▆▄▅▅▇▅▁▄▅▅▇▅▄▄▄▆</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▃▆▇▆▇▇▇▇▇▇▇▇██▇██▇▇█████████▇█▇▇█████</td></tr><tr><td>val_auc</td><td>▁▂▃▄▅▅▅▆▆▇▇▇▇███████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▃▇▇▆▇▇▇▇▇▆▇▇██▇▇▇▇▇▇█▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█</td></tr><tr><td>val_loss_epoch</td><td>▇▆▆▆▅▄▄▄█▂▃▇▂▄▂▄▄█▃▁▂▃▆▂▂▅▄▃▁▆▄▂▃▂▄▅▂▆▃▄</td></tr><tr><td>val_loss_step</td><td>▆▅▄▅▄▃▃▃▇▂▃▇▂▄▂▃▃█▃▁▂▃▆▂▂▅▄▃▁▆▄▂▃▂▄▅▂▆▃▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.69104</td></tr><tr><td>train_auc</td><td>0.71945</td></tr><tr><td>train_f1</td><td>0.56331</td></tr><tr><td>train_loss_epoch</td><td>0.60728</td></tr><tr><td>train_loss_step</td><td>0.64796</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.70438</td></tr><tr><td>val_auc</td><td>0.77706</td></tr><tr><td>val_f1</td><td>0.61244</td></tr><tr><td>val_loss_epoch</td><td>0.59604</td></tr><tr><td>val_loss_step</td><td>0.59592</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_64_onehot_1</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/yiw8dhej' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/yiw8dhej</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231231_090444-yiw8dhej\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20231231_092518-kf981t6j</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/kf981t6j' target=\"_blank\">MLP_2_64_onehot_1</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/kf981t6j' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/kf981t6j</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_2_64_onehot_1\\max\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 4.8 K \n",
      "2 | head        | Sequential       | 2.1 K \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "6.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "6.9 K     Total params\n",
      "0.028     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\torch_geometric\\warnings.py:17: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▃▄▅▅▅▆▆▆▇▆▇▇▆▇▆▇▇▇▆▇▇▇▇▇▆▇▇▇▇▇▇▇▇▆█▇▇▇</td></tr><tr><td>train_auc</td><td>▁▃▃▄▄▅▆▆▆▇▇▇█▇▇▇▇▇▇▇▇▇▇▇▇█▇▇▇▇▇▇▇█▇▆██▇▇</td></tr><tr><td>train_f1</td><td>▂▃▁▁▅▄▆▆▆▆▇▅▇▇▆▇▇▇▇▇▅▆▇▇▆▇▇▇▇▇▇▇▆▇▆▆██▇█</td></tr><tr><td>train_loss_epoch</td><td>█▆▅▄▄▃▃▃▃▂▃▃▁▂▂▂▂▂▂▂▂▁▃▂▂▁▂▁▂▁▂▂▁▁▂▃▁▁▂▂</td></tr><tr><td>train_loss_step</td><td>█▇▆▆▅▆▅▄▆▇▆▃▇▄▇▆▅▆▄▅▅▄▆▃▆▃▄▅▆▆▃▁▅▃▆█▅▅▃▆</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>▂▂▃▃▃▂▃▃▄▄▄▄▄▄▄█▅▄▄▄█▁▄▃▃▃▃▂▄▇▆▃▃▂▅▂▄▄▄▄</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_epoch</td><td>▁▁▁▁▁▁▂▃▃▃▃▅▅▅▃▃▅▅▆▅▆▇▇▇▆▅▆█▆▇█▆██▅▅▇▆▅▇</td></tr><tr><td>val_loss_step</td><td>▁▁▁▂▁▂▂▃▃▃▃▆▆▅▃▃▆▄▆▅▅▇▇█▆▄▆█▆▇▇▅█▇▃▄▆▅▃▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.68556</td></tr><tr><td>train_auc</td><td>0.71807</td></tr><tr><td>train_f1</td><td>0.60277</td></tr><tr><td>train_loss_epoch</td><td>0.6023</td></tr><tr><td>train_loss_step</td><td>0.63798</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.40876</td></tr><tr><td>val_auc</td><td>0.61888</td></tr><tr><td>val_f1</td><td>0.58031</td></tr><tr><td>val_loss_epoch</td><td>1.29473</td></tr><tr><td>val_loss_step</td><td>1.13248</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_64_onehot_1</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/kf981t6j' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/kf981t6j</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231231_092518-kf981t6j\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20231231_094511-10ixs27l</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/10ixs27l' target=\"_blank\">MLP_2_64_onehot_1</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/10ixs27l' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/10ixs27l</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_2_64_onehot_1\\sum\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 4.8 K \n",
      "2 | head        | Sequential       | 2.1 K \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "6.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "6.9 K     Total params\n",
      "0.028     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▃▃▃▄▄▆▇▇▇▇▆▆▇▇▆▇▆██▆▇███▇▇▇██▇█▇█▇▇██▇</td></tr><tr><td>train_auc</td><td>▄▄▄▃▅▅▇▆▃▄▂▆▅▂▅▅█▆▃▅█▇▅▆▆▅▆▅▁▄▄▅▅▄▂▄▃▆▅▃</td></tr><tr><td>train_f1</td><td>▂▁▃▂▄▃▅▄▅▆▇▆█▆▅▇▇▆▅▇▆▆█▇▇▇▇█▇██▇▇▇▇▇█▇▇▇</td></tr><tr><td>train_loss_epoch</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▃▆▂▂▂▂▁▁▁▂▂▁▂▂▂▂▁▂▁▁▁▁▁▂▁▂▁▁▂▁▁▂▁▂▂▁▁▂▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▃▄▂▄▄▅█▆▅▅▆▆█▆▇▇█▇▇▆▆▇▆▇▆▆▆▇▇▆▇▇█▇▇▆▇▇▇</td></tr><tr><td>val_auc</td><td>▇▇▇▆▅▇▇▆▅▅▅▇▇▄▅██▇▇███▇████▇▂▄████▆▃███▁</td></tr><tr><td>val_f1</td><td>▁▃▅▃▅▅▆█▇▆▆▇▆█▇▇██▇█▆▆▇██▇▇▆██▇▇██▇█▇▇▇█</td></tr><tr><td>val_loss_epoch</td><td>█▁▁▂▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁</td></tr><tr><td>val_loss_step</td><td>█▁▁▂▁▂▂▁▂▁▁▂▁▁▁▂▁▂▁▁▁▁▁▁▁▂▁▁▁▂▁▁▁▁▁▁▁▄▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.67733</td></tr><tr><td>train_auc</td><td>0.51577</td></tr><tr><td>train_f1</td><td>0.60647</td></tr><tr><td>train_loss_epoch</td><td>0.62799</td></tr><tr><td>train_loss_step</td><td>0.7201</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.73723</td></tr><tr><td>val_auc</td><td>0.37136</td></tr><tr><td>val_f1</td><td>0.65385</td></tr><tr><td>val_loss_epoch</td><td>0.60191</td></tr><tr><td>val_loss_step</td><td>0.58349</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_64_onehot_1</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/10ixs27l' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/10ixs27l</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231231_094511-10ixs27l\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20231231_100445-k1fjdyet</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/k1fjdyet' target=\"_blank\">MLP_2_64_onehot_1</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/k1fjdyet' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/k1fjdyet</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\torch_geometric\\deprecation.py:22: UserWarning: 'nn.glob.GlobalAttention' is deprecated, use 'nn.aggr.AttentionalAggregation' instead\n",
      "  warnings.warn(out)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_2_64_onehot_1\\attention\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 4.8 K \n",
      "2  | head        | Sequential       | 2.1 K \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 65    \n",
      "--------------------------------------------------\n",
      "7.0 K     Trainable params\n",
      "0         Non-trainable params\n",
      "7.0 K     Total params\n",
      "0.028     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▃▄▅▆▆▆▇▇▇██▇▇█▇█▇▇██▇▇▇▇▇███▇▇███▇████▇</td></tr><tr><td>train_auc</td><td>▁▂▂▄▅▆▆▇▇▇▇▇▇▇█▇▇▇▇▇▇▇▇▇▇▇██▇▇▇▇███▇██▇▇</td></tr><tr><td>train_f1</td><td>▄▁▂▄▅▅▅▇▆▆▇█▇▇▇▇█▇▇▇▇▇▇▆▇████▇▇██▇▇███▇█</td></tr><tr><td>train_loss_epoch</td><td>█▇▇▆▄▄▄▃▃▄▃▂▂▂▃▂▃▂▃▃▃▂▄▂▂▂▃▂▂▂▃▂▁▁▃▃▂▁▃▃</td></tr><tr><td>train_loss_step</td><td>▇▇▆▅▆▄█▄▅▃▄▄▄▄▃▃▁▂▅▄▅▃▂▅▄▃▄▄▂▃▅▅▃▁▆▅▄▃▃▄</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▆▆▄▇▅▆▇▆▅▄▆▇▇▇▇▇██▇▇▇█▆██▅▆████▇██▇▇▆</td></tr><tr><td>val_auc</td><td>▁▂▃▃▅▆▆▆▆▇▇▇▇█████████████████▇█████████</td></tr><tr><td>val_f1</td><td>▁▁▁▆▆▅▆▆▆▇▆▅▅▆▇▆▇████▇▇▇▇▆▇▇▆▆▇▇▇██▇▇██▆</td></tr><tr><td>val_loss_epoch</td><td>▇▇▇▆▆▆▆▆▁▆▂▇█▃▅▂▂▄▃▆▄▃▇▆▆▁▅▄▁▃▄▃▃▄▃▂▁▃▆▄</td></tr><tr><td>val_loss_step</td><td>▆▇▆▅▅▅▆▆▁▆▂▇█▄▅▃▂▅▄▆▄▄█▆▇▁▆▅▁▃▄▃▃▄▃▃▂▃▆▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.69287</td></tr><tr><td>train_auc</td><td>0.74028</td></tr><tr><td>train_f1</td><td>0.61644</td></tr><tr><td>train_loss_epoch</td><td>0.59506</td></tr><tr><td>train_loss_step</td><td>0.58849</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.67883</td></tr><tr><td>val_auc</td><td>0.77723</td></tr><tr><td>val_f1</td><td>0.46988</td></tr><tr><td>val_loss_epoch</td><td>0.56635</td></tr><tr><td>val_loss_step</td><td>0.53449</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_64_onehot_1</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/k1fjdyet' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/k1fjdyet</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231231_100445-k1fjdyet\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20231231_102434-msriy2us</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/msriy2us' target=\"_blank\">MLP_2_64_onehot_1</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/msriy2us' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/msriy2us</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_2_64_onehot_1\\attention2\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 4.8 K \n",
      "2  | head        | Sequential       | 2.1 K \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 4.2 K \n",
      "--------------------------------------------------\n",
      "11.1 K    Trainable params\n",
      "0         Non-trainable params\n",
      "11.1 K    Total params\n",
      "0.045     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▃▄▄▇▅▆▇▆▇▇▇▇█▇▇▇▇▇██▇█▇▇██▇▇▇▇███▇▇▇██▇</td></tr><tr><td>train_auc</td><td>▁▁▃▃▆▇▇▇▇██▇▇█▇█▇▇██▇▇█████▇██▇████▇████</td></tr><tr><td>train_f1</td><td>▅▃▁▄▆▄▆▇▅▆█▆▇▇▇▇▆▆▇█▇▆██▇██▇▇█▇██▇▆▆▇█▇▇</td></tr><tr><td>train_loss_epoch</td><td>█▇▆▆▃▅▃▃▄▂▃▂▃▃▂▃▃▃▂▂▃▂▂▂▁▂▁▂▁▂▂▂▃▁▂▂▁▁▃▁</td></tr><tr><td>train_loss_step</td><td>▆▆▅▅▅▆▆▄▅▅▅▆▅▅█▅▃▂▄▃▃▄▆▄▅▄▂▂▃▃▄▃▃▄▁▂▆▅▃▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▂▄▆▂▇█▆▇▇▆▇▇▇▇▆▆▇▇▇▆▇▇▇▇▇▇▆▇▇▇▇█▇▇▆▇▇</td></tr><tr><td>val_auc</td><td>▁▂▃▄▄▆▇▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇▇█████</td></tr><tr><td>val_f1</td><td>▁▁▁▄▅▆▃██▆▇▇▆▇▇▇▇▆▆▇▇▇▆▇▇▇▇▇▇▇▇▇▇▇█▇▇▆▇▇</td></tr><tr><td>val_loss_epoch</td><td>▆▅▆▅▆▅▇▅▃▄▂▆▅▁▅▂▃▆▄▃▁▃▄▅▁▂▅▄▂▇▇▇▅▂█▂▅▃▃▄</td></tr><tr><td>val_loss_step</td><td>▅▄▅▄▅▅▆▅▂▃▂▆▅▁▅▂▃▆▄▃▁▃▄▅▁▂▅▄▂██▇▅▂█▂▅▃▃▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.6819</td></tr><tr><td>train_auc</td><td>0.74563</td></tr><tr><td>train_f1</td><td>0.50986</td></tr><tr><td>train_loss_epoch</td><td>0.58241</td></tr><tr><td>train_loss_step</td><td>0.5241</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.70438</td></tr><tr><td>val_auc</td><td>0.76764</td></tr><tr><td>val_f1</td><td>0.60488</td></tr><tr><td>val_loss_epoch</td><td>0.6071</td></tr><tr><td>val_loss_step</td><td>0.6194</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_64_onehot_1</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/msriy2us' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/msriy2us</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231231_102434-msriy2us\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20231231_104335-8cf2h6yh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/8cf2h6yh' target=\"_blank\">MLP_3_16_onehot_1</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/8cf2h6yh' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/8cf2h6yh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_3_16_onehot_1\\mean\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 704   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "858       Trainable params\n",
      "0         Non-trainable params\n",
      "858       Total params\n",
      "0.003     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▂▂▂▂▂▂▂▃▃▄▅▇▆▆▆▆▆▆▇▇▆▇▆▇▇▇▇▇▆█▇▆▇▇▇█▇▇</td></tr><tr><td>train_auc</td><td>▁▂▂▂▂▃▂▂▃▃▄▆▇█▇▇▇▇▇▇████▇████████▇██████</td></tr><tr><td>train_f1</td><td>▃▂▁▁▁▁▁▁▁▁▂▄▅▇▇▆▇▇▆▇▇▇▇█▇▇▇▇▇█▆██▆█▇▇█▇█</td></tr><tr><td>train_loss_epoch</td><td>█▇▇▇▇▆▇▇▆▆▆▅▄▃▃▃▄▃▃▂▃▃▂▂▃▃▂▂▁▂▂▂▂▄▁▃▂▁▃▂</td></tr><tr><td>train_loss_step</td><td>▇█▅▇▇█▆▆▇▇▆▅▅▃▅▅▄▅▅▂▄▅▄▄▃▅▄▆▇▂▂▆▅▂▃▅▂▁▆▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▆▅▆▇▆▆▇▇▆▇█▆█▇▇▇▇▇█▆▇█▇▆█▆▇▇▇</td></tr><tr><td>val_auc</td><td>▁▅▆▇▇███████████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▅▅▆▇▇▆▇▇▆▇█▆█▇▇▇▇▇█▆▇█▇▆█▆█▇▇</td></tr><tr><td>val_loss_epoch</td><td>▇▇▆▆▆▅▆▅▇▆▆▆▂█▁▂▆▅▂▄▃▄▄▂▆▆▃▄▄▄▆▃▂▁▃▇▄▃▂▂</td></tr><tr><td>val_loss_step</td><td>▅▆▅▅▅▄▅▄▆▅▅▅▂█▁▂▆▅▂▄▃▄▄▂▆▆▃▄▄▄▆▃▂▁▃▇▄▃▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.67459</td></tr><tr><td>train_auc</td><td>0.72418</td></tr><tr><td>train_f1</td><td>0.53766</td></tr><tr><td>train_loss_epoch</td><td>0.59868</td></tr><tr><td>train_loss_step</td><td>0.56054</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.69708</td></tr><tr><td>val_auc</td><td>0.76637</td></tr><tr><td>val_f1</td><td>0.54645</td></tr><tr><td>val_loss_epoch</td><td>0.5299</td></tr><tr><td>val_loss_step</td><td>0.44677</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_16_onehot_1</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/8cf2h6yh' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/8cf2h6yh</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231231_104335-8cf2h6yh\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20231231_110336-egugr373</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/egugr373' target=\"_blank\">MLP_3_16_onehot_1</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/egugr373' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/egugr373</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_3_16_onehot_1\\max\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 704   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "858       Trainable params\n",
      "0         Non-trainable params\n",
      "858       Total params\n",
      "0.003     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\torch_geometric\\warnings.py:17: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▂▁▄▅▄▅▅▅▅▆▆▇▆▆▆▇▇▆▇▆▆▇▆▆▇█▇█▇▇▆███▇▇▇▇▇▇</td></tr><tr><td>train_auc</td><td>▁▁▃▂▂▄▂▄▃▄▆▆▆▅▆▆▇▆▇▆▆▇▇▆▇▇▇█▇▇▇▇██▆▇█▇██</td></tr><tr><td>train_f1</td><td>▆▅▂▂▁▂▁▃▃▄▅▆▆▄▆▆▆▆▆▆▇▇▆▅▇▇▇█▇▇▆▇▇█▇▆▇▇█▇</td></tr><tr><td>train_loss_epoch</td><td>█▅▄▄▄▃▄▃▃▃▂▃▃▃▃▂▃▂▂▂▃▃▂▂▁▂▁▂▁▂▂▂▁▂▁▂▂▂▂▂</td></tr><tr><td>train_loss_step</td><td>▇█▄▅▆█▅▅▅▆▄▄▅▅▅▁▃▂▄▁▆▃▅▄▆▁▃▂▄▃▂▆▂▁▄▃▂▁▃▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>▃▁▂▁▃▅▇▇▇▄▄▅▄▅▄▄▄▄▄▅▄▄▄▄▄▅▅▅▆▆▄▃▆█▇▆▆██▇</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_epoch</td><td>▆█▅▄▄▃▅▁▇▅▆▆▂▄▁▃▂▅▄▂▄▃▃▅▃▆▄▃▅▅█▄▃▄▄▆▄▄▃▅</td></tr><tr><td>val_loss_step</td><td>▆█▅▄▄▃▅▁▇▅▆▆▂▄▂▃▂▅▅▂▄▃▃▅▃▆▄▃▅▅█▄▃▄▄▆▄▄▃▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.61609</td></tr><tr><td>train_auc</td><td>0.64208</td></tr><tr><td>train_f1</td><td>0.375</td></tr><tr><td>train_loss_epoch</td><td>0.6527</td></tr><tr><td>train_loss_step</td><td>0.62325</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.59124</td></tr><tr><td>val_auc</td><td>0.62227</td></tr><tr><td>val_f1</td><td>0.0</td></tr><tr><td>val_loss_epoch</td><td>0.66691</td></tr><tr><td>val_loss_step</td><td>0.65906</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_16_onehot_1</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/egugr373' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/egugr373</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231231_110336-egugr373\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20231231_112324-yt4m7grl</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/yt4m7grl' target=\"_blank\">MLP_3_16_onehot_1</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/yt4m7grl' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/yt4m7grl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_3_16_onehot_1\\sum\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 704   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "858       Trainable params\n",
      "0         Non-trainable params\n",
      "858       Total params\n",
      "0.003     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▁▂▁▃▄▂▄▄▄▅▄▄▅▅▆▆▅▄▅▆▅▆▇▇▇▆▇▇▇▇█▇▇█████</td></tr><tr><td>train_auc</td><td>▃▅█▄▃▅▄▂▂▄▄▃▃▄▂▁▂▄▄▅▂▃▄▂▃▃▅▃▁▆▅▄▆▄▃▆▅▅▇▇</td></tr><tr><td>train_f1</td><td>▃▂▅▅▃▅▆▅▄▄▅▄▄▃▄▃▃▄▁▄▆▆▁▆▅▇▆▄▇▇▃██▅▇▆▆▆█▇</td></tr><tr><td>train_loss_epoch</td><td>█▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▂▂▂▂▂▂▁▁▁▁▂▁▁▂▂▃▆▂▄▄▄▅▄▄▇▃▄█▄▃▅▃▅▄▄</td></tr><tr><td>val_auc</td><td>███▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▂▁▁▁▁▂▁▁▁▁▁▂▂▃▂▆▃▇█</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▃▇▂▄▄▄▅▄▄▇▃▄█▄▄▆▄▆▅▅</td></tr><tr><td>val_loss_epoch</td><td>▇█▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_step</td><td>▇█▃▁▁▁▂▁▂▁▂▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.65356</td></tr><tr><td>train_auc</td><td>0.53825</td></tr><tr><td>train_f1</td><td>0.54502</td></tr><tr><td>train_loss_epoch</td><td>0.62753</td></tr><tr><td>train_loss_step</td><td>0.60057</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.62774</td></tr><tr><td>val_auc</td><td>0.7144</td></tr><tr><td>val_f1</td><td>0.22727</td></tr><tr><td>val_loss_epoch</td><td>0.5774</td></tr><tr><td>val_loss_step</td><td>0.49907</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_16_onehot_1</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/yt4m7grl' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/yt4m7grl</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231231_112324-yt4m7grl\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20231231_114253-ho2h7x5u</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/ho2h7x5u' target=\"_blank\">MLP_3_16_onehot_1</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/ho2h7x5u' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/ho2h7x5u</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\torch_geometric\\deprecation.py:22: UserWarning: 'nn.glob.GlobalAttention' is deprecated, use 'nn.aggr.AttentionalAggregation' instead\n",
      "  warnings.warn(out)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_3_16_onehot_1\\attention\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 704   \n",
      "2  | head        | Sequential       | 154   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 17    \n",
      "--------------------------------------------------\n",
      "875       Trainable params\n",
      "0         Non-trainable params\n",
      "875       Total params\n",
      "0.003     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▂▁▁▁▁▁▁▁▂▅▄▄▄▄▅▆▆▇▅▆▆▅▆▆▆▆▆▆▇█▆▆▆▇██▇▇▇▆</td></tr><tr><td>train_auc</td><td>▁▂▂▂▂▁▁▃▄▅▆▆▇▇▇▇▇▇█▇█▇▇████▇▇█▇▇███████▇</td></tr><tr><td>train_f1</td><td>▃▁▁▁▁▁▁▁▃▅▆▆▄▄▅▆█▇▅█▇▇▆▅▇▇▇▇▇█▇▆▅▇▇▇▇██▇</td></tr><tr><td>train_loss_epoch</td><td>██▇▇▇▇▇▇▆▅▅▅▄▃▃▃▅▄▃▃▃▄▃▂▂▂▃▃▂▃▂▂▃▃▁▂▁▂▃▃</td></tr><tr><td>train_loss_step</td><td>▆▅▅▆▅▅▅▅▆▄▅▃▄█▁▄▄▂▂▄▃▆▆▃▄▁▅▃▂▂▃▂▂▅▅▁▁▂▂▄</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▂▄▄▄▄▄▇▅▆▇▇█▆▆▇▇▇▇███▇█▇█▇███</td></tr><tr><td>val_auc</td><td>▁▄▅▆▇▇▇▇▇▇▇▇▇▇██████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▂▂▅▄▄▄▄▇▅▆▆▇▇▆▆▆▇▇▇▇██▇▇▇▇▇▇▇█</td></tr><tr><td>val_loss_epoch</td><td>▆▅▆▆▆▆▆▅▆▃▇▅▃▅▄▇▆▄▄▂▄▃▂▁▁▃▃▃▁▂▃▂▄█▄▃▂▄▂▄</td></tr><tr><td>val_loss_step</td><td>▄▄▅▅▄▅▅▄▅▃▆▅▃▄▄▆▅▄▃▂▄▃▂▁▁▃▃▃▁▂▃▂▄█▄▃▂▄▃▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.66545</td></tr><tr><td>train_auc</td><td>0.70367</td></tr><tr><td>train_f1</td><td>0.48596</td></tr><tr><td>train_loss_epoch</td><td>0.61613</td></tr><tr><td>train_loss_step</td><td>0.63457</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.70438</td></tr><tr><td>val_auc</td><td>0.77353</td></tr><tr><td>val_f1</td><td>0.60488</td></tr><tr><td>val_loss_epoch</td><td>0.60642</td></tr><tr><td>val_loss_step</td><td>0.60579</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_16_onehot_1</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/ho2h7x5u' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/ho2h7x5u</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231231_114253-ho2h7x5u\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20231231_120308-fqedcdaa</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/fqedcdaa' target=\"_blank\">MLP_3_16_onehot_1</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/fqedcdaa' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/fqedcdaa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_3_16_onehot_1\\attention2\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 704   \n",
      "2  | head        | Sequential       | 154   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 281   \n",
      "--------------------------------------------------\n",
      "1.1 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.1 K     Total params\n",
      "0.005     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▁▁▁▁▁▁▁▂▃▅▇▇▆█▇▆▆▇▆▆▇▆▇▇▇███▆▇▇▇▇▆█▇█▇</td></tr><tr><td>train_auc</td><td>▂▁▁▄▁▃▁▂▂▃▄▆▇█▇▇▇▇▇▇█▇██▇█▇██▇▇██████▇██</td></tr><tr><td>train_f1</td><td>▂▂▁▁▁▁▁▁▁▂▄▆▇█▇▇▇▇▇▇▆▇▇▆█▇▇█▇█▇█▇▇▇▇█▇██</td></tr><tr><td>train_loss_epoch</td><td>█▇█▇▇▇▇▇▇▇▆▄▄▃▃▂▄▂▅▃▂▅▂▁▂▂▄▂▂▃▃▂▁▂▂▂▂▃▁▂</td></tr><tr><td>train_loss_step</td><td>▇▇█▆▅▅▆▆▇▇▆▅▆▄▂▁▅▄▇▄▅▅▅▃▅▄▂▂▄▂▃▁▂▂▃▂▃▃▆▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▇▇▇▆▆▇▇▆█▆▆▇▆▆▇▅▇▇▆▆▇▇▇▇▇▇▇▇▆</td></tr><tr><td>val_auc</td><td>▁▅▇▇▇▇▇▇▇██▇▇▇█▇████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▂▇▇▇▇▇▇▇▆█▇▆█▆▇█▆▇▇▆▇▇▇▇▇▇▇▇▇▆</td></tr><tr><td>val_loss_epoch</td><td>▇█▇▇▇▇▆▆▇▇▇▆▄▅▃▃▁▃▆▅▅▆▃▅▄▃▄▄▄▄▄▅▄▆▄▃▅▃▂▁</td></tr><tr><td>val_loss_step</td><td>▆█▇▇▇▆▆▆▇▇▇▆▅▆▄▄▁▃█▅▅█▄▆▅▃▅▆▄▆▅▆▅█▄▄▆▄▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.67093</td></tr><tr><td>train_auc</td><td>0.71518</td></tr><tr><td>train_f1</td><td>0.55446</td></tr><tr><td>train_loss_epoch</td><td>0.60472</td></tr><tr><td>train_loss_step</td><td>0.56013</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.67518</td></tr><tr><td>val_auc</td><td>0.77353</td></tr><tr><td>val_f1</td><td>0.49143</td></tr><tr><td>val_loss_epoch</td><td>0.51474</td></tr><tr><td>val_loss_step</td><td>0.42233</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_16_onehot_1</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/fqedcdaa' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/fqedcdaa</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231231_120308-fqedcdaa\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20231231_122304-5twjcmya</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/5twjcmya' target=\"_blank\">MLP_3_32_onehot_1</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/5twjcmya' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/5twjcmya</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_3_32_onehot_1\\mean\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 2.4 K \n",
      "2 | head        | Sequential       | 562   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "3.0 K     Trainable params\n",
      "0         Non-trainable params\n",
      "3.0 K     Total params\n",
      "0.012     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▂▂▂▂▂▂▂▄▆▆▅▇▇▇▇▇▆▆▇▇▇▇▇▇▇▇▇█▇▇▇█▇▇▇█▇▇</td></tr><tr><td>train_auc</td><td>▁▂▁▁▁▂▂▃▃▄▆▇▆▇▇▇█▇▇▇▇█▇▇▇█▇▇▇███▇██▇████</td></tr><tr><td>train_f1</td><td>▃▁▁▁▁▁▁▁▁▄▇▇▅▇▇▇███▆▇▇███▇▇▆██▇█▇█▇███▇▇</td></tr><tr><td>train_loss_epoch</td><td>█▇███▇▇▇█▆▄▅▅▄▄▃▃▄▄▃▂▃▃▃▃▂▃▃▃▂▂▂▂▂▄▃▂▁▂▄</td></tr><tr><td>train_loss_step</td><td>▇▆▆▆▇▇▇▅▆▆▅▃▄▆▃▇▅█▆▄▅▃▅▃▃▅▄▃▃▁▅▂▆▂▆▄▂▄▃█</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▂▆█▇▇▇▇▇▇▇█▆▇▇▇▇▇▇▇▇▇▇▇▇██▇████</td></tr><tr><td>val_auc</td><td>▁▂▂▃▅▅▆▅▆▆▇▇▇▇▇▇████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▂▇█▇▆▇▇▇▇██▇▇▇▇▇▇█▆▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>val_loss_epoch</td><td>▆▆▇▆▆▆▇▆▆▅▄▅▄▇▆▃▆▅▄▄▄▃▅▇▃▁█▂▅█▁▅▄▅▆▄▆▄▂▄</td></tr><tr><td>val_loss_step</td><td>▅▄▆▅▅▅▆▅▅▄▄▅▄▇▆▃▆▅▃▄▄▃▅▇▃▁█▂▅█▁▅▄▅▆▄▆▄▂▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.69104</td></tr><tr><td>train_auc</td><td>0.72557</td></tr><tr><td>train_f1</td><td>0.55643</td></tr><tr><td>train_loss_epoch</td><td>0.62128</td></tr><tr><td>train_loss_step</td><td>0.72198</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.70438</td></tr><tr><td>val_auc</td><td>0.77634</td></tr><tr><td>val_f1</td><td>0.56216</td></tr><tr><td>val_loss_epoch</td><td>0.59509</td></tr><tr><td>val_loss_step</td><td>0.59611</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_32_onehot_1</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/5twjcmya' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/5twjcmya</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231231_122304-5twjcmya\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20231231_124335-m50mu1e2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/m50mu1e2' target=\"_blank\">MLP_3_32_onehot_1</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/m50mu1e2' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/m50mu1e2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_3_32_onehot_1\\max\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 2.4 K \n",
      "2 | head        | Sequential       | 562   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "3.0 K     Trainable params\n",
      "0         Non-trainable params\n",
      "3.0 K     Total params\n",
      "0.012     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\torch_geometric\\warnings.py:17: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▅▅▄▄▄▅▅▅▅▅▅▅▆▆▆▄▇▅▆█▆▆▇▅▆█▇▆▆▆▄▆▇▆▇▅█▇▅</td></tr><tr><td>train_auc</td><td>▂▄▁▁▃▅▄▄▃▅▆▅▆▆▄▅▄▅▄▄▅▄▃▃▄▅▃▅▄▅▆▃▅▃▆▆▇▇█▆</td></tr><tr><td>train_f1</td><td>▅▂▁▁▁▂▁▃▂▅▃▅▃▄▅▅▆▇▇▅▇▄▇▆▆▆▇▇▆▆▇▆▆▇▆▇█▆▇▇</td></tr><tr><td>train_loss_epoch</td><td>█▄▅▅▄▃▃▃▄▃▄▅▂▂▃▃▄▄▃▄▂▃▃▄▂▃▃▂▃▁▁▄▂▂▂▄▂▁▂▂</td></tr><tr><td>train_loss_step</td><td>▅▅▅▃█▆▇▃▇▄▆▃▃▆▂▆▆█▂▆▃▆▅▄▄▄▂▁▃▄▆▇▇▅▄▃▁▅▆▅</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>█████▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>▆▅▇▃▁▅▃▇▇▆▇▆▇▇▇▇▄▆▆▇▇▅▄▅▄▄▅▄▄▄▄▄█▆▇▄▄▃▁▃</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁█▂█████████████████████████████████</td></tr><tr><td>val_loss_epoch</td><td>▁▁▂▂▁▂▂▂▂▃▃▂▂▃▃▄▄▃▂▃▄▅▄▃▄▅▃▄▄▄▅▆▅▃▅▅▇▇█▇</td></tr><tr><td>val_loss_step</td><td>▁▁▂▂▁▂▂▂▂▃▃▂▂▃▂▄▃▂▁▂▃▄▄▂▃▄▂▃▃▃▄▅▄▂▄▃▇▆█▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.59049</td></tr><tr><td>train_auc</td><td>0.55603</td></tr><tr><td>train_f1</td><td>0.3863</td></tr><tr><td>train_loss_epoch</td><td>0.66359</td></tr><tr><td>train_loss_step</td><td>0.68211</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.40876</td></tr><tr><td>val_auc</td><td>0.42378</td></tr><tr><td>val_f1</td><td>0.58031</td></tr><tr><td>val_loss_epoch</td><td>0.78932</td></tr><tr><td>val_loss_step</td><td>0.81463</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_32_onehot_1</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/m50mu1e2' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/m50mu1e2</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231231_124335-m50mu1e2\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20231231_130353-2mi5ten0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/2mi5ten0' target=\"_blank\">MLP_3_32_onehot_1</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/2mi5ten0' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/2mi5ten0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_3_32_onehot_1\\sum\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 2.4 K \n",
      "2 | head        | Sequential       | 562   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "3.0 K     Trainable params\n",
      "0         Non-trainable params\n",
      "3.0 K     Total params\n",
      "0.012     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▂▂▁▂▄▃▄▄▄▄▄▅▆▆▆▆▇▆▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇█▇▇██▇▇</td></tr><tr><td>train_auc</td><td>▆▆▇▂▁▃▇▆▅▅▆▅▄▆▇▆▄▄▆▅▃▄▄▇█▅▅▃▃▄▅▆▂▅▅▄▅▄▂▁</td></tr><tr><td>train_f1</td><td>▄▄▅▄▂▄▁▃▁▃▃▃▅▆▆▅▇▇▇▆▆▇▇█▇▇▇▅▇█▇█▇▇▆▇███▇</td></tr><tr><td>train_loss_epoch</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▄▂▂▃▂▂▂▂▂▂▂▁▂▁▂▂▂▂▂▂▂▂▁▂▂▁▂▂▁▁▁▂▁▂▂▁▂▁▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▅▅▅▅▆▅▆▅▅▇▅▅▆▇▇▇▇▇█▇██▇███▇▇▇▇▇█▇▇▇▇▇▇▇</td></tr><tr><td>val_auc</td><td>███▃▇█████████████████████▇▇▃▅▇▆▇▇▇▇█▇▅▁</td></tr><tr><td>val_f1</td><td>▇▁▁▁▁▃▁▄▁▃▆▃▃▆▇▆▆▆▇█▇██▆▇██▆▆▇▇▇▇▇▇▇▆▆▇▇</td></tr><tr><td>val_loss_epoch</td><td>▅▂▄▃▃▄▄▃▃▃▃▃▃▄▄▂▃▃▂▃▂▂▂▃▂▂▄▁▃▃▁█▂▃▂▂▄▂▂▂</td></tr><tr><td>val_loss_step</td><td>▄▂▃▃▃▃▃▃▃▂▃▃▃▄▃▂▃▃▂▃▂▂▂▃▃▂▄▁▃▃▁█▂▃▂▂▄▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.67916</td></tr><tr><td>train_auc</td><td>0.44743</td></tr><tr><td>train_f1</td><td>0.58164</td></tr><tr><td>train_loss_epoch</td><td>0.62673</td></tr><tr><td>train_loss_step</td><td>0.69906</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.71898</td></tr><tr><td>val_auc</td><td>0.23727</td></tr><tr><td>val_f1</td><td>0.61307</td></tr><tr><td>val_loss_epoch</td><td>0.58676</td></tr><tr><td>val_loss_step</td><td>0.54641</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_32_onehot_1</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/2mi5ten0' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/2mi5ten0</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231231_130353-2mi5ten0\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20231231_132433-bv625poy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/bv625poy' target=\"_blank\">MLP_3_32_onehot_1</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/bv625poy' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/bv625poy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\torch_geometric\\deprecation.py:22: UserWarning: 'nn.glob.GlobalAttention' is deprecated, use 'nn.aggr.AttentionalAggregation' instead\n",
      "  warnings.warn(out)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_3_32_onehot_1\\attention\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 2.4 K \n",
      "2  | head        | Sequential       | 562   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 33    \n",
      "--------------------------------------------------\n",
      "3.0 K     Trainable params\n",
      "0         Non-trainable params\n",
      "3.0 K     Total params\n",
      "0.012     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▁▁▁▁▁▁▂▅▄▆▇▆▆▇▇▅▇▇▇▆▆▆▇▇█▇▇█▇▇▇▇▇▆▇█▆█</td></tr><tr><td>train_auc</td><td>▁▁▂▁▁▂▁▂▃▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇██▇█▇█▇█▇▇█▇██</td></tr><tr><td>train_f1</td><td>▃▁▁▁▁▁▁▁▂▅▇▇▇▇▆▇▇▅▇▇▇▇▇▇▇██▇███▇▇▇▇▆▇▇▆▇</td></tr><tr><td>train_loss_epoch</td><td>██▇█▇█▇▇▆▄▄▂▂▃▂▂▂▂▂▁▃▂▂▃▁▂▃▁▃▃▂▁▂▁▂▃▂▁▂▁</td></tr><tr><td>train_loss_step</td><td>▆▇▇▇▆▆▆█▆▄▅▄▃▂▁▄▄▄▃▂▆▅▄▃▅▅▃▅▄▅▅▆▇▅▃▃▆▄▂▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▆▁▂▄▄▅▅▆▃▅▆▇▆▆▆▆▆▆▆▆▇█▇█▇█▇▆█▇▇</td></tr><tr><td>val_auc</td><td>▅▁▅▇▇▇▇▇████████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▂▆▂▃▅▅▅▅▆▄▅▆▆▆▆▆▆▆▆▆▆▆█▇█▇█▇▆██▇</td></tr><tr><td>val_loss_epoch</td><td>▅▃▄▅▅▅▄▄▄▃█▆▄▆▃▄▂▆▃▁▅▂▅▅▂▂▄▄▅▂▁▅▃▂▁▂▃▃▃▂</td></tr><tr><td>val_loss_step</td><td>▆▃▄▅▅▅▅▄▅▃█▇▅▇▄▄▃▇▄▁▆▂▆▆▃▂▅▅▆▃▂▆▄▂▂▂▄▄▄▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.69378</td></tr><tr><td>train_auc</td><td>0.74217</td></tr><tr><td>train_f1</td><td>0.58177</td></tr><tr><td>train_loss_epoch</td><td>0.59752</td></tr><tr><td>train_loss_step</td><td>0.57636</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.67518</td></tr><tr><td>val_auc</td><td>0.76747</td></tr><tr><td>val_f1</td><td>0.45399</td></tr><tr><td>val_loss_epoch</td><td>0.58039</td></tr><tr><td>val_loss_step</td><td>0.55774</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_32_onehot_1</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/bv625poy' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/bv625poy</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231231_132433-bv625poy\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20231231_134456-s2490tv5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/s2490tv5' target=\"_blank\">MLP_3_32_onehot_1</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/s2490tv5' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/s2490tv5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_3_32_onehot_1\\attention2\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 2.4 K \n",
      "2  | head        | Sequential       | 562   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 1.1 K \n",
      "--------------------------------------------------\n",
      "4.1 K     Trainable params\n",
      "0         Non-trainable params\n",
      "4.1 K     Total params\n",
      "0.016     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▂▂▂▂▂▂▂▄▅▅▆▇▇▇▇█▇██▆▇▇███▇██▇█▇▇▇█▆█▇█</td></tr><tr><td>train_auc</td><td>▁▂▁▁▁▂▂▃▄▆▆▇▇▇█▇▇▇▇██▇█▇█████████▇████▇█</td></tr><tr><td>train_f1</td><td>▄▁▁▁▁▁▁▁▂▄▅▅▇▇▇▇▇▇▇▇▇▆████▇▇█▇▇█▇▇▆▇▆█▇█</td></tr><tr><td>train_loss_epoch</td><td>██▇███▇█▇▆▅▄▃▄▂▃▃▃▂▂▃▂▂▂▁▂▂▁▂▂▂▁▁▂▂▂▃▂▂▂</td></tr><tr><td>train_loss_step</td><td>▆▅▅▆▆▅▅▆▅▄▂▄▃▄▃▃▄▁▂▂▃▅▃▃▁█▅▂▂▂▂▃▄▁▅▃▂▇▂▄</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▆█▆▆▆▇▇▇▇▇▇█▆▆▇▇▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>val_auc</td><td>▄▁▄▇▆▆▇▇▇███████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▇█▆▆▇▇▇▇▇▇▇█▆▆▇▇▇▆▆▆▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>val_loss_epoch</td><td>▇▆▇▇▅▆▇▆▆▆▅▃▄▄▆▃▃▃█▂▅▁▂▃▇▁▇▃▁▃▁▃▅▂▃▃▂▁▆▃</td></tr><tr><td>val_loss_step</td><td>▆▅▅▆▄▅▅▅▅▅▅▂▄▃▅▃▂▃█▂▅▁▂▂▇▁▇▄▁▃▁▃▅▂▃▄▂▁▇▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.68739</td></tr><tr><td>train_auc</td><td>0.73173</td></tr><tr><td>train_f1</td><td>0.59382</td></tr><tr><td>train_loss_epoch</td><td>0.60371</td></tr><tr><td>train_loss_step</td><td>0.62986</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.70803</td></tr><tr><td>val_auc</td><td>0.77838</td></tr><tr><td>val_f1</td><td>0.56989</td></tr><tr><td>val_loss_epoch</td><td>0.56808</td></tr><tr><td>val_loss_step</td><td>0.54142</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_32_onehot_1</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/s2490tv5' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/s2490tv5</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231231_134456-s2490tv5\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20231231_140519-7qpfx784</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/7qpfx784' target=\"_blank\">MLP_3_64_onehot_1</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/7qpfx784' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/7qpfx784</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_3_64_onehot_1\\mean\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 9.0 K \n",
      "2 | head        | Sequential       | 2.1 K \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "11.1 K    Trainable params\n",
      "0         Non-trainable params\n",
      "11.1 K    Total params\n",
      "0.044     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▁▁▂▄▅▆▆▇▆▆▆▇▇▇▇▇▆▇▇▇▇▇▇▆▇▇▇▇▇▇▇▇▇▇▆▇█▇</td></tr><tr><td>train_auc</td><td>▁▁▁▃▅▆▇▇▇▇▇▇▇███▇█▇▇███████▇▇▇█▇█▇█▇███▇</td></tr><tr><td>train_f1</td><td>▃▁▁▁▂▇▅█▇▇▇▆▆▇█▇▇▇▆▇▇▇▇▇▇▆▇▇▇█▇▇▇▇▇▇▆▇█▇</td></tr><tr><td>train_loss_epoch</td><td>███▇▆▇▃▄▂▂▄▃▂▁▂▂▃▃▄▃▃▁▂▂▂▂▂▃▃▃▃▃▂▁▂▂▃▂▂▂</td></tr><tr><td>train_loss_step</td><td>▇█▇▆▅▆▄▃▅█▄▃▄▃▅▅▁▇▄▁▇▄▅▄▂▄██▅▄▄▅▃▅▄▅▄▇▆▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▄▇▄▇▆▆▇▇▇▇▇▇▇▇▇▆▇▇▇███▇▇█▇██▇██▇█▇▇▆</td></tr><tr><td>val_auc</td><td>▁▃▃▄▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇█████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▄▇▄███████▇████▆██▇███▇▇█████████▇█▆</td></tr><tr><td>val_loss_epoch</td><td>▅▅▅▆▅▂▅▅█▅▂▃▃▄▆▄▆▄▂▄▅▃▃▃▁▁▂▄▂▂▃▄▃▂▅▁▆▄▃▃</td></tr><tr><td>val_loss_step</td><td>▄▄▄▅▄▂▅▅█▅▂▃▃▄▆▄▆▄▂▄▅▃▃▃▁▁▂▄▂▂▃▄▃▂▅▁▆▄▃▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.69196</td></tr><tr><td>train_auc</td><td>0.72331</td></tr><tr><td>train_f1</td><td>0.52602</td></tr><tr><td>train_loss_epoch</td><td>0.59728</td></tr><tr><td>train_loss_step</td><td>0.57515</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.67153</td></tr><tr><td>val_auc</td><td>0.77646</td></tr><tr><td>val_f1</td><td>0.42308</td></tr><tr><td>val_loss_epoch</td><td>0.5775</td></tr><tr><td>val_loss_step</td><td>0.54628</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_64_onehot_1</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/7qpfx784' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/7qpfx784</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231231_140519-7qpfx784\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20231231_142805-tlxu5zoz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/tlxu5zoz' target=\"_blank\">MLP_3_64_onehot_1</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/tlxu5zoz' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/tlxu5zoz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_3_64_onehot_1\\max\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 9.0 K \n",
      "2 | head        | Sequential       | 2.1 K \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "11.1 K    Trainable params\n",
      "0         Non-trainable params\n",
      "11.1 K    Total params\n",
      "0.044     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\torch_geometric\\warnings.py:17: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▃▃▃▃▃▄▅▅▃▄▅▇▆▅▇▆▇▇▆▆▇▆██▆█▇▇▇▇▇▇▇▆▅▆▇▆▇</td></tr><tr><td>train_auc</td><td>▁▁▁▃▃▃▅▄▄▃▄▅▇▇▆▇▆▆▇▇▇█▇██▇▇▇███▇█▇▇▇▇▇▇▇</td></tr><tr><td>train_f1</td><td>▃▁▂▁▂▄▃▆▆▇▅▅▇▆▇█▇▇▇▆▇█▇█▇▇▇▇▇▇███▇▇▆▆███</td></tr><tr><td>train_loss_epoch</td><td>█▇▇▆▆▇▅▆▆▇▆▅▃▄▃▃▄▃▃▃▃▁▃▂▃▄▂▂▁▂▃▃▂▂▃▄▃▂▃▃</td></tr><tr><td>train_loss_step</td><td>▆▆▄▅▇▅▅▅▆▇▃▄▃▄▃▆▁▂▂▃▅▂▄▃▁▅▅█▅▅▄▄▂▇▃▄▄▅▇▅</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>▁▁▂▄▄▃▃▃▃▆▆▇▇██████▇████████████████▇▇▇▇</td></tr><tr><td>val_f1</td><td>█▁██████████████████████████████████████</td></tr><tr><td>val_loss_epoch</td><td>▁▁▁▁▁▃▂▂▃▃▄▄▄▄▄▅▃▅▅▄▃▅▆▄▇▄▇▅▇█▆▄▄█▇▆▆▆▆▇</td></tr><tr><td>val_loss_step</td><td>▂▂▂▂▂▄▃▂▄▂▄▅▅▆▄▅▃▅▅▅▃▅▆▄█▄▇▅▇█▄▁▂█▇▆▆▅▅▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.65996</td></tr><tr><td>train_auc</td><td>0.69554</td></tr><tr><td>train_f1</td><td>0.54074</td></tr><tr><td>train_loss_epoch</td><td>0.63296</td></tr><tr><td>train_loss_step</td><td>0.66188</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.40876</td></tr><tr><td>val_auc</td><td>0.6309</td></tr><tr><td>val_f1</td><td>0.58031</td></tr><tr><td>val_loss_epoch</td><td>0.92134</td></tr><tr><td>val_loss_step</td><td>0.93845</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_64_onehot_1</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/tlxu5zoz' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/tlxu5zoz</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231231_142805-tlxu5zoz\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20231231_144738-pbs73wpg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/pbs73wpg' target=\"_blank\">MLP_3_64_onehot_1</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/pbs73wpg' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/pbs73wpg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_3_64_onehot_1\\sum\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 9.0 K \n",
      "2 | head        | Sequential       | 2.1 K \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "11.1 K    Trainable params\n",
      "0         Non-trainable params\n",
      "11.1 K    Total params\n",
      "0.044     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▂▂▄▄▅▅▅▆▆▆▇▆▇▆▇▇█▇█████▇█▇▇██▇▇██▇▇█▇▇</td></tr><tr><td>train_auc</td><td>▃▃▁▃▃▅▄▃▄▄▄▅▅▅▆▅▅▆▇▆▅▇▆██▆▆▆▅█▆▆▆▆▇▅▆▇▇▆</td></tr><tr><td>train_f1</td><td>▄▃▂▂▁▄▃▃▂▅▇▇▇▆▇▇▇▆▆▆▇▆▆▆▇▅▆▆▆▇█▇██▇▅▅▇▇▅</td></tr><tr><td>train_loss_epoch</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▂▂▁▁▂▁▁▂▂▁▁▁▁▂▁▁▂▁▁▁▁▁▁▁▁▂▂▁▁▂▁▁▂▁▁▁▂▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▂▂▃▃▃▃▄▄▄▄▄▄▅▄▄▆▅▅▆▅▄▄▃▅██▇▆▄▆</td></tr><tr><td>val_auc</td><td>▁▁▂▂▂▂▃▃▄▅▅▅▆▆▆▇▇▇▆▆▇▇▇▇▆▇▇▆▇▇▇▇▇▇████▇▇</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▆▆▅▆▅▄▄▃▅██▇▆▄▆</td></tr><tr><td>val_loss_epoch</td><td>█▁▁▂▂▁▁▂▂▂▂▁▁▁▁▁▂▁▁▁▁▁▁▂▁▁▁▁▁▁▁▂▂▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_step</td><td>█▁▁▂▂▁▂▂▂▂▁▁▂▁▂▁▂▁▁▁▂▁▁▂▁▂▁▁▂▁▂▂▂▁▁▁▂▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.67276</td></tr><tr><td>train_auc</td><td>0.61715</td></tr><tr><td>train_f1</td><td>0.48857</td></tr><tr><td>train_loss_epoch</td><td>0.61846</td></tr><tr><td>train_loss_step</td><td>0.5751</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.67153</td></tr><tr><td>val_auc</td><td>0.75529</td></tr><tr><td>val_f1</td><td>0.4</td></tr><tr><td>val_loss_epoch</td><td>0.5814</td></tr><tr><td>val_loss_step</td><td>0.52913</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_64_onehot_1</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/pbs73wpg' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/pbs73wpg</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231231_144738-pbs73wpg\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20231231_150800-7jksvpag</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/7jksvpag' target=\"_blank\">MLP_3_64_onehot_1</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/7jksvpag' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/7jksvpag</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\torch_geometric\\deprecation.py:22: UserWarning: 'nn.glob.GlobalAttention' is deprecated, use 'nn.aggr.AttentionalAggregation' instead\n",
      "  warnings.warn(out)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_3_64_onehot_1\\attention\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 9.0 K \n",
      "2  | head        | Sequential       | 2.1 K \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 65    \n",
      "--------------------------------------------------\n",
      "11.2 K    Trainable params\n",
      "0         Non-trainable params\n",
      "11.2 K    Total params\n",
      "0.045     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▂▂▅▆▆▇▇█▆▇▆█▆▇██▇▆▇▇▇▇█▇▇▇█▇▇▇▇▇▆▅▆▇▆█</td></tr><tr><td>train_auc</td><td>▁▁▁▂▆▇▆▇▇▇▇▇▇▇▇▇███▇████▇█████████▇█▇███</td></tr><tr><td>train_f1</td><td>▃▁▁▁▄▆▆▇▇▇▆▆▆▇▆▇██▇█▇▇▇▇▇▇▆▇██▇▇▇█▆▄▆█▆▇</td></tr><tr><td>train_loss_epoch</td><td>███▇▅▅▅▄▄▂▃▂▄▂▃▃▂▄▂▄▃▅▂▂▃▂▃▂▃▄▁▄▂▂▄▅▄▃▃▁</td></tr><tr><td>train_loss_step</td><td>▆▇█▆▄▄▆▄▅▄▆▆▇▃▄▆▄▅▅▅▅▅▅▄▄▅▅█▂▂▁▃▄▄▃▆▆▃▂▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▃▆▅▅▅▅▆▇▆▆▇▅▅▄▄▅▆▆▆▆▆▆▇▇▅▆▆▆▆▆▇▆▂█▇▆</td></tr><tr><td>val_auc</td><td>▁▂▃▄▄▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█████████▇▇█</td></tr><tr><td>val_f1</td><td>▁▁▁▁▃▆▅▆▆▅▆█▇▇▇▆▆▅▄▆▇▆▆▆▆▆▇▇▆▆▆▆▆▆▇▆▃█▇▇</td></tr><tr><td>val_loss_epoch</td><td>▅▆▅▆▅▄▄▂▅▃▄▃▃▂█▄▄▃▄▆▃▄▂▅▇▃▅▄▇▄▄▅▃▂▃▃▄▃▃▁</td></tr><tr><td>val_loss_step</td><td>▃▅▄▅▄▃▄▂▅▂▄▃▃▂█▄▃▂▃▆▃▄▂▅▇▃▅▄▇▄▄▅▃▂▃▃▃▂▃▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.69196</td></tr><tr><td>train_auc</td><td>0.73704</td></tr><tr><td>train_f1</td><td>0.55482</td></tr><tr><td>train_loss_epoch</td><td>0.58225</td></tr><tr><td>train_loss_step</td><td>0.52858</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.68613</td></tr><tr><td>val_auc</td><td>0.77353</td></tr><tr><td>val_f1</td><td>0.51136</td></tr><tr><td>val_loss_epoch</td><td>0.52322</td></tr><tr><td>val_loss_step</td><td>0.42831</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_64_onehot_1</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/7jksvpag' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/7jksvpag</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231231_150800-7jksvpag\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20231231_152755-btdcvovu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/btdcvovu' target=\"_blank\">MLP_3_64_onehot_1</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/btdcvovu' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/btdcvovu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_3_64_onehot_1\\attention2\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 9.0 K \n",
      "2  | head        | Sequential       | 2.1 K \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 4.2 K \n",
      "--------------------------------------------------\n",
      "15.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "15.3 K    Total params\n",
      "0.061     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▂▂▃▆▆▇▇▆▇▇▆▆▆▆▇▇▇▇▇▆▇█▇▇▇██▇█▇▇█▆▇▆▇▇▇</td></tr><tr><td>train_auc</td><td>▂▁▁▂▅▆▇▇▇▇▇▇▇▇█▇████▇▇▇▇█████▇████▇█████</td></tr><tr><td>train_f1</td><td>▃▂▁▁▂▇▇▆▇▇█▇▆▆▆▆▇▇▇▇▇▆▆▇▇▇▇▇▇▇▇▇██▇▇▆▆▇▇</td></tr><tr><td>train_loss_epoch</td><td>██▇▇▆▅▃▂▃▁▃▂▂▂▃▂▂▂▂▂▂▃▂▂▂▁▂▂▁▂▂▁▂▂▂▂▂▂▃▂</td></tr><tr><td>train_loss_step</td><td>▇█▆▇█▇▆▄▄▅▂▃▃▆▇▃▂▄▅▄▂▄▄▅█▆▆▃▂▂▄▄▄▁▁▃▃▇▆▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▅▆▅▆█▆▆▇▇█▇▆▆▇▆▆▇▇▇▇▇▇▆▇▇▇▇▇▇▇▇▇█▆▇▆</td></tr><tr><td>val_auc</td><td>▁▃▃▄▆▆▆▇▇▇▇▇▇▇▇▇▇▇██████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▅▆▅▆█▇▆▇▇█▇▆▆█▇▇█▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇█▇▇▇</td></tr><tr><td>val_loss_epoch</td><td>▇▇▇▆▇▃▅▅▄██▅▃▄▄▃█▄▆▇▄▄▅▂▄▅▄▅▅▂▄▃▅▆▁▅▄▄▄▅</td></tr><tr><td>val_loss_step</td><td>▆▅▆▅▆▃▄▄▄██▅▂▄▄▃█▄▆▇▄▄▅▂▄▅▄▅▅▂▄▃▅▆▁▅▄▄▄▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.69013</td></tr><tr><td>train_auc</td><td>0.74322</td></tr><tr><td>train_f1</td><td>0.56145</td></tr><tr><td>train_loss_epoch</td><td>0.58941</td></tr><tr><td>train_loss_step</td><td>0.56805</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.68613</td></tr><tr><td>val_auc</td><td>0.77767</td></tr><tr><td>val_f1</td><td>0.57</td></tr><tr><td>val_loss_epoch</td><td>0.62315</td></tr><tr><td>val_loss_step</td><td>0.65077</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_64_onehot_1</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/btdcvovu' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/btdcvovu</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231231_152755-btdcvovu\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20231231_154718-hm2wv2yv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/hm2wv2yv' target=\"_blank\">MLP_4_16_onehot_1</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/hm2wv2yv' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/hm2wv2yv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_4_16_onehot_1\\mean\\GraphLevelMLP exists and is not empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 976   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "1.1 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.1 K     Total params\n",
      "0.005     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_auc</td><td>▆▆▃▄▆▇▇▅▂█▄▃▆▄▆▄▇▃▃▅▆▂▇▇▄▃▄▇▂▇▁█▃▂▅▄█▅▆▆</td></tr><tr><td>train_f1</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_epoch</td><td>▄▅▅▆█▄▄▆▇▁▄▄▇▄▅▇▆▂▅▅▆▅▆▆▁▆▄▁█▄▆▃▂▅▅▄▄▁▃▇</td></tr><tr><td>train_loss_step</td><td>▃▄▄▄▄▄▅▅▅▆▅▇▄▅█▄▃▄▄▂▅▄▄▄▄▅▄▁▄▃▆▅▅▄▅▆▄▅▅▇</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>▅▆▇▇▇████████████▇▇▇▇▇▇▇▆▆▅▅▅▄▅▅▁▁▁▁▁▁▁▁</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_epoch</td><td>▅▅▃▅▃▄▅▃▇▄▄▅▅█▅▄▅▄▆▅▄▂▇▆▅▅▅▅▃▄▃▁▅▅▆▄▆▄▄█</td></tr><tr><td>val_loss_step</td><td>▅▅▃▅▃▄▅▃▇▄▄▅▅█▅▄▅▄▆▅▄▂▇▆▅▅▅▅▃▄▃▁▅▅▆▄▆▄▄█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.58135</td></tr><tr><td>train_auc</td><td>0.50381</td></tr><tr><td>train_f1</td><td>0.0</td></tr><tr><td>train_loss_epoch</td><td>0.68616</td></tr><tr><td>train_loss_step</td><td>0.72019</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.59124</td></tr><tr><td>val_auc</td><td>0.5</td></tr><tr><td>val_f1</td><td>0.0</td></tr><tr><td>val_loss_epoch</td><td>0.71056</td></tr><tr><td>val_loss_step</td><td>0.74907</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_16_onehot_1</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/hm2wv2yv' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/hm2wv2yv</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231231_154718-hm2wv2yv\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20231231_160731-q84gtklf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/q84gtklf' target=\"_blank\">MLP_4_16_onehot_1</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/q84gtklf' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/q84gtklf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_4_16_onehot_1\\max\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 976   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "1.1 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.1 K     Total params\n",
      "0.005     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\torch_geometric\\warnings.py:17: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▄▄▅▅▅▅▅▅▅▅▅▆▆▆█▆▅▆▆▆▇▇▇▆▇▆▇▇▆▇▆▆██▇▇█▇█</td></tr><tr><td>train_auc</td><td>▂▂▁▂▂▂▃▂▁▃▃▃▄▄▅▅▆▅▄▄▆▆▆▆▆▆▆▆▅▆▆▇▇▆█▇███▇</td></tr><tr><td>train_f1</td><td>█▁▁▁▁▁▁▁▁▁▁▂▄▅▄▆▆▃▅▇▅▆▇▆▆▆▇▆▆▇▆▅▆▇█▇▆█▇▇</td></tr><tr><td>train_loss_epoch</td><td>█▅▅▅▅▄▄▅▅▄▄▄▄▄▃▃▄▃▄▅▃▄▄▄▃▂▃▂▄▃▃▃▂▂▁▃▂▁▂▃</td></tr><tr><td>train_loss_step</td><td>▃▄▄▅▄▄▅▅▄▆▅▇▃▃▇█▂▄▂▃▃▄▁▅▃▃▃▁▃▂▄▄▅▃▆▅▁▄▅▅</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>▅▂▁▇▇▇▆▆▆▇▆▇▆▆▆▇█▇▇▇█▇█▇▇▇██▇▇▇▇▇█▇██▇▇█</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_epoch</td><td>▆▆▄▅▄▄▆▄▇▅▅▅▆█▅▄▅▄▆▄▄▂▇▆▅▅▅▆▃▄▃▁▅▅▆▃▆▄▄▇</td></tr><tr><td>val_loss_step</td><td>▆▅▄▅▄▄▅▄▇▄▄▅▅█▅▄▅▄▆▄▄▂▇▆▅▅▅▆▃▄▃▁▅▅▇▃▆▄▄█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.61883</td></tr><tr><td>train_auc</td><td>0.62508</td></tr><tr><td>train_f1</td><td>0.33493</td></tr><tr><td>train_loss_epoch</td><td>0.66415</td></tr><tr><td>train_loss_step</td><td>0.6878</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.59124</td></tr><tr><td>val_auc</td><td>0.62916</td></tr><tr><td>val_f1</td><td>0.0</td></tr><tr><td>val_loss_epoch</td><td>0.70971</td></tr><tr><td>val_loss_step</td><td>0.75533</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_16_onehot_1</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/q84gtklf' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/q84gtklf</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231231_160731-q84gtklf\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20231231_162719-23r2pw9f</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/23r2pw9f' target=\"_blank\">MLP_4_16_onehot_1</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/23r2pw9f' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/23r2pw9f</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_4_16_onehot_1\\sum\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 976   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "1.1 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.1 K     Total params\n",
      "0.005     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▂▂▁▂▃▃▄▃▃▃▃▄▅▅▄▅▅▆▅▅▆▆▅▆▆▆▇▇▆▆▆▆▇▇▆▇███▇</td></tr><tr><td>train_auc</td><td>▆▅▅▅▆▇█▅▆▃▆▆▄▆▃▄▄▅▄▁▂▄▂▃▂▂▁▄▅▃▁▂▄▃▃▃▄▆▅▃</td></tr><tr><td>train_f1</td><td>▃▂▄▄▁▃▄▄▃▃▃▃▃▄▄▅▃▅▄▄▄▅▅▄▆▇▆▅▄▄▇▆▆▅▅▇▆▆██</td></tr><tr><td>train_loss_epoch</td><td>█▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▂▁▁▁▁▁▁▁▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▂▅▄▄▄▃▄▄▅▄▄▄▄▄▄█▄▄▇▇▅▅▅</td></tr><tr><td>val_auc</td><td>███████████▃▃▃▄▃▃▂▂▁▁▂▁▁▂▁▁▁▃▁▁▂▁▁▁▂▂▁▂▁</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▃▁▂▅▄▄▄▃▄▄▅▄▄▄▃▄▄█▄▄▇▇▅▅▅</td></tr><tr><td>val_loss_epoch</td><td>█▂▁▂▁▁▂▁▂▁▁▂▂▂▂▁▁▁▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁</td></tr><tr><td>val_loss_step</td><td>█▂▁▂▂▁▂▂▂▂▂▂▂▃▂▁▁▂▂▁▁▁▂▂▂▁▂▂▁▂▂▁▂▂▂▁▁▁▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.63803</td></tr><tr><td>train_auc</td><td>0.48038</td></tr><tr><td>train_f1</td><td>0.59091</td></tr><tr><td>train_loss_epoch</td><td>0.64816</td></tr><tr><td>train_loss_step</td><td>0.73657</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.63504</td></tr><tr><td>val_auc</td><td>0.22228</td></tr><tr><td>val_f1</td><td>0.26471</td></tr><tr><td>val_loss_epoch</td><td>0.6367</td></tr><tr><td>val_loss_step</td><td>0.64484</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_16_onehot_1</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/23r2pw9f' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/23r2pw9f</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231231_162719-23r2pw9f\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20231231_164817-qsogt2ft</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/qsogt2ft' target=\"_blank\">MLP_4_16_onehot_1</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/qsogt2ft' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/qsogt2ft</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\torch_geometric\\deprecation.py:22: UserWarning: 'nn.glob.GlobalAttention' is deprecated, use 'nn.aggr.AttentionalAggregation' instead\n",
      "  warnings.warn(out)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_4_16_onehot_1\\attention\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 976   \n",
      "2  | head        | Sequential       | 154   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 17    \n",
      "--------------------------------------------------\n",
      "1.1 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.1 K     Total params\n",
      "0.005     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▆█████████████████████████████████████</td></tr><tr><td>train_auc</td><td>▆▅▆▇▂▆▇▇▄▇▄▆▆▄▃▅▄▂▅▅▇▅▃▄▆▇▆▁▄▇▃█▆▅▅▇▇▄▂▆</td></tr><tr><td>train_f1</td><td>██▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_epoch</td><td>▅▄▄▂▅▃▃▃▂▃▅▃▃▂▃▃█▃▄▃▄▃▄▁▃▃▂▅▃▄▂▂▅▃▄▅▄▄▅▅</td></tr><tr><td>train_loss_step</td><td>▃▃▅▆▅▂▂▃▄▄▃▆▅▂▁▂▄▃▂▅▃▃█▃▇▃▁▂▅▃▆▄▅▄▅▅▄▃▄▅</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>▅▇▇▇▇▇█████▇▇▇▇▇▇▇▇▇▇▇▇▇▆▆▆▆▅▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_epoch</td><td>█▆▄▆▆▁▂▃▅▂▅▆▄▅▅▇▆▅▆▄▆▅▅▅▅▅▇▄▆▅▇▃▅▅▅▇▅█▅▆</td></tr><tr><td>val_loss_step</td><td>█▆▄▆▆▁▂▃▅▂▅▆▄▅▅▇▆▅▆▄▆▅▅▅▅▅▇▄▆▅▇▃▅▅▅▇▅█▅▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.58135</td></tr><tr><td>train_auc</td><td>0.50177</td></tr><tr><td>train_f1</td><td>0.0</td></tr><tr><td>train_loss_epoch</td><td>0.6826</td></tr><tr><td>train_loss_step</td><td>0.69931</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.59124</td></tr><tr><td>val_auc</td><td>0.5</td></tr><tr><td>val_f1</td><td>0.0</td></tr><tr><td>val_loss_epoch</td><td>0.67233</td></tr><tr><td>val_loss_step</td><td>0.66701</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_16_onehot_1</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/qsogt2ft' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/qsogt2ft</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231231_164817-qsogt2ft\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20231231_170658-cj4p709b</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/cj4p709b' target=\"_blank\">MLP_4_16_onehot_1</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/cj4p709b' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/cj4p709b</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_4_16_onehot_1\\attention2\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 976   \n",
      "2  | head        | Sequential       | 154   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 281   \n",
      "--------------------------------------------------\n",
      "1.4 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.4 K     Total params\n",
      "0.006     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁█▆▄▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆</td></tr><tr><td>train_auc</td><td>▆▃▅▃▅▅▃▄▄▄▄▅▆▅▇▁▃▆▂▃▄▆▆▂▂▃▄▆▄▆▃█▄▅▆▅▄▃▃▃</td></tr><tr><td>train_f1</td><td>█▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_epoch</td><td>▂▄█▆▄▄▃▅▆▄▄▆▄▄▂▄▂▂▃▅▃▃▂▃▄▂▅▃▁▅▃▃▄▄▁▁▁▄▆▄</td></tr><tr><td>train_loss_step</td><td>▁▅▆▅▅▆▄█▅▅▅▆▅▇▄▃▇█▅▂█▇▂▆▅▃▃▅▁▄▂▄▇▅▄▃▅▇▆▅</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>▁▇▇▆▇█████▇▇██▇▇▇▇▇▇▇▇▇▇▇▅▇▆▁▃▂▂▂▂▁▂▁▂▂▂</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_epoch</td><td>▂▇▅▃▆▅▆▅▅▃▃▂▅▄▅▅█▅▃▆▄▄▅▅▂▅▃▃▄▇█▅▃▅▃▄▅▁▅▅</td></tr><tr><td>val_loss_step</td><td>▂▇▄▃▆▅▆▅▄▃▃▂▄▄▄▅█▅▃▆▄▄▅▅▂▅▃▃▄▇█▅▃▅▃▄▅▁▅▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.58135</td></tr><tr><td>train_auc</td><td>0.47969</td></tr><tr><td>train_f1</td><td>0.0</td></tr><tr><td>train_loss_epoch</td><td>0.68039</td></tr><tr><td>train_loss_step</td><td>0.68134</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.59124</td></tr><tr><td>val_auc</td><td>0.5</td></tr><tr><td>val_f1</td><td>0.0</td></tr><tr><td>val_loss_epoch</td><td>0.68102</td></tr><tr><td>val_loss_step</td><td>0.68562</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_16_onehot_1</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/cj4p709b' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/cj4p709b</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231231_170658-cj4p709b\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20231231_172550-xhag5ll6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/xhag5ll6' target=\"_blank\">MLP_4_32_onehot_1</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/xhag5ll6' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/xhag5ll6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_4_32_onehot_1\\mean\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 3.5 K \n",
      "2 | head        | Sequential       | 562   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "4.0 K     Trainable params\n",
      "0         Non-trainable params\n",
      "4.0 K     Total params\n",
      "0.016     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▄▄▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▆▇▇▇█▇▇█▇█▇█▇█████</td></tr><tr><td>train_auc</td><td>▂▂▁▂▂▂▂▁▁▁▂▂▂▁▂▂▂▁▂▂▃▄▆▇▇▇█▇██████▇█████</td></tr><tr><td>train_f1</td><td>▇▂▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▅▆▇▇▇▆▆▇▇█▇▇▇▇▇▇█▇</td></tr><tr><td>train_loss_epoch</td><td>█▇▇▆▆▆▇▇▆▆▆▆▆▆▆▇▆▆▆▆▆▅▅▄▃▂▂▂▁▂▁▂▃▂▁▁▂▃▂▂</td></tr><tr><td>train_loss_step</td><td>▇▆▆▅▆▅▅▆▅▅▆▅▄▄▆▅▆▆▇▅▅▅█▃▄▂▃▄▄▁▄▂▄▅▄▂▃▃▂▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▇▆▇▇▇█▇▇▇▅█▇▇▇█▇▇▇</td></tr><tr><td>val_auc</td><td>▇▁▁▇▇▇▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇███████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▇▇▇▆▇█▇▇▇▆█▇█▇█▇▇▇</td></tr><tr><td>val_loss_epoch</td><td>███▇▇▇▇▇▇██▇▇▇█▇▇▇▇█▇█▆▅▄▄▆▅▅▁▅▅▄▅▅▃▆▄▅▄</td></tr><tr><td>val_loss_step</td><td>███▇▇▇▇▇▇██▇▇▆█▇▇▇▇█▇█▇▆▄▅▇▅▆▁▅▆▅▆▅▄▇▅▆▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.68007</td></tr><tr><td>train_auc</td><td>0.71591</td></tr><tr><td>train_f1</td><td>0.56359</td></tr><tr><td>train_loss_epoch</td><td>0.61186</td></tr><tr><td>train_loss_step</td><td>0.59839</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.68613</td></tr><tr><td>val_auc</td><td>0.77497</td></tr><tr><td>val_f1</td><td>0.5</td></tr><tr><td>val_loss_epoch</td><td>0.56569</td></tr><tr><td>val_loss_step</td><td>0.52609</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_32_onehot_1</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/xhag5ll6' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/xhag5ll6</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231231_172550-xhag5ll6\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20231231_174437-9y9h8113</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/9y9h8113' target=\"_blank\">MLP_4_32_onehot_1</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/9y9h8113' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/9y9h8113</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_4_32_onehot_1\\max\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 3.5 K \n",
      "2 | head        | Sequential       | 562   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "4.0 K     Trainable params\n",
      "0         Non-trainable params\n",
      "4.0 K     Total params\n",
      "0.016     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\torch_geometric\\warnings.py:17: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▃▁▅▅▅▅▆▅▃▅▅▅▅▆▅▆▆▇▆▆▆▆▇▇▇▆▇▅▇▇▆▇▆█▇▆▇█▇▇</td></tr><tr><td>train_auc</td><td>▄▂▂▂▂▄▃▁▄▄▃▃▃▅▅▄▅▆▄▅▅▅▅█▆▆▆▄█▇▆▇▇▇▇▆█▇▇█</td></tr><tr><td>train_f1</td><td>▆▅▃▂▂▂▃▁▇▁▁▁▁▁▁▂▂▅▅▅▃▃▄▅▄▄▅▄▅█▆▇▃▅▅▆▄▆▆▇</td></tr><tr><td>train_loss_epoch</td><td>█▆▅▄▄▄▅█▆▅▄▃▄▄▃▄▂▄▄▄▂▃▃▃▂▂▃▄▁▃▃▃▃▂▂▄▃▂▃▂</td></tr><tr><td>train_loss_step</td><td>█▇▄▅▄▅▃▆▄▄▅▃▄▃▅▅▆▆█▅▃▄▆▃▃▁▃▄▆▄▇▄▆▂▃▄▅▄▄▄</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>▃▄▃█▇█▇▄▅▄▅▁▃▃▃▂▅▅▁▄▄▄▃▃▂▃▄▅▅▃▄▅▁▂▃▄▄▄▄▅</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_epoch</td><td>▁▁▁▁▁▂▁▁▁▁▁▁▂▂▁▂▃▃▃▃▃▂▄▄▆▆▄▂▅▇▅▅▄▄▃▆▆▅██</td></tr><tr><td>val_loss_step</td><td>▂▂▂▂▂▂▂▂▂▂▂▂▂▃▂▃▃▄▃▃▃▂▄▄▇▆▃▁▄▇▄▅▄▄▂▆▆▄██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.60786</td></tr><tr><td>train_auc</td><td>0.62912</td></tr><tr><td>train_f1</td><td>0.43478</td></tr><tr><td>train_loss_epoch</td><td>0.65653</td></tr><tr><td>train_loss_step</td><td>0.66496</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.40876</td></tr><tr><td>val_auc</td><td>0.51392</td></tr><tr><td>val_f1</td><td>0.58031</td></tr><tr><td>val_loss_epoch</td><td>1.03043</td></tr><tr><td>val_loss_step</td><td>1.15414</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_32_onehot_1</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/9y9h8113' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/9y9h8113</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231231_174437-9y9h8113\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20231231_180302-fh9dzvc0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/fh9dzvc0' target=\"_blank\">MLP_4_32_onehot_1</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/fh9dzvc0' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/fh9dzvc0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_4_32_onehot_1\\sum\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 3.5 K \n",
      "2 | head        | Sequential       | 562   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "4.0 K     Trainable params\n",
      "0         Non-trainable params\n",
      "4.0 K     Total params\n",
      "0.016     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▂▁▃▃▃▅▅▄▅▅▅▅▆▆▆▆▇▅▆▇▇█▇██▇█▇▇▇▇▇███▇▇▇</td></tr><tr><td>train_auc</td><td>▅▂▄▁▄▃▃▄▂▄▃▅▆▆▆▅▇▆▅▃▅▆█▆▆▇▇▆▆▆▇▆▇▆▄▆▅▅▄▆</td></tr><tr><td>train_f1</td><td>▃▅▄▃▄▂▂▂▄▃▃▄▂▁▂▆▆▇▅▆▆▇▇▇▆▇▇▆▇▇▆███▇▇▇▇▇▇</td></tr><tr><td>train_loss_epoch</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▃▂▁▁▁▁▂▁▁▁▁▁▁▁▁▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▅▅▆▅▃▇▇▆▇▇▅▅▆▇▇█▆▇▇▇▇▆▆█▇▆▇▆▇▇▆▇▆▆▇▆</td></tr><tr><td>val_auc</td><td>▂▁▁▂▂▁▂▂▃▃▄▄▅▅▅▆▅▇▇██▇███▇█████▇████████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▇▅▆▆▃▇▇▆██▅▅▆▇▇█▆█▇█▇▇▆█▇▇▇▆▇▇▇▇▇▆▇▇</td></tr><tr><td>val_loss_epoch</td><td>▆█▅▆▅▅▅▅▄▅▅▄▅▄▇▄▃▅▄▄▃▄▄▃▃▃▄▃▄▁▄▂▂▄▃▂▆▃▃▃</td></tr><tr><td>val_loss_step</td><td>▆█▅▆▅▄▅▅▄▅▅▄▅▄▇▄▃▅▅▅▃▄▄▃▃▃▅▄▄▁▄▃▃▅▃▂▆▃▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.65814</td></tr><tr><td>train_auc</td><td>0.57733</td></tr><tr><td>train_f1</td><td>0.56</td></tr><tr><td>train_loss_epoch</td><td>0.60626</td></tr><tr><td>train_loss_step</td><td>0.61123</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.70073</td></tr><tr><td>val_auc</td><td>0.77188</td></tr><tr><td>val_f1</td><td>0.56383</td></tr><tr><td>val_loss_epoch</td><td>0.55729</td></tr><tr><td>val_loss_step</td><td>0.4991</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_32_onehot_1</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/fh9dzvc0' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/fh9dzvc0</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231231_180302-fh9dzvc0\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20231231_182157-yxmlos16</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/yxmlos16' target=\"_blank\">MLP_4_32_onehot_1</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/yxmlos16' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/yxmlos16</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\torch_geometric\\deprecation.py:22: UserWarning: 'nn.glob.GlobalAttention' is deprecated, use 'nn.aggr.AttentionalAggregation' instead\n",
      "  warnings.warn(out)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_4_32_onehot_1\\attention\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 3.5 K \n",
      "2  | head        | Sequential       | 562   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 33    \n",
      "--------------------------------------------------\n",
      "4.1 K     Trainable params\n",
      "0         Non-trainable params\n",
      "4.1 K     Total params\n",
      "0.016     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▅▄▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▆▇▆▆▇▇▇▇██▆▇▇██</td></tr><tr><td>train_auc</td><td>▃▂▂▁▂▁▂▁▂▃▂▁▃▁▁▂▂▃▂▂▂▂▂▂▃▅▆▇▆▇▇▇▇▇█▆▇███</td></tr><tr><td>train_f1</td><td>█▂▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▄▆▃▅▇▆▇▇▇▇▄▆▇██</td></tr><tr><td>train_loss_epoch</td><td>█▇▆▆▆▆▆▆▆▆▆▆▆▅▆▆▆▆▆▆▆▆▆▆▆▅▄▃▃▂▁▂▂▂▁▄▃▃▁▂</td></tr><tr><td>train_loss_step</td><td>█▇█▆▇▇▆▅█▄██▇▅▆▆▆▅▇▇▆▅█▄▇▆▇▅▂▂▃▇▃▂▃▄▂▁▂▆</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▃▅▂▇█▅▆██▃▆▇▇</td></tr><tr><td>val_auc</td><td>▇▅▁▅▅▅▅▅▄▄▄▄▄▅▅▅▅▅▅▅▅▅▆▆▇▇▇▇▇███████████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▃▅▃▇▇▅▅█▇▃▅▆▆</td></tr><tr><td>val_loss_epoch</td><td>▇▆▆▇▅▆▆█▇▇█▅▆█▆▆▆▇▆▅▆▆▆▅▄▆▄▄▂▃▃▁▆▄▂▃▇▁▃▄</td></tr><tr><td>val_loss_step</td><td>▆▅▅▆▄▅▅█▇▆▇▄▅▇▆▅▅▇▅▄▅▅▅▄▃▆▃▄▂▂▄▂▇▅▃▃█▁▃▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.67459</td></tr><tr><td>train_auc</td><td>0.71353</td></tr><tr><td>train_f1</td><td>0.53886</td></tr><tr><td>train_loss_epoch</td><td>0.62454</td></tr><tr><td>train_loss_step</td><td>0.68354</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.66423</td></tr><tr><td>val_auc</td><td>0.76554</td></tr><tr><td>val_f1</td><td>0.37838</td></tr><tr><td>val_loss_epoch</td><td>0.63007</td></tr><tr><td>val_loss_step</td><td>0.64596</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_32_onehot_1</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/yxmlos16' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/yxmlos16</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231231_182157-yxmlos16\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20231231_184132-ev6k8gc7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/ev6k8gc7' target=\"_blank\">MLP_4_32_onehot_1</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/ev6k8gc7' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/ev6k8gc7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_4_32_onehot_1\\attention2\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 3.5 K \n",
      "2  | head        | Sequential       | 562   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 1.1 K \n",
      "--------------------------------------------------\n",
      "5.1 K     Trainable params\n",
      "0         Non-trainable params\n",
      "5.1 K     Total params\n",
      "0.020     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▆▇▇█▇█▇▇████▇▇▇▇▇█</td></tr><tr><td>train_auc</td><td>▂▂▂▁▂▁▂▁▂▁▂▂▂▂▂▂▂▃▂▃▃▄▆▇▇▇▇█████████▇█▇█</td></tr><tr><td>train_f1</td><td>▇▂▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▄▇▇▇▇█▆▆▇█▇▇▆▆▇▆▇▇</td></tr><tr><td>train_loss_epoch</td><td>█▇▇▇▇▇▇▇▇▆▇▆▆▇▇▇▆▆▆▆▆▆▅▃▃▂▂▂▂▂▁▃▁▂▂▁▃▃▃▁</td></tr><tr><td>train_loss_step</td><td>▅▇▆▇▆▅▅▆▇▇▅▇▆▇▅▅▆▆▆▆▆▄▄▃▂▆▄▃▂▅▄▁█▆▆▃▁▂▃▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁███▇▆▇▇▇▇▆██▇▆▇▆▇▇</td></tr><tr><td>val_auc</td><td>▆▆▁▆▆▆▆▆▅▅▅▅▅▅▅▅▅▆▆▆▇▇██████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁███▇▆▆▇▇▆▆▇▇▆▆▇▅▆▇</td></tr><tr><td>val_loss_epoch</td><td>▅▆▆█▆▇▆▆▇▆▇▇▇▆▆▆▆▆▇▆▄▇▄▃▆█▃▄▅▅▅▂▁▂▂▁▇▃▂▇</td></tr><tr><td>val_loss_step</td><td>▄▄▅▆▄▅▅▄▅▄▅▆▅▄▅▄▄▅▆▄▃▆▃▃▆█▃▄▆▅▅▂▁▂▃▁▇▃▂▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.67916</td></tr><tr><td>train_auc</td><td>0.71515</td></tr><tr><td>train_f1</td><td>0.49351</td></tr><tr><td>train_loss_epoch</td><td>0.60196</td></tr><tr><td>train_loss_step</td><td>0.58103</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.69343</td></tr><tr><td>val_auc</td><td>0.7694</td></tr><tr><td>val_f1</td><td>0.5625</td></tr><tr><td>val_loss_epoch</td><td>0.69514</td></tr><tr><td>val_loss_step</td><td>0.80588</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_32_onehot_1</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/ev6k8gc7' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/ev6k8gc7</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231231_184132-ev6k8gc7\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20231231_190150-o9y2qbiq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/o9y2qbiq' target=\"_blank\">MLP_4_64_onehot_1</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/o9y2qbiq' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/o9y2qbiq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_4_64_onehot_1\\mean\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 13.1 K\n",
      "2 | head        | Sequential       | 2.1 K \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "15.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "15.3 K    Total params\n",
      "0.061     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▃▄▄▄▄▄▄▇▇▇▇█▇█▇███▇▇███▇█▇▇███▇█▇█▇█▇██</td></tr><tr><td>train_auc</td><td>▂▂▂▁▁▁▁▃▇▆▇▇▇▆█▇▇▇█▇█▇█▇▇▇█▇███▇█▇█▇█▇██</td></tr><tr><td>train_f1</td><td>▅▂▁▁▁▁▁▂▆▇█▇▇▆▇▇▇▇▇█████████▇█▇██▇▇▇██▇█</td></tr><tr><td>train_loss_epoch</td><td>█▇▇▇▇▇▇▆▄▄▃▂▃▃▂▃▃▂▂▂▂▁▂▃▂▂▂▂▂▂▁▁▂▂▁▁▂▃▂▁</td></tr><tr><td>train_loss_step</td><td>▆▆▇▇█▇▆▆▄▅▃▂▃▃▂▅▅▄▄▅▂▇▁▇▁▄▄▃█▅▁▃▁▆▅▂▄▃▃▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▅▆▆▆▇▆▆█▇▆█▆▆▇▇▇▆▇▇▆▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>val_auc</td><td>▁▁▂▁▂▁▄▅▆▆▇▇▇▇▇▇▇███████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▅▇▆▆█▇▇█▇▇█▆▆▇▇▇▆▇▇▆▇▇▇▇▇█▇▇▇▇█▇</td></tr><tr><td>val_loss_epoch</td><td>▆▆▆▆▇▆▆▆▇▆▅▂▅▃█▃▂▁▄▅▅▂▄▃▅▄▆▂▅▄▇▄▃▃▃▅▃▂▃▄</td></tr><tr><td>val_loss_step</td><td>▅▅▅▅▅▅▅▅▆▅▅▁▅▃█▃▂▁▄▅▅▂▄▃▅▄▆▂▅▄▇▄▃▃▃▅▃▂▃▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.68739</td></tr><tr><td>train_auc</td><td>0.74753</td></tr><tr><td>train_f1</td><td>0.58293</td></tr><tr><td>train_loss_epoch</td><td>0.58738</td></tr><tr><td>train_loss_step</td><td>0.5637</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.70073</td></tr><tr><td>val_auc</td><td>0.7769</td></tr><tr><td>val_f1</td><td>0.54945</td></tr><tr><td>val_loss_epoch</td><td>0.60863</td></tr><tr><td>val_loss_step</td><td>0.6225</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_64_onehot_1</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/o9y2qbiq' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/o9y2qbiq</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231231_190150-o9y2qbiq\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20231231_192135-rt4hthdb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/rt4hthdb' target=\"_blank\">MLP_4_64_onehot_1</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/rt4hthdb' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/rt4hthdb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_4_64_onehot_1\\max\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 13.1 K\n",
      "2 | head        | Sequential       | 2.1 K \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "15.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "15.3 K    Total params\n",
      "0.061     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\torch_geometric\\warnings.py:17: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▄▄▄▅▅▅▄▅▅▄▅▆▆▆▆▅▆▆▆▅▆▆▅▄█▇▆▆▇▆▆▇▇▇▅▆▆▇▆</td></tr><tr><td>train_auc</td><td>▁▃▃▁▂▃▃▂▅▃▄▃▄▄▅▅▇▇▅▆▄▅▆▆▄▇▇▇▅▆▆▇▇▇█▅▇▆▆▆</td></tr><tr><td>train_f1</td><td>▅▃▁▂▁▂▃▃▅▄▅▂▄▅▆▄▄▄▅▇█▆▆▇▇▇▇▇▆▆▅▆▇▆▆▇▇█▆▅</td></tr><tr><td>train_loss_epoch</td><td>█▅▅▅▅▄▄▄▄▄▅▄▄▄▃▄▃▂▃▄▅▃▃▄▄▂▂▂▃▃▂▁▃▄▁▃▂▄▃▂</td></tr><tr><td>train_loss_step</td><td>▆▆▆▆▇▆▄▄▅▆▄▄▄▅▄▆▄▆█▅▅▄█▇▁██▂▆▆▄▂▃▆▇▆▄▆▆▄</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>▅▅▃▅▆▄▃▇▃▄▂▂▂▂▂▄▃▂▄▃▂▃▁▃▄▄▅█▇▄▂▃▂▂▃▂▃▃▃▄</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_epoch</td><td>▁▁▁▁▁▁▂▂▂▂▂▄▂▆▅█▄▆▄▆▇▅▇▆▅▆▇▆▆█▅▇▅▆█▇▇█▄▅</td></tr><tr><td>val_loss_step</td><td>▁▁▁▁▁▁▁▂▁▁▁▄▁▆▄█▂▅▂▅▇▄▆▄▃▅▇▄▄▇▃▅▂▃▅▄▅▇▁▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.61974</td></tr><tr><td>train_auc</td><td>0.61453</td></tr><tr><td>train_f1</td><td>0.35802</td></tr><tr><td>train_loss_epoch</td><td>0.64858</td></tr><tr><td>train_loss_step</td><td>0.63858</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.40876</td></tr><tr><td>val_auc</td><td>0.4308</td></tr><tr><td>val_f1</td><td>0.58031</td></tr><tr><td>val_loss_epoch</td><td>0.99866</td></tr><tr><td>val_loss_step</td><td>0.92923</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_64_onehot_1</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/rt4hthdb' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/rt4hthdb</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231231_192135-rt4hthdb\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20231231_194134-ghzik3fp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/ghzik3fp' target=\"_blank\">MLP_4_64_onehot_1</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/ghzik3fp' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/ghzik3fp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_4_64_onehot_1\\sum\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 13.1 K\n",
      "2 | head        | Sequential       | 2.1 K \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "15.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "15.3 K    Total params\n",
      "0.061     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▂▁▃▄▃▄▃▃▄▄▄▅▅▆▆▇▇▆▆▇▇▇▇▇▇▆▇▇▇▆▆▇▆▆█▆▆▇</td></tr><tr><td>train_auc</td><td>▂▁▂▁▃▃▃▃▃▃▄▅▅▄▅▆▆▆▅▅▇▇█▆▆▆▇▇▆▆▆▇▆▆▆██▆▅▆</td></tr><tr><td>train_f1</td><td>▂▁▁▃▅▄▁▁▂▃▂▂▄▆▆▅▆▆▇▇▇▇▇▇▇▇▇▇▆▇▇▇█▇█▇███▇</td></tr><tr><td>train_loss_epoch</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▂▂▂▂▂▁▁▂▂▂▁▂▂▁▂▂▂▂▂▁▂▁▂▁▁▂▁▂▁▁▁▁▂▁▁▂▁▂▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▅▅▆▇▆▆▆▇▇▆▆▆▇▇▇▇▇▆▇▇▇▇▇▇█▇███▇██▇█▇▆██</td></tr><tr><td>val_auc</td><td>▂▇▁▇▇▅▇▇▇▇▇▇▇▇██████████████████████████</td></tr><tr><td>val_f1</td><td>▇▇▁▁▅▆▅▃▃▇▇▅▅▅▆█▇▆▆▄▆▆▇▇▆▆▇▆███▆▇█▆█▆▅▇█</td></tr><tr><td>val_loss_epoch</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_step</td><td>█▂▂▂▂▂▁▁▁▁▂▁▂▁▃▁▁▁▂▂▂▁▁▁▁▁▁▁▂▁▂▂▁▁▁▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.68921</td></tr><tr><td>train_auc</td><td>0.65219</td></tr><tr><td>train_f1</td><td>0.575</td></tr><tr><td>train_loss_epoch</td><td>0.58579</td></tr><tr><td>train_loss_step</td><td>0.55309</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.74453</td></tr><tr><td>val_auc</td><td>0.7721</td></tr><tr><td>val_f1</td><td>0.70339</td></tr><tr><td>val_loss_epoch</td><td>0.61854</td></tr><tr><td>val_loss_step</td><td>0.59928</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_64_onehot_1</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/ghzik3fp' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/ghzik3fp</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231231_194134-ghzik3fp\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20231231_200134-2f1z35kt</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/2f1z35kt' target=\"_blank\">MLP_4_64_onehot_1</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/2f1z35kt' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/2f1z35kt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\torch_geometric\\deprecation.py:22: UserWarning: 'nn.glob.GlobalAttention' is deprecated, use 'nn.aggr.AttentionalAggregation' instead\n",
      "  warnings.warn(out)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_4_64_onehot_1\\attention\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 13.1 K\n",
      "2  | head        | Sequential       | 2.1 K \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 65    \n",
      "--------------------------------------------------\n",
      "15.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "15.3 K    Total params\n",
      "0.061     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▃▄▄▄▄▄▄▄▄▄▄▄▆▆█▆▇▆▇██▆▆▇▆▇▇▇██▇▆▇▇█▇▇██</td></tr><tr><td>train_auc</td><td>▁▁▂▂▂▁▂▃▁▂▃▂▂▅▆▇▇▇▇▇█▇▇▇▇▇▇▇███▇▇▇█████▇</td></tr><tr><td>train_f1</td><td>▆▂▁▁▁▁▁▁▁▁▁▁▁▅▇▇█▆▅▆█▇▅▅▇▅▆▆▇██▇▆▆▆█▇▇▇▇</td></tr><tr><td>train_loss_epoch</td><td>█▇▇▇▇▇▇▆▇▇▇▇▆▅▄▂▄▃▄▂▃▂▄▃▂▄▃▃▁▂▁▂▃▃▃▂▂▂▁▂</td></tr><tr><td>train_loss_step</td><td>█▇▇▆▆▇▆▇▆▆▇▇▆▅▆▇▆▃▅█▄▆▆▅▄▁▆▃▂▃▂▃▃▄▂▄▂▄▂▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▇▂▇▄▆█▅▄▆█▅▇█▇▆▆▇▆▆▇▇▆▇▇██▇</td></tr><tr><td>val_auc</td><td>▂▁▃▃▃▃▃▃▃▄▄▅▇█▇▇▇███████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▇▃▇▄▆█▅▅▆█▅▇██▆▆▇▆▇▇▇▇▇███▇</td></tr><tr><td>val_loss_epoch</td><td>▅▅▄▅▅▅▄▃▅▅▅▅▅▄▃█▃▂▂▁▂▃▂▃▃▅▆▂▁▄▂▃▄▄▁▃▂▂▁▁</td></tr><tr><td>val_loss_step</td><td>▄▄▃▄▄▄▃▂▄▄▄▄▄▃▃█▃▂▂▁▂▃▂▃▃▅▆▂▁▄▂▃▄▄▁▃▂▂▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.6819</td></tr><tr><td>train_auc</td><td>0.71043</td></tr><tr><td>train_f1</td><td>0.51261</td></tr><tr><td>train_loss_epoch</td><td>0.60294</td></tr><tr><td>train_loss_step</td><td>0.53833</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.70073</td></tr><tr><td>val_auc</td><td>0.76912</td></tr><tr><td>val_f1</td><td>0.59</td></tr><tr><td>val_loss_epoch</td><td>0.56414</td></tr><tr><td>val_loss_step</td><td>0.52077</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_64_onehot_1</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/2f1z35kt' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/2f1z35kt</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231231_200134-2f1z35kt\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20231231_202059-iwdg5j95</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/iwdg5j95' target=\"_blank\">MLP_4_64_onehot_1</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/iwdg5j95' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/iwdg5j95</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_4_64_onehot_1\\attention2\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 13.1 K\n",
      "2  | head        | Sequential       | 2.1 K \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 4.2 K \n",
      "--------------------------------------------------\n",
      "19.5 K    Trainable params\n",
      "0         Non-trainable params\n",
      "19.5 K    Total params\n",
      "0.078     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▄▃▄▄▄▄▄▅▆▆█▇▇▇▇█▇▇▇▇▇▇▇█▇▇▇▇▇▇████▇▇███</td></tr><tr><td>train_auc</td><td>▁▂▁▂▂▃▂▄▆▇▇█▇█▇▇█▇▇▇▇▇██▇███▇█████▇▇▇███</td></tr><tr><td>train_f1</td><td>▅▁▁▁▁▁▁▂▇▇▆█▇▇█▇█▇▆▇▇▇▇▇▇█▇█▇▇▇████▇███▇</td></tr><tr><td>train_loss_epoch</td><td>█▇▇▇▇▇▇▆▄▄▃▂▂▃▃▂▂▃▃▁▃▂▂▂▂▂▂▃▃▃▃▁▁▁▂▂▂▁▂▂</td></tr><tr><td>train_loss_step</td><td>▇██▇█▆█▅▇▆▃█▆▇▅▅▃▅▆▂▆▂▃▄▆▇▅█▄▅▃▄▁▃▄▅▄▄▄▆</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▆▅▅▇▆▇▇▆▆▆█████▇█▇▇▇▇▆▇▇▆▇▇▇▇▇▇▇▇</td></tr><tr><td>val_auc</td><td>▁▁▁▁▂▃▃▅▆▆▇▇▇▇▇▇█▇██████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▂▆▅▅▇▆▇▇▆▆▆█▇██▇██▇▇▇▆▆▇▇▆▇▇▇▇▆▆▇▇</td></tr><tr><td>val_loss_epoch</td><td>▇▆▇▆▆▇▆▅▆▇▃▄▅▃▄▇▃▂▅█▂▄▄▆▂▇▂▅▄▆▁▃▃▄▆▄▄▃▂▃</td></tr><tr><td>val_loss_step</td><td>▆▅▆▅▅▆▅▄▆▇▃▄▅▃▄▇▃▂▄█▂▄▄▆▂▇▂▅▅▆▁▃▃▄▆▄▄▃▂▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.70475</td></tr><tr><td>train_auc</td><td>0.72496</td></tr><tr><td>train_f1</td><td>0.55814</td></tr><tr><td>train_loss_epoch</td><td>0.60548</td></tr><tr><td>train_loss_step</td><td>0.63821</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.69708</td></tr><tr><td>val_auc</td><td>0.77607</td></tr><tr><td>val_f1</td><td>0.57436</td></tr><tr><td>val_loss_epoch</td><td>0.55715</td></tr><tr><td>val_loss_step</td><td>0.50938</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_64_onehot_1</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/iwdg5j95' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/iwdg5j95</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231231_202059-iwdg5j95\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20231231_204153-mxfgsfxr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/mxfgsfxr' target=\"_blank\">MLP_2_16_onehot_2</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/mxfgsfxr' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/mxfgsfxr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_2_16_onehot_2\\mean\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 432   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "586       Trainable params\n",
      "0         Non-trainable params\n",
      "586       Total params\n",
      "0.002     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▄▅▅▅▅▅▅▅▅▆▆▇▇▇▇▇▇█▇▇█▇▇▇███▇██▇▇█▇██▇▇█</td></tr><tr><td>train_auc</td><td>▂▁▂▁▁▂▂▂▂▃▄▅▇▇▇▇▇▇█▇▇█▇▇████▇▇█▇██▇▇██▇█</td></tr><tr><td>train_f1</td><td>▇▂▁▁▁▁▁▁▁▁▄▅▆▇▇▇▆▇█▇▇█▇█▇█▇█▇▇▇▇▇█▇▇█▇▇▇</td></tr><tr><td>train_loss_epoch</td><td>█▆▆▆▆▆▆▆▅▅▅▄▃▂▂▂▃▃▂▃▃▂▁▂▂▁▂▂▂▂▂▁▂▁▂▂▁▂▂▁</td></tr><tr><td>train_loss_step</td><td>▇▆█▇▇▇▇▆▅▇█▆▃▂▅▄▄▄▇▃▂▆▅▂▅▆▄▂▄▅▄▄▁▃▂▅▂▄▄▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▃▆▇▇▇▆▆▆██▆▇█▇█▇█▇▇▆▇▇▇▇█▇▆▇█▇</td></tr><tr><td>val_auc</td><td>▁▄▅▅▆▆██████████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▃▆▇▇█▇▇▆██▆█▇█▇▇█▇█▇█▇████▇██▇</td></tr><tr><td>val_loss_epoch</td><td>▇█▇▇▇▆▇▇▆▆▆▆▅▄▁▂▃▄▃▄▄▄▃▄▂▃▄▄▅▄▃▁▄▃▂▃▃▄▄▂</td></tr><tr><td>val_loss_step</td><td>▇█▇▆▆▆▇▇▆▆▆▆▆▅▂▂▃▄▃▅▅▅▄▄▃▃▅▅▆▅▄▁▅▄▂▄▃▅▄▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.68007</td></tr><tr><td>train_auc</td><td>0.71809</td></tr><tr><td>train_f1</td><td>0.4868</td></tr><tr><td>train_loss_epoch</td><td>0.60264</td></tr><tr><td>train_loss_step</td><td>0.56555</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.69343</td></tr><tr><td>val_auc</td><td>0.77389</td></tr><tr><td>val_f1</td><td>0.49398</td></tr><tr><td>val_loss_epoch</td><td>0.52184</td></tr><tr><td>val_loss_step</td><td>0.47097</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_16_onehot_2</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/mxfgsfxr' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/mxfgsfxr</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231231_204153-mxfgsfxr\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20231231_210238-thv2ukqk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/thv2ukqk' target=\"_blank\">MLP_2_16_onehot_2</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/thv2ukqk' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/thv2ukqk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_2_16_onehot_2\\max\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 432   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "586       Trainable params\n",
      "0         Non-trainable params\n",
      "586       Total params\n",
      "0.002     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\torch_geometric\\warnings.py:17: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▆▇▇▇▆▇▇▇▇▇█▇▇▇█▇█▇█▇██</td></tr><tr><td>train_auc</td><td>▃▂▄▃▃▁▅▃▃▃▄▃▂▃▃▃▂▃▄▅▄▅▅▄▃▄▅▅▆▄▅▆▇▆▄▅▆█▄▅</td></tr><tr><td>train_f1</td><td>█▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▁▂▂▃▃▅▄▄▄▄▅▄▅▄▅▄▄▄</td></tr><tr><td>train_loss_epoch</td><td>█▄▃▃▃▃▃▃▂▂▂▃▂▃▂▂▃▃▃▃▃▂▂▂▂▂▂▁▂▂▂▁▁▂▂▂▁▂▂▂</td></tr><tr><td>train_loss_step</td><td>▂▄▇▇▆▄▅▃▂▅█▆▃▂▆▃▁▄▇▂▆▅█▄▅▄▆▅▇▄▅▃▁▅▁▄▄▂▄▄</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁██████████████▅▅▅▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>▃▅▃▂▂▂▂▂▂▁▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▆▇▇▆▆▆▆▆▆▆█▇▆▇</td></tr><tr><td>val_f1</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▅▅▅██████████████████████</td></tr><tr><td>val_loss_epoch</td><td>▂▂▂▂▂▁▂▁▁▁▂▂▂▂▁▂▂▂▂▂▃▃▃▃▄▂▄▃▄▆▅▇▆█▆▅█▅▇▅</td></tr><tr><td>val_loss_step</td><td>▂▂▂▂▂▁▂▂▁▁▂▂▂▂▁▂▂▂▂▂▃▃▃▃▅▁▄▂▃▆▅▇▅█▅▄█▄▇▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.60695</td></tr><tr><td>train_auc</td><td>0.52329</td></tr><tr><td>train_f1</td><td>0.2637</td></tr><tr><td>train_loss_epoch</td><td>0.66407</td></tr><tr><td>train_loss_step</td><td>0.67273</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.40511</td></tr><tr><td>val_auc</td><td>0.58194</td></tr><tr><td>val_f1</td><td>0.57662</td></tr><tr><td>val_loss_epoch</td><td>0.75898</td></tr><tr><td>val_loss_step</td><td>0.75105</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_16_onehot_2</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/thv2ukqk' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/thv2ukqk</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231231_210238-thv2ukqk\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20231231_212412-yupvgl6c</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/yupvgl6c' target=\"_blank\">MLP_2_16_onehot_2</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/yupvgl6c' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/yupvgl6c</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_2_16_onehot_2\\sum\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 432   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "586       Trainable params\n",
      "0         Non-trainable params\n",
      "586       Total params\n",
      "0.002     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▃▄▃▄▄▄▄▄▅▄▅▅▅▆▆▅▅▆▆▅▅▆▅▆▅▆▆▇▆▆▇▆█▇▇█▇█▇</td></tr><tr><td>train_auc</td><td>▆▄█▃▇▅▆▇▆▃▄▄▄▄▂▃▇▅▂▃▆▆▆▄▅▅▅▆▅▃▅▃▃▄▁▂▂▄▃▃</td></tr><tr><td>train_f1</td><td>▆▁▄▅▅▄▄▄▄▂▃▅▄▄▄▅▂▅▄▅▄▅▃▅▃▃▄▇▄▆▃▅▄▅▅▄█▄▆▄</td></tr><tr><td>train_loss_epoch</td><td>█▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▄▁▅▁▃▂▁▃▅▅▅▅▆▅▅▅▆▅█▄█▄▆▅▇▆▇▅▆▆▆▇▆▇▄█▄</td></tr><tr><td>val_auc</td><td>▂▂█▂█▂▄▅▃▃▄▂▄▂▂▃▅▃▂▂▄▃▂▁▁▁▂▂▄▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_f1</td><td>▁▁▁▅▁▅▂▃▂▁▃▅▅▅▅▆▄▅▅▇▅█▄█▄▇▅▇▆▆▅▆▆▆▇▆█▄▇▄</td></tr><tr><td>val_loss_epoch</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_step</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.61517</td></tr><tr><td>train_auc</td><td>0.44184</td></tr><tr><td>train_f1</td><td>0.42565</td></tr><tr><td>train_loss_epoch</td><td>0.6408</td></tr><tr><td>train_loss_step</td><td>0.62376</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.66058</td></tr><tr><td>val_auc</td><td>0.24451</td></tr><tr><td>val_f1</td><td>0.31111</td></tr><tr><td>val_loss_epoch</td><td>0.59712</td></tr><tr><td>val_loss_step</td><td>0.59466</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_16_onehot_2</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/yupvgl6c' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/yupvgl6c</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231231_212412-yupvgl6c\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20231231_214427-yrmm9orx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/yrmm9orx' target=\"_blank\">MLP_2_16_onehot_2</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/yrmm9orx' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/yrmm9orx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\torch_geometric\\deprecation.py:22: UserWarning: 'nn.glob.GlobalAttention' is deprecated, use 'nn.aggr.AttentionalAggregation' instead\n",
      "  warnings.warn(out)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_2_16_onehot_2\\attention\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 432   \n",
      "2  | head        | Sequential       | 154   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 17    \n",
      "--------------------------------------------------\n",
      "603       Trainable params\n",
      "0         Non-trainable params\n",
      "603       Total params\n",
      "0.002     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▄▄▅▄▄▄▄▅▅▅▆▇▆▇█▇█▇▇▇▇▇▇█▇▇▇██▇▇▇█▇█████</td></tr><tr><td>train_auc</td><td>▂▂▁▁▂▁▂▃▁▃▃▄▆▆▆▇▇▇█▇▇▇▆▇▇█▆▇▇██▇▇▇▇███▇█</td></tr><tr><td>train_f1</td><td>▇▂▁▁▁▁▁▁▁▃▂▅▇▅▆▇▆▇▇▆▇▇▆▇▇▆▇▇▇▇▆▆▆█▆▇▇▇▇▇</td></tr><tr><td>train_loss_epoch</td><td>█▇▇▇▆▆▇▆▇▆▅▄▃▄▄▂▃▃▃▂▂▄▃▂▃▂▃▃▂▁▂▂▂▃▃▂▂▂▂▁</td></tr><tr><td>train_loss_step</td><td>▇▆▅▇█▆▅▇▆▆▅▆▁▃▅▃▂▄▄▃▆▂▂▄▄█▅▆▁▃▂▇▄▂▁▃▆▃▂▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▆▆▆▅▆▇▇▆▆▆▇▇▆▇▇▆▇▇▇▇▇▆▇▇▆▇▇█▇</td></tr><tr><td>val_auc</td><td>▁▄▆▆▇▇██████████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▂▆▆▆▅▆▇▇▇▆▆▇▇▆▇▇▆▇▇▇▇▇▆██▆███▇</td></tr><tr><td>val_loss_epoch</td><td>▇▆█▇▇▇█▇▇▆▆▅▄▅▇▆▁▃▂▅▃▄▄▆▃▅▃▄▅▅▂▅▅▄▅▃▁▅▂▃</td></tr><tr><td>val_loss_step</td><td>▇▆█▇▇▆█▆▇▆▆▅▄▆█▇▁▄▂▆▃▅▅▇▃▆▃▅▇▇▃▆▆▅▆▃▁▇▃▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.68007</td></tr><tr><td>train_auc</td><td>0.71548</td></tr><tr><td>train_f1</td><td>0.53826</td></tr><tr><td>train_loss_epoch</td><td>0.60304</td></tr><tr><td>train_loss_step</td><td>0.57436</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.68613</td></tr><tr><td>val_auc</td><td>0.7803</td></tr><tr><td>val_f1</td><td>0.42667</td></tr><tr><td>val_loss_epoch</td><td>0.56496</td></tr><tr><td>val_loss_step</td><td>0.54813</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_16_onehot_2</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/yrmm9orx' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/yrmm9orx</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231231_214427-yrmm9orx\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20231231_220359-5oxkqi2r</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/5oxkqi2r' target=\"_blank\">MLP_2_16_onehot_2</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/5oxkqi2r' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/5oxkqi2r</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_2_16_onehot_2\\attention2\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 432   \n",
      "2  | head        | Sequential       | 154   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 281   \n",
      "--------------------------------------------------\n",
      "867       Trainable params\n",
      "0         Non-trainable params\n",
      "867       Total params\n",
      "0.003     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▄▄▅▅▅▅▅▅▅▅▆▇▇▇▇▇▇▇▇▇▇▇██▇██▇▇██████████</td></tr><tr><td>train_auc</td><td>▁▂▂▁▂▂▁▂▂▂▃▅▇▆▇▆▇▇▇▇██▇▇█▇▇▇▇█▇█▇████▇██</td></tr><tr><td>train_f1</td><td>▇▃▂▂▁▁▁▁▁▁▂▄▆▇▇▇▇▇▇▇█▇▇▇▇▇█▇▇▇██▇▇▇██▇██</td></tr><tr><td>train_loss_epoch</td><td>█▅▆▆▆▆▆▆▆▅▅▄▃▄▃▃▂▃▂▃▂▂▂▂▂▃▂▂▁▁▃▂▂▁▁▂▁▂▂▁</td></tr><tr><td>train_loss_step</td><td>▆█▆▇▆▆▇▇▆▆▅▄▆█▅▂▁▄▅▂▁▁▆▃▃▅▂▄▄▂▃▃▄▆▃▂▂▆▅▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▃▅▆▆▆▇▆▇▇▇▇█▇▇▇▇███▇▇█▇██▇██▇▇</td></tr><tr><td>val_auc</td><td>▁▅▇█████████████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▂▅▅▆▅▇▆▇▇▇▇█▇▇▇▇██▇▇██▇▇▇████▇</td></tr><tr><td>val_loss_epoch</td><td>█▇▇▇▇██▇█▆▅▆▃▅▃▄▅▂▄▃▃▃▄▃▃▃▅▅▃▅▂▄▄▃▁▃▄▂▃▄</td></tr><tr><td>val_loss_step</td><td>▇▇▇▇▆▇█▇█▆▅▇▃▆▃▅▆▃▅▄▄▄▅▃▄▄▆▇▃▆▂▄▅▄▁▄▅▃▄▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.68282</td></tr><tr><td>train_auc</td><td>0.71368</td></tr><tr><td>train_f1</td><td>0.54522</td></tr><tr><td>train_loss_epoch</td><td>0.5971</td></tr><tr><td>train_loss_step</td><td>0.56391</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.69708</td></tr><tr><td>val_auc</td><td>0.77428</td></tr><tr><td>val_f1</td><td>0.55615</td></tr><tr><td>val_loss_epoch</td><td>0.59606</td></tr><tr><td>val_loss_step</td><td>0.62797</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_16_onehot_2</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/5oxkqi2r' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/5oxkqi2r</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231231_220359-5oxkqi2r\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20231231_222406-hnl267w7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/hnl267w7' target=\"_blank\">MLP_2_32_onehot_2</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/hnl267w7' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/hnl267w7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_2_32_onehot_2\\mean\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 1.4 K \n",
      "2 | head        | Sequential       | 562   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.008     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▁▁▁▁▃▄▄▆▇▆▅▆▆▆▆▇▇▆▇▇▇▇▆▇▇▆▇▇▇▇▇▇▇▆█▆▇▇</td></tr><tr><td>train_auc</td><td>▂▁▁▁▂▄▄▇▇▇▇▇▇█▇▇▇██▇▇███▇██████▇████████</td></tr><tr><td>train_f1</td><td>▁▁▁▁▂▁▄▄▆▇█▇▆▇▇▇▇██▇▇█▇▇▇▇▇▇▇█▇▇▇▇▇██▆▇█</td></tr><tr><td>train_loss_epoch</td><td>██▇█▇▇▆▅▄▄▄▃▃▃▃▃▃▂▂▃▃▃▂▂▃▃▂▂▃▁▂▃▂▂▁▃▂▃▁▂</td></tr><tr><td>train_loss_step</td><td>▆▇▇▇▅▆▅▅▆▄▅▆▄▃▃▂▄▄▂▂▃▄▃▂▁▅▃▄▃▃▂▅▃▄▄█▂▃▃▄</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▃▆▇▇▇█▇▆▇█▇▇██▇▇█▇▇█▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>val_auc</td><td>▁▇██████████████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▃▆▇▇▇█▆▇▇▇▇▇▇▇▇▇▇▇▇█▇▇▇▇▇▇▇▇▇▇█▇▆▇</td></tr><tr><td>val_loss_epoch</td><td>█▆██▇▇▇▄▃▄▆▆▃▂▄▄▄▃▃▅▂▄▃▄▂▆▁▂▃▁▂▃▅▂▃▂▁▂▃▂</td></tr><tr><td>val_loss_step</td><td>▇▅▇▇▆▆▇▄▄▅██▄▃▅▆▆▄▄▇▃▅▄▆▃▇▂▃▄▂▃▅▆▃▄▂▁▃▄▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.6947</td></tr><tr><td>train_auc</td><td>0.7246</td></tr><tr><td>train_f1</td><td>0.57722</td></tr><tr><td>train_loss_epoch</td><td>0.60327</td></tr><tr><td>train_loss_step</td><td>0.60212</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.70438</td></tr><tr><td>val_auc</td><td>0.77649</td></tr><tr><td>val_f1</td><td>0.56684</td></tr><tr><td>val_loss_epoch</td><td>0.54187</td></tr><tr><td>val_loss_step</td><td>0.52454</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_32_onehot_2</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/hnl267w7' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/hnl267w7</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231231_222406-hnl267w7\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20231231_224405-ua35y5y1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/ua35y5y1' target=\"_blank\">MLP_2_32_onehot_2</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/ua35y5y1' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/ua35y5y1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_2_32_onehot_2\\max\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 1.4 K \n",
      "2 | head        | Sequential       | 562   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.008     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\torch_geometric\\warnings.py:17: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▃▃▃▃▄▄▄▄▄▅▆▅▆▇▆▆▇▆▇▇█▇▇▇▇▇▆▇█▇▇█▆▆▇▇█▇</td></tr><tr><td>train_auc</td><td>▁▁▁▁▂▁▂▃▄▄▅▄▅▅▆▇▇▅▆▇▇▆▇▇▆▇▇▇▆▇▇▆▇▇▇▆▇▇▇█</td></tr><tr><td>train_f1</td><td>▅▃▂▁▁▁▂▃▄▅▄▄▆▄▆▇▆▇▇▆▇█▇▇▇▇▇▇▇▆█▇▇▇▇▇▇▆██</td></tr><tr><td>train_loss_epoch</td><td>█▇▆▇▆▆▅▅▅▆▅▅▄▄▄▃▃▄▄▃▂▂▂▂▃▃▃▂▄▂▁▄▂▁▂▃▁▃▁▄</td></tr><tr><td>train_loss_step</td><td>▆▆▆█▃▆▄▄▆▅▅▅▂▁▆▂▁▄▁▃▆▅▂▃▂▄▃▃▂▅▁▂▃▃▄▅▄▄▃▇</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▇▇▇▇▇▇▇▇██▇▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>▁▁▂▂▅▅▇▇▇▇▇█▇█████▇▇▇▇█████▇▇▇▇▇██▇▇▇▇▇▇</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▆▇██████████████████████████████</td></tr><tr><td>val_loss_epoch</td><td>▂▁▂▂▂▂▂▂▂▂▂▃▃▃▃▂▄▄▃▅▄▄▄▄▃▄▅▄▅▆▆▇▅▄▅▆█▆▅▆</td></tr><tr><td>val_loss_step</td><td>▂▁▂▂▂▂▃▂▂▂▂▃▃▄▃▁▄▄▃▅▄▄▄▃▃▃▅▃▅▆▅▆▄▃▄▅█▆▃▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.65722</td></tr><tr><td>train_auc</td><td>0.70941</td></tr><tr><td>train_f1</td><td>0.5541</td></tr><tr><td>train_loss_epoch</td><td>0.64365</td></tr><tr><td>train_loss_step</td><td>0.71597</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.40511</td></tr><tr><td>val_auc</td><td>0.65332</td></tr><tr><td>val_f1</td><td>0.57662</td></tr><tr><td>val_loss_epoch</td><td>0.91231</td></tr><tr><td>val_loss_step</td><td>0.91371</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_32_onehot_2</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/ua35y5y1' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/ua35y5y1</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231231_224405-ua35y5y1\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20231231_230351-fs9jzk86</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/fs9jzk86' target=\"_blank\">MLP_2_32_onehot_2</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/fs9jzk86' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/fs9jzk86</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_2_32_onehot_2\\sum\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 1.4 K \n",
      "2 | head        | Sequential       | 562   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.008     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▂▃▂▃▄▄▃▄▄▄▅▅▅▅▆▇▆▆▆▆▆▇▆▇▇▆▆▇█▇▇▇▇▇▇█▇▇</td></tr><tr><td>train_auc</td><td>▄▁▅▄▃▁▂▃▂▁▄▄▄▄▄▄▅▄▃▅▂▂▂▅▄▃▄▃▃▃▅▃▆▇▅▇▆█▇▇</td></tr><tr><td>train_f1</td><td>▃▁▁▂▁▂▃▂▃▂▂▃▄▄▄▃▅▆▆▅▅▇▆▆▆▆▆▆▇▅▇▇▇▆▇▇█▇▅▆</td></tr><tr><td>train_loss_epoch</td><td>█▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▃▂▂▂▂▁▂▁▂▂▂▁▁▁▁▂▁▁▁▂▁▁▁▁▂▁▁▁▁▁▁▂▁▁▂▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▃▂▂▂▃▃▃▅▃▅▃▇▇▇▇▇██▆▆█▇██▇▇██▇▇█▇██▇▆▇█</td></tr><tr><td>val_auc</td><td>▁▂▇▃▂▂▂▃▂▂▂▄▃▃▃▅▄▃▂▂▁▂▂▃▃▂▁▁▃▂▃▄▆▇▇█▇███</td></tr><tr><td>val_f1</td><td>▁▆▃▃▂▂▄▃▃▅▃▅▃▆▆▆▇▆▇▇▆▆█▇▇▇▇▇▇█▇▇█▇████▇█</td></tr><tr><td>val_loss_epoch</td><td>█▄▃▃▂▃▃▂▂▃▃▂▃▂▂▄▃▂▂▃▂▃▃▃▂▃▁▂▂▁▁▂▂▂▂▁▁▂▂▂</td></tr><tr><td>val_loss_step</td><td>█▆▅▄▃▄▅▃▄▄▅▃▅▃▄▇▆▂▄▅▃▅▅▅▃▅▂▃▄▂▂▃▄▄▃▂▁▃▂▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.66636</td></tr><tr><td>train_auc</td><td>0.55317</td></tr><tr><td>train_f1</td><td>0.56287</td></tr><tr><td>train_loss_epoch</td><td>0.6175</td></tr><tr><td>train_loss_step</td><td>0.64344</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.72263</td></tr><tr><td>val_auc</td><td>0.76206</td></tr><tr><td>val_f1</td><td>0.63107</td></tr><tr><td>val_loss_epoch</td><td>0.55208</td></tr><tr><td>val_loss_step</td><td>0.53759</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_32_onehot_2</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/fs9jzk86' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/fs9jzk86</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231231_230351-fs9jzk86\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20231231_232317-osofjti9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/osofjti9' target=\"_blank\">MLP_2_32_onehot_2</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/osofjti9' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/osofjti9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\torch_geometric\\deprecation.py:22: UserWarning: 'nn.glob.GlobalAttention' is deprecated, use 'nn.aggr.AttentionalAggregation' instead\n",
      "  warnings.warn(out)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_2_32_onehot_2\\attention\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 1.4 K \n",
      "2  | head        | Sequential       | 562   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 33    \n",
      "--------------------------------------------------\n",
      "2.0 K     Trainable params\n",
      "0         Non-trainable params\n",
      "2.0 K     Total params\n",
      "0.008     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▁▁▁▂▅▆▆▇▇▇▇▇▆▆▅▆▇▆▆▅▆▇▆▇▆▇█▇▇▆▇▇▇▇█▇▇▇</td></tr><tr><td>train_auc</td><td>▁▂▂▂▄▃▇▇▇▇▇▇▇█▇▇█▇█▇▇▇▇█▇██████▇████████</td></tr><tr><td>train_f1</td><td>▁▁▁▁▁▂▆▆▇▇▇▇▇▇▇▆▅▇▇▇█▇▆▇▇▇▆▇▇██▇▇▇▇▇█▇██</td></tr><tr><td>train_loss_epoch</td><td>█▇▇█▇▇▄▄▄▄▄▄▄▁▃▄▃▂▂▃▄▄▄▃▃▃▃▄▂▃▂▂▄▂▁▂▂▂▄▃</td></tr><tr><td>train_loss_step</td><td>▆█▆▆▅▅▃▃▄▄▄▄▅▁▂▃▅▅▅▄▂▅▅▅▃▄▄▄█▅▅▅▄▅▄▄▂█▇▄</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▂▄▆▆▅▅▅▆▅▆▇▅▆▇▇▆▇▇█▇█▇▇▆▇▇█▇█▇▇█▇▇▇</td></tr><tr><td>val_auc</td><td>▁▆▇█████████████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▂▄▆▅▅▅▅▆▅▆▇▅▆▇▆▆█▇█▇█▆▆▆▇▇█▇█▇▇▇▇▇▇</td></tr><tr><td>val_loss_epoch</td><td>▇███▇▅▂▂▆▆▃▆▇▇▂▄▄▄▅▆▄▅▃▆▅▅▃▄▄▂▇▄▂▅▁▃▂▅▂▅</td></tr><tr><td>val_loss_step</td><td>▆▇▆▇▆▄▂▂▇▆▃▇██▃▅▄▄▆▆▄▅▄▇▅▆▃▄▅▂█▅▂▆▁▃▃▆▃▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.69104</td></tr><tr><td>train_auc</td><td>0.73164</td></tr><tr><td>train_f1</td><td>0.62528</td></tr><tr><td>train_loss_epoch</td><td>0.61334</td></tr><tr><td>train_loss_step</td><td>0.60478</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.70073</td></tr><tr><td>val_auc</td><td>0.78743</td></tr><tr><td>val_f1</td><td>0.49383</td></tr><tr><td>val_loss_epoch</td><td>0.5943</td></tr><tr><td>val_loss_step</td><td>0.61314</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_32_onehot_2</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/osofjti9' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/osofjti9</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231231_232317-osofjti9\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20231231_234220-thft6ljs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/thft6ljs' target=\"_blank\">MLP_2_32_onehot_2</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/thft6ljs' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/thft6ljs</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_2_32_onehot_2\\attention2\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 1.4 K \n",
      "2  | head        | Sequential       | 562   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 1.1 K \n",
      "--------------------------------------------------\n",
      "3.0 K     Trainable params\n",
      "0         Non-trainable params\n",
      "3.0 K     Total params\n",
      "0.012     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▁▁▁▂▂▆▆▆▅▆▆▆▆▆▇▇█▆█▇▇██▇▆▇▇▇▇███▇▇▇▇██</td></tr><tr><td>train_auc</td><td>▂▁▂▃▂▃▅▇▇█▇▇▇▇▇▇█▇█▇█▇█████▇████████████</td></tr><tr><td>train_f1</td><td>▁▁▁▁▁▂▂▆▇▆▅▆█▇▆▇▇▇▇▇█▇▇██▇▆█▇▇▇▇▇▇▇▇▇▇█▇</td></tr><tr><td>train_loss_epoch</td><td>█▇▇▇▇▇▅▃▅▃▄▃▃▂▃▃▄▂▁▃▂▂▂▂▂▂▁▃▁▂▁▂▂▂▂▁▂▂▂▁</td></tr><tr><td>train_loss_step</td><td>▇█▆▇▆▇▅▇█▄▃▄█▃▁▆▄▂▁▅▁▃▅▄▁█▂▆▆▅▄▄▄▆▃█▆▅▃▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▃▆█▇▆▆▆▇▇▇█▇█▇▇█▇█▇█▇▇█▇▇▇██▇▇▇▇▇█</td></tr><tr><td>val_auc</td><td>▁▇██████████████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▃▅██▇▆▆█▇▇▇▆█▇▇█▇███▆██▇█▇██▇▇▇▇▇█</td></tr><tr><td>val_loss_epoch</td><td>█▇▇▇▆▇▆▃▃▄▄▃▂▃▄▂▃▃▂▂▃▄▄▂▃▄▃▂▁▁▁▂▃▅▄▂▆▄▂▃</td></tr><tr><td>val_loss_step</td><td>█▇▇▇▅▇▆▂▄▅▅▃▂▄▅▃▃▃▂▂▄▅▅▃▄▅▄▂▁▁▁▃▄▇▅▂█▅▃▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.69013</td></tr><tr><td>train_auc</td><td>0.72627</td></tr><tr><td>train_f1</td><td>0.54374</td></tr><tr><td>train_loss_epoch</td><td>0.59419</td></tr><tr><td>train_loss_step</td><td>0.57958</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.70073</td></tr><tr><td>val_auc</td><td>0.77781</td></tr><tr><td>val_f1</td><td>0.57732</td></tr><tr><td>val_loss_epoch</td><td>0.55881</td></tr><tr><td>val_loss_step</td><td>0.55699</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_32_onehot_2</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/thft6ljs' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/thft6ljs</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231231_234220-thft6ljs\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240101_000208-br9phkge</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/br9phkge' target=\"_blank\">MLP_2_64_onehot_2</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/br9phkge' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/br9phkge</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_2_64_onehot_2\\mean\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 4.8 K \n",
      "2 | head        | Sequential       | 2.1 K \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "6.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "6.9 K     Total params\n",
      "0.028     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▃▄▄▅▆▇▇▇▇▆█▇▇▇▇▇▇██▇▇█▇█████▇▇▇▇▇▇█▇▇▇▇</td></tr><tr><td>train_auc</td><td>▁▂▂▄▄▆▆▇▇█▇▇▇█▇████▇████▇████▇██████▇███</td></tr><tr><td>train_f1</td><td>▄▂▁▁▄▄▆▇▇▇▇▇██▇█▇▇▇▇▇▇█▇▇▇▇▇▇▇█▇▇█▇▇▇▇▇▇</td></tr><tr><td>train_loss_epoch</td><td>█▇▆▆▆▅▄▃▃▂▄▃▂▂▂▃▃▂▁▃▂▁▂▂▂▂▁▂▁▂▂▁▂▂▁▃▂▁▂▂</td></tr><tr><td>train_loss_step</td><td>▇█▅▇▅▅▆▅▄▃▅▅▄▅▆█▆▄▃▂▂█▄▂▃▁▃▆▇█▅▆▅▆▅▄▄▅▅▅</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▃▇▆▆▇▇▇▆▆▇▇▇██▇▇▇▇▇▇█▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>val_auc</td><td>▁▁▁▂▃▄▄▅▅▅▆▇▇▇▇▇▇▇▇▇▇▆▆▆▇▇▇██▇▇▇▇▇▇▇▇██▇</td></tr><tr><td>val_f1</td><td>▁▁▁▃▆▆▆▇▇▇▆▆▆▇▇██▇▇▇▇▇▇█▇▇▇▇▇▇▇▇▇█▇▇▆▇▇▇</td></tr><tr><td>val_loss_epoch</td><td>█▇▇▆▆▅▄▄▃▄▅▂▁▂▃▄▅▂▃▄▃▂▁▅▂▅▄▅▆▄▄▅▄▃▃▇▅▃▅▄</td></tr><tr><td>val_loss_step</td><td>▇▆▆▆▆▅▄▅▄▄▆▂▁▃▄▄▆▃▄▅▄▂▁▆▂▇▅▆▇▄▅▆▄▄▄█▅▃▆▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.68647</td></tr><tr><td>train_auc</td><td>0.7318</td></tr><tr><td>train_f1</td><td>0.5469</td></tr><tr><td>train_loss_epoch</td><td>0.60034</td></tr><tr><td>train_loss_step</td><td>0.61572</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.70073</td></tr><tr><td>val_auc</td><td>0.77566</td></tr><tr><td>val_f1</td><td>0.60194</td></tr><tr><td>val_loss_epoch</td><td>0.55938</td></tr><tr><td>val_loss_step</td><td>0.55179</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_64_onehot_2</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/br9phkge' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/br9phkge</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240101_000208-br9phkge\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240101_002126-1c4h4ith</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/1c4h4ith' target=\"_blank\">MLP_2_64_onehot_2</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/1c4h4ith' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/1c4h4ith</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_2_64_onehot_2\\max\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 4.8 K \n",
      "2 | head        | Sequential       | 2.1 K \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "6.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "6.9 K     Total params\n",
      "0.028     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\torch_geometric\\warnings.py:17: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▃▄▄▄▅▆▇▆▆▇▇▆▇▇▇▇█▇█▇█▆▇▆▇█▇▇▇█▇▇▇████▇▇</td></tr><tr><td>train_auc</td><td>▁▂▂▃▃▄▆▆▆▆▇▆▆▇▆▇▇▇▇▇▇█▆▇▆▇▇▇▆▇▇▇▇▇▇▇███▇</td></tr><tr><td>train_f1</td><td>▄▃▃▁▅▄▆▆▆▇▇▇▇▇▆█▇▇▇▇▇██▇▇▇▇█▇▇▇▇▇▇▇▇██▆▇</td></tr><tr><td>train_loss_epoch</td><td>█▅▅▅▅▄▄▃▂▃▃▃▃▂▂▃▃▃▂▂▃▂▄▂▂▂▁▂▂▃▂▁▂▂▂▃▁▂▃▂</td></tr><tr><td>train_loss_step</td><td>▇▆▄▆▅▅▄▇▄▆▂▅▄▃▆▆▆▅▆▃▄▅▅▃▅▁▄▆▄▆▆▇█▃▅▃▄▃▅▅</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>▇▆▆▆▆▇█████▇██████▆▆▆▆▆▃▂▅▅▃▂▁▁▂▆▅▅▁▂▃▅▂</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_epoch</td><td>▁▁▁▁▁▂▂▃▂▃▃▄▅▄▄▄▄▅▆▅▇█▆▆▆▄▆█▆▇▅▅▇▆▅▆▆▆▅▄</td></tr><tr><td>val_loss_step</td><td>▁▁▁▁▁▁▂▃▂▂▃▄▅▃▃▃▄▄▆▄▆█▆▆▄▃▅█▅▆▄▄▆▅▂▃▅▄▃▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.6691</td></tr><tr><td>train_auc</td><td>0.69781</td></tr><tr><td>train_f1</td><td>0.52865</td></tr><tr><td>train_loss_epoch</td><td>0.61068</td></tr><tr><td>train_loss_step</td><td>0.63003</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.40511</td></tr><tr><td>val_auc</td><td>0.61206</td></tr><tr><td>val_f1</td><td>0.57662</td></tr><tr><td>val_loss_epoch</td><td>0.97519</td></tr><tr><td>val_loss_step</td><td>0.82439</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_64_onehot_2</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/1c4h4ith' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/1c4h4ith</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240101_002126-1c4h4ith\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240101_004100-bja1jyxd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/bja1jyxd' target=\"_blank\">MLP_2_64_onehot_2</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/bja1jyxd' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/bja1jyxd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_2_64_onehot_2\\sum\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 4.8 K \n",
      "2 | head        | Sequential       | 2.1 K \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "6.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "6.9 K     Total params\n",
      "0.028     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▄▃▄▅▄▆▆▇▇▇█▇▇█▇██████▇█▇▇███▆▇▇█▇██▇▇▇▇</td></tr><tr><td>train_auc</td><td>▆▇▆▇▇▇██▆▇▆▅▅▆▆▅▄▅▅▅▃▄▂▂▂▄▃▃▃▅▅▅▃▃▂▁▄▅▃▂</td></tr><tr><td>train_f1</td><td>▂▁▁▃▃▃▅▅▆▆▅▆▇▇▇▆▆▇▇▇▆▇█▇▆▆▇▇▇▇█▇▇▇▅▆▄▄▄█</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▂▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▅▁▇█▆▆█▇█████▇█▇████▇██▇▇██▇▇█▇▇▇▇▇▇█▇▇▇</td></tr><tr><td>val_auc</td><td>▇█▆█████▇▇▇▄▅▅▅▆▇▆▇▅▄▃▂▂▂▂▂▂▂▃▅▅▂▁▁▁▄▃▁▁</td></tr><tr><td>val_f1</td><td>▁▇█▇▅▅█▇▇██▇▇██████▇█████▇▇▇▇▇▇▇▇██████▆</td></tr><tr><td>val_loss_epoch</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_step</td><td>█▂▁▁▂▁▁▂▁▂▂▁▁▁▁▁▂▁▁▂▁▁▁▁▁▂▁▁▂▁▂▁▁▁▁▂▂▁▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.65265</td></tr><tr><td>train_auc</td><td>0.43981</td></tr><tr><td>train_f1</td><td>0.64083</td></tr><tr><td>train_loss_epoch</td><td>0.63732</td></tr><tr><td>train_loss_step</td><td>0.62348</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.70803</td></tr><tr><td>val_auc</td><td>0.23794</td></tr><tr><td>val_f1</td><td>0.51807</td></tr><tr><td>val_loss_epoch</td><td>0.62461</td></tr><tr><td>val_loss_step</td><td>0.6925</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_64_onehot_2</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/bja1jyxd' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/bja1jyxd</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240101_004100-bja1jyxd\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240101_010008-2q22t0qv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/2q22t0qv' target=\"_blank\">MLP_2_64_onehot_2</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/2q22t0qv' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/2q22t0qv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\torch_geometric\\deprecation.py:22: UserWarning: 'nn.glob.GlobalAttention' is deprecated, use 'nn.aggr.AttentionalAggregation' instead\n",
      "  warnings.warn(out)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_2_64_onehot_2\\attention\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 4.8 K \n",
      "2  | head        | Sequential       | 2.1 K \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 65    \n",
      "--------------------------------------------------\n",
      "7.0 K     Trainable params\n",
      "0         Non-trainable params\n",
      "7.0 K     Total params\n",
      "0.028     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▃▄▅▆▇▇▇▆▆▇████▇▇▇▇▇▇█▇▇▇█████▇▇██▇██▇▇█</td></tr><tr><td>train_auc</td><td>▁▂▂▅▅▇▇▇▇▇▇▇▇▇█▇▇▇█████▇▇█████▇███████▇█</td></tr><tr><td>train_f1</td><td>▄▁▂▄▅▇▇▇▅▅▆▇███▇█▇▆▇▇▇▇▆▇█████▆▇█▇▇██▇▇▇</td></tr><tr><td>train_loss_epoch</td><td>█▇▇▆▅▃▃▃▄▄▄▃▂▃▂▂▃▃▃▂▂▁▂▃▂▁▂▂▁▂▃▃▂▂▂▂▂▂▂▂</td></tr><tr><td>train_loss_step</td><td>▇█▇▇▅▄▆▄▄▆▆▆▃▅▅▅▃▇▃█▃█▄▅▄▇▆▆▆▅▆▄▃▂▅▃▂▁▃▄</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▅▆▄▆▅▇▇▆▅▅▅▆▄▆▇▇████▇█▇▇▇▆▇▇▇▇▇▇▇▇▆▆▇</td></tr><tr><td>val_auc</td><td>▂▁▁▁▃▅▆▆▆▆▇▇▇▇▇▇▇▇▇█▇▇▆▆▆▇▇▇▇▇▇▇█▇▇▇█▆▆▆</td></tr><tr><td>val_f1</td><td>▁▁▁▆▆▄▅▅▇▇▆▅▅▅▆▄▆█▇█████▇▇▇▇▆▇██▇███▇██▇</td></tr><tr><td>val_loss_epoch</td><td>██▇▆▅▆▄▄▃▅▄▅▂▂▆▄▅▂▃▄▁▁▃▄▂▄▅▄▃▄▄▃▆▂▂▄▂▅▆▆</td></tr><tr><td>val_loss_step</td><td>▇▇▆▅▅▆▄▄▃▅▄▅▂▂▇▄▅▂▄▄▁▁▄▅▃▅▆▅▄▅▅▄█▃▂▅▂▆██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.69561</td></tr><tr><td>train_auc</td><td>0.74153</td></tr><tr><td>train_f1</td><td>0.59141</td></tr><tr><td>train_loss_epoch</td><td>0.58658</td></tr><tr><td>train_loss_step</td><td>0.54899</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.71168</td></tr><tr><td>val_auc</td><td>0.78898</td></tr><tr><td>val_f1</td><td>0.59067</td></tr><tr><td>val_loss_epoch</td><td>0.64271</td></tr><tr><td>val_loss_step</td><td>0.73974</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_64_onehot_2</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/2q22t0qv' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/2q22t0qv</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240101_010008-2q22t0qv\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240101_011949-xlwaoty8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/xlwaoty8' target=\"_blank\">MLP_2_64_onehot_2</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/xlwaoty8' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/xlwaoty8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_2_64_onehot_2\\attention2\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 4.8 K \n",
      "2  | head        | Sequential       | 2.1 K \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 4.2 K \n",
      "--------------------------------------------------\n",
      "11.1 K    Trainable params\n",
      "0         Non-trainable params\n",
      "11.1 K    Total params\n",
      "0.045     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▃▃▆▆▅▇▇▇▇▇▇▇▇▇▇▆█▇▇▇▇█▇██▇▇▇█▇▇█▇▇▇▇▇▇</td></tr><tr><td>train_auc</td><td>▁▁▂▂▆▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█████████▇███▇████</td></tr><tr><td>train_f1</td><td>▅▄▁▃▅▅▄█▇██▇██▇▇▆▆▇▇▇▇▇█▇██▇▇█████▇▆▇█▇█</td></tr><tr><td>train_loss_epoch</td><td>█▇▇▇▄▅▄▄▃▃▄▃▃▂▃▃▃▄▃▃▄▂▂▂▂▂▁▂▂▂▂▂▃▂▂▃▂▃▂▁</td></tr><tr><td>train_loss_step</td><td>█▇▇▆▅▅▆▅▆▅▂▄▇▆▆▃▅▅▆▆▄▄▄▆▄█▆▆▃▅▆▅▄▁█▅▅▄▃▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▃▅█▄▆▆▆▆▇▇▇▇▇▆▇▇▇▇▇▇▇▇▇█▆▇██▇▇▇▇▇▇▇█▇</td></tr><tr><td>val_auc</td><td>▄▃▂▁▁▆▇▇▆▇▇▇▇▇▇▆▆▇▇▇▇██████▇▇▇▇▇███████▇</td></tr><tr><td>val_f1</td><td>▁▁▁▃▆█▄▅▆▅▆▆▇▆▇▇█▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇██▇▇█▇</td></tr><tr><td>val_loss_epoch</td><td>█▇█▇▅▆▆▃▃▅▄▃▅▆▅▄▃▅▄▃▆▂▃▅▄▂▄▃▂▄▃▆▄▂▆▆▅▁▄▃</td></tr><tr><td>val_loss_step</td><td>▇▆▇▇▅▆▆▃▄▆▄▃▇▇▆▅▂▆▅▃▇▂▃▇▄▃▅▃▂▅▄█▄▂▇▇▆▁▄▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.69013</td></tr><tr><td>train_auc</td><td>0.73646</td></tr><tr><td>train_f1</td><td>0.59882</td></tr><tr><td>train_loss_epoch</td><td>0.58893</td></tr><tr><td>train_loss_step</td><td>0.54344</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.69343</td></tr><tr><td>val_auc</td><td>0.78406</td></tr><tr><td>val_f1</td><td>0.56701</td></tr><tr><td>val_loss_epoch</td><td>0.53626</td></tr><tr><td>val_loss_step</td><td>0.51494</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_64_onehot_2</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/xlwaoty8' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/xlwaoty8</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240101_011949-xlwaoty8\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240101_013951-l03rbrr3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/l03rbrr3' target=\"_blank\">MLP_3_16_onehot_2</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/l03rbrr3' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/l03rbrr3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_3_16_onehot_2\\mean\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 704   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "858       Trainable params\n",
      "0         Non-trainable params\n",
      "858       Total params\n",
      "0.003     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▂▂▂▂▂▂▂▂▃▃▅▆▇▇▇▇▇▆▆▆▇▇▇▇██▇█▇▇▇▇▇▇▆▇▇▇</td></tr><tr><td>train_auc</td><td>▁▂▃▂▂▃▂▂▂▂▄▄▆▇▇▇██▇███▇▇████▇███████▇█▇█</td></tr><tr><td>train_f1</td><td>▃▂▁▁▁▁▁▁▁▁▂▂▅▆▇▇▇█▇▆▇▇▇▇▇█▇▇▇█▇▇█▇▇▇▇▇▇▇</td></tr><tr><td>train_loss_epoch</td><td>█▇▇▇▆▆▇▇▆▆▆▆▄▃▁▂▂▃▃▂▂▃▂▃▂▃▂▂▃▂▂▃▂▃▁▂▂▁▁▂</td></tr><tr><td>train_loss_step</td><td>▇█▅▇▇█▆▇▇▇▆▆▆▅▇▂▄▁▅▆▅▅▄▆▄▆▆▇▅▅▆▆▅▃▅▃▄▃▆▅</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▂▆▇▇█▇▆▇▇██▆▇▇█▇▇▇█▇▇▇▇▆█▇▇▇█</td></tr><tr><td>val_auc</td><td>▁▄▅▆▆▇▇█████████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▂▅▇▇█▇▆█▇██▆█▇███▇█▇▇█▇▇█▇███</td></tr><tr><td>val_loss_epoch</td><td>██▇▇▇▇▇▆█▇▇▇▅▃▁▅▄▂▄▃▄▄▃▄▃▃▄▃▃▃▄▄▄▅▃▇▃▅▃▄</td></tr><tr><td>val_loss_step</td><td>▆▇▆▆▆▅▆▅▇▆▆▆▅▃▁▅▄▂▄▃▅▄▃▄▃▄▄▃▃▃▄▅▄▅▃█▃▅▃▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.67733</td></tr><tr><td>train_auc</td><td>0.71285</td></tr><tr><td>train_f1</td><td>0.52362</td></tr><tr><td>train_loss_epoch</td><td>0.61176</td></tr><tr><td>train_loss_step</td><td>0.61733</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.70438</td></tr><tr><td>val_auc</td><td>0.77522</td></tr><tr><td>val_f1</td><td>0.58462</td></tr><tr><td>val_loss_epoch</td><td>0.57541</td></tr><tr><td>val_loss_step</td><td>0.5804</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_16_onehot_2</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/l03rbrr3' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/l03rbrr3</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240101_013951-l03rbrr3\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240101_015935-qie0gkw0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/qie0gkw0' target=\"_blank\">MLP_3_16_onehot_2</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/qie0gkw0' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/qie0gkw0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_3_16_onehot_2\\max\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 704   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "858       Trainable params\n",
      "0         Non-trainable params\n",
      "858       Total params\n",
      "0.003     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\torch_geometric\\warnings.py:17: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▂▁▄▄▅▅▅▅▅▅▆▆▆▅▆▆▆▆▇▇▇▆▇▇▆█▇▇▇▇▇▇█▆▅▆▇█▇▇</td></tr><tr><td>train_auc</td><td>▁▁▃▂▁▂▂▃▂▃▃▃▄▃▄▄▄▅▄▆▆▆▅▅▆▆▆▅▆▆▆█▇▆▆▅▆▆▇▆</td></tr><tr><td>train_f1</td><td>▇▆▂▁▁▁▁▂▄▃▅▅▅▅▅▆▄▆▅▇▇▆▇▅▇██▇▆▇▇▇██▅▆█▇▇▇</td></tr><tr><td>train_loss_epoch</td><td>█▅▄▄▃▃▄▃▃▃▂▃▂▃▂▃▂▃▂▂▂▃▂▂▂▁▁▂▂▂▂▁▂▃▁▂▂▁▂▁</td></tr><tr><td>train_loss_step</td><td>██▃▆▆▇▄▄▅▆▄▅▄▄▅▂▃▂▃▅▆▃▇▅▆▃▆▆▅▄▄█▅▁▄▄▃▄▄▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>▃▄▂▂▂▂▃▇▄▅▁▁▆▆▅▅▃▂▅▅▅▇▇▇▆▄▃▆▅▅▅▃▃█▇▅▂▂▂▄</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_epoch</td><td>▆▇▅▄▄▃▅▁▇▄▆▆▂▃▂▃▂▄▅▁▄▃▂▅▃▅▄▃▄▅█▄▄▅▄▆▃▄▂▄</td></tr><tr><td>val_loss_step</td><td>▆▆▅▄▄▃▄▁▇▄▆▆▂▃▂▃▂▄▅▁▄▃▂▅▃▆▄▃▄▅█▄▄▅▄▆▃▄▂▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.60878</td></tr><tr><td>train_auc</td><td>0.61487</td></tr><tr><td>train_f1</td><td>0.29836</td></tr><tr><td>train_loss_epoch</td><td>0.65549</td></tr><tr><td>train_loss_step</td><td>0.63576</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.59489</td></tr><tr><td>val_auc</td><td>0.63771</td></tr><tr><td>val_f1</td><td>0.0</td></tr><tr><td>val_loss_epoch</td><td>0.66106</td></tr><tr><td>val_loss_step</td><td>0.65209</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_16_onehot_2</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/qie0gkw0' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/qie0gkw0</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240101_015935-qie0gkw0\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240101_021930-mm4f325v</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/mm4f325v' target=\"_blank\">MLP_3_16_onehot_2</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/mm4f325v' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/mm4f325v</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_3_16_onehot_2\\sum\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 704   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "858       Trainable params\n",
      "0         Non-trainable params\n",
      "858       Total params\n",
      "0.003     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▄▂▁▂▄▃▃▅▅▄▄▅▅▅▅▅▅▅▇▆▆▆▆▇▆▆▆▇▇▇█▇▇▇▇█▇█▇</td></tr><tr><td>train_auc</td><td>▃▆█▄▃▇▅▆▆▃▃▇▆▃▅▅▅▄▆▅▃▅▃▃▆▄▅▂▃▂▁▄▃▃▃▂▁▃▂▃</td></tr><tr><td>train_f1</td><td>▃▂▆▆▅█▇▅▅▄▃▅▄▂▇▄▂▃▄▄▄▄▃▁▅▄▃▂▆▇▅▅▄▇▄▃▅▁▇▇</td></tr><tr><td>train_loss_epoch</td><td>█▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▂▂▂▂▂▁▁▁▁▂▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▃▃▂▃▃▂▃█</td></tr><tr><td>val_auc</td><td>███▂▂▂▂▁▁▂▂▁▁▂▂▂▁▂▁▁▁▁▂▁▁▁▁▁▁▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_f1</td><td>▁▁▁▂▂▂▂▂▁▁▁▁▂▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▃▃▂▃▃▂▃█</td></tr><tr><td>val_loss_epoch</td><td>█▇▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_step</td><td>██▄▂▂▁▁▁▂▁▂▂▁▁▁▁▁▂▂▁▁▁▂▂▁▂▁▂▁▂▂▁▁▁▁▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.60146</td></tr><tr><td>train_auc</td><td>0.45302</td></tr><tr><td>train_f1</td><td>0.44949</td></tr><tr><td>train_loss_epoch</td><td>0.65698</td></tr><tr><td>train_loss_step</td><td>0.64715</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.62774</td></tr><tr><td>val_auc</td><td>0.25192</td></tr><tr><td>val_f1</td><td>0.16393</td></tr><tr><td>val_loss_epoch</td><td>0.57272</td></tr><tr><td>val_loss_step</td><td>0.53099</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_16_onehot_2</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/mm4f325v' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/mm4f325v</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240101_021930-mm4f325v\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240101_023843-gmnakx2e</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/gmnakx2e' target=\"_blank\">MLP_3_16_onehot_2</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/gmnakx2e' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/gmnakx2e</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\torch_geometric\\deprecation.py:22: UserWarning: 'nn.glob.GlobalAttention' is deprecated, use 'nn.aggr.AttentionalAggregation' instead\n",
      "  warnings.warn(out)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_3_16_onehot_2\\attention\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 704   \n",
      "2  | head        | Sequential       | 154   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 17    \n",
      "--------------------------------------------------\n",
      "875       Trainable params\n",
      "0         Non-trainable params\n",
      "875       Total params\n",
      "0.003     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▂▁▂▂▂▂▂▂▂▂▃▄▅▃▆▄▇▆▆▇▆▇▇▇▆▇▇▆▇█▇▇▄█▇▇▆█▆▇</td></tr><tr><td>train_auc</td><td>▁▁▁▂▃▂▃▃▃▃▅▆▇▆▇▇▇▇▇▇▇▇▇▇▇██▇████▇██▇▇███</td></tr><tr><td>train_f1</td><td>▃▁▁▁▁▁▁▁▁▃▄▃▅▂▇▄▇▇▆▇▇▇▇█▇▆▇▇▇▇█▇▅▇▇▇▆█▇▆</td></tr><tr><td>train_loss_epoch</td><td>██▇▇▇▇▆▇▆▆▆▄▃▆▃▃▄▂▃▃▃▂▃▂▂▂▂▃▂▁▁▁▂▁▁▃▃▂▁▃</td></tr><tr><td>train_loss_step</td><td>█▅▆▆▆▆▆▅▇▆▆▄▃▄▄▄▄▂▅▇▄▃▄▇▅▃▂▅▄█▄▃▄▇▁▁▃▃▁▇</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▂▂▆▂▆▅▆▆▆▆▇▇▆▇▇▇▇▆▇▇▇▇▇▇▇▇▇▇▇█</td></tr><tr><td>val_auc</td><td>▁▄▅▆▇▇▇▇████████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▂▂▅▂▅▄▅▅▅▅▆▆▅▆▇▇▇▆▇▇▆▇▇▇▇▇▇▆▆█</td></tr><tr><td>val_loss_epoch</td><td>▇▇██▇▇█▇█▆▇▇▇▆▇▆▆▇▅▄▆▄▁▃▄▄▄▁▄▃▃▄▄▅▃▄▃▃▅▆</td></tr><tr><td>val_loss_step</td><td>▇▆▇▇▆▇▇▇▇▆▇▇▇▆▇▆▆█▅▄▇▅▁▄▄▅▄▂▅▄▃▄▅▆▄▄▄▄▅▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.67642</td></tr><tr><td>train_auc</td><td>0.71689</td></tr><tr><td>train_f1</td><td>0.45031</td></tr><tr><td>train_loss_epoch</td><td>0.62879</td></tr><tr><td>train_loss_step</td><td>0.70517</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.71168</td></tr><tr><td>val_auc</td><td>0.77571</td></tr><tr><td>val_f1</td><td>0.64574</td></tr><tr><td>val_loss_epoch</td><td>0.61736</td></tr><tr><td>val_loss_step</td><td>0.65411</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_16_onehot_2</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/gmnakx2e' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/gmnakx2e</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240101_023843-gmnakx2e\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240101_025842-2k2bsjxs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/2k2bsjxs' target=\"_blank\">MLP_3_16_onehot_2</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/2k2bsjxs' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/2k2bsjxs</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_3_16_onehot_2\\attention2\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 704   \n",
      "2  | head        | Sequential       | 154   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 281   \n",
      "--------------------------------------------------\n",
      "1.1 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.1 K     Total params\n",
      "0.005     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▂▄▅▆▇▅▆▇▇▆▇█▆▇▇▆▇█▇██▇█▇▇▇▇█▇</td></tr><tr><td>train_auc</td><td>▂▂▂▂▁▂▂▂▂▂▃▄▆▇▇▇▇▇▇▇█▇█▇▇█▇▇█▇███▇▇██▇██</td></tr><tr><td>train_f1</td><td>▂▂▁▁▁▁▁▁▁▁▁▂▅▇▇▇▅▆▇▇▆▇█▆▇▇▆▇▇▇█▇▇▇▇▇▇▇██</td></tr><tr><td>train_loss_epoch</td><td>█▇█▇▇█▇▇▇▇▇▆▄▆▄▄▄▃▃▃▃▃▅▂▂▃▃▂▂▄▃▁▁▃▂▁▁▃▂▁</td></tr><tr><td>train_loss_step</td><td>▇▇█▆▅▆▆▆▇▇▇▆▄▄▃▅▄▄▃▄▄▆▅▅▆▃▁▃▃▄█▃▅▇▆▇▃▄▇▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▂▄▆▆█▇▇██▇▆▇▆██▇██▇██▇████▇█▇</td></tr><tr><td>val_auc</td><td>▁▄▆▆▇▇▇█████████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▂▄▅▅█▇▆▇█▇▆█▆▇█▇▇█▇▇█▇▇█▇▇▇█▇</td></tr><tr><td>val_loss_epoch</td><td>▇█▇▇▇▇▆▆▇▇▇▇▆▃▅▅▄▁▄▅▃▅▂▅▃▃▁▃▅▂▅▅▂▆▅▃▃▁▂▄</td></tr><tr><td>val_loss_step</td><td>▆█▇▇▇▆▆▆▇▇▇▇▆▄▆▆▄▁▅▆▃▆▁▆▃▄▁▃▆▂▆▆▃█▆▄▃▁▃▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.67185</td></tr><tr><td>train_auc</td><td>0.71706</td></tr><tr><td>train_f1</td><td>0.55293</td></tr><tr><td>train_loss_epoch</td><td>0.60129</td></tr><tr><td>train_loss_step</td><td>0.54815</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.69708</td></tr><tr><td>val_auc</td><td>0.77328</td></tr><tr><td>val_f1</td><td>0.53107</td></tr><tr><td>val_loss_epoch</td><td>0.57562</td></tr><tr><td>val_loss_step</td><td>0.57286</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_16_onehot_2</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/2k2bsjxs' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/2k2bsjxs</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240101_025842-2k2bsjxs\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240101_031926-lupwn55b</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/lupwn55b' target=\"_blank\">MLP_3_32_onehot_2</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/lupwn55b' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/lupwn55b</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_3_32_onehot_2\\mean\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 2.4 K \n",
      "2 | head        | Sequential       | 562   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "3.0 K     Trainable params\n",
      "0         Non-trainable params\n",
      "3.0 K     Total params\n",
      "0.012     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▂▂▂▂▂▂▂▃▆▆▅▆▆▇▆▆▆▅▇▇▇▇▇▇▇▇█▇▆██▇▇▇▇█▇▇</td></tr><tr><td>train_auc</td><td>▂▃▁▂▃▂▂▃▃▄▇▆▆▅▇█▇▇▇▆▇█▇██▇█▇█▇▇▇████████</td></tr><tr><td>train_f1</td><td>▃▁▁▁▁▁▁▁▁▂▇▇▅▇▇█▇██▅█▇▇██▇▇▇██▇███▇▇████</td></tr><tr><td>train_loss_epoch</td><td>█████▇███▇▄▆▄▄▂▃▃▄▄▄▂▃▃▃▃▂▃▂▁▃▂▃▂▂▂▃▂▂▁▂</td></tr><tr><td>train_loss_step</td><td>▅▆▆▅█▆▆▄▆▄▄▄▂▁▄▂▅█▃▂▃▃▂▆▁▃▁▂▂▃▃▄▄▄▃▃▄▂▅▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▅▆▅▆▆█▇▇▇█▇█▇███▇▇▇▇▇▇▇▇█▇▇▇▇▇▇</td></tr><tr><td>val_auc</td><td>▄▄▁▅▇▇▇▇████████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▄▆█▆▆▇▇▇▇█▇▇▇▇▇▇▇▇▆▆▇▇▇▇█▇▇▇▇▇▇</td></tr><tr><td>val_loss_epoch</td><td>▇▆█▇▇▇█▇▇▅▃▅▅▂▄▁▂▃▅▅▂▂▃▄▃▄▄▃▄▅▅▂▃▃▅▅▄▁▂▃</td></tr><tr><td>val_loss_step</td><td>▆▆▇▇▆▇█▇▇▅▄▅▅▂▆▂▂▃▆▆▃▂▄▆▃▅▅▄▅▆▆▃▄▄▆▆▅▁▂▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.68007</td></tr><tr><td>train_auc</td><td>0.70852</td></tr><tr><td>train_f1</td><td>0.57729</td></tr><tr><td>train_loss_epoch</td><td>0.60463</td></tr><tr><td>train_loss_step</td><td>0.60404</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.69708</td></tr><tr><td>val_auc</td><td>0.77323</td></tr><tr><td>val_f1</td><td>0.58291</td></tr><tr><td>val_loss_epoch</td><td>0.54389</td></tr><tr><td>val_loss_step</td><td>0.5183</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_32_onehot_2</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/lupwn55b' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/lupwn55b</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240101_031926-lupwn55b\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240101_033937-c262ofeb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/c262ofeb' target=\"_blank\">MLP_3_32_onehot_2</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/c262ofeb' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/c262ofeb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_3_32_onehot_2\\max\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 2.4 K \n",
      "2 | head        | Sequential       | 562   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "3.0 K     Trainable params\n",
      "0         Non-trainable params\n",
      "3.0 K     Total params\n",
      "0.012     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\torch_geometric\\warnings.py:17: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▄▃▄▃▄▄▄▄▄▄▅▄▅▃▆▅▇▁▄▅▃▄▅▅▆▆▆▅▄▇▅█▇▆▆▇▆██</td></tr><tr><td>train_auc</td><td>▃▂▁▂▃▃▄▅▄▅▄▆▄▆▅▅▅▅▄▆▆▆▆▅▇▆▆▆▆▆▇▇▇▇▇▇▇█▇█</td></tr><tr><td>train_f1</td><td>▅▂▁▂▁▂▂▃▂▄▅▅▆▃▅▅▅▇▅▃▆▄▇▆▅▆▆▆▆▃▇▆▆▇▄▇█▇▆▇</td></tr><tr><td>train_loss_epoch</td><td>█▆▇▆▆▅▅▅▆▄▅▆▅▃▅▄▅▆▅▅▄▄▄▆▅▃▅▃▄▄▃▄▂▅▃▅▃▁▂▃</td></tr><tr><td>train_loss_step</td><td>▄▅▅▃▇▆▅▃▄▃▆▅▃▄▃▅▅█▃▅▁▅▅▆▄▄▃▄▁▄▅▆▆▂▄▅▄▄▅▅</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>████▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>▇▆▅▄▆▇▆▅▄▅▅▅▅▃▅▄▇▅▅▅▇▅▅▅▅▅▅▅▁▁█▁▂▁▃▆▅▃▃▃</td></tr><tr><td>val_f1</td><td>▁▁▁▂████████████████████████████████████</td></tr><tr><td>val_loss_epoch</td><td>▁▁▁▁▁▂▁▂▂▃▃▂▂▃▂▃▃▃▂▂▃▄▄▄▃▄▃▄▃▃▄▄▅▄▅▄▇▆██</td></tr><tr><td>val_loss_step</td><td>▁▁▂▂▂▂▁▂▂▃▃▂▂▃▂▃▃▂▁▂▃▄▃▃▃▃▂▃▂▂▃▄▄▂▄▃▇▆█▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.62706</td></tr><tr><td>train_auc</td><td>0.62353</td></tr><tr><td>train_f1</td><td>0.41379</td></tr><tr><td>train_loss_epoch</td><td>0.65802</td></tr><tr><td>train_loss_step</td><td>0.67693</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.40511</td></tr><tr><td>val_auc</td><td>0.43851</td></tr><tr><td>val_f1</td><td>0.57662</td></tr><tr><td>val_loss_epoch</td><td>0.95132</td></tr><tr><td>val_loss_step</td><td>1.00016</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_32_onehot_2</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/c262ofeb' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/c262ofeb</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240101_033937-c262ofeb\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240101_035939-nitolkk6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/nitolkk6' target=\"_blank\">MLP_3_32_onehot_2</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/nitolkk6' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/nitolkk6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_3_32_onehot_2\\sum\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 2.4 K \n",
      "2 | head        | Sequential       | 562   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "3.0 K     Trainable params\n",
      "0         Non-trainable params\n",
      "3.0 K     Total params\n",
      "0.012     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▃▁▁▃▃▃▃▃▄▄▅▅▆▅▅▆▅▆▅▆▆▆▇▇▆▆█▇▇▇▇▇▇▇███▇██</td></tr><tr><td>train_auc</td><td>██▄▇▇▇▆▅▆▆▆▅▅▄▄▅▄▅▃▂▄▄▃▂▃▃▂▁▂▃▂▂▂▂▁▂▂▂▂▂</td></tr><tr><td>train_f1</td><td>▅▄▅▄▄▂▃▂▁▅▄▃▅▅▃▆▆▇▇▆▆▆▇▇▇█▇▆▆▆▆▇██▇▇▇▇██</td></tr><tr><td>train_loss_epoch</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▃▂▂▂▂▂▁▂▂▂▂▁▁▂▁▂▂▂▁▁▂▁▂▁▁▁▁▁▁▁▂▁▁▁▂▂▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▄▄▄▄▅▄▇▁▆█▁▄█▃▄▇▅▅▅▃▄▅▅▅▆▆▄▇▇▅▆▆▆▄▄▃▆█▇▅</td></tr><tr><td>val_auc</td><td>█▅▆▆▅▆▅▂▁▁▂▁▁▁▂▁▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁</td></tr><tr><td>val_f1</td><td>▁▁▁▁█▂▇▇██▇█▇▇█████▇█████████████████▇██</td></tr><tr><td>val_loss_epoch</td><td>▅▆▆▆█▆▇█▇▆▇▇▆▇▆▄▅▄▆▇▅▆▅▅▄▄▅▄▄▅▄▄▅▆▅█▆▁▂▅</td></tr><tr><td>val_loss_step</td><td>▅▄▇▆▇▆▇█▇▆▇▇▇▆▆▄▅▃▆▇▅▆▅▆▄▅▅▄▅▆▅▄▆▅▅██▁▂▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.67367</td></tr><tr><td>train_auc</td><td>0.31525</td></tr><tr><td>train_f1</td><td>0.60639</td></tr><tr><td>train_loss_epoch</td><td>0.60712</td></tr><tr><td>train_loss_step</td><td>0.62376</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.62044</td></tr><tr><td>val_auc</td><td>0.24772</td></tr><tr><td>val_f1</td><td>0.65789</td></tr><tr><td>val_loss_epoch</td><td>0.61326</td></tr><tr><td>val_loss_step</td><td>0.59425</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_32_onehot_2</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/nitolkk6' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/nitolkk6</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240101_035939-nitolkk6\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240101_041904-qt88bxqp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/qt88bxqp' target=\"_blank\">MLP_3_32_onehot_2</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/qt88bxqp' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/qt88bxqp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\torch_geometric\\deprecation.py:22: UserWarning: 'nn.glob.GlobalAttention' is deprecated, use 'nn.aggr.AttentionalAggregation' instead\n",
      "  warnings.warn(out)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_3_32_onehot_2\\attention\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 2.4 K \n",
      "2  | head        | Sequential       | 562   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 33    \n",
      "--------------------------------------------------\n",
      "3.0 K     Trainable params\n",
      "0         Non-trainable params\n",
      "3.0 K     Total params\n",
      "0.012     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▂▂▂▂▂▂▃▆▆▇▇█▇▆▇▆█▇▇▇▇▇▆▇█▇██▇▇██▇▇▇█▇█</td></tr><tr><td>train_auc</td><td>▁▂▂▁▁▂▂▁▁▅▆▇▇▇▇▇▇▇▇▆█▇▇█▇█▇▇█▇▇████▇▇█▇█</td></tr><tr><td>train_f1</td><td>▃▁▁▁▁▁▁▁▃▇▇▆▆▇▇▆▇▇█▇▇▇▇▇▆▇▇▇▇██▇██▇▆▇█▇▇</td></tr><tr><td>train_loss_epoch</td><td>████▇█▇▇▆▄▄▂▃▃▃▂▂▂▃▃▂▃▂▂▂▁▂▃▂▂▃▁▂▂▂▂▁▃▂▁</td></tr><tr><td>train_loss_step</td><td>▆█▇▇▇▆▆█▆▆▄▄▄▆▇▅▄▄▅▅▇▆▄▅▄▄▁▇▄▇▆▅▇▅▃▄▅▅▅▄</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▂▅▄▆▇▆▆▆▆▆▆▆██▇▆▆▆█▇▆▇▆▇█▇▇▇▇▇█▆</td></tr><tr><td>val_auc</td><td>▇█▇▂▁▁▁▁████████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▂▅▄▆▆▆▆▆▆▆▅▆▇▇▆▆▆▆▇▇▆▇▇▆▇▇██▇▆▇▇</td></tr><tr><td>val_loss_epoch</td><td>█▅▆▇█▇▇▆▆▄▆▃▄▃▅▄▅█▅▂▄▃▅▃▂▃▆▄▃▄▂▅▄▆▁▃▄▆▅▃</td></tr><tr><td>val_loss_step</td><td>▇▃▅▆▆▆▅▅▅▃▆▃▄▃▆▄▅█▄▂▄▄▅▃▂▃▇▄▃▄▃▅▄▇▁▄▅▆▆▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.69013</td></tr><tr><td>train_auc</td><td>0.73011</td></tr><tr><td>train_f1</td><td>0.56031</td></tr><tr><td>train_loss_epoch</td><td>0.60156</td></tr><tr><td>train_loss_step</td><td>0.5837</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.67883</td></tr><tr><td>val_auc</td><td>0.77527</td></tr><tr><td>val_f1</td><td>0.5</td></tr><tr><td>val_loss_epoch</td><td>0.55023</td></tr><tr><td>val_loss_step</td><td>0.5307</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_32_onehot_2</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/qt88bxqp' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/qt88bxqp</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240101_041904-qt88bxqp\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240101_043905-stytkg08</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/stytkg08' target=\"_blank\">MLP_3_32_onehot_2</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/stytkg08' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/stytkg08</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_3_32_onehot_2\\attention2\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 2.4 K \n",
      "2  | head        | Sequential       | 562   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 1.1 K \n",
      "--------------------------------------------------\n",
      "4.1 K     Trainable params\n",
      "0         Non-trainable params\n",
      "4.1 K     Total params\n",
      "0.016     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▂▂▂▂▂▂▂▂▂▃▅▆▆▆▆▆▇▇▇▆▇▇▇▇▇▇▇▇▇▇▇█▆█▇▇▇█</td></tr><tr><td>train_auc</td><td>▁▂▂▂▃▂▂▂▂▂▂▃▆▇▇▇▇▇█▇██▇▇▇████▇████▇█▇███</td></tr><tr><td>train_f1</td><td>▃▁▁▁▁▁▁▁▁▁▁▃▆▇▅▇▇▇▇▇█▇█▇▇▇███▇▇█▇▇▇█▇█▇█</td></tr><tr><td>train_loss_epoch</td><td>██▇█▇█▇█▇█▇▆▅▄▄▃▄▄▄▃▂▃▄▃▃▂▁▂▃▃▂▄▁▂▃▃▃▂▂▂</td></tr><tr><td>train_loss_step</td><td>▆▆▅▇▆▆▅▆▆▅▆▄▃▅▄▄▂▆▂▅▃▆▆▃▁▃▁▃▃▄▁▃█▂▄▂▃▃▆▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▂█▆▇▇▇▇▇▇▇▇▇▇▇▇▆▇▇▇▇▇▇▇▇▆▇▇▇▇</td></tr><tr><td>val_auc</td><td>▆▁▅▇▅▆██████████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▂██▇▇▇▇▇▇▇▇▇█▇▇▆▇▇█▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>val_loss_epoch</td><td>█▇▇█▅▆▇▇▇▇▆▅▃▅▃▄▃▅▄▄▂▃▆▃▄▁▆▂▄▃▂▂▁▁▂▂▃▅▂▃</td></tr><tr><td>val_loss_step</td><td>█▆▇█▄▆▇▇▇▆▆▅▃▆▃▅▃▇▆▄▃▄█▄▄▁█▂▅▃▃▂▂▁▂▃▄▇▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.7011</td></tr><tr><td>train_auc</td><td>0.72192</td></tr><tr><td>train_f1</td><td>0.5866</td></tr><tr><td>train_loss_epoch</td><td>0.59584</td></tr><tr><td>train_loss_step</td><td>0.55905</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.70438</td></tr><tr><td>val_auc</td><td>0.77527</td></tr><tr><td>val_f1</td><td>0.53714</td></tr><tr><td>val_loss_epoch</td><td>0.56285</td></tr><tr><td>val_loss_step</td><td>0.55634</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_32_onehot_2</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/stytkg08' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/stytkg08</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240101_043905-stytkg08\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240101_045840-5uz14kvp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/5uz14kvp' target=\"_blank\">MLP_3_64_onehot_2</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/5uz14kvp' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/5uz14kvp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_3_64_onehot_2\\mean\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 9.0 K \n",
      "2 | head        | Sequential       | 2.1 K \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "11.1 K    Trainable params\n",
      "0         Non-trainable params\n",
      "11.1 K    Total params\n",
      "0.044     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▂▂▃▄▆▇▇▇▆▇▇█▇█▇▇▇▇▇▇▇█▇▆█▇▇▇█▇██▇▇▇█▇▇</td></tr><tr><td>train_auc</td><td>▂▁▁▃▅▆▇▇▇▇▇▇▇▇▇█████▇████████████▇██▇██▇</td></tr><tr><td>train_f1</td><td>▃▁▁▁▂▇▆▇▇██▆▇▇▇▇▇▇▇▇▇▇▇▇▇▆██▇████▇▇▆▇▇█▇</td></tr><tr><td>train_loss_epoch</td><td>███▇▆▆▄▃▂▄▄▃▃▃▂▁▁▃▂▂▃▂▂▂▂▂▂▃▃▂▄▂▂▃▃▃▂▁▂▃</td></tr><tr><td>train_loss_step</td><td>██▇▇▆▃▃▃▄▇▄▄▂█▁▂▇▆▄▄▂▇▂▆▅▄▄▃▄▃▆▄▇▄▂▃▄▅▄▅</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▂▇▇▇▆▇▇████████▇█▇▇▇█▇▇▇▇▇▇▇███▇█▇▇▇</td></tr><tr><td>val_auc</td><td>▁▃▄▆▇▇▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇██▇█████▇▇▇▇▇▇▇▇▇</td></tr><tr><td>val_f1</td><td>▁▁▁▁▂▆▇▇▆▆▇███▇█▇▇▇▇█▇▇▇█▆▆▇▇▇▇▇███▇▇▇▇▆</td></tr><tr><td>val_loss_epoch</td><td>▇▇▇█▇▄▄▄▄▅▄▄▃▅▂▇▆▂▃▃▄▁▄▆▆▅▃▃▄▃▃▃▃▅▂▁▁▂▃▂</td></tr><tr><td>val_loss_step</td><td>▅▅▅▆▅▃▄▄▄▅▃▄▃▅▂█▇▃▃▃▄▁▄▇▇▅▃▃▅▃▃▃▄▅▂▁▁▂▄▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.6691</td></tr><tr><td>train_auc</td><td>0.71814</td></tr><tr><td>train_f1</td><td>0.49441</td></tr><tr><td>train_loss_epoch</td><td>0.60905</td></tr><tr><td>train_loss_step</td><td>0.61156</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.70073</td></tr><tr><td>val_auc</td><td>0.77279</td></tr><tr><td>val_f1</td><td>0.5</td></tr><tr><td>val_loss_epoch</td><td>0.55142</td></tr><tr><td>val_loss_step</td><td>0.53105</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_64_onehot_2</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/5uz14kvp' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/5uz14kvp</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240101_045840-5uz14kvp\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240101_051818-8ptu96th</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/8ptu96th' target=\"_blank\">MLP_3_64_onehot_2</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/8ptu96th' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/8ptu96th</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_3_64_onehot_2\\max\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 9.0 K \n",
      "2 | head        | Sequential       | 2.1 K \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "11.1 K    Trainable params\n",
      "0         Non-trainable params\n",
      "11.1 K    Total params\n",
      "0.044     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\torch_geometric\\warnings.py:17: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▃▄▃▅▄▄▆▆▅▃▄▄▅▄▅▆▆▆▅▅▅▇▆▅▆▅▆▅▇▇▅█▅▆▇█▇█▆</td></tr><tr><td>train_auc</td><td>▁▁▂▄▅▅▅▆▆▆▅▆▆▆▅▆▆▆▆▆▆▇▇▇▇▇▇▆█▇▇▆█▆▇▇▇▆█▇</td></tr><tr><td>train_f1</td><td>▄▁▁▁▃▅▃▅▆▇▄▄▅▆▆▅▆▇▆▅▆▆▇▇▄▇▇▅▅▇█▇▇▆▇▆▆▆▇▅</td></tr><tr><td>train_loss_epoch</td><td>█▇▆▅▅▅▅▅▃▅▅▂▅▅▅▃▆▅▄▄▄▂▃▄▄▃▃▃▄▁▃▄▁▄▄▁▄▅▁▂</td></tr><tr><td>train_loss_step</td><td>▇▇▅▅▇▄▆▇▇▅▃▂▅▇▄▁▆▃▄█▆▅▇▄▇▆▆▄▃▄▆▅▆▇▃▅▃▅▅▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>███▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>▃▁▄▄▄▃▃▄▅▅▄▅█▅▃▄▅▂▃▃▃▃▃█▃▆▃▃▆▃▃▄▃▄▄▃▃▃▅▅</td></tr><tr><td>val_f1</td><td>▁▁▂█████████████████████████████████████</td></tr><tr><td>val_loss_epoch</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▄▄▃▄▄▄▄▄▃▄▄▃▅▄▅▄▅▇▅▄▄▆▇▆█▆▇█</td></tr><tr><td>val_loss_step</td><td>▂▂▂▂▂▃▃▂▃▂▃▃▅▅▃▄▃▅▄▅▃▄▅▃▆▃▆▃▅▇▃▁▂▇▇▆█▅▅█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.60878</td></tr><tr><td>train_auc</td><td>0.63197</td></tr><tr><td>train_f1</td><td>0.34154</td></tr><tr><td>train_loss_epoch</td><td>0.6544</td></tr><tr><td>train_loss_step</td><td>0.62084</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.40511</td></tr><tr><td>val_auc</td><td>0.48693</td></tr><tr><td>val_f1</td><td>0.57662</td></tr><tr><td>val_loss_epoch</td><td>1.02273</td></tr><tr><td>val_loss_step</td><td>1.04453</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_64_onehot_2</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/8ptu96th' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/8ptu96th</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240101_051818-8ptu96th\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240101_053740-cbvn4j2x</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/cbvn4j2x' target=\"_blank\">MLP_3_64_onehot_2</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/cbvn4j2x' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/cbvn4j2x</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_3_64_onehot_2\\sum\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 9.0 K \n",
      "2 | head        | Sequential       | 2.1 K \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "11.1 K    Trainable params\n",
      "0         Non-trainable params\n",
      "11.1 K    Total params\n",
      "0.044     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▂▃▃▄▄▅▅▆▅▆▇▆▆▇▆▇█▇██▇▇▇▇▇▇▇▇▇▇▇▇█▇▇███</td></tr><tr><td>train_auc</td><td>▃▁▃▃▄▄▃▄▅▄▅▅▅▆▆▆▅▆▆▅▆▅▄▆▆▆▆▆▇▆▆▆▆▅▆▅▆███</td></tr><tr><td>train_f1</td><td>▄▄▅▂▁▃▂▂▄▆▆▆▆▆▇▇▆▆▆▆▇▆▆▇▅▆▇▇▇▇███▇▇▄▆▇▇▇</td></tr><tr><td>train_loss_epoch</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▅▂▂▂▂▂▁▂▂▁▁▁▂▁▂▂▁▁▁▁▂▁▂▂▂▁▂▁▂▁▂▂▂▁▂▁▂▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▂▂▁▃▄▄▃▃▄▄▄▄▅▄▄▄▄▄▄▄▄▄▄▄▆▇█▇▆▇▄▄</td></tr><tr><td>val_auc</td><td>▁▂▃▃▄▅▆▅▆▆▆▆▇▆▅▅▆▇▇████▇▇▆▇▇▇▇▇▇███▇██▇▇</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▂▂▁▃▄▄▃▃▅▅▅▅▅▅▄▄▅▄▅▅▅▅▄▅▆▇█▇▆▇▅▅</td></tr><tr><td>val_loss_epoch</td><td>█▃▂▂▂▁▂▂▂▂▂▂▁▂▁▂▂▁▁▁▂▁▁▃▁▂▂▂▃▁▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_step</td><td>█▂▂▂▃▁▂▃▃▂▂▂▁▂▁▂▃▁▂▁▃▁▂▄▁▃▂▂▄▂▂▃▄▂▁▁▁▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.68007</td></tr><tr><td>train_auc</td><td>0.6503</td></tr><tr><td>train_f1</td><td>0.56897</td></tr><tr><td>train_loss_epoch</td><td>0.60944</td></tr><tr><td>train_loss_step</td><td>0.60213</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.64964</td></tr><tr><td>val_auc</td><td>0.76306</td></tr><tr><td>val_f1</td><td>0.33333</td></tr><tr><td>val_loss_epoch</td><td>0.57287</td></tr><tr><td>val_loss_step</td><td>0.5424</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_64_onehot_2</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/cbvn4j2x' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/cbvn4j2x</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240101_053740-cbvn4j2x\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240101_055650-rpwpc90k</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/rpwpc90k' target=\"_blank\">MLP_3_64_onehot_2</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/rpwpc90k' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/rpwpc90k</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\torch_geometric\\deprecation.py:22: UserWarning: 'nn.glob.GlobalAttention' is deprecated, use 'nn.aggr.AttentionalAggregation' instead\n",
      "  warnings.warn(out)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_3_64_onehot_2\\attention\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 9.0 K \n",
      "2  | head        | Sequential       | 2.1 K \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 65    \n",
      "--------------------------------------------------\n",
      "11.2 K    Trainable params\n",
      "0         Non-trainable params\n",
      "11.2 K    Total params\n",
      "0.045     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▂▂▃▇▇█▇█▇█▇▇▇▇▇▇▇▅█▇███▇██▇▇██▇▆█▇▅███</td></tr><tr><td>train_auc</td><td>▁▁▁▂▅▆▇█▇█▇█▇██▇███▇█▇█████▇████▇▇██▇███</td></tr><tr><td>train_f1</td><td>▃▁▁▁▂▇▇█▇▇▇▇▆█▇▇▆███▇▇█▇██▇███▇▇▇██▆▄▇██</td></tr><tr><td>train_loss_epoch</td><td>████▆▅▃▁▃▃▃▁▂▂▁▃▁▂▁▅▃▂▂▂▂▂▃▄▂▂▂▂▁▃▂▃▄▃▃▃</td></tr><tr><td>train_loss_step</td><td>▄▆▆▅▄▂▂▃▁█▃▅▄▂█▆▂▄▅▂▂▄▂▄▂▂▅▇▃▃▂▁▄▁▂▂▅▃▂▆</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▅▆▆▆▆▆▆▆█▆█▆▇▆▆▅▆█▆▆▆▆▆▇▆▇▆▆▇▆▆█▅▆▇▆</td></tr><tr><td>val_auc</td><td>▁▂▃▄▇▇█████████▇▇▇▇▇▇▇█▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>val_f1</td><td>▁▁▁▁▄▆▅▅▅▅▅▅▇▅▇▆▆▅▅▄▆▇▅▅▅▅▅▆▅▆▆▇▇▆▆█▅▅▆▅</td></tr><tr><td>val_loss_epoch</td><td>▅▇▆▇▆▃▂▂▁▂▄▃▄▂▁▃▅▄▅█▄▃▃▁▃▄▃▄▃█▃▄▃▅▄▄▄▃▂▄</td></tr><tr><td>val_loss_step</td><td>▄▅▄▅▅▃▂▂▁▂▄▃▄▂▂▃▅▄▄█▄▄▃▁▃▄▃▄▃█▃▅▃▅▅▄▃▃▂▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.6883</td></tr><tr><td>train_auc</td><td>0.72212</td></tr><tr><td>train_f1</td><td>0.57849</td></tr><tr><td>train_loss_epoch</td><td>0.61819</td></tr><tr><td>train_loss_step</td><td>0.67708</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.67883</td></tr><tr><td>val_auc</td><td>0.77129</td></tr><tr><td>val_f1</td><td>0.40541</td></tr><tr><td>val_loss_epoch</td><td>0.59531</td></tr><tr><td>val_loss_step</td><td>0.61005</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_64_onehot_2</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/rpwpc90k' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/rpwpc90k</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240101_055650-rpwpc90k\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240101_061549-89t6zzz2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/89t6zzz2' target=\"_blank\">MLP_3_64_onehot_2</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/89t6zzz2' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/89t6zzz2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_3_64_onehot_2\\attention2\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 9.0 K \n",
      "2  | head        | Sequential       | 2.1 K \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 4.2 K \n",
      "--------------------------------------------------\n",
      "15.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "15.3 K    Total params\n",
      "0.061     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▂▂▂▃▅▇▇▆▇▇▇▆▇▇▇▇▇▇▇▇▇▇█▇▇▇▇▇█▇█▇▇▇▇▇▆█</td></tr><tr><td>train_auc</td><td>▂▁▂▂▃▅▇▇▇▇▇▇▇█▇▇██▇███████▇▇▇██▇████████</td></tr><tr><td>train_f1</td><td>▃▂▁▁▁▂▆▇▇▇█▇▇▆▇▇▇▇███▇▇▇▇▇▇▇▇▇▇▇███▇▇▇▇█</td></tr><tr><td>train_loss_epoch</td><td>██▇▇▇▆▄▃▄▃▃▂▃▂▃▃▃▂▃▃▃▂▂▂▁▂▃▂▂▁▁▃▁▁▃▂▂▂▂▁</td></tr><tr><td>train_loss_step</td><td>▇▇▆▆▇▅▆█▃▃▂▃▃▆▆▅▄▃▄▃▁▄▃▁▃▄▅▆▃▆▄▄▄▃▆▃▃▇▄▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▆▆▆▇▆▆▇▇▇▇▇▇▇▇▇█▇████▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>val_auc</td><td>▁▄▄▅▆▇▇▇▇▇▇▇▇▇▇▇▇███▇▇▇▇▇▇▇▇▇██████████▇</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▇▆▆▆▆▆▇▇▇▇▇▇█▇▆██████▇▇▇▇▇▇▇▇▇▇▇▇▇█</td></tr><tr><td>val_loss_epoch</td><td>█▇█▇█▆▄▄▃▁▆▄▄▄▄▅▃▃▅▂▅▅▃▂▃▁▆▃▃▃▂▂▁▂▁▅▂▃▃▆</td></tr><tr><td>val_loss_step</td><td>█▇▇▆█▆▅▅▃▁█▅▅▅▅▆▄▄▇▂▆▆▃▂▄▁▇▄▄▄▃▂▁▂▂▇▂▄▄█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.69653</td></tr><tr><td>train_auc</td><td>0.73662</td></tr><tr><td>train_f1</td><td>0.58396</td></tr><tr><td>train_loss_epoch</td><td>0.5836</td></tr><tr><td>train_loss_step</td><td>0.50482</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.70438</td></tr><tr><td>val_auc</td><td>0.77267</td></tr><tr><td>val_f1</td><td>0.60488</td></tr><tr><td>val_loss_epoch</td><td>0.64747</td></tr><tr><td>val_loss_step</td><td>0.73698</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_64_onehot_2</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/89t6zzz2' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/89t6zzz2</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240101_061549-89t6zzz2\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240101_063446-b8d3yoam</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/b8d3yoam' target=\"_blank\">MLP_4_16_onehot_2</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/b8d3yoam' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/b8d3yoam</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_4_16_onehot_2\\mean\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 976   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "1.1 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.1 K     Total params\n",
      "0.005     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁███████████████████████████████████████</td></tr><tr><td>train_auc</td><td>▄▅▁▁▃▆▂▄▅▅▃▁▃▃▄▄▄▃▅▃▃▂▄▆▃▂▅▃▂█▁▅▃▄▄▁▄▄▅▄</td></tr><tr><td>train_f1</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_epoch</td><td>▃▄▅▅█▃▄▄▅▂▃▄▅▄▅▅▅▂▄▅▅▄▅▅▁▅▃▁▆▃▅▃▁▄▄▄▃▁▃▆</td></tr><tr><td>train_loss_step</td><td>▃▄▄▅▄▅▅▅▄▇▅▇▄▅█▄▃▄▄▂▆▄▄▄▄▅▄▁▅▃▆▆▅▄▅▆▄▅▅█</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>▃▄▁▃▄▆▅▇▇██████████▇▇▇▇▇▇▇▆▃▃▂▅▂▇▁▁▁▁▁▁▁</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_epoch</td><td>▅▅▃▅▃▄▅▃▆▄▄▅▅█▅▄▅▄▆▄▄▃█▆▅▅▅▅▃▄▃▁▅▅▆▄▅▄▄█</td></tr><tr><td>val_loss_step</td><td>▅▅▃▅▃▄▅▃▆▄▄▅▅█▅▄▅▄▆▄▄▃█▆▅▅▅▅▃▄▃▁▅▅▆▄▅▄▄█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.58044</td></tr><tr><td>train_auc</td><td>0.49798</td></tr><tr><td>train_f1</td><td>0.0</td></tr><tr><td>train_loss_epoch</td><td>0.6862</td></tr><tr><td>train_loss_step</td><td>0.71861</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.59489</td></tr><tr><td>val_auc</td><td>0.5</td></tr><tr><td>val_f1</td><td>0.0</td></tr><tr><td>val_loss_epoch</td><td>0.70987</td></tr><tr><td>val_loss_step</td><td>0.74898</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_16_onehot_2</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/b8d3yoam' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/b8d3yoam</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240101_063446-b8d3yoam\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240101_065444-xc73fy6h</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/xc73fy6h' target=\"_blank\">MLP_4_16_onehot_2</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/xc73fy6h' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/xc73fy6h</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_4_16_onehot_2\\max\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 976   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "1.1 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.1 K     Total params\n",
      "0.005     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\torch_geometric\\warnings.py:17: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▄▄▄▄▄▄▄▄▄▄▄▄▄▅▄▅▄▅▆▆▅▆▇█▆█▆▅▆▆▆█▆▆▆▆▇▆▆</td></tr><tr><td>train_auc</td><td>▂▂▁▂▂▃▃▄▃▃▃▃▃▃▄▄▅▆▆▆▆▇▇▇▇▇▇▇▅██▆██▆▇▇█▇▇</td></tr><tr><td>train_f1</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▃▂▁▁▅▆▅▅▆█▆█▆▄▆▅▇▆▆█▆▇▆▇▇</td></tr><tr><td>train_loss_epoch</td><td>█▅▅▅▆▄▅▅▅▄▄▅▅▄▄▅▄▃▄▃▄▃▃▂▂▄▃▂▄▂▂▃▁▁▃▂▂▂▂▃</td></tr><tr><td>train_loss_step</td><td>▄▅▅▅▅▆▅▆▅█▅▇▅▇█▅▃▄▄▂▄▅▁▅▄▄▅▂▃▂▇▅▃█▆█▁▅▅▄</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>████████████████▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>▆▃▂▄▃▄▆▆▆▇▇▆▁▇▇▆▆▆▆▇▆█▇▇▇▇▇▇▇▆▆▆▆▇▆▇▇▇▇█</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁████████████████████████</td></tr><tr><td>val_loss_epoch</td><td>▂▂▁▂▁▁▂▁▂▂▁▂▂▃▂▂▂▃▃▃▄▄▃▄▅▅▅▅▆▆▆█▅▆▅▆▅▆▆▃</td></tr><tr><td>val_loss_step</td><td>▂▂▁▂▁▁▂▁▃▂▂▂▂▃▂▂▃▃▃▃▄▄▂▃▄▄▄▄▆▅▆█▄▅▃▅▄▅▅▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.60695</td></tr><tr><td>train_auc</td><td>0.6134</td></tr><tr><td>train_f1</td><td>0.29508</td></tr><tr><td>train_loss_epoch</td><td>0.66136</td></tr><tr><td>train_loss_step</td><td>0.65854</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.40511</td></tr><tr><td>val_auc</td><td>0.66299</td></tr><tr><td>val_f1</td><td>0.57662</td></tr><tr><td>val_loss_epoch</td><td>0.70857</td></tr><tr><td>val_loss_step</td><td>0.65624</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_16_onehot_2</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/xc73fy6h' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/xc73fy6h</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240101_065444-xc73fy6h\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240101_071344-ukbc5ecd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/ukbc5ecd' target=\"_blank\">MLP_4_16_onehot_2</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/ukbc5ecd' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/ukbc5ecd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_4_16_onehot_2\\sum\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 976   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "1.1 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.1 K     Total params\n",
      "0.005     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▂▁▁▁▃▄▃▃▃▄▄▄▄▄▃▅▆▃▅▅▅▅▆▆▅▇▆▇▆▇▇▆▆▆▆▆▇█▇▆</td></tr><tr><td>train_auc</td><td>▇▅▆▄▇█▇▅▅▅▇▆▆▅▄▅▄▄▄▅▁▂▄▃▂▃▁▃▃▃▆▅▃▃▃▃▆▄▄▆</td></tr><tr><td>train_f1</td><td>▄▃▄▄▂▄▆▄▄▃▂▃▁▂▄▅▃▄▂▄▄▅▆▃▇▇▄▇▄▆▆█▅▅█▆▇▇▇█</td></tr><tr><td>train_loss_epoch</td><td>█▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▄▂▂▂▁▂▂▁▂▂▂▁▁▂▂▁▁▁▁▁▂▁▂▁▁▁▁▁▁▁▁▁▂▂▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▃▁▂▃▃▄▂▇▂▄▆▃▃▃█▃▄▇▆▅▅▄</td></tr><tr><td>val_auc</td><td>▇▇▇█▇▇███▇█████▇▇▄▆▆▂▃▃▁▁▁▁▁▂▂▁▂▁▂▂▁▂▅▅▄</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▃▁▂▃▃▅▃▇▃▄▆▃▄▃█▄▅▇▆▅▆▅</td></tr><tr><td>val_loss_epoch</td><td>█▂▁▁▁▂▂▁▂▁▁▁▂▂▁▁▁▁▂▁▁▁▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▂</td></tr><tr><td>val_loss_step</td><td>█▃▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▂▂▂▁▁▂▂▂▁▁▂▁▂▁▂▁▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.6234</td></tr><tr><td>train_auc</td><td>0.51702</td></tr><tr><td>train_f1</td><td>0.54222</td></tr><tr><td>train_loss_epoch</td><td>0.64738</td></tr><tr><td>train_loss_step</td><td>0.64221</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.64964</td></tr><tr><td>val_auc</td><td>0.45692</td></tr><tr><td>val_f1</td><td>0.30435</td></tr><tr><td>val_loss_epoch</td><td>0.70257</td></tr><tr><td>val_loss_step</td><td>0.82</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_16_onehot_2</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/ukbc5ecd' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/ukbc5ecd</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240101_071344-ukbc5ecd\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240101_073340-0ql44e1h</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/0ql44e1h' target=\"_blank\">MLP_4_16_onehot_2</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/0ql44e1h' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/0ql44e1h</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\torch_geometric\\deprecation.py:22: UserWarning: 'nn.glob.GlobalAttention' is deprecated, use 'nn.aggr.AttentionalAggregation' instead\n",
      "  warnings.warn(out)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_4_16_onehot_2\\attention\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 976   \n",
      "2  | head        | Sequential       | 154   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 17    \n",
      "--------------------------------------------------\n",
      "1.1 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.1 K     Total params\n",
      "0.005     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▅█▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆</td></tr><tr><td>train_auc</td><td>▅▄▇▅▆█▅▅▄▅▃▄▆▃▃▅▅▂▄▃▆▆▅▄▆▇▃▁▄▃▆▄▄▇█▅▅▃▅▅</td></tr><tr><td>train_f1</td><td>██▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_epoch</td><td>▅▄▃▃▄▁▃▃▃▄▅▃▄▃▃▃█▃▄▃▄▃▃▁▄▃▂▅▄▅▃▂▅▃▄▅▄▄▅▅</td></tr><tr><td>train_loss_step</td><td>▃▃▅▆▄▂▂▂▄▄▃▆▅▂▁▂▃▃▂▅▃▃█▃▆▂▁▂▅▂▅▄▄▃▅▅▄▃▄▅</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>▇██████▇▇████████▇▇▇▇▇▇▇▇▇▇▇▆▁▁▁▂▁▁▁▁▁▁▁</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_epoch</td><td>█▆▅▆▆▁▂▃▆▂▅▆▄▆▆▇▆▅▆▄▆▅▆▅▆▅▇▅▆▆▇▃▄▆▅▇▅█▄▆</td></tr><tr><td>val_loss_step</td><td>█▆▅▆▆▁▂▃▆▂▅▆▄▆▆▇▆▅▆▄▆▅▆▅▆▅▇▅▆▆▇▃▄▆▅▇▅█▄▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.58044</td></tr><tr><td>train_auc</td><td>0.50139</td></tr><tr><td>train_f1</td><td>0.0</td></tr><tr><td>train_loss_epoch</td><td>0.68346</td></tr><tr><td>train_loss_step</td><td>0.70239</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.59489</td></tr><tr><td>val_auc</td><td>0.5</td></tr><tr><td>val_f1</td><td>0.0</td></tr><tr><td>val_loss_epoch</td><td>0.67195</td></tr><tr><td>val_loss_step</td><td>0.66729</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_16_onehot_2</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/0ql44e1h' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/0ql44e1h</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240101_073340-0ql44e1h\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240101_075314-vze5cj31</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/vze5cj31' target=\"_blank\">MLP_4_16_onehot_2</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/vze5cj31' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/vze5cj31</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_4_16_onehot_2\\attention2\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 976   \n",
      "2  | head        | Sequential       | 154   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 281   \n",
      "--------------------------------------------------\n",
      "1.4 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.4 K     Total params\n",
      "0.006     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▇██████████████████████████████████████</td></tr><tr><td>train_auc</td><td>▄▂▅▁▅▅▃▃▂▃▃▄▅▃▆▁▇▄▄▃▂▅▃▂▂▄▄▅▃▆▂█▃▅▆▄▃▂▂▃</td></tr><tr><td>train_f1</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_epoch</td><td>▃▆█▆▅▄▃▅▇▅▄▇▅▄▂▄▂▂▃▅▄▄▃▄▅▂▆▄▂▆▃▃▄▅▂▁▁▅▆▄</td></tr><tr><td>train_loss_step</td><td>▂▅▆▆▄▆▅█▅▅▅▆▅▇▄▃▇█▅▂██▃▆▅▃▃▅▁▄▃▄▇▅▄▃▅█▆▅</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>▅▆▇█▇█████▇██▁▁▁▁▂▁▂▄▃▂▄▄▄▄▄▄▄▅▄▄▄▅▅▄▄▄▄</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_epoch</td><td>▃▇▅▄▆▅▆▆▅▄▄▃▅▄▅▅█▆▄▆▄▄▆▆▃▅▃▃▄▇█▆▃▅▄▄▅▁▅▅</td></tr><tr><td>val_loss_step</td><td>▃▇▅▄▆▅▆▆▅▄▄▃▅▄▅▅█▆▄▆▄▄▆▆▃▅▃▃▄▇█▆▃▅▄▄▅▁▅▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.58044</td></tr><tr><td>train_auc</td><td>0.49008</td></tr><tr><td>train_f1</td><td>0.0</td></tr><tr><td>train_loss_epoch</td><td>0.68052</td></tr><tr><td>train_loss_step</td><td>0.68112</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.59489</td></tr><tr><td>val_auc</td><td>0.47325</td></tr><tr><td>val_f1</td><td>0.0</td></tr><tr><td>val_loss_epoch</td><td>0.68046</td></tr><tr><td>val_loss_step</td><td>0.68557</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_16_onehot_2</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/vze5cj31' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/vze5cj31</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240101_075314-vze5cj31\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240101_081224-ht68k58e</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/ht68k58e' target=\"_blank\">MLP_4_32_onehot_2</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/ht68k58e' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/ht68k58e</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_4_32_onehot_2\\mean\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 3.5 K \n",
      "2 | head        | Sequential       | 562   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "4.0 K     Trainable params\n",
      "0         Non-trainable params\n",
      "4.0 K     Total params\n",
      "0.016     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▇▇█████████████████████████████████████</td></tr><tr><td>train_auc</td><td>▅▅▄▃▃▅▂▆▃▆▇▄▂▁▄▃▇▄▅▃▄▄▁▄▆▅▆▆▂▆▅▂▇▅▄▃▆█▅▄</td></tr><tr><td>train_f1</td><td>█▃▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_epoch</td><td>█▃▄▂▃▁▃▄▃▁▁▁▂▃▂▃▁▃▁▃▁▁▃▂▂▂▂▂▂▁▂▁▂▁▂▃▃▂▂▃</td></tr><tr><td>train_loss_step</td><td>▅▇▅▄▄▄▄▅▅▄▆▃▁▂▅▄▄▆█▄▄▅▆▅▄▄▄▆▆▄▇▄▆▄▅▃▅▅▄▆</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>█▁▃▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_epoch</td><td>▅▆▆▄▄▄▄▄▄▅▅▄▃▂▆▄▃▂▄▅▄▆▃▄▁▃▆█▅▂▅▃▄▄▇▃▄▅▁▂</td></tr><tr><td>val_loss_step</td><td>▅▅▅▄▄▄▄▄▄▅▅▄▃▂▆▃▃▂▄▅▄▆▃▄▁▃▆█▅▂▅▃▄▄▇▃▄▅▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.58044</td></tr><tr><td>train_auc</td><td>0.49966</td></tr><tr><td>train_f1</td><td>0.0</td></tr><tr><td>train_loss_epoch</td><td>0.68365</td></tr><tr><td>train_loss_step</td><td>0.70125</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.59489</td></tr><tr><td>val_auc</td><td>0.75673</td></tr><tr><td>val_f1</td><td>0.0</td></tr><tr><td>val_loss_epoch</td><td>0.64251</td></tr><tr><td>val_loss_step</td><td>0.60588</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_32_onehot_2</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/ht68k58e' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/ht68k58e</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240101_081224-ht68k58e\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240101_083238-ftq28si6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/ftq28si6' target=\"_blank\">MLP_4_32_onehot_2</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/ftq28si6' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/ftq28si6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_4_32_onehot_2\\max\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 3.5 K \n",
      "2 | head        | Sequential       | 562   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "4.0 K     Trainable params\n",
      "0         Non-trainable params\n",
      "4.0 K     Total params\n",
      "0.016     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\torch_geometric\\warnings.py:17: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▃▅▄▅▅▅▅▄▅▆▆▆▆▇▆▅▅▆▅▆▆▆▇█▆▆▆▆▇▆▆▆▆▇▇▆▇▆█</td></tr><tr><td>train_auc</td><td>▂▁▁▂▁▃▃▄▃▄▄▅▅▃▅▅▅▅▅▄▄▄▄▆▆▄▆▅▆▇▆▆▆▇▆▆▆▇▆█</td></tr><tr><td>train_f1</td><td>▆▆▃▁▁▁▃▃▆▁▃▆▃▅▃▄▂▃▆█▄▆▂▆▆▂▅▄▇▆▅█▃▆▆▅▄▆▇▇</td></tr><tr><td>train_loss_epoch</td><td>█▇▆▄▅▃▅▆▅▄▃▅▃▄▄▄▂▄▂▅▃▃▅▃▃▃▃▄▄▂▂▄▄▁▅▂▂▂▄▂</td></tr><tr><td>train_loss_step</td><td>▄▆▄▃▂▁▃▄▂▃▃▂▂▁▄▃▁▄█▂▃▃▄▃▂▃▂▄▂▃▇▂▂▃▅▂▂▁▃▅</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>▃▅▇█▇▇▄▅▅▆█▅█▇▄▄▄▆▇██▇▇▅▆▅▆▃▁▁▄▃▆▅▄▅▂▁▇▄</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_epoch</td><td>▁▁▁▁▁▂▂▂▂▂▂▂▂▂▁▂▃▃▃▃▂▂▃▂▄▃▂▁▃▅▃▅▄▃▂▄▄▄██</td></tr><tr><td>val_loss_step</td><td>▂▂▂▂▂▃▃▃▃▂▃▃▃▃▂▃▄▄▃▃▃▂▃▃▅▄▂▁▃▆▃▅▄▄▂▄▄▄██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.6298</td></tr><tr><td>train_auc</td><td>0.63531</td></tr><tr><td>train_f1</td><td>0.46358</td></tr><tr><td>train_loss_epoch</td><td>0.65826</td></tr><tr><td>train_loss_step</td><td>0.69767</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.40511</td></tr><tr><td>val_auc</td><td>0.43348</td></tr><tr><td>val_f1</td><td>0.57662</td></tr><tr><td>val_loss_epoch</td><td>0.96692</td></tr><tr><td>val_loss_step</td><td>1.07015</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_32_onehot_2</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/ftq28si6' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/ftq28si6</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240101_083238-ftq28si6\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240101_085241-8k5yqmf0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/8k5yqmf0' target=\"_blank\">MLP_4_32_onehot_2</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/8k5yqmf0' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/8k5yqmf0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_4_32_onehot_2\\sum\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 3.5 K \n",
      "2 | head        | Sequential       | 562   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "4.0 K     Trainable params\n",
      "0         Non-trainable params\n",
      "4.0 K     Total params\n",
      "0.016     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▂▁▂▄▃▄▄▄▃▅▅▅▅▅▅▆▅▅▇▅▆▆▆▇▇▇█▇▇█▇▅▆▇▇███▇█</td></tr><tr><td>train_auc</td><td>▄▂▅▂▁▃▄▄▃▃▁▅▄▅▃▂▅▂▄▅▅▄▃▃▃▅▄▄▃▃▃▃▂▅▄▆▇▆█▅</td></tr><tr><td>train_f1</td><td>▃▄▆▅▃▅▃▄▄▃▄▄▃▁▄▇▄▄▆█▅▄▅▇▇▇▇▆▆███▇▇▇█▇▇▇▇</td></tr><tr><td>train_loss_epoch</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▄▃▂▂▂▂▂▁▂▁▁▁▂▂▁▁▂▂▂▁▁▂▁▂▁▁▂▁▁▂▂▁▁▂▁▁▁▁▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▅▅▅▅▅▂▅▅▃▇▆▁▆▇█▆█▇▇▇▆▇▇▆▇▇▆▆▅▇▇█▇▇▆▇▇</td></tr><tr><td>val_auc</td><td>▂▁▂▁▁▂▂▃▃▄▄▄▄▄▅▄▅▅▄▄▃▄▁▆▅▆▅▆▆▆▆▆▂▅▇▆▇▆▇█</td></tr><tr><td>val_f1</td><td>▁▁▁▆▆▆▆▇▃▆▆▃▇▆▂▆▇█▆▇█▇▇▆▇▇▇█▇▆▆▅▇██▇▇▇▇▇</td></tr><tr><td>val_loss_epoch</td><td>▇██▆▅▅▆▆▅▆▆▅▅▄▆▄▅▃▄▅▅▄▄▄▂▁▃▃▃▅▅▂▆▅▃▂▄▂▂▂</td></tr><tr><td>val_loss_step</td><td>▅▇█▅▄▅▅▆▄▅▅▅▅▄▆▃▅▃▄▅▅▄▄▄▂▁▄▄▃▅▆▂▆▅▄▂▄▃▂▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.67276</td></tr><tr><td>train_auc</td><td>0.54405</td></tr><tr><td>train_f1</td><td>0.55362</td></tr><tr><td>train_loss_epoch</td><td>0.61785</td></tr><tr><td>train_loss_step</td><td>0.69661</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.72628</td></tr><tr><td>val_auc</td><td>0.77135</td></tr><tr><td>val_f1</td><td>0.61929</td></tr><tr><td>val_loss_epoch</td><td>0.53331</td></tr><tr><td>val_loss_step</td><td>0.49121</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_32_onehot_2</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/8k5yqmf0' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/8k5yqmf0</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240101_085241-8k5yqmf0\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240101_091323-nzteo4er</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/nzteo4er' target=\"_blank\">MLP_4_32_onehot_2</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/nzteo4er' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/nzteo4er</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\torch_geometric\\deprecation.py:22: UserWarning: 'nn.glob.GlobalAttention' is deprecated, use 'nn.aggr.AttentionalAggregation' instead\n",
      "  warnings.warn(out)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_4_32_onehot_2\\attention\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 3.5 K \n",
      "2  | head        | Sequential       | 562   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 33    \n",
      "--------------------------------------------------\n",
      "4.1 K     Trainable params\n",
      "0         Non-trainable params\n",
      "4.1 K     Total params\n",
      "0.016     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁█▇█████████████████████████████████████</td></tr><tr><td>train_auc</td><td>▆▆▂▆▆▇▆▆▄▄▅▅▅▅█▁▇▅█▃▅▄▅▇▆▂▇▆▅▆▄▂▇█▄▃▄█▂▆</td></tr><tr><td>train_f1</td><td>█▂▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_epoch</td><td>█▃▄▃▄▃▂▃▂▃▂▂▂▁▂▃▂▂▂▂▂▃▂▁▂▃▂▂▃▂▂▂▂▃▄▂▃▃▃▃</td></tr><tr><td>train_loss_step</td><td>▇▄▆▅▅▆▄▂█▁█▇▅▂▄▅▃▂▆▆▄▃▆▁▅▄▅▆▃▃▄▅▁▂▂▄▅▆▅▅</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>█▂▂▄▅▄▄▄▄▄▄▄▄▄▅▄▄▄▄▄▄▄▄▄▄▄▄▄▃▃▃▃▃▃▃▃▂▂▂▁</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_epoch</td><td>▄▃▃▅▂▄▃▆▅▅▆▂▄▆▄▄▄▅▃▂▄▃▃▂▁▅▂▄▁▃▄▆▃▃█▃▅▃▄▄</td></tr><tr><td>val_loss_step</td><td>▄▃▃▄▂▄▃▆▅▅▆▂▄▆▄▄▄▅▃▂▄▃▃▂▁▅▂▄▁▃▄▆▃▃█▃▅▃▄▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.58044</td></tr><tr><td>train_auc</td><td>0.50673</td></tr><tr><td>train_f1</td><td>0.0</td></tr><tr><td>train_loss_epoch</td><td>0.68224</td></tr><tr><td>train_loss_step</td><td>0.69014</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.59489</td></tr><tr><td>val_auc</td><td>0.70232</td></tr><tr><td>val_f1</td><td>0.0</td></tr><tr><td>val_loss_epoch</td><td>0.6806</td></tr><tr><td>val_loss_step</td><td>0.68541</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_32_onehot_2</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/nzteo4er' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/nzteo4er</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240101_091323-nzteo4er\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240101_093406-ymrimos8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/ymrimos8' target=\"_blank\">MLP_4_32_onehot_2</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/ymrimos8' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/ymrimos8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_4_32_onehot_2\\attention2\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 3.5 K \n",
      "2  | head        | Sequential       | 562   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 1.1 K \n",
      "--------------------------------------------------\n",
      "5.1 K     Trainable params\n",
      "0         Non-trainable params\n",
      "5.1 K     Total params\n",
      "0.020     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▄▄▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▆▇▇█▇██▇▇▇█▇▇█</td></tr><tr><td>train_auc</td><td>▂▂▂▁▂▂▃▂▂▁▂▂▂▁▂▁▂▂▂▂▂▂▂▂▃▅▇▇▇▇▇██▇▇██▇██</td></tr><tr><td>train_f1</td><td>█▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▅█▆▇▇█▇▆▆▆▇▇██</td></tr><tr><td>train_loss_epoch</td><td>█▆▆▆▇▆▆▆▆▆▆▆▆▇▆▇▆▆▆▆▆▆▇▆▆▆▃▄▂▂▂▁▂▂▂▂▃▂▂▁</td></tr><tr><td>train_loss_step</td><td>▆▇▇▇▇▆▆▇█▇▆█▇█▆▆▇▇▆▇▇▆▇▆▆▆▅▇▆▅▅▁▃▃▁▅▄▂▂▅</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▇▅▇▆▇█▇▇██▆▇▇</td></tr><tr><td>val_auc</td><td>█▅▁▅▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇██████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▆▅▆▆▇█▇▇█▇▆▇▇</td></tr><tr><td>val_loss_epoch</td><td>▆▆▇█▆▇▇▆▇▆▇█▇▆▇▇▇▇█▆▆█▇▇▇▆▄▇▃▄▅▄▄▃▃▃▁▄▂▅</td></tr><tr><td>val_loss_step</td><td>▅▅▆▆▅▆▅▅▅▅▆▇▆▅▅▅▅▆▆▅▄▇▆▆▆▅▃█▃▄▅▄▄▃▃▃▁▄▂▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.67916</td></tr><tr><td>train_auc</td><td>0.70948</td></tr><tr><td>train_f1</td><td>0.5471</td></tr><tr><td>train_loss_epoch</td><td>0.61403</td></tr><tr><td>train_loss_step</td><td>0.63823</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.70438</td></tr><tr><td>val_auc</td><td>0.77643</td></tr><tr><td>val_f1</td><td>0.55249</td></tr><tr><td>val_loss_epoch</td><td>0.62412</td></tr><tr><td>val_loss_step</td><td>0.68886</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_32_onehot_2</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/ymrimos8' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/ymrimos8</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240101_093406-ymrimos8\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240101_095454-55tsh22t</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/55tsh22t' target=\"_blank\">MLP_4_64_onehot_2</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/55tsh22t' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/55tsh22t</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_4_64_onehot_2\\mean\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 13.1 K\n",
      "2 | head        | Sequential       | 2.1 K \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "15.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "15.3 K    Total params\n",
      "0.061     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▄▄▄▄▄▄▄▄▅▆▇▇▇▇▇▇▇▇████▇█▇▇▇▇█▇▇▇▇█▇████</td></tr><tr><td>train_auc</td><td>▁▂▂▁▁▁▁▂▂▄▆▇▇▇▇▆▇▇▇███▇▇█▇██████████████</td></tr><tr><td>train_f1</td><td>▅▂▁▂▁▁▁▁▁▄▇▇▇▆▇▆▆▆▇██▇▇▇█▇▇█▇█▇▇█▇▇▇██▇▇</td></tr><tr><td>train_loss_epoch</td><td>█▇▇▇▇▇▇▇▇▆▄▃▄▃▂▃▃▃▃▂▂▁▂▄▁▂▁▂▁▂▂▂▂▁▁▂▁▂▂▂</td></tr><tr><td>train_loss_step</td><td>▆▅▇▆█▇▇▅▇▆▁▂▄▃▄▃▅▂▆▂▅▄▅▂▄▃▅▃▆▆▃▄▁▃▃▅▅▄▃▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▅▆▆▇▇█▆█▇█▇█▆██▆▇█▇▇█▇▇█▇▇███▇▇</td></tr><tr><td>val_auc</td><td>▄▃▃▁▂▁▄▄▆▇▇▇▇▇▇▇▇▇▇▇▇███████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▄█▆▇▇▇██▇▇▆▇▆▇▇▆▆▇▆▆▇▇▇▇▇▇▇▆▇▇▇</td></tr><tr><td>val_loss_epoch</td><td>▇▇█████▇▇▇▂▂▄▁▃▅▄▃▃▅▃▁▆▃▁▃▂▂▆▄▃▃▃▁▄▄▃▁▃▃</td></tr><tr><td>val_loss_step</td><td>▆▆▇▆▇▇▇▆▆▇▂▃▅▁▄▆▄▃▄▇▄▂█▄▁▄▂▃█▅▄▄▄▁▅▅▃▁▃▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.68739</td></tr><tr><td>train_auc</td><td>0.7193</td></tr><tr><td>train_f1</td><td>0.57778</td></tr><tr><td>train_loss_epoch</td><td>0.60031</td></tr><tr><td>train_loss_step</td><td>0.55046</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.69708</td></tr><tr><td>val_auc</td><td>0.77461</td></tr><tr><td>val_f1</td><td>0.56545</td></tr><tr><td>val_loss_epoch</td><td>0.56978</td></tr><tr><td>val_loss_step</td><td>0.58149</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_64_onehot_2</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/55tsh22t' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/55tsh22t</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240101_095454-55tsh22t\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240101_101457-w1a3s07o</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/w1a3s07o' target=\"_blank\">MLP_4_64_onehot_2</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/w1a3s07o' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/w1a3s07o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_4_64_onehot_2\\max\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 13.1 K\n",
      "2 | head        | Sequential       | 2.1 K \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "15.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "15.3 K    Total params\n",
      "0.061     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\torch_geometric\\warnings.py:17: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▄▅▅▄▅▅▆▆▇▅▆▆▇█▆▅▇▇▅▄▆▅▆▆▇▇▇▆▆▆▆▇█▇█▇▆▇▆</td></tr><tr><td>train_auc</td><td>▂▂▁▂▁▂▄▄▄▄▅▄▄▆▆▇▅▆▇▆▃▃▄▅▅▅▅▆▆▆▅▆▇▇▇▇█▆▆█</td></tr><tr><td>train_f1</td><td>▅▃▁▃▁▂▁▄▄▆█▄▄▆▇▅▅▅▆▇█▅▆▅▆▇▆▇▅▇▅▇▆▇▆▆▇█▅▃</td></tr><tr><td>train_loss_epoch</td><td>█▅▅▅▅▅▄▃▃▂▅▄▃▂▂▂▄▃▃▂▅▂▄▄▂▂▂▂▂▄▂▂▃▁▁▂▂▃▄▁</td></tr><tr><td>train_loss_step</td><td>▆▃▆▆▇▆▄▃▄▆▃▅▄▄▁▄▃▄▆█▅▂▇▅▅▆▅▂▇▅█▄▃▄▆▄▂▃▅▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>▅▃▄▃▅▇▆▅▃▇█▂▂▃▃▂▂▃▆▆▄▆▇▄▇▆▁▁▃▃▄▄▄▅▃▆▄▅▃▃</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_epoch</td><td>▁▁▁▁▁▁▂▂▂▂▃▅▂▆▅█▃▅▃▄▆▃▄▃▃▄▅▄▄▆▃▅▃▄▆▆▇█▃▄</td></tr><tr><td>val_loss_step</td><td>▂▂▂▂▂▂▂▂▂▂▂▅▂▇▄█▂▄▂▃▅▃▄▃▂▄▅▃▃▅▂▄▁▂▅▄▅▇▁▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.59781</td></tr><tr><td>train_auc</td><td>0.62145</td></tr><tr><td>train_f1</td><td>0.19708</td></tr><tr><td>train_loss_epoch</td><td>0.65104</td></tr><tr><td>train_loss_step</td><td>0.62508</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.40511</td></tr><tr><td>val_auc</td><td>0.41334</td></tr><tr><td>val_f1</td><td>0.57662</td></tr><tr><td>val_loss_epoch</td><td>0.79006</td></tr><tr><td>val_loss_step</td><td>0.75525</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_64_onehot_2</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/w1a3s07o' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/w1a3s07o</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240101_101457-w1a3s07o\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240101_103603-adv2keqv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/adv2keqv' target=\"_blank\">MLP_4_64_onehot_2</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/adv2keqv' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/adv2keqv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_4_64_onehot_2\\sum\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 13.1 K\n",
      "2 | head        | Sequential       | 2.1 K \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "15.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "15.3 K    Total params\n",
      "0.061     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▃▂▁▄▄▄▄▃▄▄▅▅▆▇▆▆▆█▇▇█▇▇▇▆▇█▇▇▇▆▇███▆██</td></tr><tr><td>train_auc</td><td>▃▄▃▁▃▆▆▅▅▄▄▃▃▅▄▅▅▅▄▅▇▅▆▆▆▅▄▇▆█▇▇▆▆▆▆██▇█</td></tr><tr><td>train_f1</td><td>▂▂▁▅▄▃▄▃▂▄▃▂▆▃▆▅▆▅▅▇▇▆█▇██▇▇▇█▇▇█████▇▇▇</td></tr><tr><td>train_loss_epoch</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▅▅▆▆▅▅▅▆▇▅▇▆▆███▇▇█▇▇▇▇▇███▇██▇▇▇████▇</td></tr><tr><td>val_auc</td><td>▂▆▁▇▇▇▇▇▇▇█▇▇▇▇██▇▇▇████████████████████</td></tr><tr><td>val_f1</td><td>▇▇▁▁▇▅▂▂▃▄▇▁▆▄▄▇█▇▆▆▇▇▇▇▆▆█▇▇▇▇▇▇▇▇█▇██▇</td></tr><tr><td>val_loss_epoch</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_step</td><td>█▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▂▁▁▂▂▁▂▁▁▂▁▁▂▁▁▂▁▁▁▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.6755</td></tr><tr><td>train_auc</td><td>0.65277</td></tr><tr><td>train_f1</td><td>0.57485</td></tr><tr><td>train_loss_epoch</td><td>0.6137</td></tr><tr><td>train_loss_step</td><td>0.58307</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.70438</td></tr><tr><td>val_auc</td><td>0.77118</td></tr><tr><td>val_f1</td><td>0.53714</td></tr><tr><td>val_loss_epoch</td><td>0.64396</td></tr><tr><td>val_loss_step</td><td>0.72061</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_64_onehot_2</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/adv2keqv' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/adv2keqv</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240101_103603-adv2keqv\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240101_105550-bfzh3bnm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/bfzh3bnm' target=\"_blank\">MLP_4_64_onehot_2</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/bfzh3bnm' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/bfzh3bnm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\torch_geometric\\deprecation.py:22: UserWarning: 'nn.glob.GlobalAttention' is deprecated, use 'nn.aggr.AttentionalAggregation' instead\n",
      "  warnings.warn(out)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_4_64_onehot_2\\attention\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 13.1 K\n",
      "2  | head        | Sequential       | 2.1 K \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 65    \n",
      "--------------------------------------------------\n",
      "15.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "15.3 K    Total params\n",
      "0.061     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▄▄▄▄▄▄▄▄▄▆▅▆▄▇▆█▇▇█▇▇███▇▇████▇▇██▆█▇██</td></tr><tr><td>train_auc</td><td>▁▁▂▁▂▁▁▂▁▃▆▄▆▅▇▇█▇▆▇▇▇▇▇▇█▇███▇▇▇▇▇▇▇▇█▇</td></tr><tr><td>train_f1</td><td>▆▂▁▁▁▁▁▁▁▁▆▄▆▂▇▇▇▆▆▇█████▆▇▇▇█▇▆▆▇▇██▆█▇</td></tr><tr><td>train_loss_epoch</td><td>█▇▇▇▇▇▇▇▇▆▄▆▆▅▃▄▃▄▄▂▃▂▃▂▃▃▂▂▁▂▂▃▄▂▂▅▃▃▂▃</td></tr><tr><td>train_loss_step</td><td>█▇▇▅▆▇▆▇▇▆█▆▆▅▆▇▃▃▄▅▄▆▆█▄▃▅▂▃▄▅▂▃▃▁▆▄▅▁▆</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▃▆▂▂▄▆▆▇▇▇▃▅▇▆▆▆▇▇▇▆▇█▇▆▇▅▆███</td></tr><tr><td>val_auc</td><td>▂▁▁▂▂▂▃▄▄▇█████▇▇▇▇▇▇▇██▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁█▅▂▂▃▅▅▇▇▆▃▄▆▅▅▅▆▇▇▆▆▇▇▆▇▄▅▇▇▇</td></tr><tr><td>val_loss_epoch</td><td>██▇▇█▇▇▆█▇▅▇▇▆▅▄▆▄▅▂▅▆▄▄▄▃▄▃▁▂▃▃▃▂▂▆▄▅▅▁</td></tr><tr><td>val_loss_step</td><td>█▇▆▇▇▇▆▅█▆▅▇▇▅▅▅▇▅▅▂▅▇▅▄▅▃▅▄▁▂▄▄▄▂▃▇▄▆▇▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.6755</td></tr><tr><td>train_auc</td><td>0.70438</td></tr><tr><td>train_f1</td><td>0.50899</td></tr><tr><td>train_loss_epoch</td><td>0.62997</td></tr><tr><td>train_loss_step</td><td>0.65074</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.70073</td></tr><tr><td>val_auc</td><td>0.7708</td></tr><tr><td>val_f1</td><td>0.56383</td></tr><tr><td>val_loss_epoch</td><td>0.51568</td></tr><tr><td>val_loss_step</td><td>0.45977</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_64_onehot_2</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/bfzh3bnm' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/bfzh3bnm</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240101_105550-bfzh3bnm\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240101_111556-1zsutamu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/1zsutamu' target=\"_blank\">MLP_4_64_onehot_2</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/1zsutamu' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/1zsutamu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_4_64_onehot_2\\attention2\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 13.1 K\n",
      "2  | head        | Sequential       | 2.1 K \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 4.2 K \n",
      "--------------------------------------------------\n",
      "19.5 K    Trainable params\n",
      "0         Non-trainable params\n",
      "19.5 K    Total params\n",
      "0.078     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▃▃▃▃▃▃▄▅▇▇▇▇▇▇▇█▇▇▇▆▆▇▇▇▇█▇▇██▇▇███▇█▇▇</td></tr><tr><td>train_auc</td><td>▁▂▁▁▁▁▂▃▆▇▇█▇▇▇██▇██▇▇▇▇███████▇████████</td></tr><tr><td>train_f1</td><td>▆▁▁▁▁▁▁▃▆▇▇▇▇▇███▇▆▇▅▆▆▆▇██████▇▇▇█▇█▇▇▇</td></tr><tr><td>train_loss_epoch</td><td>██▇▇▇▇▇▇▄▃▃▂▂▃▄▃▁▃▃▃▄▃▄▃▃▃▂▂▂▂▁▂▂▁▁▂▂▃▂▂</td></tr><tr><td>train_loss_step</td><td>▇▇▆▆▇▅▇▄█▃▄▄▇▇▃▄▂▆▂▃▅▃▆▃▅▂▇▅▅▅▅▇▄▆▂▄▁▃▂▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▂▆█▇▇▅█▇▆▇▆█▇▇▇█▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇▇</td></tr><tr><td>val_auc</td><td>▃▁▁▃▄▄▆▇▇▇▇▇▇▇▇█████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▂▆▇▇▇██▇▆▆▆▇██▇▇▆▆▇▇▆▇▇▇▇▇▇▆▇▆▇▇▇</td></tr><tr><td>val_loss_epoch</td><td>█▇█▇▇██▅▅▄▂▆▃▄▅▄▆▂▅▂▄▄▅▃▄▂▂▅▅▃▁▂▂▂▄▄▅▄▃▃</td></tr><tr><td>val_loss_step</td><td>▇▆█▆▆▇▇▅▅▄▂█▂▅▆▅█▂▆▂▄▄▇▄▅▂▂▇▇▄▁▂▃▃▆▅▆▆▄▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.6819</td></tr><tr><td>train_auc</td><td>0.71694</td></tr><tr><td>train_f1</td><td>0.51667</td></tr><tr><td>train_loss_epoch</td><td>0.59997</td></tr><tr><td>train_loss_step</td><td>0.54862</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.70438</td></tr><tr><td>val_auc</td><td>0.77505</td></tr><tr><td>val_f1</td><td>0.59296</td></tr><tr><td>val_loss_epoch</td><td>0.57125</td></tr><tr><td>val_loss_step</td><td>0.57917</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_64_onehot_2</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/1zsutamu' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/1zsutamu</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240101_111556-1zsutamu\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240101_113551-zsn96lzm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/zsn96lzm' target=\"_blank\">MLP_2_16_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/zsn96lzm' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/zsn96lzm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_2_16_onehot_3\\mean\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 432   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "586       Trainable params\n",
      "0         Non-trainable params\n",
      "586       Total params\n",
      "0.002     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▄▅▅▅▅▅▅▅▅▅▆▇▇▇▇█▇▇▇██▇███▇▇▇█▇██▇▇▇██▇█</td></tr><tr><td>train_auc</td><td>▂▂▂▁▁▂▂▁▂▂▃▅▆▇▇▇▇▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▆▇▇█▇█</td></tr><tr><td>train_f1</td><td>█▃▁▁▁▁▁▁▁▂▃▆▇▇▇▇█▇▇▇███▇██▇▇▇▇▇█▇█▇▇▇█▇▇</td></tr><tr><td>train_loss_epoch</td><td>█▇▆▆▆▆▆▆▆▆▅▄▃▅▃▂▃▃▂▂▃▂▂▂▂▃▂▂▂▂▂▂▃▂▂▃▁▁▃▃</td></tr><tr><td>train_loss_step</td><td>▆▇▇█▇███▇▆▆▄▄▅▆▅▄▅▆▇▅▄▇▆▇▆▆▇▅▄▅▅▇▆▅▄▄▁▅▇</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▂▅▇▇▇▅▇▇▇▇█▇█▇█▇████▇███▇▇████</td></tr><tr><td>val_auc</td><td>▁▄▅▅▇███▇███████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▂▅▇▇▇▅█▇▇▇█▇██▇█▇██▇▇▇▇█▇▇▇███</td></tr><tr><td>val_loss_epoch</td><td>▇▇▇▇██▇█▇▇▆▅▄▃▂█▅▃▄▄▅▅▃▁▃▅▅▂▅▄▇▃▄▄▄▃▄▃▃▄</td></tr><tr><td>val_loss_step</td><td>▅▅▆▆▆▆▅▆▅▅▅▄▄▃▂█▅▃▄▄▆▅▃▁▃▅▆▃▅▄▇▃▄▅▄▃▄▃▃▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.68767</td></tr><tr><td>train_auc</td><td>0.73401</td></tr><tr><td>train_f1</td><td>0.5072</td></tr><tr><td>train_loss_epoch</td><td>0.61686</td></tr><tr><td>train_loss_step</td><td>0.67958</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.72527</td></tr><tr><td>val_auc</td><td>0.78013</td></tr><tr><td>val_f1</td><td>0.60733</td></tr><tr><td>val_loss_epoch</td><td>0.58768</td></tr><tr><td>val_loss_step</td><td>0.606</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_16_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/zsn96lzm' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/zsn96lzm</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240101_113551-zsn96lzm\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240101_115530-1ephe6oi</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/1ephe6oi' target=\"_blank\">MLP_2_16_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/1ephe6oi' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/1ephe6oi</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_2_16_onehot_3\\max\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 432   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "586       Trainable params\n",
      "0         Non-trainable params\n",
      "586       Total params\n",
      "0.002     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\torch_geometric\\warnings.py:17: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▆▆▇▆▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▆▇▇▇▇█▇██▇█▇█▇███▇█</td></tr><tr><td>train_auc</td><td>▁▁▃▂▁▂▁▁▂▂▂▃▁▁▁▂▂▃▂▄▃▃▃▄▅▃▄▄▇▇▅▇▇▅▅▇▆▆▇█</td></tr><tr><td>train_f1</td><td>█▂▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▄▃▃▄▄▄▆▄▅▆▅▆▅▆▅▆▅▅▅▆</td></tr><tr><td>train_loss_epoch</td><td>█▄▃▃▃▃▃▃▃▃▃▃▃▃▃▂▃▃▃▃▂▃▂▂▂▃▂▂▁▁▂▂▂▂▂▂▂▁▂▁</td></tr><tr><td>train_loss_step</td><td>▄▆▅▇▆█▇▆▆▅▇▅▅▄▃▇▆▅▅▆▆▆▅▆█▇▅▇▅▁▄▄▇▄▅▅▅▁▃▄</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁███████████████▁▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>▃▄▄▄▅▅▄▄▄▁▃▄▅▅▅▃▄▆▆▆▇███████████████▇▇▇█</td></tr><tr><td>val_f1</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁████████████████████████</td></tr><tr><td>val_loss_epoch</td><td>▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▂▂▃▄▂▃▂▅▃▃▄▄▅▅██▆▂</td></tr><tr><td>val_loss_step</td><td>▄▃▃▃▃▃▃▄▃▃▃▃▃▃▃▃▃▃▃▄▄▃▄▄▄▆▃▄▄▆▄▃▄▅▆▆█▇▆▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.61735</td></tr><tr><td>train_auc</td><td>0.62723</td></tr><tr><td>train_f1</td><td>0.34016</td></tr><tr><td>train_loss_epoch</td><td>0.65</td></tr><tr><td>train_loss_step</td><td>0.64535</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.42125</td></tr><tr><td>val_auc</td><td>0.70427</td></tr><tr><td>val_f1</td><td>0.59278</td></tr><tr><td>val_loss_epoch</td><td>0.71068</td></tr><tr><td>val_loss_step</td><td>0.56842</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_16_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/1ephe6oi' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/1ephe6oi</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240101_115530-1ephe6oi\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240101_121527-qjruz0s0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/qjruz0s0' target=\"_blank\">MLP_2_16_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/qjruz0s0' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/qjruz0s0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_2_16_onehot_3\\sum\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 432   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "586       Trainable params\n",
      "0         Non-trainable params\n",
      "586       Total params\n",
      "0.002     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▄▃▃▂▄▄▅▆▅▅▅▆▅▆▆▅▆▆▆▇▇█▇▇▇▇▆▇▇▇▇▇█▇▇████</td></tr><tr><td>train_auc</td><td>▁▃▄▅▃██▄▅█▆▆▆▃▃▆▆▆▇▃▄▄▆▄▄▃▇▆▅▂▂▄▁▃▄▃▄▂▂▃</td></tr><tr><td>train_f1</td><td>▆▁▂▂▂▃▄▃▄▄▄▂▄▂▃▄▆▄▅▃▅▄▇▅▄▅▆▄▅▂▄▅▅▅▅▆▇▅▆█</td></tr><tr><td>train_loss_epoch</td><td>█▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▄▂▂▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▂▃▄▄▃▃▆▄▄▄▃▄▂█▃▇▄▆▅▆▆▅█▆▆▆▆▆▆▅▆▅▇▆▆▆▇</td></tr><tr><td>val_auc</td><td>▂▂▇▃▇▇▇▃▅█▆▇▇▄▄█▆█▇▅▅▇▇▃▃▂▅▆▄▃▂▂▂▂▂▁▂▂▄▁</td></tr><tr><td>val_f1</td><td>▁▁▁▂▄▄▄▃▃▅▄▄▄▃▄▂█▄▇▅▆▅▇▆▅█▆▆▆▆▆▆▅▆▅▇▆▆▆▇</td></tr><tr><td>val_loss_epoch</td><td>█▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_step</td><td>█▂▃▂▂▂▁▂▂▁▁▁▂▁▁▂▂▂▂▂▂▂▁▁▁▂▂▁▂▁▂▁▂▂▁▁▁▂▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.65023</td></tr><tr><td>train_auc</td><td>0.47581</td></tr><tr><td>train_f1</td><td>0.54132</td></tr><tr><td>train_loss_epoch</td><td>0.63034</td></tr><tr><td>train_loss_step</td><td>0.6587</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.69597</td></tr><tr><td>val_auc</td><td>0.27116</td></tr><tr><td>val_f1</td><td>0.53631</td></tr><tr><td>val_loss_epoch</td><td>0.63129</td></tr><tr><td>val_loss_step</td><td>0.68125</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_16_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/qjruz0s0' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/qjruz0s0</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240101_121527-qjruz0s0\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240101_123455-i7iyti05</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/i7iyti05' target=\"_blank\">MLP_2_16_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/i7iyti05' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/i7iyti05</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\torch_geometric\\deprecation.py:22: UserWarning: 'nn.glob.GlobalAttention' is deprecated, use 'nn.aggr.AttentionalAggregation' instead\n",
      "  warnings.warn(out)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_2_16_onehot_3\\attention\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 432   \n",
      "2  | head        | Sequential       | 154   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 17    \n",
      "--------------------------------------------------\n",
      "603       Trainable params\n",
      "0         Non-trainable params\n",
      "603       Total params\n",
      "0.002     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▄▄▅▄▅▅▅▅▅▅▆▇▆▇▇▇▇▇█▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇█▆██▇</td></tr><tr><td>train_auc</td><td>▂▂▃▂▁▂▂▁▃▃▃▅▇▆▆▆▆▇▇▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇███</td></tr><tr><td>train_f1</td><td>█▂▁▁▁▁▁▁▁▁▃▆▇█▇▆▇█▆▇█▅▇▇▇▇▇██▇▇▇▇▇██▅█▇▇</td></tr><tr><td>train_loss_epoch</td><td>█▆▆▆▆▆▆▆▅▅▅▃▂▃▂▃▃▄▂▂▃▂▂▂▃▃▂▂▂▂▃▂▂▂▂▂▃▂▂▁</td></tr><tr><td>train_loss_step</td><td>▇███▇▇█▇█▆█▅▆▄▆▄▅▄▅▆▄▇▅▂▅▃▇▇▄▅▄▄▃▅▃▄▅▅▄▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▃▃▃▃▅▆▃▅▆▄▆▃▅▅▅▅▅▅█▅▅▅▅▅▅█▅▅▇▆</td></tr><tr><td>val_auc</td><td>▁▅▇▇▇███████████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▃▄▃▄▅▆▄▅▆▄▆▄▅▆▆▅▅▅█▆▅▆▆▅▆█▅▅▇▆</td></tr><tr><td>val_loss_epoch</td><td>▇▅▇▇█▆▆▇▆▅▆▅▅▄▆▂▄▂▄▅▅▄▅▄▃▁▂▄█▁▄▃▂▆▅▃▆▅▃▅</td></tr><tr><td>val_loss_step</td><td>▅▄▅▆▆▅▄▅▄▄▅▄▄▄▅▂▄▂▄▅▅▃▅▄▃▁▂▄█▁▄▃▂▆▅▄▅▅▄▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.66575</td></tr><tr><td>train_auc</td><td>0.72661</td></tr><tr><td>train_f1</td><td>0.46491</td></tr><tr><td>train_loss_epoch</td><td>0.59649</td></tr><tr><td>train_loss_step</td><td>0.51815</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.68132</td></tr><tr><td>val_auc</td><td>0.78173</td></tr><tr><td>val_f1</td><td>0.46626</td></tr><tr><td>val_loss_epoch</td><td>0.62514</td></tr><tr><td>val_loss_step</td><td>0.67236</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_16_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/i7iyti05' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/i7iyti05</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240101_123455-i7iyti05\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240101_125421-d143dbjd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/d143dbjd' target=\"_blank\">MLP_2_16_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/d143dbjd' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/d143dbjd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_2_16_onehot_3\\attention2\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 432   \n",
      "2  | head        | Sequential       | 154   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 281   \n",
      "--------------------------------------------------\n",
      "867       Trainable params\n",
      "0         Non-trainable params\n",
      "867       Total params\n",
      "0.003     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▅▅▅▅▅▅▅▅▅▅▆▇▇▇▇▇▇▇▇▇████████▇█████▇██▇▇</td></tr><tr><td>train_auc</td><td>▁▂▂▁▁▂▂▂▂▃▃▅▆▇▆▆▇▇▇█▇▇█▇▇▇▇▇█▇█▇██████▇█</td></tr><tr><td>train_f1</td><td>▇▃▂▁▁▁▁▁▁▁▂▅▅▆▇▅▅▇▆▇▆▇▇▇▇▇▇▇▇▇█▇▇▇▇▆▇▇▇▇</td></tr><tr><td>train_loss_epoch</td><td>█▆▆▆▆▆▆▆▆▆▅▄▄▄▃▃▄▂▃▂▂▂▁▂▂▂▂▂▃▂▂▂▂▁▃▂▁▁▂▃</td></tr><tr><td>train_loss_step</td><td>▇█▆▇▆▇▆▇▆▆▇▆▆█▄▃▄▃▅▄▄▃▅▄█▂▆▁▂▄▆▄▆▆▄▆▅▅▇▆</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▅▇▇▆▇▅▅▇▇▇▆█▇▇▇█▇▇▇██▇▇█▇█▇█▇</td></tr><tr><td>val_auc</td><td>▁▅▇█████████████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▅▇▇▆▇▅▅▇▇▇▆▇▇▇▇█▇▇▇██▇▇█▇▇▇█▇</td></tr><tr><td>val_loss_epoch</td><td>▇▇▇█▆▇▆▆▆▆▆▅▃▃▃▃▃▄▄▄▄▄▂▂▂▅▂▂▄▃▁▅▆▄▄▃▆▄▃▃</td></tr><tr><td>val_loss_step</td><td>▆▇▆█▆▆▆▆▆▆▆▅▃▃▃▄▃▄▅▅▅▄▃▂▃▆▃▂▅▃▁▆▇▅▅▄▇▄▄▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.67032</td></tr><tr><td>train_auc</td><td>0.71232</td></tr><tr><td>train_f1</td><td>0.48794</td></tr><tr><td>train_loss_epoch</td><td>0.62098</td></tr><tr><td>train_loss_step</td><td>0.64682</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.70696</td></tr><tr><td>val_auc</td><td>0.78283</td></tr><tr><td>val_f1</td><td>0.55056</td></tr><tr><td>val_loss_epoch</td><td>0.56396</td></tr><tr><td>val_loss_step</td><td>0.55485</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_16_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/d143dbjd' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/d143dbjd</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240101_125421-d143dbjd\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240101_131347-ptzqxruz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/ptzqxruz' target=\"_blank\">MLP_2_32_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/ptzqxruz' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/ptzqxruz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_2_32_onehot_3\\mean\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 1.4 K \n",
      "2 | head        | Sequential       | 562   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.008     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▁▁▁▁▃▄▆▆▆▆▇▆▇█▇█▇▇▇▇▇█▇▇█▇▆██▇▇▇▇█▇██▇</td></tr><tr><td>train_auc</td><td>▁▁▂▃▃▅▅▇▇▇▇▇█▇▇█▇▇██▇██████▇▇████████▇██</td></tr><tr><td>train_f1</td><td>▁▁▁▁▁▁▅▅▆▆▇▆▇▆▇█▇█▇▇▇▇▇▇▇▇▇▇▇▇█▇▇▇██▇▇▇▇</td></tr><tr><td>train_loss_epoch</td><td>██▇▇▇▆▆▄▄▃▃▃▂▄▃▂▄▃▁▃▃▃▃▂▃▂▃▂▃▁▁▁▃▂▁▂▂▂▂▂</td></tr><tr><td>train_loss_step</td><td>▅█▅▆▅▆▃▂▃▄▃▆▁▅▂▅▂▂▃▅▅▃▂▁▃▆▅▃▄▄▅▄▄▁▅▄▆▄▄▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▄▇▇▇▇▇▇▇▅▇▇▇▇█▇▆██▇██▇▇██▇█▇▇██▇▇▇</td></tr><tr><td>val_auc</td><td>▁▆▇▇▇███████████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▅█▇▇▇▇▇▇▅█▇███▇▆████▇▇█████▇▇██▇▇█</td></tr><tr><td>val_loss_epoch</td><td>█▇▆▆▇▆▅▄▂▃▂▃▄▂▃▃▃▂▅▅▄▄▃▃▂▃▃▄▂▃▄▂▁▄▄▃▅▃▂▃</td></tr><tr><td>val_loss_step</td><td>█▇▆▅▆▅▅▄▂▃▂▃▅▂▂▄▄▃▆▆▅▄▃▃▃▃▃▅▂▄▅▃▁▅▅▄▆▃▂▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.68128</td></tr><tr><td>train_auc</td><td>0.72771</td></tr><tr><td>train_f1</td><td>0.50636</td></tr><tr><td>train_loss_epoch</td><td>0.59886</td></tr><tr><td>train_loss_step</td><td>0.58259</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.71429</td></tr><tr><td>val_auc</td><td>0.77898</td></tr><tr><td>val_f1</td><td>0.6422</td></tr><tr><td>val_loss_epoch</td><td>0.55994</td></tr><tr><td>val_loss_step</td><td>0.54783</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_32_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/ptzqxruz' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/ptzqxruz</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240101_131347-ptzqxruz\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240101_133416-zzxjxskr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/zzxjxskr' target=\"_blank\">MLP_2_32_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/zzxjxskr' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/zzxjxskr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_2_32_onehot_3\\max\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 1.4 K \n",
      "2 | head        | Sequential       | 562   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.008     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\torch_geometric\\warnings.py:17: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▃▂▄▄▄▄▄▄▄▅▅▅▇▆▆█▅▇▆▆▇▇█▆█▇▇▇▇█▇▇▆▆▇█▇▇▇</td></tr><tr><td>train_auc</td><td>▂▁▂▃▁▂▂▃▄▄▅▆▆▆▇▆▇▆▇▇▇▇▇▇▇██▆▇▆▇█▇▇▇█▇▆▇▇</td></tr><tr><td>train_f1</td><td>▆▁▂▁▁▁▁▂▄▄▄▄▆▆▇▆█▇▇▇▆█▇▇▆█▇▇▇▇▇██▇▇█▇▇▇▆</td></tr><tr><td>train_loss_epoch</td><td>█▇▆▆▅▅▆▆▄▅▄▄▃▄▂▄▂▄▃▃▃▃▄▂▂▁▃▂▄▂▂▁▃▂▂▂▂▁▂▃</td></tr><tr><td>train_loss_step</td><td>▆█▅▆▅▇▄▄▆▇▆▇▁▄▃▅▄▁▄▂▁▂█▄▃▇▅▅▄▆▅▅▅▃▅▇▇▄▄▆</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▅▅▅▅▅▅▅▅▅██▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>▁▁▁▂▂▂▅▇▇█████████▇█████▇██▇▇▇▇▇█▇▇█████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁███████████████████████████████</td></tr><tr><td>val_loss_epoch</td><td>▄▃▁▁▂▁▁▂▁▁▂▂▃▂▄▄▅▄▂▃▃▃▃▃▃▅▂▄▄▄█▅▅▃▅▅▃▅▄▄</td></tr><tr><td>val_loss_step</td><td>▅▃▂▁▂▂▂▃▂▂▂▂▃▂▄▄▅▄▁▃▂▃▃▃▃▄▂▄▃▄█▄▄▃▄▄▃▅▃▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.63927</td></tr><tr><td>train_auc</td><td>0.67005</td></tr><tr><td>train_f1</td><td>0.38949</td></tr><tr><td>train_loss_epoch</td><td>0.6379</td></tr><tr><td>train_loss_step</td><td>0.66404</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.42125</td></tr><tr><td>val_auc</td><td>0.6967</td></tr><tr><td>val_f1</td><td>0.59278</td></tr><tr><td>val_loss_epoch</td><td>0.76833</td></tr><tr><td>val_loss_step</td><td>0.80663</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_32_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/zzxjxskr' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/zzxjxskr</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240101_133416-zzxjxskr\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240101_135354-7sy6273d</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/7sy6273d' target=\"_blank\">MLP_2_32_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/7sy6273d' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/7sy6273d</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_2_32_onehot_3\\sum\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 1.4 K \n",
      "2 | head        | Sequential       | 562   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.008     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▂▁▂▂▃▅▃▄▄▄▅▄▅▅▅▅▅▅▆▆▆▇▆▆▇▇▆▆▆▆▇▇▇▆▇▇█▇█▇</td></tr><tr><td>train_auc</td><td>▁▂▄▃▂▄▄▁▆▆▃▅▅▃▄▅▂▂▃▄▅▂▅▁▅▅▅▅▃▆▄▆▆▃▃█▆▆▅▇</td></tr><tr><td>train_f1</td><td>▄▃▄▃▂▃▄▁▂▂▃▂▄▂▅▃▅▅▅▆▆▆▅▇▇▆▅▆▆▇▇█▇▇▇█▇▇▇▇</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▂▃▁▃▃▁▃▂▃▄▃▃▃▄▆▅▃▅▄▅▆█▇▅▄█▆███▆██▅▇██</td></tr><tr><td>val_auc</td><td>▁▅▄▄▂▃▃▃▄▄▅▆▅▃▅▆▃▂▂▅▅▅▂▂▂▅▂▂▃▇▇▆▄▄██▇▇▇▇</td></tr><tr><td>val_f1</td><td>▁▁▁▂▄▂▃▃▂▃▂▃▄▃▃▄▅▆▆▃▆▅▅▆▇▇▅▅█▆███▆██▆▇██</td></tr><tr><td>val_loss_epoch</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_step</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.67671</td></tr><tr><td>train_auc</td><td>0.54992</td></tr><tr><td>train_f1</td><td>0.56618</td></tr><tr><td>train_loss_epoch</td><td>0.61545</td></tr><tr><td>train_loss_step</td><td>0.63697</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.73626</td></tr><tr><td>val_auc</td><td>0.73919</td></tr><tr><td>val_f1</td><td>0.65714</td></tr><tr><td>val_loss_epoch</td><td>0.53207</td></tr><tr><td>val_loss_step</td><td>0.48722</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_32_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/7sy6273d' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/7sy6273d</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240101_135354-7sy6273d\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240101_141334-z9k84k4u</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/z9k84k4u' target=\"_blank\">MLP_2_32_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/z9k84k4u' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/z9k84k4u</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\torch_geometric\\deprecation.py:22: UserWarning: 'nn.glob.GlobalAttention' is deprecated, use 'nn.aggr.AttentionalAggregation' instead\n",
      "  warnings.warn(out)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_2_32_onehot_3\\attention\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 1.4 K \n",
      "2  | head        | Sequential       | 562   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 33    \n",
      "--------------------------------------------------\n",
      "2.0 K     Trainable params\n",
      "0         Non-trainable params\n",
      "2.0 K     Total params\n",
      "0.008     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▁▁▁▃▃▅▆▅▅▆▅▅▅▆▇▆▇▇▇▇▇▇▇▆▅▇▆▇▇██▆▆▆▇▆▇▆</td></tr><tr><td>train_auc</td><td>▂▁▁▂▃▄▆▇▇▇▇▇▇▇▇▇█▇██████▇█▇▇▇█████▇█████</td></tr><tr><td>train_f1</td><td>▁▁▁▁▁▃▃▆▇▅▅▇▆▅▇▆█▇▇▇███▇▇▆▅▇▇▇▇██▇▇▇▇▇▇▇</td></tr><tr><td>train_loss_epoch</td><td>█▇█▇▇▆▄▄▂▃▃▃▃▂▂▂▂▃▂▂▁▁▁▂▃▂▃▃▂▂▂▁▂▁▂▃▂▂▂▁</td></tr><tr><td>train_loss_step</td><td>▇▇▆▆▇▅▃▆▅▄▆▄▂▅▅▄▁█▂▆▆▃▂▆▅▂▅▃▅▅▄▃▁▇▅▄▄▅▁▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▄▇▅█▅▅▇▅▅▅▅▇▇▅▆▅▅▆█▇▇▄▆█▇▇▇▇▇▇▇▇▇█</td></tr><tr><td>val_auc</td><td>▁▇▇▇▇███████████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▄▇▆█▅▅▇▅▅▅▅▇▇▆▇▅▅▇██▇▄▆█▇▇▇█▇▇▇▇██</td></tr><tr><td>val_loss_epoch</td><td>██▆▇▆▇█▄▅▃▃▆▂▃▅▆█▅▃▄▂▅▅▅▁▄▃▂▅▅▇▂▅▃▅▃▃▃▅▅</td></tr><tr><td>val_loss_step</td><td>▆▆▅▅▅▆▇▄▅▄▃▇▂▃▅▆█▅▃▄▂▅▅▆▁▅▃▁▅▆█▃▆▄▅▃▄▄▆▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.6758</td></tr><tr><td>train_auc</td><td>0.71859</td></tr><tr><td>train_f1</td><td>0.50763</td></tr><tr><td>train_loss_epoch</td><td>0.59741</td></tr><tr><td>train_loss_step</td><td>0.54522</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.71795</td></tr><tr><td>val_auc</td><td>0.78547</td></tr><tr><td>val_f1</td><td>0.58378</td></tr><tr><td>val_loss_epoch</td><td>0.61958</td></tr><tr><td>val_loss_step</td><td>0.68166</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_32_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/z9k84k4u' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/z9k84k4u</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240101_141334-z9k84k4u\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240101_143345-82ex2fjq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/82ex2fjq' target=\"_blank\">MLP_2_32_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/82ex2fjq' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/82ex2fjq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_2_32_onehot_3\\attention2\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 1.4 K \n",
      "2  | head        | Sequential       | 562   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 1.1 K \n",
      "--------------------------------------------------\n",
      "3.0 K     Trainable params\n",
      "0         Non-trainable params\n",
      "3.0 K     Total params\n",
      "0.012     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▁▁▂▁▄▆▆▇▇▆▇▇▇▅▇▇█▆▆▆▇▇▇▇▇▇▇▇▆▇▇█▇███▇▇</td></tr><tr><td>train_auc</td><td>▁▁▂▂▂▄▅▆▇█▇▇▇▇█▇██▇▇▇██▇█▇▇▇█▇▇▇█████▇▇█</td></tr><tr><td>train_f1</td><td>▁▁▁▁▂▁▆▇▇▇█▇█▇█▆▇███▇▇▇▇▇█▇▇▇▇▇█▇█▇█▇▇▇▇</td></tr><tr><td>train_loss_epoch</td><td>██▇█▇▇▆▅▃▃▃▄▅▃▂▃▄▂▂▂▃▂▂▃▂▃▂▃▂▃▃▃▂▃▂▁▂▄▂▂</td></tr><tr><td>train_loss_step</td><td>▇▇▇▇▇▆▅▄▄▆▄▅▇▅▁▅▅▃█▂▃▄▃▄▆▄▇▄▄▄▆▆▄▃▃▂▅▄▄▄</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▅▅▇▇▇▇▇▇▇▇▇▆▇▇▇▇▇▇▇▇█▇▇▇▇▇▇█▇██▇▆█</td></tr><tr><td>val_auc</td><td>▁▇▇▇▇███████████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▅▆███▇█▇███▆█▇▇▇▇▇▇▇█▇▇▇▇▇▇█▇███▆█</td></tr><tr><td>val_loss_epoch</td><td>▇█▇▆▅▆▅▃▄▃▁▂▃▅▆▁▅▄▂▂▃▅▂▂▂▂▂▅▁▄▂▁▂▄▅▃▃▄▅▆</td></tr><tr><td>val_loss_step</td><td>▆█▆▆▄▆▅▃▅▄▁▂▃▆█▁▆▅▂▃▄▆▃▃▃▃▃▇▁▅▂▁▃▅▇▄▄▅▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.68219</td></tr><tr><td>train_auc</td><td>0.73118</td></tr><tr><td>train_f1</td><td>0.52329</td></tr><tr><td>train_loss_epoch</td><td>0.59708</td></tr><tr><td>train_loss_step</td><td>0.59709</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.72161</td></tr><tr><td>val_auc</td><td>0.78107</td></tr><tr><td>val_f1</td><td>0.62</td></tr><tr><td>val_loss_epoch</td><td>0.65162</td></tr><tr><td>val_loss_step</td><td>0.75201</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_32_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/82ex2fjq' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/82ex2fjq</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240101_143345-82ex2fjq\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240101_145346-299qxop0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/299qxop0' target=\"_blank\">MLP_2_64_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/299qxop0' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/299qxop0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_2_64_onehot_3\\mean\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 4.8 K \n",
      "2 | head        | Sequential       | 2.1 K \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "6.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "6.9 K     Total params\n",
      "0.028     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▃▃▄▄▇▇▇▇▇▇▇▇▇▇▇▇▇█▇▇▇▇▇▇█▇▇▇█▇▇▇▇▇█▇▇▇▇</td></tr><tr><td>train_auc</td><td>▁▂▃▃▅▇▇▇▇▇▇▇██▇██████████████████▇██▇██▇</td></tr><tr><td>train_f1</td><td>▅▄▁▂▃▆█▇▇▇▇▇██▇▇██████▇▇▇▇▇█▇▇▇▆█▇██▇▇▇▇</td></tr><tr><td>train_loss_epoch</td><td>█▆▆▆▅▃▄▄▃▃▂▃▂▃▂▂▂▂▃▂▂▂▂▁▂▂▂▂▂▂▂▂▃▃▂▁▂▂▁▃</td></tr><tr><td>train_loss_step</td><td>█▇█▇▆▅▅▆▆▄▃▆▅▄▆▃▅▄▄▇▂▁▃▃▅▅▄▃▃▆▃▂▄▅▅▆▅▄▇█</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▄▅▅▇▇▇█▇██▇█▇█▇███▇███▇███▇▇▇██▇▇▇██</td></tr><tr><td>val_auc</td><td>▁▃▄▅▅▆▆▆▇▇▇▇▇▇██████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▂▄▅▆▇▇▇██████▇▇█▇██████▇███▇▇▇████▇██</td></tr><tr><td>val_loss_epoch</td><td>▇▇▆▇▅▅█▅▄▄▃▇▅▂▄▂▁▅▃▄▄▃▅▃▁▂▃▃▂▂▅▅▄▄▄▄▅▃▄▃</td></tr><tr><td>val_loss_step</td><td>▆▅▅▆▄▅█▅▄▄▃█▅▂▄▂▁▅▃▄▄▄▅▃▁▂▃▄▂▂▅▆▄▅▄▄▅▃▅▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.68767</td></tr><tr><td>train_auc</td><td>0.71919</td></tr><tr><td>train_f1</td><td>0.56489</td></tr><tr><td>train_loss_epoch</td><td>0.61674</td></tr><tr><td>train_loss_step</td><td>0.68495</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.72527</td></tr><tr><td>val_auc</td><td>0.77925</td></tr><tr><td>val_f1</td><td>0.62687</td></tr><tr><td>val_loss_epoch</td><td>0.54824</td></tr><tr><td>val_loss_step</td><td>0.53481</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_64_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/299qxop0' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/299qxop0</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240101_145346-299qxop0\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240101_151324-rwnhbfga</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/rwnhbfga' target=\"_blank\">MLP_2_64_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/rwnhbfga' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/rwnhbfga</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_2_64_onehot_3\\max\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 4.8 K \n",
      "2 | head        | Sequential       | 2.1 K \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "6.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "6.9 K     Total params\n",
      "0.028     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\torch_geometric\\warnings.py:17: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▃▄▅▅▆▆▆▆▇▇▆▆▆▇▇▆▆▇▇▇▆▇▇▆▇█▆▇▇▆▆▇▇▆▆▇▇▇</td></tr><tr><td>train_auc</td><td>▁▂▃▃▄▇▆▆▇▇▇▇▇▆▇▇▇█▇▇▇▇▇▇▇▇▇██▇█▇▇██▇▇█▇▇</td></tr><tr><td>train_f1</td><td>▄▅▁▃▄▅▆▆▇▆▆▇▇▇▆▇▇▇▇▇█▇▆▇▇▆▇█▆▇▇▆▇███▇▆▇▇</td></tr><tr><td>train_loss_epoch</td><td>█▅▅▄▄▂▃▃▃▂▂▃▃▃▂▂▁▁▂▂▂▂▂▁▂▁▂▁▂▂▁▂▃▁▁▃▂▂▁▂</td></tr><tr><td>train_loss_step</td><td>█▇█▄▄▃▄▅▅▆▄▇▂▆▅▆▆▁▂▅▃▂▂▁▆▃▅▃▂▃▅▆▅▇▆█▆█▆▇</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>▇▇███████▇▇▇▇▇▇▅▆█▅▂▂▂▂▂▂▁▂▁▁▁▂▂▂▂▂▂▂▁▁▂</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_epoch</td><td>▁▁▁▁▁▁▁▂▂▄▃▃▄▄▄▄▆▄▅▅▆▇▆▆▇▅▇▇▆▆▇▅▇▄█▅▄▅▄▅</td></tr><tr><td>val_loss_step</td><td>▁▁▁▁▁▁▁▂▁▄▃▂▅▄▃▄▆▂▃▄▅▆▆▅█▅▆▆▄▄▆▄▆▁█▃▂▅▂▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.66301</td></tr><tr><td>train_auc</td><td>0.67443</td></tr><tr><td>train_f1</td><td>0.54835</td></tr><tr><td>train_loss_epoch</td><td>0.63099</td></tr><tr><td>train_loss_step</td><td>0.66324</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.42125</td></tr><tr><td>val_auc</td><td>0.63068</td></tr><tr><td>val_f1</td><td>0.59278</td></tr><tr><td>val_loss_epoch</td><td>1.24964</td></tr><tr><td>val_loss_step</td><td>1.22823</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_64_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/rwnhbfga' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/rwnhbfga</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240101_151324-rwnhbfga\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240101_153318-kigj26k0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/kigj26k0' target=\"_blank\">MLP_2_64_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/kigj26k0' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/kigj26k0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_2_64_onehot_3\\sum\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 4.8 K \n",
      "2 | head        | Sequential       | 2.1 K \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "6.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "6.9 K     Total params\n",
      "0.028     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▄▂▃▅▆▇▆▆▇▇▇▇▇▇█▇▇▇█▇█▇▇███▆▇██████▇▆███</td></tr><tr><td>train_auc</td><td>▇█▆▇▅▆▆▆▅▅▅▅▆▅▅▅▅▆▄▅▅▆▄▄▃▃▄▄▄▅▄▅▄▃▃▄▃▁▂▃</td></tr><tr><td>train_f1</td><td>▂▁▃▃▃▅▆▃▆▆▆▆▆▇▅▆▇▇▇███▇▇█▇▇█▅▆▆▅▇▆▇█▇▇▆█</td></tr><tr><td>train_loss_epoch</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▃▄▂▂▁▂▂▂▁▁▂▁▂▁▁▂▂▂▂▁▁▂▁▂▂▁▁▁▂▁▁▁▂▂▂▁▂▂▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▆▆▆▄▄▆▆▅▅▇▆▅▆▆▆▅▅▇▆▆▅▇▆▆▅▆▇▇██▇█▆▅▅▇▇▆█</td></tr><tr><td>val_auc</td><td>▇▆█▆▄▇▅▄▄▃▃▄▄▃▃▅▃▃▂▂▃▂▂▂▂▁▁▂▁▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>val_f1</td><td>▁█▇▇▅▅▆▆▆▆▇▆▆▇▇▇▆▆▇▇▆▆▇▇▆▆▇▇▇██▇█▇▆▆▇▇▇█</td></tr><tr><td>val_loss_epoch</td><td>█▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_step</td><td>█▁▁▁▁▁▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▂▁▁▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.68858</td></tr><tr><td>train_auc</td><td>0.42011</td></tr><tr><td>train_f1</td><td>0.61469</td></tr><tr><td>train_loss_epoch</td><td>0.61749</td></tr><tr><td>train_loss_step</td><td>0.71</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.73626</td></tr><tr><td>val_auc</td><td>0.24562</td></tr><tr><td>val_f1</td><td>0.65714</td></tr><tr><td>val_loss_epoch</td><td>0.55955</td></tr><tr><td>val_loss_step</td><td>0.55686</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_64_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/kigj26k0' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/kigj26k0</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240101_153318-kigj26k0\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240101_155241-ejl2ape2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/ejl2ape2' target=\"_blank\">MLP_2_64_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/ejl2ape2' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/ejl2ape2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\torch_geometric\\deprecation.py:22: UserWarning: 'nn.glob.GlobalAttention' is deprecated, use 'nn.aggr.AttentionalAggregation' instead\n",
      "  warnings.warn(out)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_2_64_onehot_3\\attention\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 4.8 K \n",
      "2  | head        | Sequential       | 2.1 K \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 65    \n",
      "--------------------------------------------------\n",
      "7.0 K     Trainable params\n",
      "0         Non-trainable params\n",
      "7.0 K     Total params\n",
      "0.028     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▃▃▄▅▆▇▇▇▇█▇▇▇▇▇█▇▇█▇▇██▇▇▇█▇▇▇█▇▇▇██▇█▇</td></tr><tr><td>train_auc</td><td>▁▂▂▄▆▇▇▇▇▇▇█▇▇▇▇█▇███████▇██▇▇▇████████▇</td></tr><tr><td>train_f1</td><td>▅▃▁▂▄▆▇▇▇▇█▇██▇████▇▆▆█▇▇▆▇▇▇▆▆▇██▇▇█▇▇█</td></tr><tr><td>train_loss_epoch</td><td>█▇▇▆▅▄▃▃▃▃▃▃▄▂▂▂▂▃▃▂▂▂▂▁▃▃▂▂▂▃▃▂▃▂▁▁▂▃▁▃</td></tr><tr><td>train_loss_step</td><td>▅▅▅▅▅▃▂▄▄▆▃▄▃▄▆▅▅▄▄▃▃▄▃▃▃▅▃▃▁▄▄▅▄▃▅█▃▂▅▄</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▃▅▅▆▄▄▄▅▅▄▄▅▄▅▅▆▇▇█▇▇█▇▆▆▇█▇▆▆▅▇▇▆▇▇▅</td></tr><tr><td>val_auc</td><td>▁▃▄▅▇▇▇▇▇▇▇▇▇▇▇▇▇▇██▇▇██▇▇█▇████▇▇█████▇</td></tr><tr><td>val_f1</td><td>▁▁▁▃▅▅▆▄▄▅▅▅▄▄▅▅▅▅▆▇▇▇▇▇█▇▇▇██▇▆▆▆▇▇▆▇▇▅</td></tr><tr><td>val_loss_epoch</td><td>▇▇█▇▄▄▃▄▄▄▄▇▇▄▃▄▇▄▅▂▄▃▁▂▄▄▅▄▄▃▃▁▄▄▂▄▄▃▁▂</td></tr><tr><td>val_loss_step</td><td>▆▇▇▆▃▄▃▄▄▄▄██▄▃▄▇▄▆▂▅▄▂▂▄▄▆▅▄▄▃▁▅▄▂▅▄▃▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.67397</td></tr><tr><td>train_auc</td><td>0.72554</td></tr><tr><td>train_f1</td><td>0.61322</td></tr><tr><td>train_loss_epoch</td><td>0.61023</td></tr><tr><td>train_loss_step</td><td>0.62802</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.67033</td></tr><tr><td>val_auc</td><td>0.79631</td></tr><tr><td>val_f1</td><td>0.42308</td></tr><tr><td>val_loss_epoch</td><td>0.49357</td></tr><tr><td>val_loss_step</td><td>0.39577</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_64_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/ejl2ape2' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/ejl2ape2</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240101_155241-ejl2ape2\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240101_161222-nu4qhr74</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/nu4qhr74' target=\"_blank\">MLP_2_64_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/nu4qhr74' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/nu4qhr74</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_2_64_onehot_3\\attention2\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 4.8 K \n",
      "2  | head        | Sequential       | 2.1 K \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 4.2 K \n",
      "--------------------------------------------------\n",
      "11.1 K    Trainable params\n",
      "0         Non-trainable params\n",
      "11.1 K    Total params\n",
      "0.045     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▃▃▅▆▆▆▆█▇▆▇█▇██▇█▇▇██▇▇██▇█▇█▇▇▇█████▇</td></tr><tr><td>train_auc</td><td>▁▁▂▃▅▆▆▇▆▇▇▇▇█▇▇▇▇██▇▇▇██▇███▇█▇▇▇██▇█▇▇</td></tr><tr><td>train_f1</td><td>▅▄▁▂▅▇▆▆▆█▆▆▇█▇███▇▆▇▇█▇██▇▇███▇▇▇▇▇▇███</td></tr><tr><td>train_loss_epoch</td><td>███▇▆▄▅▄▃▃▃▃▂▂▂▃▃▃▂▃▃▂▃▂▄▃▂▁▁▃▂▁▃▃▁▂▂▁▂▃</td></tr><tr><td>train_loss_step</td><td>▇▆▆▇█▇▃▄█▄▃▆▇▅▅█▅▇▄▆▄█▄▄▅▅▄▃▅▆▃▄▅▅▃▄▃▅▁▄</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▄▅▇█▇██▆▆▆▆▆▆▆▇▇▇▆▆▆▆▇▇▇█▆█▇▇▇▆▇▆██▆</td></tr><tr><td>val_auc</td><td>▁▃▃▄▇▇█████████▇████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▅▅▇█▇██▇▇▇▇▇▇▇▇█▇▆▇▇▆▇▇██▇███▇▇▇▆██▇</td></tr><tr><td>val_loss_epoch</td><td>█▇█▇▅▃▄▄▄▅▄▅▅▆▇▅▃▇▄▅▅▃▅▄▆▄▅▆▃▄▄▃▅▅▄▃▃▄▄▁</td></tr><tr><td>val_loss_step</td><td>▇▇█▆▅▃▄▄▅▆▄▆▅▇█▆▃█▄▆▆▃▆▅▇▄▆▇▃▅▅▄▅▆▅▃▃▅▅▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.66393</td></tr><tr><td>train_auc</td><td>0.72352</td></tr><tr><td>train_f1</td><td>0.60515</td></tr><tr><td>train_loss_epoch</td><td>0.60812</td></tr><tr><td>train_loss_step</td><td>0.58437</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.7033</td></tr><tr><td>val_auc</td><td>0.78712</td></tr><tr><td>val_f1</td><td>0.54237</td></tr><tr><td>val_loss_epoch</td><td>0.43424</td></tr><tr><td>val_loss_step</td><td>0.28985</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_64_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/nu4qhr74' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/nu4qhr74</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240101_161222-nu4qhr74\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240101_163207-7wz4oevh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/7wz4oevh' target=\"_blank\">MLP_3_16_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/7wz4oevh' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/7wz4oevh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_3_16_onehot_3\\mean\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 704   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "858       Trainable params\n",
      "0         Non-trainable params\n",
      "858       Total params\n",
      "0.003     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▂▂▂▂▂▂▂▂▃▃▅▆▆▆█▆▆▇▇▆█▇▇▇▆██▇▇██▇▇▇▇▇▇▇</td></tr><tr><td>train_auc</td><td>▂▂▁▂▁▂▂▂▁▃▃▄▆▇▇▇█▇▇██▇███▇████████▇█████</td></tr><tr><td>train_f1</td><td>▂▁▁▁▁▁▁▁▁▁▂▃▅▆▆▆█▆▆▇▇▇▇▇▇▇▆▇█▇▇▇█▇▇▇▇▇▇▇</td></tr><tr><td>train_loss_epoch</td><td>█▇█▇█▇▇▇▇▆▆▆▄▂▃▄▂▃▄▂▂▃▁▁▂▂▁▃▂▂▁▁▁▂▃▂▂▁▂▁</td></tr><tr><td>train_loss_step</td><td>▆▆█▆▇▇▇██▇▆▆▄▄▂▄▄▃▅▅▁▅▂▁▂▅▇▄▆▄▄▅▅▄▇▃▂▂▃▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▅▅▇▇▆▇▇▆▇████▆███▇██▇█▇▇▇████</td></tr><tr><td>val_auc</td><td>▁▄▆▆▆▇▇▇▇▇▇█████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▅▅▇▇▆▇█▆▇███▇▆▇█▇▇█▇▇█▇▇▇▇█▇▇</td></tr><tr><td>val_loss_epoch</td><td>▇▇▇▇█▆▇▇▆▇▆▆▅▆▃▄▅▄▃▄▃▃▃▅▂▄▄▄▂▁▂▄▄▃▄▂▁▂▅▂</td></tr><tr><td>val_loss_step</td><td>▆▇▆▆█▆▇▆▆▇▅▆▆▇▄▅▆▅▃▅▄▄▄▆▂▄▅▅▃▁▃▅▅▃▄▃▁▃▆▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.6758</td></tr><tr><td>train_auc</td><td>0.71014</td></tr><tr><td>train_f1</td><td>0.49929</td></tr><tr><td>train_loss_epoch</td><td>0.60395</td></tr><tr><td>train_loss_step</td><td>0.55596</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.71429</td></tr><tr><td>val_auc</td><td>0.78046</td></tr><tr><td>val_f1</td><td>0.56667</td></tr><tr><td>val_loss_epoch</td><td>0.52563</td></tr><tr><td>val_loss_step</td><td>0.47548</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_16_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/7wz4oevh' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/7wz4oevh</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240101_163207-7wz4oevh\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240101_165114-6yqt5imb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/6yqt5imb' target=\"_blank\">MLP_3_16_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/6yqt5imb' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/6yqt5imb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_3_16_onehot_3\\max\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 704   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "858       Trainable params\n",
      "0         Non-trainable params\n",
      "858       Total params\n",
      "0.003     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\torch_geometric\\warnings.py:17: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▃▆▅▅▅▅▅▅▆▆▅▆▆▆▆▆▆▆▇▇▆▆▆▇▇▅▇▇▇█▆▆▆▆▇█▇▆▇</td></tr><tr><td>train_auc</td><td>▁▂▂▂▁▂▂▂▁▁▂▃▆▅▄▄▆▄▆▇▆▅▅▆▇▇▅▆▆▇█▆▆▆█▆▆██▇</td></tr><tr><td>train_f1</td><td>▇▇▂▁▁▁▁▁▁▁▂▂▃▄▃▄▃▅▄▆▆▆▄▄▆▆▆▅█▆█▅▅▅▆█▇█▅▇</td></tr><tr><td>train_loss_epoch</td><td>█▄▃▃▃▃▃▃▃▃▂▃▂▂▃▃▂▃▃▂▂▂▂▂▁▁▂▃▂▂▁▂▂▂▂▂▂▂▃▁</td></tr><tr><td>train_loss_step</td><td>▆▅█▅▆▆▇█▇▆▆▆▃▄▅▇▄▅▄▄▃▆▃▃▃▆▆▆▄▄▄▅▆▅▆▄▇▂▄▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▇▇▅█▇▇▁▁▁▁</td></tr><tr><td>val_auc</td><td>▂▄▇▇▅▇▄▂▁▂█▆▆▆▆▆▆▆▆▆▆▆▆▆▆▄▅▆▄▄▄▇▅▄▄▄▄▅▅▅</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁██▁███▇▇▇▇</td></tr><tr><td>val_loss_epoch</td><td>▄▆▄▄█▃▅▄▃▆▂▄▁▅▅▄▄▂▃▂▃▄▂▄▂▁▅▄▃▃▄▅▄▄▅▅▅▇▆█</td></tr><tr><td>val_loss_step</td><td>▄▆▄▄█▃▅▄▃▆▂▅▁▆▆▄▅▂▃▂▃▄▂▄▂▁▅▄▃▃▄▄▃▄▅▄▅▆▅▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.60183</td></tr><tr><td>train_auc</td><td>0.59769</td></tr><tr><td>train_f1</td><td>0.2899</td></tr><tr><td>train_loss_epoch</td><td>0.65069</td></tr><tr><td>train_loss_step</td><td>0.59244</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.42125</td></tr><tr><td>val_auc</td><td>0.69133</td></tr><tr><td>val_f1</td><td>0.59278</td></tr><tr><td>val_loss_epoch</td><td>0.7153</td></tr><tr><td>val_loss_step</td><td>0.70053</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_16_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/6yqt5imb' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/6yqt5imb</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240101_165114-6yqt5imb\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240101_171306-dngwccts</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/dngwccts' target=\"_blank\">MLP_3_16_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/dngwccts' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/dngwccts</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_3_16_onehot_3\\sum\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 704   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "858       Trainable params\n",
      "0         Non-trainable params\n",
      "858       Total params\n",
      "0.003     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▃▃▁▁▄▂▄▃▃▄▆▄▆▅▅▅▄▅▅▅▅▄▇▆▆▆▆▆▇▆▆▄▆▆▆▇▆█▇█</td></tr><tr><td>train_auc</td><td>▆▇▅▇▆█▅▄▅▅▅▄▅▄▅▇▃▅▄▆▃▅▄▁▃▄▃▃▃▂▂▂▃▃▃▃▃▃▁▂</td></tr><tr><td>train_f1</td><td>▅▃▅▇▄█▇▆▇▅▆▆▅▄▂▄▂▃▁▃▄▂▂▂▃▂▃▃▅▃▄▃▅▃▁▅▄▅▄▆</td></tr><tr><td>train_loss_epoch</td><td>█▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▅▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▁▃▃▆▃▆▃▃▃██▆▃▃█▆▃▆</td></tr><tr><td>val_auc</td><td>███▅▇▃▂▂▂▂▂▂▂▂▂▂▂▁▁▂▂▂▂▂▂▂▂▁▂▂▂▂▂▂▂▂▁▂▂▂</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▅▁▅▁▁▁██▅▁▁█▅▁▅</td></tr><tr><td>val_loss_epoch</td><td>██▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_step</td><td>▇█▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.62557</td></tr><tr><td>train_auc</td><td>0.41521</td></tr><tr><td>train_f1</td><td>0.4209</td></tr><tr><td>train_loss_epoch</td><td>0.64276</td></tr><tr><td>train_loss_step</td><td>0.64494</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.58242</td></tr><tr><td>val_auc</td><td>0.26868</td></tr><tr><td>val_f1</td><td>0.01724</td></tr><tr><td>val_loss_epoch</td><td>0.65512</td></tr><tr><td>val_loss_step</td><td>0.62694</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_16_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/dngwccts' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/dngwccts</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240101_171306-dngwccts\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240101_173459-v2v7cqzl</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/v2v7cqzl' target=\"_blank\">MLP_3_16_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/v2v7cqzl' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/v2v7cqzl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\torch_geometric\\deprecation.py:22: UserWarning: 'nn.glob.GlobalAttention' is deprecated, use 'nn.aggr.AttentionalAggregation' instead\n",
      "  warnings.warn(out)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_3_16_onehot_3\\attention\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 704   \n",
      "2  | head        | Sequential       | 154   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 17    \n",
      "--------------------------------------------------\n",
      "875       Trainable params\n",
      "0         Non-trainable params\n",
      "875       Total params\n",
      "0.003     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▂▂▂▂▂▂▂▂▃▂▅▄▅▅▆▇█▇▇█▇▆▇██▆▇███▇▇▇▇▇▇▇▇</td></tr><tr><td>train_auc</td><td>▂▁▂▂▂▁▂▃▃▂▄▅▆▆▇▆▇█▇▇▇▇▇▇▇█▇▇▇██▇████▇█▇█</td></tr><tr><td>train_f1</td><td>▄▁▁▁▁▁▁▁▁▁▄▁▇▄▅▄▆▇▇▇▇▇▇▇▇█▇▅▆█▇▇██▆▇▇█▇▇</td></tr><tr><td>train_loss_epoch</td><td>▇█▆▇▇▇▇▇▆▇▆▆▅▅▅▅▃▃▃▃▃▃▂▃▃▃▂▃▃▁▃▃▂▃▂▁▂▂▂▂</td></tr><tr><td>train_loss_step</td><td>▆▆█▆▆▇▅▆▆▇▇▅▆▅▆▄▆▃▄▃▆▅▁▃▂▅▃▄▄▄▂█▆▃▅▆▆▆▃▄</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▃▃▃▅▄▅▅▅▆▇▅▆▆█▇▇▇▇▇▇▇▇▇█▇█▇▇</td></tr><tr><td>val_auc</td><td>▁▃▄▅▅▇▇▇▇███████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▃▄▄▅▄▅▅▆▆▇▅▆▆▇▇▇▆▇█▇▇██▇▇██▇</td></tr><tr><td>val_loss_epoch</td><td>▆▆▇▇▇▇▆▇▆▅▆█▄▄▄▄▆▆▄▅▅▄▄▃▄▃▂▂▂▃▂▃▁▂▃▆▂▄▃▃</td></tr><tr><td>val_loss_step</td><td>▅▅▆▆▆▇▅▇▅▄▅█▃▄▄▃▇▆▄▅▅▄▄▃▄▃▂▂▂▄▂▃▁▂▄█▃▅▃▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.6758</td></tr><tr><td>train_auc</td><td>0.7217</td></tr><tr><td>train_f1</td><td>0.5298</td></tr><tr><td>train_loss_epoch</td><td>0.61164</td></tr><tr><td>train_loss_step</td><td>0.61676</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.71429</td></tr><tr><td>val_auc</td><td>0.78008</td></tr><tr><td>val_f1</td><td>0.59375</td></tr><tr><td>val_loss_epoch</td><td>0.56396</td></tr><tr><td>val_loss_step</td><td>0.5566</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_16_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/v2v7cqzl' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/v2v7cqzl</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240101_173459-v2v7cqzl\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240101_175415-aetdnjga</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/aetdnjga' target=\"_blank\">MLP_3_16_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/aetdnjga' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/aetdnjga</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_3_16_onehot_3\\attention2\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 704   \n",
      "2  | head        | Sequential       | 154   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 281   \n",
      "--------------------------------------------------\n",
      "1.1 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.1 K     Total params\n",
      "0.005     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▂▂▂▂▂▂▂▃▄▄▆▆▇▅▆▇▅▇▇██▇▇▇▇███▆▇▆▇▇█▇██▇</td></tr><tr><td>train_auc</td><td>▂▃▁▁▂▁▂▁▂▄▆▆▇▇▇▇▇▇▇▇██▇██▇██▇██▇▇▇▇█████</td></tr><tr><td>train_f1</td><td>▃▁▁▁▁▁▁▁▁▃▅▄▇▇▇▅▇▇▅▇▇█▇█▇▇██▇█▇▇▆█▇▇▇▇▇▇</td></tr><tr><td>train_loss_epoch</td><td>█▇███▇▇▇▇▆▅▅▃▅▄▃▃▄▃▂▃▁▃▂▂▃▂▂▂▃▃▂▃▄▃▁▃▂▁▂</td></tr><tr><td>train_loss_step</td><td>▇█▆▇▇▆▅▇▆▆▄▂▇▄▅▅▆▅▆▄▅█▄▅▆▆▅▁▃▅▄▆▄▅▅▂▄▇▃▄</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▃▅▆▅█▅▆█▅▆█▇▇▇▇▇█▇▇▇▇▇▆▇█▇▇▇▇▇</td></tr><tr><td>val_auc</td><td>▁▃▅▅▆▇▇█████████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▄▆▇▆█▆▆█▆▇█▇▇█▇▇██▇▇▇▇▇▇█▇▇▇▇▇</td></tr><tr><td>val_loss_epoch</td><td>▇███▇▇▆▆▇▆▅▅▃▁▅▇▅▅▄▆▃▃▅▄▃▄▅▅▄▂▂▅▃▄▆▅▂▅█▁</td></tr><tr><td>val_loss_step</td><td>▆▆▆▆▆▅▅▅▆▅▄▅▃▁▅▇▅▅▄▆▄▄▅▄▄▅▅▅▅▂▃▅▄▄▆▆▂▅█▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.67215</td></tr><tr><td>train_auc</td><td>0.71491</td></tr><tr><td>train_f1</td><td>0.50619</td></tr><tr><td>train_loss_epoch</td><td>0.60666</td></tr><tr><td>train_loss_step</td><td>0.62131</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.69963</td></tr><tr><td>val_auc</td><td>0.78321</td></tr><tr><td>val_f1</td><td>0.5119</td></tr><tr><td>val_loss_epoch</td><td>0.50823</td></tr><tr><td>val_loss_step</td><td>0.42879</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_16_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/aetdnjga' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/aetdnjga</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240101_175415-aetdnjga\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240101_181245-2cs23cfa</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/2cs23cfa' target=\"_blank\">MLP_3_32_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/2cs23cfa' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/2cs23cfa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_3_32_onehot_3\\mean\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 2.4 K \n",
      "2 | head        | Sequential       | 562   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "3.0 K     Trainable params\n",
      "0         Non-trainable params\n",
      "3.0 K     Total params\n",
      "0.012     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▁▁▁▁▁▁▁▂▅▆▆▅▆▆▇▆▆▆▆▆▆▆▇▇▆▇▇▆█▇▇▇▆▆▆▇▇▆</td></tr><tr><td>train_auc</td><td>▂▂▂▂▁▂▂▁▃▃▆▆▆▇▇▇▇▆▇▇▇▇▇██▇▇████▇██▇▇▇███</td></tr><tr><td>train_f1</td><td>▃▁▁▁▁▁▁▁▁▂▆█▇▅▇▇█▇▇▆▆▇▇▇▇█▇▇▇▇█▇▇▇▇▇▇▇▇▇</td></tr><tr><td>train_loss_epoch</td><td>█▇▇▇▇▇▇▇▇▆▅▄▅▃▃▄▃▄▄▃▃▃▃▃▁▃▂▃▃▂▂▄▂▂▃▂▃▂▂▂</td></tr><tr><td>train_loss_step</td><td>█▇▆▇█▇▆█▆▄▅▄▆▄▅▅▆▆▄▂▄▁▃▁▅▃▆▂▃▅▆▆▆▁▄▄▁▄▆▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▄▆▄▇▅▇▇▇▇█▇▇▇▇███▇▇█▇██▇▇██▇█▇▇</td></tr><tr><td>val_auc</td><td>▅▃▁▁▁▁▁▄████████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▄▇▄▇▅▇█▇▇███▇█████▇█▇██████▇██▇</td></tr><tr><td>val_loss_epoch</td><td>▆███▇███▇▇▅▆▆▆▃▂▃▄▅▄▆▃▄▄▄▂▃▃▁▂▄▄▄▄▄▆▃▂▄▅</td></tr><tr><td>val_loss_step</td><td>▅█▇▇▇▇▇█▇▇▅▆▆▇▄▂▄▅▆▄█▃▄▅▅▃▄▄▁▂▅▅▅▆▆█▃▂▅▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.67671</td></tr><tr><td>train_auc</td><td>0.71899</td></tr><tr><td>train_f1</td><td>0.5305</td></tr><tr><td>train_loss_epoch</td><td>0.59529</td></tr><tr><td>train_loss_step</td><td>0.57775</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.72161</td></tr><tr><td>val_auc</td><td>0.78151</td></tr><tr><td>val_f1</td><td>0.5914</td></tr><tr><td>val_loss_epoch</td><td>0.61067</td></tr><tr><td>val_loss_step</td><td>0.65832</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_32_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/2cs23cfa' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/2cs23cfa</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240101_181245-2cs23cfa\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240101_183116-yvvbi6el</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/yvvbi6el' target=\"_blank\">MLP_3_32_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/yvvbi6el' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/yvvbi6el</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_3_32_onehot_3\\max\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 2.4 K \n",
      "2 | head        | Sequential       | 562   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "3.0 K     Trainable params\n",
      "0         Non-trainable params\n",
      "3.0 K     Total params\n",
      "0.012     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\torch_geometric\\warnings.py:17: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▄▄▄▄▄▄▅▄▄▆▇▆▅▆▅▇▄█▆▆▅▆█▆▆▆▆▆█▅▆▇▆▆▇▅▆▄▆</td></tr><tr><td>train_auc</td><td>▂▃▂▂▁▂▁▂▄▅▆▆▆▅▅▄▆▅▇▅▇▅██▇▆▇▆▇▇▆▆▆▅▆█▇█▆▇</td></tr><tr><td>train_f1</td><td>▅▂▁▁▁▁▁▂▃▂▅▆▅▆▅▅█▄▆▆▆▆▄█▅▇▆▇▆█▅▆▇▆▆▆█▅▆▆</td></tr><tr><td>train_loss_epoch</td><td>█▅▅▅▅▃▄▄▃▃▂▂▃▁▂▂▂▂▁▃▂▂▂▃▁▃▃▃▃▃▃▃▃▃▂▃▃▃▃▂</td></tr><tr><td>train_loss_step</td><td>▇▅▄▅▇▄▃█▄▂▃▂▃▁▆▇▇▅▆▃▆▃▆▄▄▂▆▃▃▄▅█▄▇▄█▃▅▁▄</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>███████▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>▆█▄▆▅▆█▂▃▁▂▂▂▂▄▆▆▆▇▅▆▃▅▆▅▄▅█▃▅▃▆▅▅▅▄▅▄▆▆</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁█████████████████████████████████</td></tr><tr><td>val_loss_epoch</td><td>▁▂▂▂▂▂▂▃▄▃▃▄▄▄▅▄▅▄▃▅▄▆▅▆▅▆▅▅▅▅▅▆▅▅▄▆██▇▇</td></tr><tr><td>val_loss_step</td><td>▁▃▂▂▂▃▃▃▄▃▃▄▃▄▄▃▄▄▂▅▃▆▅▆▅▆▅▄▅▄▅▆▅▄▂▅██▇▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.60274</td></tr><tr><td>train_auc</td><td>0.56803</td></tr><tr><td>train_f1</td><td>0.3318</td></tr><tr><td>train_loss_epoch</td><td>0.65895</td></tr><tr><td>train_loss_step</td><td>0.66207</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.42125</td></tr><tr><td>val_auc</td><td>0.5131</td></tr><tr><td>val_f1</td><td>0.59278</td></tr><tr><td>val_loss_epoch</td><td>0.76403</td></tr><tr><td>val_loss_step</td><td>0.76915</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_32_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/yvvbi6el' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/yvvbi6el</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240101_183116-yvvbi6el\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240101_185047-hobofabe</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/hobofabe' target=\"_blank\">MLP_3_32_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/hobofabe' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/hobofabe</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_3_32_onehot_3\\sum\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 2.4 K \n",
      "2 | head        | Sequential       | 562   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "3.0 K     Trainable params\n",
      "0         Non-trainable params\n",
      "3.0 K     Total params\n",
      "0.012     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▂▃▄▄▄▅▄▆▅▅▅▇▆▇▇▇▆▇▅▇▇▇▆▅██▇█▇▇███▇▇███</td></tr><tr><td>train_auc</td><td>█▆██▇▇▅▇▇▅▆▆▇▅▅▆▅▇▂▅▇▆▅▅▆▅▂▆▅▄▄▃▁▁▅▆▇▅▃▆</td></tr><tr><td>train_f1</td><td>▄▃▃▁▃▂▁▂▂▄▄▃▄▆▆▆▆▆█▇▃▆▇▇▇█▆▇▆▆▇▇█▇▆▆▇▇██</td></tr><tr><td>train_loss_epoch</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▂▂▂▂▂▁▂▂▂▁▁▂▁▂▂▂▂▂▁▂▁▁▁▂▁▂▁▁▂▂▂▂▁▂▂▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▄▄▄▇▆▃▆▇▇▅▇▇▇▅▇▇▇▆▆▇▆▇▆▆▇██▇█▇████▇▆███</td></tr><tr><td>val_auc</td><td>██████▇████▇█▅▂▅▆▃▁▄▆▄▄▄▆▃▃▅▃▁▁▁▁▁▅▄▆▅▂▄</td></tr><tr><td>val_f1</td><td>▇▁▁▁██▇▄█▇▇██▇██▆▇▅█▇▅▆▅▆▆█▇███████▇▅██▇</td></tr><tr><td>val_loss_epoch</td><td>█▆▅▆▆▆▆▅▆▅▅▅▆▅▅▅▄▄▅▅▅▃▃▃▄▂▄▃▃▂▄▄▄▄▃▅▂▁▃▄</td></tr><tr><td>val_loss_step</td><td>█▆▅▆▆▆▆▅▆▅▅▅▆▅▅▅▄▅▅▅▅▃▄▃▄▃▅▄▃▂▅▅▅▄▄▇▂▁▄▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.67489</td></tr><tr><td>train_auc</td><td>0.49297</td></tr><tr><td>train_f1</td><td>0.5908</td></tr><tr><td>train_loss_epoch</td><td>0.59886</td></tr><tr><td>train_loss_step</td><td>0.54412</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.72527</td></tr><tr><td>val_auc</td><td>0.41266</td></tr><tr><td>val_f1</td><td>0.61929</td></tr><tr><td>val_loss_epoch</td><td>0.59689</td></tr><tr><td>val_loss_step</td><td>0.62841</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_32_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/hobofabe' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/hobofabe</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240101_185047-hobofabe\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240101_191001-9uddx9vj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/9uddx9vj' target=\"_blank\">MLP_3_32_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/9uddx9vj' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/9uddx9vj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\torch_geometric\\deprecation.py:22: UserWarning: 'nn.glob.GlobalAttention' is deprecated, use 'nn.aggr.AttentionalAggregation' instead\n",
      "  warnings.warn(out)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_3_32_onehot_3\\attention\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 2.4 K \n",
      "2  | head        | Sequential       | 562   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 33    \n",
      "--------------------------------------------------\n",
      "3.0 K     Trainable params\n",
      "0         Non-trainable params\n",
      "3.0 K     Total params\n",
      "0.012     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▃▃▃▃▃▃▃▅▅▆▅▆▇█▇▇▇▇▇█▇█▇▇▇▇▇▇▇▇█▇▇▇▇▇▇▇█</td></tr><tr><td>train_auc</td><td>▂▁▁▂▁▁▂▂▄▅▆▆▆▇▇▇▇▇▇▇▇█▇█▇▇███▇██▇█▇██▇██</td></tr><tr><td>train_f1</td><td>▃▁▁▁▁▁▁▁▅▇▇▅▆█▇▆▆▇▆▆▇▇▇▇▇▇▇▇▇▇▇█▇▇█▆▇▇█▇</td></tr><tr><td>train_loss_epoch</td><td>█▇▇▇███▇▆▅▅▅▂▄▂▃▃▃▃▄▃▃▂▂▃▁▁▃▂▃▁▂▁▁▂▂▂▁▂▂</td></tr><tr><td>train_loss_step</td><td>▆▆▆▇▇▆▇▇▇▅▇▆▅▆▄▇▅▅▇▃▄▂▅▅▅▃▅▂▆▇▅▁▄▅▅█▄█▅▅</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▃▄▄▅▄▄▅▅▄▅▅▅▅▅▆▅▅▅▅▅▅▅▅▅▆▆▅▅▅▅█▆</td></tr><tr><td>val_auc</td><td>▂▁▄▅▆▆▆▇▇███████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▃▄▅▅▄▅▆▅▄▆▆▆▅▆▆▆▆▅▆▆▆▆▆▆▆▆▆▆▆▆█▇</td></tr><tr><td>val_loss_epoch</td><td>▇▇▇▇▇▇▇▅▆▇▅▅▆▅▅▃▆▅▄▁▆▅▃▆▂▅▃▄▃▆▅▃█▄▃▄▃▄▄▂</td></tr><tr><td>val_loss_step</td><td>▆▆▆▆▆▆▆▄▅▆▅▅▅▄▅▃▅▅▄▁▆▅▃▆▂▄▃▄▃▆▅▃█▄▃▄▃▄▄▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.68037</td></tr><tr><td>train_auc</td><td>0.72467</td></tr><tr><td>train_f1</td><td>0.56683</td></tr><tr><td>train_loss_epoch</td><td>0.61033</td></tr><tr><td>train_loss_step</td><td>0.63125</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.66667</td></tr><tr><td>val_auc</td><td>0.78074</td></tr><tr><td>val_f1</td><td>0.42038</td></tr><tr><td>val_loss_epoch</td><td>0.48381</td></tr><tr><td>val_loss_step</td><td>0.36364</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_32_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/9uddx9vj' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/9uddx9vj</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240101_191001-9uddx9vj\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240101_192916-nadiae9m</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/nadiae9m' target=\"_blank\">MLP_3_32_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/nadiae9m' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/nadiae9m</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_3_32_onehot_3\\attention2\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 2.4 K \n",
      "2  | head        | Sequential       | 562   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 1.1 K \n",
      "--------------------------------------------------\n",
      "4.1 K     Trainable params\n",
      "0         Non-trainable params\n",
      "4.1 K     Total params\n",
      "0.016     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▂▂▂▂▂▂▂▅▆▆▇▇▇▇▆▇▇▆▇▇▇▇▇▇▇▇▆▇█▇▇▇▆▇▇▇██</td></tr><tr><td>train_auc</td><td>▂▂▁▂▂▁▂▂▁▄▆▅▆▆▇▇▆▇▇▆▇█▇▆▇▇▇▆▇▇▆▇▇▇▇▇▇███</td></tr><tr><td>train_f1</td><td>▃▁▁▁▁▁▁▁▁▆█▇▇▇██▆▇▇█▇▇▇▇▇█▇▆▇▇█▇▇▇▇█▇██▇</td></tr><tr><td>train_loss_epoch</td><td>██▇███▇▇▇▄▄▃▄▃▃▃▃▃▃▃▂▁▂▂▁▃▃▁▂▃▂▃▂▃▂▂▂▂▂▂</td></tr><tr><td>train_loss_step</td><td>▅▆▆▅▆▅▅▆▅▇▆▄▄▃▆▁▂▄▃▄▃▄▇▃▆▂▄█▅▂▃▃▃▄▄▃▅▄▅▅</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▇▄▅▇▇▇▇█▇▆▆██▇▇▇▇█▇▇▇▆███▇▇▇▇█▇</td></tr><tr><td>val_auc</td><td>▆▁▂▁▁▁▁▁▁███████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▇▄▅▇▇▇█▇▇▆▆████████▇▇▆███▇██▇█▇</td></tr><tr><td>val_loss_epoch</td><td>▆█▆█▆▆▇▆▇▅▆▇▄▃▂▄▂▂▅▃▃▂▂▆▃▅▁▆▄▂▃▅▂▃▄▅▃▁▃▆</td></tr><tr><td>val_loss_step</td><td>▅▇▅▇▄▄▆▅▆▅▆█▄▃▃▄▂▂▅▃▄▂▂█▄▆▁▇▅▂▄▇▃▄▅▆▄▁▃▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.68767</td></tr><tr><td>train_auc</td><td>0.71033</td></tr><tr><td>train_f1</td><td>0.55352</td></tr><tr><td>train_loss_epoch</td><td>0.61098</td></tr><tr><td>train_loss_step</td><td>0.66944</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.71062</td></tr><tr><td>val_auc</td><td>0.78129</td></tr><tr><td>val_f1</td><td>0.55367</td></tr><tr><td>val_loss_epoch</td><td>0.63945</td></tr><tr><td>val_loss_step</td><td>0.71926</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_32_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/nadiae9m' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/nadiae9m</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240101_192916-nadiae9m\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240101_195141-wqnjzuqi</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/wqnjzuqi' target=\"_blank\">MLP_3_64_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/wqnjzuqi' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/wqnjzuqi</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_3_64_onehot_3\\mean\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 9.0 K \n",
      "2 | head        | Sequential       | 2.1 K \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "11.1 K    Trainable params\n",
      "0         Non-trainable params\n",
      "11.1 K    Total params\n",
      "0.044     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▂▂▂▅▇▇▆▇▇▇▇▇▇▇▇▇▇██▇█▇█▇▆█▇██▇▇▇▇▇▇▇▇▇</td></tr><tr><td>train_auc</td><td>▁▂▂▂▄▆▇▇▇▇▇▇▇▇▇▇▇▇▇█▇▇███▇▇█▇████▇▇▇█▇█▇</td></tr><tr><td>train_f1</td><td>▃▁▁▁▁▅▇▇▅▆█▇▆▇▇██▇▇██▇████▆▇▇█▇▇▇▇██▇▇▇▇</td></tr><tr><td>train_loss_epoch</td><td>████▇▅▃▄▅▃▃▃▄▄▂▃▃▃▂▃▂▂▂▂▂▄▄▁▂▂▂▂▂▂▃▂▂▃▂▂</td></tr><tr><td>train_loss_step</td><td>▇▆▆▅▅▅▆▃▄▄▅▅▄▃▂▅█▅▄▅▃▄▁▅▅▇▄▄▆▃▂▄▂▂▇▄▃▅▃▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▅▇▇▇▇▇▇▇▇▇▇▇▇▇▇██▇▇▆▇█▇█▇▇▇█▇██▇█▇█</td></tr><tr><td>val_auc</td><td>▁▄▅▆▇▇▇▇▇▇▇█████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▅▇██▇▇▇▇▇▇▇██▇▇█▇█▇▇██▇█▇███▇████▇█</td></tr><tr><td>val_loss_epoch</td><td>▇██▆▅▃▂▆▄▄▃▂▅▅▄▂▂▁▇▁▂▄▂▁▇▄▄▁▂▂▃▆▄▃▃▅▂▁▃▂</td></tr><tr><td>val_loss_step</td><td>▆▇▇▄▃▃▂▆▃▅▃▂▆▅▄▂▂▁█▁▂▅▂▁▇▄▅▂▂▂▄▇▅▃▄▅▂▁▃▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.67671</td></tr><tr><td>train_auc</td><td>0.72797</td></tr><tr><td>train_f1</td><td>0.5097</td></tr><tr><td>train_loss_epoch</td><td>0.58876</td></tr><tr><td>train_loss_step</td><td>0.53283</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.72527</td></tr><tr><td>val_auc</td><td>0.78195</td></tr><tr><td>val_f1</td><td>0.61929</td></tr><tr><td>val_loss_epoch</td><td>0.53321</td></tr><tr><td>val_loss_step</td><td>0.50567</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_64_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/wqnjzuqi' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/wqnjzuqi</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240101_195141-wqnjzuqi\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240101_201348-lo1iz2jv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/lo1iz2jv' target=\"_blank\">MLP_3_64_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/lo1iz2jv' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/lo1iz2jv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_3_64_onehot_3\\max\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 9.0 K \n",
      "2 | head        | Sequential       | 2.1 K \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "11.1 K    Trainable params\n",
      "0         Non-trainable params\n",
      "11.1 K    Total params\n",
      "0.044     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\torch_geometric\\warnings.py:17: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▂▃▂▃▃▃▄▄▄▄▄▅▄▄▄▅▄▄▄▄▅█▅▃▄▅▄▅▆▄▆▇▆▆▆▅▇▅</td></tr><tr><td>train_auc</td><td>▁▃▂▄▆▄▅▅▅▆▅▆▆▆▆▆▆▇▅▆▆▇▇█▇▇█▆▆▅███▇██▇▆▇█</td></tr><tr><td>train_f1</td><td>▃▁▁▂▂▃▃▂▄▅▇▄▃▆▅▅▃▅▆▄▄▃▅▇▇█▄▄▅▆▅▃▅▆▇▇▅▅▆▅</td></tr><tr><td>train_loss_epoch</td><td>█▆▇▇▅▄▅▄▅▃▅▄▄▆▃▃▅▄▄▃▅▂▂▂▁▆▃▃▄▄▂▃▄▁▃▃▄▄▄▁</td></tr><tr><td>train_loss_step</td><td>▆▃▃▃▂▄▄▃▄▅▅▃▃▄▃▃▅▅▁▄▃▃▃▂▄█▃▂▅▅▁▃▂▃▄▃▂▃▂▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>▃▅▄▄▆▄▃▂▃▃▃▃▅▆▇▆▆▃▄▃▃▂▂▂▂▃▂▄▂▁█▇▃▃▆▃▄▅▅▅</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_epoch</td><td>▁▁▁▁▂▁▂▂▂▂▃▃▂▂▂▂▄▅▄▄▄▂▅▆▆▄▂▂▃▅▅▄▆▇▇█▅█▆▆</td></tr><tr><td>val_loss_step</td><td>▁▁▁▂▂▂▂▂▂▂▃▃▁▂▂▁▅▅▃▄▄▁▅▅▅▃▂▂▃▅▅▃▅▇▇█▃█▄▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.61553</td></tr><tr><td>train_auc</td><td>0.6492</td></tr><tr><td>train_f1</td><td>0.32855</td></tr><tr><td>train_loss_epoch</td><td>0.64551</td></tr><tr><td>train_loss_step</td><td>0.6185</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.42125</td></tr><tr><td>val_auc</td><td>0.40281</td></tr><tr><td>val_f1</td><td>0.59278</td></tr><tr><td>val_loss_epoch</td><td>0.97943</td></tr><tr><td>val_loss_step</td><td>0.947</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_64_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/lo1iz2jv' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/lo1iz2jv</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240101_201348-lo1iz2jv\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240101_203300-x64r3lhw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/x64r3lhw' target=\"_blank\">MLP_3_64_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/x64r3lhw' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/x64r3lhw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_3_64_onehot_3\\sum\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 9.0 K \n",
      "2 | head        | Sequential       | 2.1 K \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "11.1 K    Trainable params\n",
      "0         Non-trainable params\n",
      "11.1 K    Total params\n",
      "0.044     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▂▄▄▄▆▆▆▆▆▇▇▆▇▇▇▇▇▇▆▇▇▇█▇▇█▇▇█▇▇▇▇▆▇▇█▇</td></tr><tr><td>train_auc</td><td>▅▃▅▆▇▇▇▅▄▅▅▅▅▆▅▅▄▄▅▃▃▅▂▃▃▃▁▃▅▄▅▆▆▅▅▄▇▇██</td></tr><tr><td>train_f1</td><td>▂▃▃▂▁▂▄▅▃▆▇▆▄▅▆▇▇▇██▆▆▇▇▇███▇▇█▅▇▅█████▅</td></tr><tr><td>train_loss_epoch</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▃▂▂▂▂▂▁▂▂▂▁▂▂▁▂▂▂▁▂▁▁▁▂▂▂▂▁▂▁▂▂▁▁▂▁▂▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▂▂▃▂▃▃▅▆▅▄▄▄▃▄▆▅▆▄▄▂▆▅▅▆▅▅▅▅▆▅▆▅▆█</td></tr><tr><td>val_auc</td><td>▇▄▇▇▇▇▆▃▃▃▅▅▅▅▆█▅▆▆▄▄▄▂▂▁▁▁▁▂▆█▆█▄▇▆▇▇█▇</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▂▂▂▃▅▆▄▄▄▄▂▄▆▅▆▄▄▁▆▅▅▆▄▅▆▆▆▅▆▅▆█</td></tr><tr><td>val_loss_epoch</td><td>█▄▃▂▁▂▂▂▂▂▂▂▃▂▂▃▂▁▃▁▂▂▁▂▃▃▂▂▂▁▂▂▂▂▂▂▁▁▂▁</td></tr><tr><td>val_loss_step</td><td>█▇▄▃▁▂▃▃▂▃▃▂▄▃▃▅▂▁▄▂▃▃▂▂▄▄▄▃▃▂▃▃▃▃▃▂▂▁▃▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.66941</td></tr><tr><td>train_auc</td><td>0.58709</td></tr><tr><td>train_f1</td><td>0.50681</td></tr><tr><td>train_loss_epoch</td><td>0.61014</td></tr><tr><td>train_loss_step</td><td>0.57463</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.67399</td></tr><tr><td>val_auc</td><td>0.70165</td></tr><tr><td>val_f1</td><td>0.47953</td></tr><tr><td>val_loss_epoch</td><td>0.53792</td></tr><tr><td>val_loss_step</td><td>0.49391</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_64_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/x64r3lhw' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/x64r3lhw</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240101_203300-x64r3lhw\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240101_205326-i0vl44k1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/i0vl44k1' target=\"_blank\">MLP_3_64_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/i0vl44k1' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/i0vl44k1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\torch_geometric\\deprecation.py:22: UserWarning: 'nn.glob.GlobalAttention' is deprecated, use 'nn.aggr.AttentionalAggregation' instead\n",
      "  warnings.warn(out)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_3_64_onehot_3\\attention\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 9.0 K \n",
      "2  | head        | Sequential       | 2.1 K \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 65    \n",
      "--------------------------------------------------\n",
      "11.2 K    Trainable params\n",
      "0         Non-trainable params\n",
      "11.2 K    Total params\n",
      "0.045     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▂▂▄▅█▄▅▆▆▆▇▇█▇▇▇▇█████▇▇▆█████▇█▇▇█▇█▇</td></tr><tr><td>train_auc</td><td>▁▁▂▃▅▆▇▆▆▇▇▇▇▇▇▇▇▇▇█▇▇▇█▇▇▇█████▇█▇█████</td></tr><tr><td>train_f1</td><td>▃▁▁▁▄▆██▅▅▆▇▇█▇▇█▇▇██▇█▇█▇▆▇█▇██▇█▇▇█▇▇▇</td></tr><tr><td>train_loss_epoch</td><td>█▇█▇▆▅▄▅▄▃▂▃▂▂▁▃▃▃▂▂▃▂▁▂▂▂▃▂▂▂▂▂▂▁▂▁▁▁▂▃</td></tr><tr><td>train_loss_step</td><td>▆▆▅▅▅▅▄▃▄▄▃▄▂▂▄▅▄▅▄▄▅▅▄▁▅▂▃▃▃▅▆▄▆▄▄▂█▅▅▆</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▇▃▃▇▅▇▆▆▄▇▇▅▆█▆▅▆▇▇▇▇▇█▇▇▅▇█▇▇▇▇▇▇▇</td></tr><tr><td>val_auc</td><td>▁▃▅▅▇███████████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▂▇▄▄▇▆█▇▇▅▇▇▅▇█▆▆▆▇▇▇▇██▇▇▅▇█▇▇█▇▇██</td></tr><tr><td>val_loss_epoch</td><td>▆▆▆█▇▄▇▅▅▅▂▁▄▄▂▂▆▄▂▂▁▃▃▂▅▄▂▃▁▃▄▁▃▇▃▃▃▃▄▅</td></tr><tr><td>val_loss_step</td><td>▄▅▅▇▆▄▇▄▅▅▃▁▅▄▃▂▆▄▃▂▁▄▃▂▅▄▂▄▂▃▄▁▃█▄▄▃▃▅▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.67854</td></tr><tr><td>train_auc</td><td>0.72293</td></tr><tr><td>train_f1</td><td>0.52304</td></tr><tr><td>train_loss_epoch</td><td>0.61689</td></tr><tr><td>train_loss_step</td><td>0.69055</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.72161</td></tr><tr><td>val_auc</td><td>0.78156</td></tr><tr><td>val_f1</td><td>0.65138</td></tr><tr><td>val_loss_epoch</td><td>0.61656</td></tr><tr><td>val_loss_step</td><td>0.67272</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_64_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/i0vl44k1' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/i0vl44k1</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240101_205326-i0vl44k1\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240101_211347-c0p4sb2k</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/c0p4sb2k' target=\"_blank\">MLP_3_64_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/c0p4sb2k' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/c0p4sb2k</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_3_64_onehot_3\\attention2\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 9.0 K \n",
      "2  | head        | Sequential       | 2.1 K \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 4.2 K \n",
      "--------------------------------------------------\n",
      "15.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "15.3 K    Total params\n",
      "0.061     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▂▂▂▅▆▆▆▇▇▇█▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇▇▇▆▇▇█▇██</td></tr><tr><td>train_auc</td><td>▁▂▂▃▄▆▇▇▇▇▇▇█▇▇█▇█▇████▇██▇███████▇▇████</td></tr><tr><td>train_f1</td><td>▃▁▁▁▁▆▇▇▇▇█▇█▇▇█▇▇▇▇▇█▇▇▇▇█▇▇▇▇▇▇▆▇▇██▇▇</td></tr><tr><td>train_loss_epoch</td><td>█▇█▇▇▄▃▃▃▃▃▄▂▂▃▃▃▂▂▂▁▂▄▃▁▃▄▃▃▂▁▁▁▃▃▂▃▃▃▂</td></tr><tr><td>train_loss_step</td><td>▆█▆▆▅▆▆▃▃▄▃▃▂▂▃▄▃▂▅▁▂▂▅▃▄▃▂▃▂▅▃▃▄▄▂█▄▁▄▅</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▄▅▅▅▆▇▅▇▇▇▆▇▇██████▇▇█▆██▇██▇█▇▇▆▇██</td></tr><tr><td>val_auc</td><td>▁▃▄▅▅▇▇▇▇▇▇▇▇▇▇█████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▄▅▆▅▆▇▅▇▇█▆▇███▇▇▇▇▇▇▇▆█████▇█▇▇▆▇██</td></tr><tr><td>val_loss_epoch</td><td>███▇▆▅▃▄▃▄▂▅▁▇▄▄▃▃▂▃▂▄▄▄▃▆▄▄▂▁▄▅▅▂▃▄▃▄▁▂</td></tr><tr><td>val_loss_step</td><td>▆▇▇▆▅▅▃▄▃▄▂▆▁█▅▄▃▄▃▄▂▅▄▄▃▇▅▅▃▁▄▆▆▃▄▅▃▅▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.69406</td></tr><tr><td>train_auc</td><td>0.74181</td></tr><tr><td>train_f1</td><td>0.55629</td></tr><tr><td>train_loss_epoch</td><td>0.59736</td></tr><tr><td>train_loss_step</td><td>0.65338</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.72527</td></tr><tr><td>val_auc</td><td>0.78123</td></tr><tr><td>val_f1</td><td>0.63768</td></tr><tr><td>val_loss_epoch</td><td>0.52365</td></tr><tr><td>val_loss_step</td><td>0.48472</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_64_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/c0p4sb2k' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/c0p4sb2k</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240101_211347-c0p4sb2k\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240101_213433-kcsm209r</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/kcsm209r' target=\"_blank\">MLP_4_16_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/kcsm209r' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/kcsm209r</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_4_16_onehot_3\\mean\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 976   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "1.1 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.1 K     Total params\n",
      "0.005     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▃▄▇▆▆▇▆▇▆█▇▆▆▇▇</td></tr><tr><td>train_auc</td><td>▂▂▂▁▂▂▁▂▃▂▁▂▂▃▁▂▃▂▂▂▃▂▃▃▄▆▇██▇▇██▇██▇███</td></tr><tr><td>train_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▃▅▇▇▅▇▆███▇▇▇▇▇</td></tr><tr><td>train_loss_epoch</td><td>▇█▇▇▇▇█▇▇▇█▇▇▇▇▇▇▇▇▇▇█▇▇▆▆▄▃▄▃▃▂▁▄▂▂▁▃▄▂</td></tr><tr><td>train_loss_step</td><td>▅▅▅▄▅▄▄▅▆▇▆▅▆▅▆▆▅▅▅▇▅▆▅▇▅▂█▂▄▂▄▁▂▃▃▃▅▂▄▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▄▅█▇▇▆▅▅▆▇██▆▇█▇</td></tr><tr><td>val_auc</td><td>▁▇▆▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇███████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▃▅█▇█▆▆▅▆▆█▇▆▇▇▇</td></tr><tr><td>val_loss_epoch</td><td>▇▇▆▇▇▇▇▇▆██▆▆▇▇▆▇▇█▇▆▇▇▇▆▆▃▄▄▃▁▅▆▄▄▁▄▄▃█</td></tr><tr><td>val_loss_step</td><td>▅▅▄▅▆▅▆▅▄▆▆▅▄▅▅▄▅▆▆▅▄▆▅▅▅▅▃▄▄▃▁▅▆▄▄▁▄▄▃█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.66575</td></tr><tr><td>train_auc</td><td>0.69627</td></tr><tr><td>train_f1</td><td>0.49307</td></tr><tr><td>train_loss_epoch</td><td>0.61719</td></tr><tr><td>train_loss_step</td><td>0.59538</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.7033</td></tr><tr><td>val_auc</td><td>0.77931</td></tr><tr><td>val_f1</td><td>0.55249</td></tr><tr><td>val_loss_epoch</td><td>0.70485</td></tr><tr><td>val_loss_step</td><td>0.83686</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_16_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/kcsm209r' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/kcsm209r</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240101_213433-kcsm209r\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240101_215537-tyyqc8jo</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/tyyqc8jo' target=\"_blank\">MLP_4_16_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/tyyqc8jo' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/tyyqc8jo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_4_16_onehot_3\\max\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 976   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "1.1 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.1 K     Total params\n",
      "0.005     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\torch_geometric\\warnings.py:17: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▄▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▆▅▆▇▆▆▆▇▆█▇▆▇█▆▇▇▆</td></tr><tr><td>train_auc</td><td>▂▂▂▁▂▃▁▂▃▂▂▄▅▅▃▃▄▄▄▄▆▅▇▆▅▇█▅▆▆▆██▆▇█▇██▇</td></tr><tr><td>train_f1</td><td>▆▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▃▃▅▅▅▆▅▆▅▇▆▆▆█▆▇█▆</td></tr><tr><td>train_loss_epoch</td><td>█▆▅▄▄▄▅▄▄▄▅▄▄▃▃▄▄▃▄▄▄▄▃▃▄▃▂▄▃▃▃▁▁▃▂▂▁▁▄▂</td></tr><tr><td>train_loss_step</td><td>▆▅▅▃▄▃▃▄▆█▇▅▇▃▇▆▅▆▄▇▄▇▃▅▄▂▆▅▄▁▄▄▅█▇▄▅▅▄▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>█████████████████▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>▇▆▇▆▆▇▇▇▇▇█████▂▂▂▁▁▂▂██▇▇▇▇██▇▇▇▇█▇▇▇▇█</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁███████████████████████</td></tr><tr><td>val_loss_epoch</td><td>▂▂▁▂▂▂▂▂▁▃▃▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▄▅▆▆▇▄▄▆▅▇█▅█▅</td></tr><tr><td>val_loss_step</td><td>▂▃▂▂▃▂▃▂▁▄▃▂▂▃▃▃▃▃▃▄▅▄▄▄▅▅▄▅▇▆▇▃▃▅▄▇█▃█▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.60548</td></tr><tr><td>train_auc</td><td>0.61963</td></tr><tr><td>train_f1</td><td>0.28713</td></tr><tr><td>train_loss_epoch</td><td>0.65536</td></tr><tr><td>train_loss_step</td><td>0.63906</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.42125</td></tr><tr><td>val_auc</td><td>0.68993</td></tr><tr><td>val_f1</td><td>0.59278</td></tr><tr><td>val_loss_epoch</td><td>0.74232</td></tr><tr><td>val_loss_step</td><td>0.7102</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_16_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/tyyqc8jo' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/tyyqc8jo</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240101_215537-tyyqc8jo\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240101_221525-3d4eg9h9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/3d4eg9h9' target=\"_blank\">MLP_4_16_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/3d4eg9h9' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/3d4eg9h9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_4_16_onehot_3\\sum\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 976   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "1.1 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.1 K     Total params\n",
      "0.005     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▂▃▄▄▄▄▂▅▄▄▅▅▅▅▅▅▆▅▅▄▆▆▅▆▇▇▆▆▅▆█▇▇▇▇▇█▇</td></tr><tr><td>train_auc</td><td>▅▃▆▅▅▅▅▄▃▃▂▇▄▄▃▅▆▅▄▄▇▄▇▁▃▆▅▃▃▃▅▄▅▆▅█▄▃▆▄</td></tr><tr><td>train_f1</td><td>▄▂▁▅▅▄▃▄▅▂▁▅▃▅▃▃▆▁▄▄▃▂▆▄▅▅▅▆▅▄▆▅▇█▆▇▆▇█▆</td></tr><tr><td>train_loss_epoch</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▄▁▁▁▁▂▂▂▃▃▂▃▆▁▂▂▃▂█▅▂▄▇▂</td></tr><tr><td>val_auc</td><td>▆▆▆▇▆▆▇▇▇▇▇▇█▇▇▇▇▇▇▇▇▇▃▁▇▆▄▅▇▇▆▆▇▇▇▇███▇</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▄▁▁▁▁▂▂▂▃▃▂▃▆▁▂▂▂▂█▅▂▄▇▂</td></tr><tr><td>val_loss_epoch</td><td>█▃▂▃▃▂▂▁▁▂▂▁▁▁▁▁▁▂▂▁▁▂▁▁▁▁▂▁▁▁▁▂▂▂▁▁▁▂▁▂</td></tr><tr><td>val_loss_step</td><td>█▄▂▄▄▂▂▂▁▃▃▁▁▂▁▁▂▃▃▂▂▃▂▂▂▂▂▂▁▁▁▃▃▃▂▂▂▃▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.64018</td></tr><tr><td>train_auc</td><td>0.51772</td></tr><tr><td>train_f1</td><td>0.47326</td></tr><tr><td>train_loss_epoch</td><td>0.63485</td></tr><tr><td>train_loss_step</td><td>0.58947</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.58608</td></tr><tr><td>val_auc</td><td>0.70501</td></tr><tr><td>val_f1</td><td>0.03419</td></tr><tr><td>val_loss_epoch</td><td>0.65032</td></tr><tr><td>val_loss_step</td><td>0.67984</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_16_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/3d4eg9h9' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/3d4eg9h9</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240101_221525-3d4eg9h9\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240101_223634-87gp33mu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/87gp33mu' target=\"_blank\">MLP_4_16_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/87gp33mu' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/87gp33mu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\torch_geometric\\deprecation.py:22: UserWarning: 'nn.glob.GlobalAttention' is deprecated, use 'nn.aggr.AttentionalAggregation' instead\n",
      "  warnings.warn(out)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_4_16_onehot_3\\attention\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 976   \n",
      "2  | head        | Sequential       | 154   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 17    \n",
      "--------------------------------------------------\n",
      "1.1 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.1 K     Total params\n",
      "0.005     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_auc</td><td>▇▆▇▇█▃▇▅▆▄▇▆▇▆█▅▇▃▆█▃▅▇▄▅▂▄▆▆▅▄▅▆▁▇▅▇▅▃▇</td></tr><tr><td>train_f1</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_epoch</td><td>▅▆▅▄▅▃▅▇▆▆▁▇▂▅▁▂▄▄▁▂█▆▅▅▄▆▆▅▇▇█▅▅▆▃▆▄▄▄▃</td></tr><tr><td>train_loss_step</td><td>▂▃▄▃█▄▄▆▃▂▅▇▁▄▇█▂▄▅▅▃▃▂▇▆▆▄▂▃▅▅▄▄▂▅▅▄▄▂▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>▃▃▅▅▆█▇▇▇▇▇▇▇▇▇▇▇▇▆▆▆▆▅▄▃▂▂▂▂▁▁▂▁▁▁▁▁▂▁▁</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_epoch</td><td>▆▃▆▄▃▄▅▆▅▄▃▃▄▄▆▂▆▃▆▃▅▃▆▃▄▅▂▄▃▁▃▄▅█▄▅▄▆▃▆</td></tr><tr><td>val_loss_step</td><td>▆▃▆▄▃▄▅▆▅▄▃▃▄▄▆▂▆▃▆▃▅▃▆▃▄▅▂▄▃▁▃▄▅█▄▅▄▆▃▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.58447</td></tr><tr><td>train_auc</td><td>0.52242</td></tr><tr><td>train_f1</td><td>0.0</td></tr><tr><td>train_loss_epoch</td><td>0.67657</td></tr><tr><td>train_loss_step</td><td>0.66646</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.57875</td></tr><tr><td>val_auc</td><td>0.5</td></tr><tr><td>val_f1</td><td>0.0</td></tr><tr><td>val_loss_epoch</td><td>0.69968</td></tr><tr><td>val_loss_step</td><td>0.72081</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_16_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/87gp33mu' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/87gp33mu</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240101_223634-87gp33mu\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240101_225631-nkqr7t56</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/nkqr7t56' target=\"_blank\">MLP_4_16_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/nkqr7t56' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/nkqr7t56</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_4_16_onehot_3\\attention2\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 976   \n",
      "2  | head        | Sequential       | 154   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 281   \n",
      "--------------------------------------------------\n",
      "1.4 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.4 K     Total params\n",
      "0.006     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>█▁██████████████████████████████████████</td></tr><tr><td>train_auc</td><td>▆▁▃▇▄▄▃▇▃▅▆▇▂▂▅▇▄█▄▃▃▃▃▂▅█▅▇▄▁▃█▂▃▆▂▆▅▅▄</td></tr><tr><td>train_f1</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_epoch</td><td>▄█▅▂▃▄▅▄▂▇▇▃▄▃▃▂▄▃▁▄▇▆▃▅▅▂▅▃█▅▂▄▂▃▄▄▅▂▃▃</td></tr><tr><td>train_loss_step</td><td>▇▆▅▆██▅▆█▇▆▆▃▅▃▇▇▆▁▅▇▆▄▆█▆▆▅▄▆▅▅▆▇▆▇▄▆▅▅</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>▃▃▅▅▆▆▇▇▇██████▇▇▇▇▇▇▇▆▄▄▃▃▁▁▂▁▂▁▁▁▁▁▁▁▁</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_epoch</td><td>▄▃▅▇▅▆▆▆▃▆▂▅▂█▆▃▄▅▇▆▅▆▆▆▆▆▂▁▁▃▂▆▅▃▄▃▆▆▅▅</td></tr><tr><td>val_loss_step</td><td>▄▃▅▇▅▆▆▆▃▆▂▅▂█▆▃▄▅▇▆▅▆▆▆▆▆▂▁▁▃▂▆▅▃▄▃▆▆▅▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.58447</td></tr><tr><td>train_auc</td><td>0.49372</td></tr><tr><td>train_f1</td><td>0.0</td></tr><tr><td>train_loss_epoch</td><td>0.67717</td></tr><tr><td>train_loss_step</td><td>0.66683</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.57875</td></tr><tr><td>val_auc</td><td>0.49684</td></tr><tr><td>val_f1</td><td>0.0</td></tr><tr><td>val_loss_epoch</td><td>0.68862</td></tr><tr><td>val_loss_step</td><td>0.69728</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_16_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/nkqr7t56' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/nkqr7t56</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240101_225631-nkqr7t56\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240101_231631-ezxc17ia</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/ezxc17ia' target=\"_blank\">MLP_4_32_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/ezxc17ia' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/ezxc17ia</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_4_32_onehot_3\\mean\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 3.5 K \n",
      "2 | head        | Sequential       | 562   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "4.0 K     Trainable params\n",
      "0         Non-trainable params\n",
      "4.0 K     Total params\n",
      "0.016     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▄▄▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▆██▇█████▇██████████</td></tr><tr><td>train_auc</td><td>▂▂▁▂▂▂▁▂▂▁▁▂▂▂▂▃▂▂▂▅▆██▇▇█▇██▇███████▇██</td></tr><tr><td>train_f1</td><td>█▂▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▄█▇▇▇█▇█▇▇▇██▇▇▇█▇▇█</td></tr><tr><td>train_loss_epoch</td><td>█▇▇▆▇▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▄▃▂▂▃▂▃▂▁▂▁▂▂▂▂▂▂▂▂▁</td></tr><tr><td>train_loss_step</td><td>▄▆▅▇▆▇▆▆▅▄▆▅▄▇▆▆▅▆▅▅▄▄▂▄▄▃▄▂▃▁▂▆▃▄▂▄▄█▃▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▇▆▅▇▇▇█▆▇▇█▇██▇███▇█</td></tr><tr><td>val_auc</td><td>▇█▁▇▇▇▆▆▆▆▆▆▆▇▆▇▇▇▇▇████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▇▆▅█▇██▆▇██▇▇█████▇█</td></tr><tr><td>val_loss_epoch</td><td>▆▇▇▇▇█▇▇▇▇▇▇▆▇▇█▇▆▆▆▄▇▃▄▇▃▃▅▅▅▃▄▄▃▂▇▁▅▃▄</td></tr><tr><td>val_loss_step</td><td>▅▅▆▅▆▇▆▆▆▆▅▆▅▆▆▆▆▅▅▅▄▇▃▄█▃▃▅▅▅▃▅▄▄▃█▁▆▄▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.67854</td></tr><tr><td>train_auc</td><td>0.71236</td></tr><tr><td>train_f1</td><td>0.51648</td></tr><tr><td>train_loss_epoch</td><td>0.61028</td></tr><tr><td>train_loss_step</td><td>0.62414</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.72161</td></tr><tr><td>val_auc</td><td>0.77777</td></tr><tr><td>val_f1</td><td>0.62</td></tr><tr><td>val_loss_epoch</td><td>0.59734</td></tr><tr><td>val_loss_step</td><td>0.62863</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_32_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/ezxc17ia' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/ezxc17ia</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240101_231631-ezxc17ia\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240101_233659-p2e1mu56</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/p2e1mu56' target=\"_blank\">MLP_4_32_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/p2e1mu56' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/p2e1mu56</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_4_32_onehot_3\\max\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 3.5 K \n",
      "2 | head        | Sequential       | 562   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "4.0 K     Trainable params\n",
      "0         Non-trainable params\n",
      "4.0 K     Total params\n",
      "0.016     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\torch_geometric\\warnings.py:17: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▃▄▅▄▅▅▅▅▅▅▇▅▆▆▅▇▇▆▇▅▇▇▆▇▇▆█▆▆▆▆▇▆▇▆▆▇▇▇</td></tr><tr><td>train_auc</td><td>▃▂▁▄▂▂▂▄▄▄▄▃▅▅▅▆▆▆▇▆▄▄▄▄▄▅▄▅▆▅▆▆▆▆▆▆▆▆█▆</td></tr><tr><td>train_f1</td><td>▇▅▄▃▄▃█▂▄▁▃▅▅▅█▄▇▆█▆▇█▆▂▇▆▂█▃▅▆▇▅▅▇▆▇▆▆█</td></tr><tr><td>train_loss_epoch</td><td>█▆▅▃▅▄▅▅▃▄▃▅▁▂▄▂▃▂▂▃▃▃▃▄▃▄▃▂▃▂▃▂▂▃▃▂▅▂▁▃</td></tr><tr><td>train_loss_step</td><td>▄▇▅█▅█▅▆▄▄▆▃▁▆▄▇▄▅▂▅▆▅▃▆▇▃▅▂▄▃▅█▅▄▅▅▄▅▄▅</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>▃▂▂▂▁▄▇█▅▄▄▆▆▅▆▇▅▇▆▇▆▅▄▆▆▅▆▆▆▆▄▃▃▃▄▆▆▁▁▆</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_epoch</td><td>▂▂▁▂▂▁▂▁▂▁▂▂▂▂▂▁▂▃▃▃▂▁▃▂▂▃▂▂▃▃▂▃▂▂▄▃█▅▄▆</td></tr><tr><td>val_loss_step</td><td>▃▂▂▂▂▁▂▂▂▂▂▂▃▂▂▂▂▃▃▄▂▁▃▂▂▃▂▃▃▃▂▃▂▂▃▂█▄▂▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.61918</td></tr><tr><td>train_auc</td><td>0.59768</td></tr><tr><td>train_f1</td><td>0.39827</td></tr><tr><td>train_loss_epoch</td><td>0.66411</td></tr><tr><td>train_loss_step</td><td>0.66441</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.42125</td></tr><tr><td>val_auc</td><td>0.48875</td></tr><tr><td>val_f1</td><td>0.59278</td></tr><tr><td>val_loss_epoch</td><td>0.8599</td></tr><tr><td>val_loss_step</td><td>0.86937</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_32_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/p2e1mu56' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/p2e1mu56</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240101_233659-p2e1mu56\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240101_235629-xzjcez9z</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/xzjcez9z' target=\"_blank\">MLP_4_32_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/xzjcez9z' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/xzjcez9z</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_4_32_onehot_3\\sum\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 3.5 K \n",
      "2 | head        | Sequential       | 562   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "4.0 K     Trainable params\n",
      "0         Non-trainable params\n",
      "4.0 K     Total params\n",
      "0.016     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▂▃▁▄▃▄▄▄▅▅▅▆▇▇▅▅▆▇▆▇▆▇▇█▆▇████▆▇█▇▇▇█▇</td></tr><tr><td>train_auc</td><td>▄▂▃▃▁▃▃▁▃▄▃▃▂▃▃▆▄▂▄▅▆▄▄▅▅▆▄▄▇▆▆▅▇▄▅▇█▆▇▆</td></tr><tr><td>train_f1</td><td>▁▁▄▃▄▂▅▄▅▂▃▃▄▃▄▆▇▄▇▅▄▆█▆▆█▅▇▆▆▆▇█▅▆▅█▆▇▇</td></tr><tr><td>train_loss_epoch</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▅▂▄▂▃▃▃▂▂▃▃▂▃▃▃▁▃▂▂▃▃▁▂▃▁▃▁▁▁▂▅▃▂▂▃▂▄▂▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▃▂▂▁▃▅▅▅▅▄▅▆▅▇▇▄▄▄▇▇▆▄▄▇▅▅▅▅▇█▆▅▇▇▇▄▅▅▄▅</td></tr><tr><td>val_auc</td><td>▃▁▃▃▃▃▄▅▄▄▅▆▅▆▆▅▆▅▆▆▆▇▆▆▇▇▆▆▇▇▇▇███▇▇▇▇▇</td></tr><tr><td>val_f1</td><td>██▄▁▅▇█▆▅▅▅▇▅██▅▄▅▇▇▇▅▅▇▆▅▅▆▇█▆▆▇▇▇▅▆▅▅▆</td></tr><tr><td>val_loss_epoch</td><td>██▇▅▇█▇▇▇▅▆▅▅▆▅▆▅▃▄▄▅▇▃▅█▅▅▅▆▆▄▄▄▄▃▆▁▄▄▃</td></tr><tr><td>val_loss_step</td><td>▇▇▆▄▆▇▆▆▆▄▅▄▅▅▅▆▄▃▄▄▅▇▃▅█▅▅▅▆▆▅▅▄▄▄▆▁▅▄▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.67489</td></tr><tr><td>train_auc</td><td>0.58051</td></tr><tr><td>train_f1</td><td>0.56585</td></tr><tr><td>train_loss_epoch</td><td>0.59933</td></tr><tr><td>train_loss_step</td><td>0.56812</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.67033</td></tr><tr><td>val_auc</td><td>0.76538</td></tr><tr><td>val_f1</td><td>0.45783</td></tr><tr><td>val_loss_epoch</td><td>0.53388</td></tr><tr><td>val_loss_step</td><td>0.48891</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_32_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/xzjcez9z' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/xzjcez9z</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240101_235629-xzjcez9z\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240102_001555-tefgofnq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/tefgofnq' target=\"_blank\">MLP_4_32_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/tefgofnq' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/tefgofnq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\torch_geometric\\deprecation.py:22: UserWarning: 'nn.glob.GlobalAttention' is deprecated, use 'nn.aggr.AttentionalAggregation' instead\n",
      "  warnings.warn(out)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_4_32_onehot_3\\attention\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 3.5 K \n",
      "2  | head        | Sequential       | 562   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 33    \n",
      "--------------------------------------------------\n",
      "4.1 K     Trainable params\n",
      "0         Non-trainable params\n",
      "4.1 K     Total params\n",
      "0.016     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▅▄▅▅▅▅▅▅▅▅▅▅▅▅▆▅▇▆▇▇▆▇▇████▇███▇███████</td></tr><tr><td>train_auc</td><td>▃▂▃▂▂▁▂▃▃▂▃▄▃▃▄▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇██▇▇██▇▇██</td></tr><tr><td>train_f1</td><td>▇▂▂▁▁▁▁▁▁▁▁▁▁▁▁▅▂▇▃▆▆▅▆▆████▇█▇██▇▇▇███▆</td></tr><tr><td>train_loss_epoch</td><td>█▇▆▆▆▇▆▆▆▆▆▆▆▆▆▅▆▅▅▄▄▃▃▃▃▃▃▂▂▂▂▂▄▃▁▂▃▂▁▂</td></tr><tr><td>train_loss_step</td><td>▆▆▆▇▇▇▆██▆▇▆▇█▆▄▆▆▅▃▄▄▁▇▆▅▃▄▃▆▇▅▄▇▆▆▆▆▃▅</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▄▁▃▄▄▅▅▅▄▅▅▄▄▄▅▅▅▅▅▅▅▅▃█</td></tr><tr><td>val_auc</td><td>█▅▃▂▂▂▁▁▁▁▂▂▃▄▆▇▇▇▇▇▇▇▇▇▇▇██████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▄▁▄▄▅▅▅▆▅▆▅▄▄▄▅▅▅▅▆▅▅▅▄█</td></tr><tr><td>val_loss_epoch</td><td>▆▅▇▇▅▆▆▇▆▆▆▆▆▆▆▆▆▅▄▅▆▅▂▅▄▅▁█▄▆▅▅▄▁▄▂▂▅▃▆</td></tr><tr><td>val_loss_step</td><td>▅▅▆▆▅▆▅▆▅▅▅▆▆▅▆▆▆▅▄▅▆▅▂▅▄▅▁█▄▆▆▅▄▁▅▁▂▅▁█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.67215</td></tr><tr><td>train_auc</td><td>0.7083</td></tr><tr><td>train_f1</td><td>0.46177</td></tr><tr><td>train_loss_epoch</td><td>0.62064</td></tr><tr><td>train_loss_step</td><td>0.62416</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.7033</td></tr><tr><td>val_auc</td><td>0.77402</td></tr><tr><td>val_f1</td><td>0.50909</td></tr><tr><td>val_loss_epoch</td><td>0.69304</td></tr><tr><td>val_loss_step</td><td>0.81928</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_32_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/tefgofnq' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/tefgofnq</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240102_001555-tefgofnq\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240102_003525-tqi3eplg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/tqi3eplg' target=\"_blank\">MLP_4_32_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/tqi3eplg' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/tqi3eplg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_4_32_onehot_3\\attention2\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 3.5 K \n",
      "2  | head        | Sequential       | 562   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 1.1 K \n",
      "--------------------------------------------------\n",
      "5.1 K     Trainable params\n",
      "0         Non-trainable params\n",
      "5.1 K     Total params\n",
      "0.020     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▅▄▅▅▅▅▅▅▅▅▅▆▇▇▇▇██▇█▇██▇██▇███▇▇████▇██</td></tr><tr><td>train_auc</td><td>▃▃▂▂▁▁▂▂▃▂▂▃▃▆▆▆▇▇█▇█▇▇▇████████████████</td></tr><tr><td>train_f1</td><td>▇▂▃▁▁▁▁▁▁▁▁▁▃▇▇▆▇██▇▆▇▇▇▇▇▇██▇▇█▇▇▇▇▇▇▇█</td></tr><tr><td>train_loss_epoch</td><td>█▆▆▆▆▇▆▆▆▆▆▆▅▄▄▃▂▂▃▃▂▂▂▃▂▁▂▃▃▁▂▃▂▂▂▃▃▂▂▂</td></tr><tr><td>train_loss_step</td><td>▇▆▆▆▇▇▆▆▆▅▄▆▆▄▅▃▃▂▄▄▂▄▅▂▆▄█▄▃▂▄▄▃▁▂▃▃▂▂▄</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▅▇▇▇▅▅▇▇▇▇▇▇▇▇▇▇▇█▇██▇▇▇█▇█</td></tr><tr><td>val_auc</td><td>▄▅▂▂▁▁▁▁▁▂▂▂▄▅▆▇▇▇▇▇▇▇██▇███████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▅█▇▇▅▅█▇▇▇▇██▇▇▇███████▇███</td></tr><tr><td>val_loss_epoch</td><td>█▇▇▇▇▇█▇▇▆▆█▇▆▅▅▄▄▄▅▄▂▅▃▄▄▄▄▅▄▃▄▄▃▁▃▄▃▅▁</td></tr><tr><td>val_loss_step</td><td>█▇▆▆▇▇█▇▇▅▅█▇▇▆▆▅▄▅▆▄▂▆▄▅▄▅▅▆▆▄▅▄▃▂▄▅▄▇▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.68584</td></tr><tr><td>train_auc</td><td>0.71934</td></tr><tr><td>train_f1</td><td>0.56566</td></tr><tr><td>train_loss_epoch</td><td>0.6125</td></tr><tr><td>train_loss_step</td><td>0.62679</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.72527</td></tr><tr><td>val_auc</td><td>0.77887</td></tr><tr><td>val_f1</td><td>0.61538</td></tr><tr><td>val_loss_epoch</td><td>0.48832</td></tr><tr><td>val_loss_step</td><td>0.40261</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_32_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/tqi3eplg' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/tqi3eplg</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240102_003525-tqi3eplg\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240102_005422-xwzw9swk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/xwzw9swk' target=\"_blank\">MLP_4_64_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/xwzw9swk' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/xwzw9swk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_4_64_onehot_3\\mean\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 13.1 K\n",
      "2 | head        | Sequential       | 2.1 K \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "15.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "15.3 K    Total params\n",
      "0.061     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▄▄▄▄▄▄▄▄▄▆▆▆▇▇▇▇▆▆▇▇███▇█▇██▇▇█▇▇▇███▇█</td></tr><tr><td>train_auc</td><td>▁▂▁▁▁▁▂▂▃▂▆▆▇▇▇▇█▇▇▇████▇▇▇██▇▇███▇█████</td></tr><tr><td>train_f1</td><td>▅▃▁▁▁▁▁▁▁▁▆▆▆▇▇▆▇▆▅▇██▇▇▇██▇▇▇██▇▇▇█▇▇██</td></tr><tr><td>train_loss_epoch</td><td>█▆▆▆▆▆▆▆▆▅▄▄▃▂▂▂▁▂▄▂▂▂▂▂▁▂▂▂▂▂▂▃▁▂▂▂▂▁▄▂</td></tr><tr><td>train_loss_step</td><td>█▇█▇▇▆▆▇▆▅█▆▆█▆▆▃▅▆▅▆▄▃▆▅▆▂▆▅▆▃▇▅▅▄▁▄▆▆▄</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▅▇▇▅▇█▇███▄█▇███████▆████▇███▇▆</td></tr><tr><td>val_auc</td><td>▁▂▃▄▃▃▄▃▅▅▇▇▇▇██████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▅▇▇▅███▇▇█▄▇▇██▇█▇▇▇▆▇█▇▇▇███▇▆</td></tr><tr><td>val_loss_epoch</td><td>█▇▇████▇▇▇▄▆█▃▃▄▃▄▃▄▄▆▇▅▄▂▄▄▂▁▂▇▃▁▄▃▄▃▂▃</td></tr><tr><td>val_loss_step</td><td>▆▅▅▆▆▆▆▅▆▆▄▆█▃▃▄▃▄▃▄▄▆▇▆▅▂▅▅▃▁▂█▄▁▅▄▄▃▂▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.68493</td></tr><tr><td>train_auc</td><td>0.72254</td></tr><tr><td>train_f1</td><td>0.55826</td></tr><tr><td>train_loss_epoch</td><td>0.61096</td></tr><tr><td>train_loss_step</td><td>0.58303</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.67766</td></tr><tr><td>val_auc</td><td>0.77903</td></tr><tr><td>val_f1</td><td>0.46341</td></tr><tr><td>val_loss_epoch</td><td>0.57289</td></tr><tr><td>val_loss_step</td><td>0.55471</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_64_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/xwzw9swk' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/xwzw9swk</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240102_005422-xwzw9swk\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240102_011325-g1dtdhsf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/g1dtdhsf' target=\"_blank\">MLP_4_64_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/g1dtdhsf' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/g1dtdhsf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_4_64_onehot_3\\max\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 13.1 K\n",
      "2 | head        | Sequential       | 2.1 K \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "15.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "15.3 K    Total params\n",
      "0.061     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\torch_geometric\\warnings.py:17: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▄▄▄▄▄▄▄▅▅▅▆▆▇▇▆▆▆▆▆▆█▆▆▆▅▆▇▆▆▅▆▆▆▆▆▆▇▆</td></tr><tr><td>train_auc</td><td>▁▂▁▂▂▁▂▄▃▁▃▅▅▆▇▅▇▆▆▇▆▆█▇▇▇▅▇▇▇█▆▇▆█▇▆▇█▇</td></tr><tr><td>train_f1</td><td>▅▇▁▂▁▁▁▁▁▂▂▄▅▅▆▅▆▆▅▇█▇▆▆▆▇▆▆▇██▇▆▇▇▇▆▇█▆</td></tr><tr><td>train_loss_epoch</td><td>█▅▅▄▄▅▃▄▄▄▄▃▃▂▁▃▂▂▄▂▂▂▁▂▂▂▃▂▂▂▃▄▃▃▂▃▂▂▁▃</td></tr><tr><td>train_loss_step</td><td>█▆▇▆▇▄▂▃▄▅▆▆▁▅▃█▅▆▅▆▆▂▁▂▅▂▂▇▆▂▄▃▆▄▆▂▆▆▁▇</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>█▄▃▃▂▁▅▃▄▃▃▄▇▆▃▃▄▆▇▅▃▃▄▂▂▂▄▃▃▅▅▅▅▅▃▃▁▁█▃</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_epoch</td><td>▁▁▁▁▁▁▁▁▂▂▂▂▂▃▄▆▅▄▃▄▃▃▅▅▆▇▆▅▆█▅▄▅▅▅▅▄▆▇▇</td></tr><tr><td>val_loss_step</td><td>▁▁▁▁▁▁▁▁▁▂▁▂▁▂▃▆▄▃▁▃▂▁▄▄▅▆▄▄▆█▄▂▃▄▃▄▂▅▅▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.61279</td></tr><tr><td>train_auc</td><td>0.62227</td></tr><tr><td>train_f1</td><td>0.40449</td></tr><tr><td>train_loss_epoch</td><td>0.66204</td></tr><tr><td>train_loss_step</td><td>0.68551</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.42125</td></tr><tr><td>val_auc</td><td>0.37133</td></tr><tr><td>val_f1</td><td>0.59278</td></tr><tr><td>val_loss_epoch</td><td>1.25249</td></tr><tr><td>val_loss_step</td><td>1.2038</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_64_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/g1dtdhsf' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/g1dtdhsf</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240102_011325-g1dtdhsf\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240102_013344-lip18voe</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/lip18voe' target=\"_blank\">MLP_4_64_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/lip18voe' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/lip18voe</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_4_64_onehot_3\\sum\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 13.1 K\n",
      "2 | head        | Sequential       | 2.1 K \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "15.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "15.3 K    Total params\n",
      "0.061     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▃▂▃▄▃▄▃▄▄▄▅▅▅▆▆▇▆▆▆▆▇▇▇▇█▇▇▇▇▇▇▇▇▇▇█▇▇</td></tr><tr><td>train_auc</td><td>▅▄▂▅█▆▆▆▃▅▆▃▃▄▅▃▅▄▆▅▂▇▆▅▃▃▃▃▅▂▂▃▄▂▃▂▄▁▁▁</td></tr><tr><td>train_f1</td><td>▂▂▃▆▄▂▄▁▃▃▃▃▃▄▅▄▆▆▄▄██▇▆▇▇▇▇███▇▆█▆▇▇██▇</td></tr><tr><td>train_loss_epoch</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▄▃▃▂▂▁▁▂▁▂▂▁▂▁▂▁▁▂▂▂▁▁▁▂▂▁▂▁▁▁▂▂▂▁▁▁▁▁▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▅▅▅▅▆▅▅▅▅▅▆▇▇█▇▇█▅▇▇█▇▇▇▇▇▇▇██▇▇▇███▇█</td></tr><tr><td>val_auc</td><td>▃▃▃▇▇▇▇▇▃▇▇▆▄▅▇▄▆▅█▄▃▇▇▃▄▃▂▃▃▂▁▁▁▁▁▁▁▁▂▂</td></tr><tr><td>val_f1</td><td>██▁▁▅▁▂▁▁▁▁▂▂▅▅▆▅▆▇▁▄▆▇▆▅▆▆▅▅▅▆▇▅▆▆▇▇▆▆▆</td></tr><tr><td>val_loss_epoch</td><td>█▂▁▁▂▁▁▁▂▁▁▁▂▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_step</td><td>█▂▁▂▂▂▁▁▂▁▂▂▂▂▁▁▁▂▁▂▁▂▂▂▁▁▂▂▂▁▁▂▂▁▂▂▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.67123</td></tr><tr><td>train_auc</td><td>0.4452</td></tr><tr><td>train_f1</td><td>0.5288</td></tr><tr><td>train_loss_epoch</td><td>0.61794</td></tr><tr><td>train_loss_step</td><td>0.6562</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.67033</td></tr><tr><td>val_auc</td><td>0.28514</td></tr><tr><td>val_f1</td><td>0.45783</td></tr><tr><td>val_loss_epoch</td><td>0.56538</td></tr><tr><td>val_loss_step</td><td>0.54345</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_64_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/lip18voe' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/lip18voe</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240102_013344-lip18voe\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240102_015324-ynkzt9bm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/ynkzt9bm' target=\"_blank\">MLP_4_64_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/ynkzt9bm' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/ynkzt9bm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\torch_geometric\\deprecation.py:22: UserWarning: 'nn.glob.GlobalAttention' is deprecated, use 'nn.aggr.AttentionalAggregation' instead\n",
      "  warnings.warn(out)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_4_64_onehot_3\\attention\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 13.1 K\n",
      "2  | head        | Sequential       | 2.1 K \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 65    \n",
      "--------------------------------------------------\n",
      "15.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "15.3 K    Total params\n",
      "0.061     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▃▄▄▄▄▄▄▄▄▄▅▆▆▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇▆▇█▇▆█▇▇███</td></tr><tr><td>train_auc</td><td>▁▂▂▂▂▂▂▃▂▂▃▅▆▆▇▇▇▇▇▇▇▇▇▇▇▇█▇▇███▇▇██████</td></tr><tr><td>train_f1</td><td>▆▂▁▁▁▁▁▁▁▁▁▇▅▅▇████▆▆██▇▇▆▇▆▆▅▆▇▆▅████▇█</td></tr><tr><td>train_loss_epoch</td><td>█▆▇▆▆▆▆▆▆▆▆▅▄▃▄▄▃▂▃▂▂▃▃▂▃▃▂▃▂▃▂▂▃▃▂▂▂▁▂▂</td></tr><tr><td>train_loss_step</td><td>▆█▇▇▆▇▆▇█▇▅▄▆▃▂▅█▅▄▅█▅▂▅▂▃▁█▂▅▃▂▄▃▃▄▃▅▆▅</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▅▃██▄▃▄▅▅▆▇▅▅▅▅▅████▅▇█▅▅▅▅▅▇▅</td></tr><tr><td>val_auc</td><td>▁▁▂▂▃▂▂▂▄▄▇▇▇███████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▄▃█▇▄▄▄▅▅▆▆▅▅▅▅▆▇█▇█▅▇█▅▆▅▅▅▇▅</td></tr><tr><td>val_loss_epoch</td><td>▇▇▆▆▆▇▇▇▇▇▆▄▄▃█▅▇▄▄▁▃▂▄▇▅▂▃▃▅▁▆▄▄▆▄█▅▃▃▇</td></tr><tr><td>val_loss_step</td><td>▆▇▅▅▅▇▆▆▆▆▆▄▄▃█▄▇▄▃▁▄▂▄▇▅▂▄▃▆▁▆▅▅▆▅█▅▄▄▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.68311</td></tr><tr><td>train_auc</td><td>0.71497</td></tr><tr><td>train_f1</td><td>0.59604</td></tr><tr><td>train_loss_epoch</td><td>0.61948</td></tr><tr><td>train_loss_step</td><td>0.63532</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.65934</td></tr><tr><td>val_auc</td><td>0.77733</td></tr><tr><td>val_f1</td><td>0.35862</td></tr><tr><td>val_loss_epoch</td><td>0.68854</td></tr><tr><td>val_loss_step</td><td>0.77618</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_64_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/ynkzt9bm' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/ynkzt9bm</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240102_015324-ynkzt9bm\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240102_021248-ssg452ap</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/ssg452ap' target=\"_blank\">MLP_4_64_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/ssg452ap' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/ssg452ap</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_4_64_onehot_3\\attention2\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 13.1 K\n",
      "2  | head        | Sequential       | 2.1 K \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 4.2 K \n",
      "--------------------------------------------------\n",
      "19.5 K    Trainable params\n",
      "0         Non-trainable params\n",
      "19.5 K    Total params\n",
      "0.078     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▃▄▄▄▄▄▄▄▄▄▄▆▇▇▆▇▇▇▇▇▇█▇█▇▇▇█▇▇▇█▇▇▇▆▇█▇</td></tr><tr><td>train_auc</td><td>▂▂▂▁▂▂▁▁▂▂▂▃▇█▇▇▇▇▇▇▇▇█▇█▇████▇████▇█▇██</td></tr><tr><td>train_f1</td><td>▆▂▁▁▁▁▁▁▁▁▁▁▆▇▇▆▇▇▇▇▇▇▇▇▇▇██▇▇▇▇█▇▇▇▅▆█▆</td></tr><tr><td>train_loss_epoch</td><td>█▇▇▇▇▇▇▇▇▇▇▆▃▃▃▄▃▃▃▃▂▂▁▃▂▂▂▂▂▂▃▁▂▁▂▁▃▂▂▂</td></tr><tr><td>train_loss_step</td><td>█▇▇▆▇▆▇▇▇▇▇▅▁▅▃▄▅▄▅▄▄▃▄▃▄▅▄▄▆▇▄▆▃▄▅▄▄▅▅▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▇▆▇▇▇▇▇▇▇█▇▆█▇▆▅▇▆▆▆▆▇▇█▇▇██</td></tr><tr><td>val_auc</td><td>▂▁▂▂▃▂▃▃▄▄▆▇▇█▇█████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁██▇████▇▇██▆▇▇▆▅▇▆▆▆▆▇██▇▇▇▇</td></tr><tr><td>val_loss_epoch</td><td>▇▇▇▇▆▇▇▆█▆▇▆▅▅▄▅▄▆▄▆▃▅▃▃▄▄▃▄▃▄▅▄▆▃▆▅▄▁▄▃</td></tr><tr><td>val_loss_step</td><td>▆▆▇▆▆▇▇▆█▆▇▆▆▅▄▆▄▆▅█▄▆▃▄▅▅▄▄▄▄▆▅▇▄█▆▅▁▅▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.66484</td></tr><tr><td>train_auc</td><td>0.71588</td></tr><tr><td>train_f1</td><td>0.44978</td></tr><tr><td>train_loss_epoch</td><td>0.60675</td></tr><tr><td>train_loss_step</td><td>0.53857</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.72161</td></tr><tr><td>val_auc</td><td>0.7803</td></tr><tr><td>val_f1</td><td>0.60417</td></tr><tr><td>val_loss_epoch</td><td>0.52777</td></tr><tr><td>val_loss_step</td><td>0.47901</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_64_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/ssg452ap' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/ssg452ap</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240102_021248-ssg452ap\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240102_023250-mdznmxut</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/mdznmxut' target=\"_blank\">MLP_2_16_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/mdznmxut' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/mdznmxut</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_2_16_onehot_4\\mean\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 432   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "586       Trainable params\n",
      "0         Non-trainable params\n",
      "586       Total params\n",
      "0.002     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▅▅▅▅▅▅▅▅▅▆▇▇▇▇▇█▇▇▇▇▇███████▇▇▇██▇▇▇▇▇█</td></tr><tr><td>train_auc</td><td>▁▁▂▂▂▂▂▂▃▃▅▅▆▇▇▆▇▇▇▇▇▇██▇█▇▇▇▇████▇█▇█▇█</td></tr><tr><td>train_f1</td><td>▇▂▁▁▁▁▁▁▁▂▄▆▅▆▇▇█▇▆▇▇▇██████▇▇▆▇██▆█▇█▇▇</td></tr><tr><td>train_loss_epoch</td><td>█▇▇▆▆▆▆▆▆▆▅▄▄▂▄▃▃▃▃▂▃▃▂▁▂▁▂▂▂▂▁▁▂▂▂▂▂▁▂▁</td></tr><tr><td>train_loss_step</td><td>▅▆▆█▇▇▇▇▆▅▆▆▃▄▂▅▅▄▄█▆▃▃▇▄▅▆▅▂▃▃▃▆▆▂▂▄▃▃▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▂▆▇▇▇▇▇█▇▇█▇████████▇▇▇█▆█▇▇█▇</td></tr><tr><td>val_auc</td><td>▁▃▃▂▃▄██████████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▃▆▇▇▇▇██▇▇█▇████████▇███▇█▇▇██</td></tr><tr><td>val_loss_epoch</td><td>▇▆▆▆▇▇▆▇▆▆▆▅▄▅▄▅▃▄▆▃▅▇▄▅▃▄▄▄▇▃█▆▅▃▁▄▂▂▃▃</td></tr><tr><td>val_loss_step</td><td>▅▅▅▅▅▆▅▆▅▅▅▅▄▅▄▅▃▄▆▃▅▇▄▅▃▄▄▄▇▃█▆▅▃▁▄▂▂▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.68584</td></tr><tr><td>train_auc</td><td>0.74117</td></tr><tr><td>train_f1</td><td>0.51685</td></tr><tr><td>train_loss_epoch</td><td>0.58095</td></tr><tr><td>train_loss_step</td><td>0.51796</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.68498</td></tr><tr><td>val_auc</td><td>0.72173</td></tr><tr><td>val_f1</td><td>0.57</td></tr><tr><td>val_loss_epoch</td><td>0.58035</td></tr><tr><td>val_loss_step</td><td>0.54584</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_16_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/mdznmxut' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/mdznmxut</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240102_023250-mdznmxut\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240102_025213-wijsfa19</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/wijsfa19' target=\"_blank\">MLP_2_16_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/wijsfa19' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/wijsfa19</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_2_16_onehot_4\\max\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 432   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "586       Trainable params\n",
      "0         Non-trainable params\n",
      "586       Total params\n",
      "0.002     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\torch_geometric\\warnings.py:17: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▆▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇██▇▇██▇█▇██▇▇██▇</td></tr><tr><td>train_auc</td><td>▄▃▅▄▄▄▆▅▇▃▅▃▄▅▅▁▃▅▅▅▆▆▅▅▅▅▇▆▆▆▆▇▆▆▆▅▆▇██</td></tr><tr><td>train_f1</td><td>█▃▂▂▁▁▁▁▁▁▂▂▃▃▄▃▄▄▄▄▅▅▅▄▆▅▅▅▅▆▄▅▅▆▅▅▅▅▅▅</td></tr><tr><td>train_loss_epoch</td><td>█▄▃▃▃▃▃▃▃▃▃▃▃▂▃▂▂▃▂▂▃▃▂▂▂▂▂▂▂▁▁▂▁▂▂▂▂▁▂▁</td></tr><tr><td>train_loss_step</td><td>▃▄▄▅▅▅▆▅▅▃▅▄▂▁▃▅▄▂▂▅▄▃▄▆█▅▄▅▃▂▂▃▆▃▄▃▃▁▃▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▆▆▆▆▆▆▆▆▆▆▆▆▆██▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>▂▂▂▂▂▂▂▁▁▁▁▁▁▁▆▁▂███████████▇▇▇▇████████</td></tr><tr><td>val_f1</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁██████████████████████████</td></tr><tr><td>val_loss_epoch</td><td>▃▂▂▂▂▃▂▃▂▂▂▂▂▂▂▂▃▃▃▃▄▂▅▄▅▆▄▅▄▆▄▄▆▆█▇██▇▁</td></tr><tr><td>val_loss_step</td><td>▅▄▅▄▅▅▄▅▄▄▅▅▅▄▅▄▅▅▄▅▅▃▆▅▆▇▅▆▅▆▅▅▆▆█▇██▆▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.61735</td></tr><tr><td>train_auc</td><td>0.56922</td></tr><tr><td>train_f1</td><td>0.30972</td></tr><tr><td>train_loss_epoch</td><td>0.6428</td></tr><tr><td>train_loss_step</td><td>0.58952</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.43956</td></tr><tr><td>val_auc</td><td>0.63241</td></tr><tr><td>val_f1</td><td>0.61069</td></tr><tr><td>val_loss_epoch</td><td>0.66675</td></tr><tr><td>val_loss_step</td><td>0.55843</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_16_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/wijsfa19' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/wijsfa19</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240102_025213-wijsfa19\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240102_031136-buqk39rq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/buqk39rq' target=\"_blank\">MLP_2_16_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/buqk39rq' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/buqk39rq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_2_16_onehot_4\\sum\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 432   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "586       Trainable params\n",
      "0         Non-trainable params\n",
      "586       Total params\n",
      "0.002     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▃▂▄▂▄▄▅▄▆▅▅▅▆▅▅▅▇▆▅▆▇▆▅▆▆▆▆▇▆▇▇▇▇▆▆▇▇▇█</td></tr><tr><td>train_auc</td><td>▄▅▆▆▅▄▆▇█▇▆▆▆▆▆▅▇▇▅▅▅▅▅▆▆▄▄▄▄▄▃▂▃▂▄▃▃▁▂▁</td></tr><tr><td>train_f1</td><td>█▁▅▅▄▄▆▄▅▅▄▆▄▄▄▂▅▅▄▃▅▅▆▃▆▄▅▆▂▆▃▅▆▆▃▅▅▇▄█</td></tr><tr><td>train_loss_epoch</td><td>█▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▃▃▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▂▂▂▂▂▅▄▃▅▂▃▂▂▇▂▅▃▅▄▅▄▇▅▆▅▅▇▃▇▄▇▃█▄▇▅▆</td></tr><tr><td>val_auc</td><td>▂▃▄▃▇▂█▅▇▆▃▃▃▃▃▄▄▃▂▂▂▂▂▃▄▁▁▁▁▁▁▁▁▁▂▂▁▁▁▁</td></tr><tr><td>val_f1</td><td>▁▁▁▂▂▃▃▂▅▅▃▅▃▄▃▃▇▃▅▄▅▄▆▄▇▆▇▆▅▇▄▇▅▇▄█▅▇▅▇</td></tr><tr><td>val_loss_epoch</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_step</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▂▁▂▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.65753</td></tr><tr><td>train_auc</td><td>0.39896</td></tr><tr><td>train_f1</td><td>0.52107</td></tr><tr><td>train_loss_epoch</td><td>0.61709</td></tr><tr><td>train_loss_step</td><td>0.56092</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.66667</td></tr><tr><td>val_auc</td><td>0.24984</td></tr><tr><td>val_f1</td><td>0.50273</td></tr><tr><td>val_loss_epoch</td><td>0.65567</td></tr><tr><td>val_loss_step</td><td>0.69623</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_16_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/buqk39rq' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/buqk39rq</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240102_031136-buqk39rq\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240102_033106-46l9eghd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/46l9eghd' target=\"_blank\">MLP_2_16_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/46l9eghd' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/46l9eghd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\torch_geometric\\deprecation.py:22: UserWarning: 'nn.glob.GlobalAttention' is deprecated, use 'nn.aggr.AttentionalAggregation' instead\n",
      "  warnings.warn(out)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_2_16_onehot_4\\attention\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 432   \n",
      "2  | head        | Sequential       | 154   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 17    \n",
      "--------------------------------------------------\n",
      "603       Trainable params\n",
      "0         Non-trainable params\n",
      "603       Total params\n",
      "0.002     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▄▅▅▅▅▅▅▅▆▆█▇█▇▇▇▇▇█▇▇███▇▇██▇██▇▇▇▇███▇</td></tr><tr><td>train_auc</td><td>▂▂▂▂▁▂▂▁▃▄▅▇▇▇▇▇▇▇▆▇▇▇▇▇▇▇▇█▇███▇▇▇█████</td></tr><tr><td>train_f1</td><td>▇▂▁▁▁▁▁▁▁▃▄▇▆█▆▇▆▆▇▇█▆▇▇▇▇▇██▅▇▇▇▇█▆▆█▇▆</td></tr><tr><td>train_loss_epoch</td><td>█▆▆▆▆▆▆▆▅▅▄▃▄▃▃▃▄▃▄▁▃▂▂▃▃▂▃▂▃▂▂▁▄▂▂▂▂▂▂▃</td></tr><tr><td>train_loss_step</td><td>▆▇▇█▇▆▇▆▇▃█▃▄▄▄▂█▆▆▄▁▃▂▄▃▅▆▇▅▂▆▃▂▅▁▁▅▃▆▆</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▃▄▃▄▄▅▅▄▄▄▆▆▄▄▅▄▅▄█▅▄▆▆▄▇▇▄▇▇▄</td></tr><tr><td>val_auc</td><td>▁▅▆▇▇███████████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▃▄▃▄▄▅▅▄▄▄▆▆▄▅▆▅▆▅█▆▅▆▆▄▇█▄▇▇▅</td></tr><tr><td>val_loss_epoch</td><td>▅▄▅▅▆▅▄▅▄▄▅▅▃▃▅▅▄▂▄▄▃▂▄▃▅▃▁▅▂▃▄▄▃█▃▂▅▃▃▂</td></tr><tr><td>val_loss_step</td><td>▅▃▅▅▅▄▄▄▃▄▅▅▂▃▅▅▄▂▄▄▄▂▄▂▅▃▁▅▂▃▄▄▃█▃▂▅▃▃▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.67306</td></tr><tr><td>train_auc</td><td>0.7271</td></tr><tr><td>train_f1</td><td>0.43711</td></tr><tr><td>train_loss_epoch</td><td>0.61547</td></tr><tr><td>train_loss_step</td><td>0.65045</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.62271</td></tr><tr><td>val_auc</td><td>0.72849</td></tr><tr><td>val_f1</td><td>0.3268</td></tr><tr><td>val_loss_epoch</td><td>0.59098</td></tr><tr><td>val_loss_step</td><td>0.53211</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_16_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/46l9eghd' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/46l9eghd</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240102_033106-46l9eghd\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240102_035010-hzwo4ibx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/hzwo4ibx' target=\"_blank\">MLP_2_16_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/hzwo4ibx' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/hzwo4ibx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_2_16_onehot_4\\attention2\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 432   \n",
      "2  | head        | Sequential       | 154   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 281   \n",
      "--------------------------------------------------\n",
      "867       Trainable params\n",
      "0         Non-trainable params\n",
      "867       Total params\n",
      "0.003     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▄▅▅▅▅▅▅▅▅▆▇█▇▇█▇▇▇█▇██▇██████▇███▇▇████</td></tr><tr><td>train_auc</td><td>▁▂▃▁▁▂▂▂▂▄▅▆▇▇▇▆▇▇▇▇█▇█▇█▇▇▇██████▇▇████</td></tr><tr><td>train_f1</td><td>▇▂▁▁▁▁▁▁▂▂▅▇█▆▇█▆▇▇▇▆▇▇▇█▇▇▇▇▇██▇▇█▆▇▇█▇</td></tr><tr><td>train_loss_epoch</td><td>█▆▆▆▆▆▆▆▅▅▄▃▄▃▃▃▄▂▂▂▃▃▂▂▃▃▂▃▂▁▃▁▁▁▂▂▂▂▁▂</td></tr><tr><td>train_loss_step</td><td>▇▇▆▆▆▇▆▆▆▄▇▄█▅▄▂▇▂▃▆▅▂▅▄▅▁▂▁▂▅▂▄▂█▅▆▃▄▆▄</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▃▅▆▆▄▆▇▅▇▇▆▆▆▇▇▇█▇▆▆▇██▇▆▆▇▇█▇</td></tr><tr><td>val_auc</td><td>▁▃▆▇████████████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▂▃▅▆▆▄▇▇▅▇▇▇▆▇▇█▇██▇▇▇██▇█▆█▇█▇</td></tr><tr><td>val_loss_epoch</td><td>▇▇▇█▆▇▆▆▅▅▅▄▄▅▃▃▂▃▅▄▃▅▂▁▂▃▃▅▄▃▂▄▄▅▃▄▄▄▃▇</td></tr><tr><td>val_loss_step</td><td>▆▇▆█▅▇▆▅▅▅▅▅▄▆▂▃▂▃▆▄▄▆▃▁▂▄▄▆▅▃▂▄▄▆▃▄▅▅▄█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.68676</td></tr><tr><td>train_auc</td><td>0.7289</td></tr><tr><td>train_f1</td><td>0.52295</td></tr><tr><td>train_loss_epoch</td><td>0.59483</td></tr><tr><td>train_loss_step</td><td>0.60945</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.67766</td></tr><tr><td>val_auc</td><td>0.72565</td></tr><tr><td>val_f1</td><td>0.55102</td></tr><tr><td>val_loss_epoch</td><td>0.68666</td></tr><tr><td>val_loss_step</td><td>0.77281</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_16_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/hzwo4ibx' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/hzwo4ibx</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240102_035010-hzwo4ibx\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240102_040837-xs1z8bpi</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/xs1z8bpi' target=\"_blank\">MLP_2_32_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/xs1z8bpi' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/xs1z8bpi</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_2_32_onehot_4\\mean\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 1.4 K \n",
      "2 | head        | Sequential       | 562   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.008     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▁▁▁▁▃▅▅▆▇▇█▆▇▅█▇▇▇█▇▇▇▇▆▇▇█▇█▇▇███▇▇█▇</td></tr><tr><td>train_auc</td><td>▁▁▁▂▃▄▄▇▇▇▇▇█▇▇█████████████████████████</td></tr><tr><td>train_f1</td><td>▁▁▁▁▁▁▄▅▆▇█▇▇▅█▅██▇▇▇█▇▇▇▇█▇████▇███▇▇▇▇</td></tr><tr><td>train_loss_epoch</td><td>██▇▇▇▆▆▄▅▃▃▃▂▆▄▄▃▂▁▂▂▂▂▂▃▃▂▂▂▂▂▂▂▂▁▂▂▂▂▂</td></tr><tr><td>train_loss_step</td><td>▆█▅▆▆▆▅▃▅▆▄▂▂▄▃▃▃▃▅▃▁▅▂▄▃▆▄▃▅▄▃▆▁▃▁▄▃▄▃▄</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▅██▇▇█▆▇▅▇▇██▇▇▇███▇███▇█▇▇▆▇▇▇▇▇█</td></tr><tr><td>val_auc</td><td>▁▇██████████████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▅██▇███▇▅▇▇██▆▇▇█▇▇▇███▇██▇▇▇█▇▇▇▇</td></tr><tr><td>val_loss_epoch</td><td>█▇▆▆▇▅▅▇▄▅▄▃▆▁▂▃▂▃█▄▅▄▅▄▃▇▆▃▃▂▁▁▆▄▄▆▄▃▄▅</td></tr><tr><td>val_loss_step</td><td>▆▆▅▄▆▄▅▇▄▅▄▄▆▁▂▃▂▃█▄▅▄▅▄▃▇▆▃▃▂▂▁▆▄▄▆▄▃▄▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.68676</td></tr><tr><td>train_auc</td><td>0.74488</td></tr><tr><td>train_f1</td><td>0.52949</td></tr><tr><td>train_loss_epoch</td><td>0.58626</td></tr><tr><td>train_loss_step</td><td>0.60061</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.69231</td></tr><tr><td>val_auc</td><td>0.72168</td></tr><tr><td>val_f1</td><td>0.59223</td></tr><tr><td>val_loss_epoch</td><td>0.64073</td></tr><tr><td>val_loss_step</td><td>0.67611</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_32_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/xs1z8bpi' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/xs1z8bpi</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240102_040837-xs1z8bpi\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240102_042701-uoczvzb2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/uoczvzb2' target=\"_blank\">MLP_2_32_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/uoczvzb2' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/uoczvzb2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_2_32_onehot_4\\max\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 1.4 K \n",
      "2 | head        | Sequential       | 562   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.008     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\torch_geometric\\warnings.py:17: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▃▂▃▃▃▃▃▄▅▆▅▇▆▆▅▅▇▇▇▇█▇▇▇▆█▆▆▇▇▆█▇▇█▇▇▇▇</td></tr><tr><td>train_auc</td><td>▂▁▁▂▂▂▁▃▃▄▅▅▆▆▆▅▆▆▆▇▇▇▇▇█▇▆▇▇▇▇▆▇▇▇█▇█▇▇</td></tr><tr><td>train_f1</td><td>▅▁▄▁▁▁▁▁▁▄▆▅▇▅█▅▇▇▇█▇█▇▇▇▇█▇▇▇█▇█▇▇█▇█▇▇</td></tr><tr><td>train_loss_epoch</td><td>█▇▆▆▆▅▆▆▅▄▄▄▂▅▄▄▄▃▂▃▂▃▂▃▂▃▁▃▃▂▂▃▂▂▁▁▂▁▁▃</td></tr><tr><td>train_loss_step</td><td>▆▇▅▆▆▆▆▅▆▆▆▅▂▄▇▄▇▁▃▄▃▄▃▆█▆▄▅▇▃▅▆▂▅▅▃▅▃▃▇</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▆▆▆▆▆▆▆▆▆█▇▇▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>▁▁▁▁▁▂▁▃████████▇▇▇▇██▇▇▇██▇▇▇███▇▇▆▇▇▇▇</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▆██████████████████████████████</td></tr><tr><td>val_loss_epoch</td><td>▄▂▁▁▂▁▁▂▁▁▂▁▂▂▂▂▃▃▁▄▂▃▅▄▃▄▃▅▄▄█▄▄▂▃▄▄▆▅▅</td></tr><tr><td>val_loss_step</td><td>▄▃▂▁▂▁▁▂▂▂▂▂▃▃▃▂▃▃▁▃▂▃▅▄▃▄▂▅▄▄█▃▄▁▂▃▄▆▄▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.65845</td></tr><tr><td>train_auc</td><td>0.66912</td></tr><tr><td>train_f1</td><td>0.44179</td></tr><tr><td>train_loss_epoch</td><td>0.63812</td></tr><tr><td>train_loss_step</td><td>0.68426</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.43956</td></tr><tr><td>val_auc</td><td>0.62497</td></tr><tr><td>val_f1</td><td>0.61069</td></tr><tr><td>val_loss_epoch</td><td>0.81569</td></tr><tr><td>val_loss_step</td><td>0.84499</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_32_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/uoczvzb2' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/uoczvzb2</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240102_042701-uoczvzb2\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240102_044434-vezghfyh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/vezghfyh' target=\"_blank\">MLP_2_32_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/vezghfyh' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/vezghfyh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_2_32_onehot_4\\sum\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 1.4 K \n",
      "2 | head        | Sequential       | 562   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.008     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▂▁▃▂▃▃▃▄▄▅▅▄▆▅▅▅▇▆▇▇▆▆▆▇▇▇█▇█▇▆▆██▇█▇█▇█</td></tr><tr><td>train_auc</td><td>▅▃█▇▇▆▆▅▄▆▄▆▆▄▃▃▃▃▃▃▃▃▁▁▂▂▃▃▃▁▂▂▂▂▁▂▂▂▃▄</td></tr><tr><td>train_f1</td><td>▄▂▃▃▂▃▂▂▃▅▂▄▃▁▆▃▆▆▅▅▅▆▆▇▇▇▇▆███▇▇▇██▆█▇▇</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▃▃▂▁▂▁▁▁▂▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▂▁▂▂▂▂▄▃▃▄▃▃▆▅▆▇▅▆▄▆█▇█▇▇▇▇█▇▇▆▇▇▇▇█▇</td></tr><tr><td>val_auc</td><td>▂▄██▇▆▃▃▄▅▃▄▂▂▂▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▂▁▁▁▁▁▁▁▁</td></tr><tr><td>val_f1</td><td>▁▂▁▂▁▂▃▃▂▅▄▄▆▃▃▇▅▆█▆▇▄▇███▇▇▇▇██▇▆▇█▇▇█▇</td></tr><tr><td>val_loss_epoch</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_step</td><td>█▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.6968</td></tr><tr><td>train_auc</td><td>0.45381</td></tr><tr><td>train_f1</td><td>0.54891</td></tr><tr><td>train_loss_epoch</td><td>0.58498</td></tr><tr><td>train_loss_step</td><td>0.5466</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.7033</td></tr><tr><td>val_auc</td><td>0.24706</td></tr><tr><td>val_f1</td><td>0.61244</td></tr><tr><td>val_loss_epoch</td><td>0.63196</td></tr><tr><td>val_loss_step</td><td>0.65496</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_32_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/vezghfyh' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/vezghfyh</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240102_044434-vezghfyh\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240102_050247-4dzcs3kn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/4dzcs3kn' target=\"_blank\">MLP_2_32_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/4dzcs3kn' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/4dzcs3kn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\torch_geometric\\deprecation.py:22: UserWarning: 'nn.glob.GlobalAttention' is deprecated, use 'nn.aggr.AttentionalAggregation' instead\n",
      "  warnings.warn(out)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_2_32_onehot_4\\attention\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 1.4 K \n",
      "2  | head        | Sequential       | 562   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 33    \n",
      "--------------------------------------------------\n",
      "2.0 K     Trainable params\n",
      "0         Non-trainable params\n",
      "2.0 K     Total params\n",
      "0.008     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▁▁▁▃▃▆▆▅▆█▇▅▆▆▆▇▇▇▇▇▆▇▇▇▅▇▆▇▇█▇▇██▇▆██</td></tr><tr><td>train_auc</td><td>▁▁▁▂▂▄▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇███▇▇██</td></tr><tr><td>train_f1</td><td>▁▂▁▁▁▃▄▇▆▆▇█▇▆▆▇▇▇▇▇█▇▇▇▇▇▅▇▇▇███▇█▇▇▇▇█</td></tr><tr><td>train_loss_epoch</td><td>█▇▇▇▇▆▄▅▄▅▄▂▃▃▃▄▃▃▃▄▃▃▃▃▂▃▄▃▃▃▃▂▃▂▃▁▂▃▂▂</td></tr><tr><td>train_loss_step</td><td>██▆▆▇▄▄▄▇▅▅▇▄▄▄▅▃▃▆▆▅▄▅▆▇▃▅▄▅▃▄▃▁▃▅▇▄▃▇▄</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▂▄▄▄▄▄▅▇▅▅▄▄▆▆▄▇▆▆▇▇█▇▄▆▆▇▇▇▇▇▇▇▆██</td></tr><tr><td>val_auc</td><td>▁▇██████████████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▂▅▄▄▄▄▅▇▆▅▅▅▇▇▅▇▇▇▇▇█▇▄▇▇▇▇▇▇███▆██</td></tr><tr><td>val_loss_epoch</td><td>▅▅▄▄▄▄▅▁▄▅▃▅▃▃▄▅▄▄▁▅▂▆▃▄▃▂▄▂▃▃█▂▅▂▅▂▃▃▃▂</td></tr><tr><td>val_loss_step</td><td>▅▄▄▄▃▄▅▁▄▅▃▅▃▃▄▅▄▅▂▅▃▆▃▅▄▂▄▂▃▄█▂▅▃▅▃▃▄▄▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.70411</td></tr><tr><td>train_auc</td><td>0.75735</td></tr><tr><td>train_f1</td><td>0.59194</td></tr><tr><td>train_loss_epoch</td><td>0.57963</td></tr><tr><td>train_loss_step</td><td>0.58988</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.67766</td></tr><tr><td>val_auc</td><td>0.73235</td></tr><tr><td>val_f1</td><td>0.49425</td></tr><tr><td>val_loss_epoch</td><td>0.5898</td></tr><tr><td>val_loss_step</td><td>0.56167</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_32_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/4dzcs3kn' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/4dzcs3kn</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240102_050247-4dzcs3kn\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240102_052048-y29h0fym</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/y29h0fym' target=\"_blank\">MLP_2_32_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/y29h0fym' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/y29h0fym</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_2_32_onehot_4\\attention2\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 1.4 K \n",
      "2  | head        | Sequential       | 562   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 1.1 K \n",
      "--------------------------------------------------\n",
      "3.0 K     Trainable params\n",
      "0         Non-trainable params\n",
      "3.0 K     Total params\n",
      "0.012     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▁▁▁▁▄▅▆▆▆▆▆▇▆▆▆▆▆▇▆▇▇▆▆█▆▇▇▆██▇▇▆▆▆▇▆▇</td></tr><tr><td>train_auc</td><td>▁▁▁▂▃▃▅▇▇▇█▇▇█▇▇██████▇██████▇██████████</td></tr><tr><td>train_f1</td><td>▁▁▁▁▁▁▅▆▆▇▆▇▇▇▇▆▆▇▇▇▆▇▇▇▇█▇▇▇▇▇█▇▇▇▇▇▇▆▇</td></tr><tr><td>train_loss_epoch</td><td>██▇█▇▇▆▅▄▅▂▃▃▂▃▄▃▂▃▂▃▃▃▃▂▂▂▂▂▃▂▂▁▂▂▂▂▃▂▂</td></tr><tr><td>train_loss_step</td><td>▆▆▆▇▆▆▆▆▄█▃▃▅▂▂▅▇▅▂▃▅▅▃▂▃▃▄▁▆▃▄▂▁▃▇▂▁▅▃▅</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▄▄▆▇▇▇▇▇█▇▇▇██▇▇▆▇▇▇▇▇▇▇▇▇▆▇▆▇██▇█</td></tr><tr><td>val_auc</td><td>▁▇▇█████████████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▄▄▇▇▇▇▇██▇▇▇██▇▇▆▇▇▇██▇███▇█▇███▇█</td></tr><tr><td>val_loss_epoch</td><td>▆▇▆▆▄▆▅▄▄▃▁▆▅▃▃▃█▂▃▄▄▄▅▃▃▂▅▄▅▄▃▂▅▄▁▆▇▄▆▄</td></tr><tr><td>val_loss_step</td><td>▅▆▅▄▃▅▄▃▄▃▁▆▅▃▃▃█▂▃▄▄▄▅▃▃▂▅▄▅▄▃▂▅▄▁▆▇▄▆▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.69132</td></tr><tr><td>train_auc</td><td>0.7467</td></tr><tr><td>train_f1</td><td>0.54933</td></tr><tr><td>train_loss_epoch</td><td>0.59299</td></tr><tr><td>train_loss_step</td><td>0.63337</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.69963</td></tr><tr><td>val_auc</td><td>0.72309</td></tr><tr><td>val_f1</td><td>0.60952</td></tr><tr><td>val_loss_epoch</td><td>0.63569</td></tr><tr><td>val_loss_step</td><td>0.66791</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_32_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/y29h0fym' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/y29h0fym</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240102_052048-y29h0fym\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240102_053844-wrea0nok</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/wrea0nok' target=\"_blank\">MLP_2_64_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/wrea0nok' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/wrea0nok</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_2_64_onehot_4\\mean\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 4.8 K \n",
      "2 | head        | Sequential       | 2.1 K \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "6.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "6.9 K     Total params\n",
      "0.028     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▃▃▃▅▆▇▇▇▆▇▇█▇▇█▇█▇▇▇▇▇▇██▇▇▇█▇▇▇▇▇▇▇▇██</td></tr><tr><td>train_auc</td><td>▁▂▂▃▅▆▇▇▇▇██▇█▇█████████████████▇██▇████</td></tr><tr><td>train_f1</td><td>▄▂▁▁▅▇█▇▇▇▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▆▇▇▆▇▇█▇▆▇▇▇</td></tr><tr><td>train_loss_epoch</td><td>█▆▇▆▅▄▄▂▂▃▂▃▂▂▁▂▁▁▂▂▂▂▂▂▂▂▁▁▂▃▁▂▂▃▂▂▂▂▂▃</td></tr><tr><td>train_loss_step</td><td>▆▆▆▅▄▅▃▅▂▆▃▄▄▃▅▅▃▄▇▄▅▂▃▂▄▄▄▁▃▃▄▆▃▄▆▅▂▃▄█</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▄▄▅▆▆███▇▇████▇▇▇▇████▇██▇▇▇▇▆█▇▇▇▇▇</td></tr><tr><td>val_auc</td><td>▁▅▆▆▇████▇▆▆▆▆▆▆▆▆▆▆▆▅▆▆▆▆▆▆▅▅▅▅▅▅▅▆▆▆▅▆</td></tr><tr><td>val_f1</td><td>▁▁▁▁▄▄▆▇▇███▇▇█████▇█▇████▇██▇▇▇▇▇██▇▇▇▇</td></tr><tr><td>val_loss_epoch</td><td>▇▆▆▇▅▇▃▄▆▃▂▆▂▃▆▅▃▆▅▄▃▂▃▁▂▄▇▁▅▄▆▄▃█▂▆▄▅▅▅</td></tr><tr><td>val_loss_step</td><td>▆▅▅▆▅▆▃▄▇▃▂▇▂▃▆▅▃▆▅▄▃▂▃▁▂▄█▁▅▄▆▄▃█▂▆▄▅▅▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.70137</td></tr><tr><td>train_auc</td><td>0.74639</td></tr><tr><td>train_f1</td><td>0.56225</td></tr><tr><td>train_loss_epoch</td><td>0.60988</td></tr><tr><td>train_loss_step</td><td>0.75136</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.68498</td></tr><tr><td>val_auc</td><td>0.72168</td></tr><tr><td>val_f1</td><td>0.57426</td></tr><tr><td>val_loss_epoch</td><td>0.65134</td></tr><tr><td>val_loss_step</td><td>0.69497</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_64_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/wrea0nok' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/wrea0nok</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240102_053844-wrea0nok\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240102_055645-rob57lvp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/rob57lvp' target=\"_blank\">MLP_2_64_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/rob57lvp' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/rob57lvp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_2_64_onehot_4\\max\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 4.8 K \n",
      "2 | head        | Sequential       | 2.1 K \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "6.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "6.9 K     Total params\n",
      "0.028     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\torch_geometric\\warnings.py:17: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▃▃▂▄▄▆▅▆▆▆▆▆▆▇▇▇▇▇▇▇▆▇▇█▇▆▇▆▆▇▇▇▇█▇▇▆▇▇</td></tr><tr><td>train_auc</td><td>▁▃▃▂▄▅▆▆▆▇▇▇▇▇▇▇▇▇▇▇█▇██▇▇▇█▇▇▇▇▇▇██▇▇▇█</td></tr><tr><td>train_f1</td><td>▄▄▁▁▄▅▆▅▆▆▆▆█▇▆▇▇█▇▇█▇▆▇█▆▆▇▆▆▇▇▇▇██▇▆▇▇</td></tr><tr><td>train_loss_epoch</td><td>█▅▅▅▄▄▃▃▂▃▂▂▃▂▁▂▂▂▂▂▁▂▂▂▂▂▂▁▂▂▁▁▂▂▁▂▂▂▁▂</td></tr><tr><td>train_loss_step</td><td>▇█▇▇▅▅▅▆▂▆▄▆▅▃█▅▄▅█▆▅▆▄▁▅▆▅▂▄▅█▄▅▄▅▆▃▆▂█</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>▇▇▇▇▇█▇▇███████▇▇▇▇██▆▆▇▇▆▆▆▆▆▆▆▆▆▆▆▆▆▁▁</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_epoch</td><td>▁▁▁▁▁▂▂▂▂▃▃▂▄▄▄▅▇▅▆▆▇▆▅▆▇▅▆▆▆▅▆▅▆▄█▅▄▆▅▆</td></tr><tr><td>val_loss_step</td><td>▁▁▁▁▁▂▂▁▁▃▃▁▄▄▃▅▇▃▄▄▆▆▅▅▇▄▅▅▄▄▅▃▆▁█▃▂▆▃▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.68858</td></tr><tr><td>train_auc</td><td>0.73762</td></tr><tr><td>train_f1</td><td>0.55886</td></tr><tr><td>train_loss_epoch</td><td>0.61193</td></tr><tr><td>train_loss_step</td><td>0.69686</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.43956</td></tr><tr><td>val_auc</td><td>0.41517</td></tr><tr><td>val_f1</td><td>0.61069</td></tr><tr><td>val_loss_epoch</td><td>1.39106</td></tr><tr><td>val_loss_step</td><td>1.27584</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_64_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/rob57lvp' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/rob57lvp</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240102_055645-rob57lvp\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240102_061424-ew65b1ru</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/ew65b1ru' target=\"_blank\">MLP_2_64_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/ew65b1ru' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/ew65b1ru</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_2_64_onehot_4\\sum\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 4.8 K \n",
      "2 | head        | Sequential       | 2.1 K \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "6.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "6.9 K     Total params\n",
      "0.028     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▃▂▄▅▆▆▇▇▇▇▇▇▇█▇▇▇█▇▇▇▇▇▇█▇▇█▇▆▇█▆▇▆█▇█</td></tr><tr><td>train_auc</td><td>▁▂▄▂▂▂▃▃▃▅▄▅▅▅▆▅▆█▇▇▇▆▆▆▆▇█▇▆▆▅▆▄▆▄▅▇▆▆█</td></tr><tr><td>train_f1</td><td>▁▅▄▃▅▆▆▃▇▇▆▆▇▇▆▇█▇▆█▇█▆█▇▆█▇▇▇▅▄▆▇▇█▆▇▇▇</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▄▃▂▂▂▁▂▂▃▁▂▂▁▂▂▁▁▂▂▂▁▁▁▂▂▂▁▂▁▁▂▁▂▁▂▁▂▂▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▅▄▅▆██▇▇███▇▇▇███▇██████▇███▇▇███▇▇████</td></tr><tr><td>val_auc</td><td>▇▇▇▄▁▄▇▆▆▇▇█▇▇▇▇████▆▇▇▇▇▇▇▇▇▇▅▇▆▇▇▇████</td></tr><tr><td>val_f1</td><td>▇▂▁▃▅▇█▇▆███▇▇████████████████████▇▇████</td></tr><tr><td>val_loss_epoch</td><td>█▃▂▃▂▂▂▂▂▁▁▂▁▁▂▂▁▂▂▂▁▁▂▁▁▂▂▁▂▂▃▂▂▂▁▃▁▂▂▂</td></tr><tr><td>val_loss_step</td><td>█▄▂▅▃▃▃▃▅▂▂▄▂▂▄▃▂▃▃▃▂▁▂▁▂▃▅▂▃▃▅▂▃▄▂▅▂▃▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.69863</td></tr><tr><td>train_auc</td><td>0.67548</td></tr><tr><td>train_f1</td><td>0.58853</td></tr><tr><td>train_loss_epoch</td><td>0.61468</td></tr><tr><td>train_loss_step</td><td>0.77101</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.72527</td></tr><tr><td>val_auc</td><td>0.76002</td></tr><tr><td>val_f1</td><td>0.67811</td></tr><tr><td>val_loss_epoch</td><td>0.59591</td></tr><tr><td>val_loss_step</td><td>0.58133</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_64_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/ew65b1ru' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/ew65b1ru</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240102_061424-ew65b1ru\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240102_063300-jipe8a9e</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/jipe8a9e' target=\"_blank\">MLP_2_64_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/jipe8a9e' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/jipe8a9e</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\torch_geometric\\deprecation.py:22: UserWarning: 'nn.glob.GlobalAttention' is deprecated, use 'nn.aggr.AttentionalAggregation' instead\n",
      "  warnings.warn(out)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_2_64_onehot_4\\attention\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 4.8 K \n",
      "2  | head        | Sequential       | 2.1 K \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 65    \n",
      "--------------------------------------------------\n",
      "7.0 K     Trainable params\n",
      "0         Non-trainable params\n",
      "7.0 K     Total params\n",
      "0.028     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▃▄▅▇▇▇▇▇▇█▇▇▇▇▇▇▇██▇▇▇████▇▇▇██▇▇▇███▇</td></tr><tr><td>train_auc</td><td>▁▃▂▃▅▆▇▇▇▇▇█▇▇▇▇▇▇▇███████████▇█████████</td></tr><tr><td>train_f1</td><td>▅▃▁▂▅▆▆▇▇█▇▇▇▇▆▇▇██▇▇▇▇▇▇▇██▇▆▆▇███▇██▇█</td></tr><tr><td>train_loss_epoch</td><td>█▇▇▆▆▅▃▃▃▃▃▂▃▂▃▃▃▂▄▃▂▂▃▂▂▂▂▃▂▃▂▁▃▃▃▃▂▁▂▂</td></tr><tr><td>train_loss_step</td><td>█▇▇▄▇▃▃▃█▅▇▅▄▇▅█▆▄▃▄▃▇▂▂▁▅▃▅▄▅▃█▆▆▇▇▃▄▄▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▂▅▄█▄▄▄▅▆▆▇▇▇█▅▆████▇▇█▆▆██▇█▇▅▇███▇▇</td></tr><tr><td>val_auc</td><td>▁▃▄▅▅██████▇▇▇▇▆▇▇▇▇▇▇▇█▇▇███▇██▇▇▇▇██▇█</td></tr><tr><td>val_f1</td><td>▁▁▁▂▅▄▇▄▄▄▅▆▆▇█▇▇▅▆███████▆▆▇█▇▇▇▆▇█▇▇▇▇</td></tr><tr><td>val_loss_epoch</td><td>▆▇█▅▇▇▆▃▄▅▆█▃▁▄▅▄▆▇▃▆▄▂▂▂▂▅▄▃▄▃▄▂▆▃▁▃▃▆▂</td></tr><tr><td>val_loss_step</td><td>▄▆▇▄▇▇▆▂▄▄▆█▃▁▄▆▄▅▇▃▆▄▂▂▂▂▄▄▄▄▃▅▂▆▃▁▃▃▇▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.69863</td></tr><tr><td>train_auc</td><td>0.75533</td></tr><tr><td>train_f1</td><td>0.62838</td></tr><tr><td>train_loss_epoch</td><td>0.58049</td></tr><tr><td>train_loss_step</td><td>0.55952</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.66667</td></tr><tr><td>val_auc</td><td>0.74069</td></tr><tr><td>val_f1</td><td>0.49162</td></tr><tr><td>val_loss_epoch</td><td>0.56313</td></tr><tr><td>val_loss_step</td><td>0.50126</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_64_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/jipe8a9e' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/jipe8a9e</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240102_063300-jipe8a9e\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240102_065040-8eoj97xd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/8eoj97xd' target=\"_blank\">MLP_2_64_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/8eoj97xd' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/8eoj97xd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_2_64_onehot_4\\attention2\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 4.8 K \n",
      "2  | head        | Sequential       | 2.1 K \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 4.2 K \n",
      "--------------------------------------------------\n",
      "11.1 K    Trainable params\n",
      "0         Non-trainable params\n",
      "11.1 K    Total params\n",
      "0.045     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▃▄▆▇▅▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇███▇██▇██▇█▇▇████</td></tr><tr><td>train_auc</td><td>▁▂▂▄▆▇▆▇▇▇▇▇▇▇▇▇█▇█▇▇▇████████████▇█████</td></tr><tr><td>train_f1</td><td>▅▃▁▃▆▆▅▅▇█▇▇▇█▇▇▇▇▇▆▇▇█▇██▇████▇▇▇▇▆▇███</td></tr><tr><td>train_loss_epoch</td><td>█▇▇▇▄▃▅▄▃▄▃▃▃▃▂▂▄▁▃▄▃▃▂▂▂▂▂▂▂▃▃▃▂▂▂▃▁▃▂▂</td></tr><tr><td>train_loss_step</td><td>▆▆▆▅█▃▄▄▇▅▃▅▆▁▄▁▃▄▃▄▃▅▂▅▃▆▂▅█▃▃▃▂▅▃▂▆▄▃▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▂▅▇█▇▆▇██▇▆▇▇▇▇▆▇▇█▇▇▇▇▇▇▇▇▇█▇▆▇▇▇▇▇▇</td></tr><tr><td>val_auc</td><td>▁▄▅▆▅████▇▇▇▇▇▇▇▆▆▆▇▇▇▆▅▆▆▆▆▆▇▆▆▆▆▆▆▇▆▄▅</td></tr><tr><td>val_f1</td><td>▁▁▁▂▅▇█▆▆▇▇▇▇▇▇▇▇▇████▇▇▇▇▇▇▇▇▇███▇▇▇▇▇▇</td></tr><tr><td>val_loss_epoch</td><td>█▆▇▅▃▃▄▄▂▄▄█▅▃▄▂▄▅▄▃▅▃▅▄▃▃▆▅▂▄▂▁▄▆▄▁▅▃▂▂</td></tr><tr><td>val_loss_step</td><td>▇▅▆▄▂▃▅▄▂▄▄█▅▃▅▂▄▅▄▃▅▃▆▄▄▃▆▆▃▅▂▁▅▆▄▁▅▃▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.71233</td></tr><tr><td>train_auc</td><td>0.75117</td></tr><tr><td>train_f1</td><td>0.64326</td></tr><tr><td>train_loss_epoch</td><td>0.58081</td></tr><tr><td>train_loss_step</td><td>0.54714</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.68132</td></tr><tr><td>val_auc</td><td>0.7244</td></tr><tr><td>val_f1</td><td>0.54922</td></tr><tr><td>val_loss_epoch</td><td>0.56129</td></tr><tr><td>val_loss_step</td><td>0.49719</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_64_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/8eoj97xd' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/8eoj97xd</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240102_065040-8eoj97xd\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240102_070822-7plurzmw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/7plurzmw' target=\"_blank\">MLP_3_16_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/7plurzmw' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/7plurzmw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_3_16_onehot_4\\mean\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 704   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "858       Trainable params\n",
      "0         Non-trainable params\n",
      "858       Total params\n",
      "0.003     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▂▂▂▂▂▂▂▄▅▆▆▆▇▇▇▆▇▇▆▇▇▇▇▇▇▇▇█▇█▇▇▇▇██▆▇</td></tr><tr><td>train_auc</td><td>▂▁▂▁▁▂▃▂▃▅▆▆▇▇█▇█▇█▇█▇████████████████▇█</td></tr><tr><td>train_f1</td><td>▂▁▁▁▁▁▁▁▁▄▆▇▆▆▇██▇█▇▇██▇▇█▇▇████▇▇▇███▇▇</td></tr><tr><td>train_loss_epoch</td><td>█▇▇██▇▇▇▇▆▅▄▃▂▂▄▃▃▂▂▂▃▂▂▂▁▃▃▂▂▃▁▃▃▃▂▂▁▃▂</td></tr><tr><td>train_loss_step</td><td>▇▆▇▆▆▇▇█▇▇▇▅▅▃▂▆▂▃▃▅▁▄▁▆▄▅▃▃▂▅▂▃▄▂▅▃▃▄▆▄</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▂▆▇▆▇▅▇▇▇▇▇▇██▇▅▇▇█▇▇▇▇█▆▆▇▇▇▇▇</td></tr><tr><td>val_auc</td><td>▁▅▇▇▇▇██████████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▂▆█▇▇▆▇█▇▇▇▇██▇▆▇▇█▇▇███▇▆█▇▇█▇</td></tr><tr><td>val_loss_epoch</td><td>▇▇▇▇█▆▇▆▆▆▅▆▃█▇▅▃▁▄▂▂▃▄▃▂▂█▁▅▁▅▃▃▅▆▃▄▅▃▄</td></tr><tr><td>val_loss_step</td><td>▅▆▅▅▇▅▆▅▅▅▅▆▃█▇▅▃▁▄▂▂▃▅▄▂▂█▁▅▁▅▃▃▅▆▃▄▅▃▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.67854</td></tr><tr><td>train_auc</td><td>0.7301</td></tr><tr><td>train_f1</td><td>0.48083</td></tr><tr><td>train_loss_epoch</td><td>0.59963</td></tr><tr><td>train_loss_step</td><td>0.60381</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.67766</td></tr><tr><td>val_auc</td><td>0.72151</td></tr><tr><td>val_f1</td><td>0.56</td></tr><tr><td>val_loss_epoch</td><td>0.62107</td></tr><tr><td>val_loss_step</td><td>0.63268</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_16_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/7plurzmw' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/7plurzmw</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240102_070822-7plurzmw\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240102_072628-6iz451n2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/6iz451n2' target=\"_blank\">MLP_3_16_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/6iz451n2' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/6iz451n2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_3_16_onehot_4\\max\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 704   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "858       Trainable params\n",
      "0         Non-trainable params\n",
      "858       Total params\n",
      "0.003     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\torch_geometric\\warnings.py:17: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▄▅▄▄▅▄▅▅▅▅▅▅▅▅▆▆▅▇▆▆▅▆▅▆▆▅▆█▇▆█▆█▇▇▆▆█</td></tr><tr><td>train_auc</td><td>▁▁▂▂▁▃▄▃▃▃▅▃▅▄▅▅▆▄▅▆▆▅▆▆▄▆▇▆██▇▆▆▇▇▆▇▇▆▇</td></tr><tr><td>train_f1</td><td>▆▅▁▁▁▂▂▂▃▃▅▄▅▄▄▅▆▅▅▇▆▆▆▆▆▅▆▅▇▇▇▆▇▇█▇▇▇▅█</td></tr><tr><td>train_loss_epoch</td><td>█▄▄▄▄▃▃▃▄▃▃▃▃▃▃▃▃▂▃▂▂▂▃▂▂▂▁▃▂▁▂▂▁▂▁▂▂▂▃▁</td></tr><tr><td>train_loss_step</td><td>▅▅▇▄▅▅▅▇▅▄▅▆▄▂▂▅▂▃▃▄▂▃▂▅▅▄█▂▄▅▁▄▄▄▁▂▄▅▅▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>▁▃▁▃▃▁▂▁▁▂▃▃▃▄▇▇▆▇███▆▇▇▆▄▄▆▄▆▆▅▄▆▆▅█▇▇▇</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_epoch</td><td>▅▆▅▅█▄▆▄▃▆▄▅▃▇▇▅▅▃▄▂▃▅▃▅▃▁▆▆▃▃▅▄▂▄▅▄▄▃▅▃</td></tr><tr><td>val_loss_step</td><td>▅▆▄▄█▄▅▄▃▆▄▅▃▇▇▅▅▃▄▂▄▅▃▅▃▁▆▇▃▃▅▄▂▅▅▅▄▃▅▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.64384</td></tr><tr><td>train_auc</td><td>0.6369</td></tr><tr><td>train_f1</td><td>0.42308</td></tr><tr><td>train_loss_epoch</td><td>0.64012</td></tr><tr><td>train_loss_step</td><td>0.62701</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.56044</td></tr><tr><td>val_auc</td><td>0.62034</td></tr><tr><td>val_f1</td><td>0.0</td></tr><tr><td>val_loss_epoch</td><td>0.66784</td></tr><tr><td>val_loss_step</td><td>0.658</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_16_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/6iz451n2' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/6iz451n2</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240102_072628-6iz451n2\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240102_074404-rtm2zfe7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/rtm2zfe7' target=\"_blank\">MLP_3_16_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/rtm2zfe7' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/rtm2zfe7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_3_16_onehot_4\\sum\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 704   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "858       Trainable params\n",
      "0         Non-trainable params\n",
      "858       Total params\n",
      "0.003     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▁▁▃▂▂▂▃▃▃▃▂▃▃▃▄▄▄▅▅▅▆▅▅▅▅▅▅▆█▆▆▇▇▇▆█▇▇</td></tr><tr><td>train_auc</td><td>▅▅▆▆▄▃▄▄▂▄▄▄▄▂▄▃▄▄▄▆▇▅▅▄▃▂▁▃▂▃▅▃▄▄▅▃▄▂▆█</td></tr><tr><td>train_f1</td><td>▅▄▆▅▃▆▅▆▅▅▅▃▄▂▂▃▂▄▁▅▅▃▂▃▃▁▅▄▅▅▆▃▆▅▅█▄▇▇▆</td></tr><tr><td>train_loss_epoch</td><td>█▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>██▃▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▅▁▁▅▁▁▁▅▁▁▅▁█▁▁▅▁▁▁▁▁▁▅</td></tr><tr><td>val_auc</td><td>▇▇▇▄▂▂▂▂▂▂▁▂▁▁▁▁▁▂▂▁▁▂▂▁▂▁▁▁▁▂▃▄▅▃▁▁▆▇▇█</td></tr><tr><td>val_f1</td><td>▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▃▁▁▃▁▁▁▃▁▁▃▁▆▁▁▃▁▁▁▁▁▁▃</td></tr><tr><td>val_loss_epoch</td><td>█▇▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_step</td><td>██▂▁▃▂▂▁▁▁▂▂▁▂▂▂▂▁▂▁▁▂▂▂▁▁▂▂▁▁▂▂▁▁▂▂▁▁▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.6347</td></tr><tr><td>train_auc</td><td>0.55603</td></tr><tr><td>train_f1</td><td>0.43503</td></tr><tr><td>train_loss_epoch</td><td>0.64324</td></tr><tr><td>train_loss_step</td><td>0.60981</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.5641</td></tr><tr><td>val_auc</td><td>0.73012</td></tr><tr><td>val_f1</td><td>0.01653</td></tr><tr><td>val_loss_epoch</td><td>0.78285</td></tr><tr><td>val_loss_step</td><td>0.83541</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_16_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/rtm2zfe7' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/rtm2zfe7</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240102_074404-rtm2zfe7\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240102_080223-oeyrgs19</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/oeyrgs19' target=\"_blank\">MLP_3_16_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/oeyrgs19' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/oeyrgs19</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\torch_geometric\\deprecation.py:22: UserWarning: 'nn.glob.GlobalAttention' is deprecated, use 'nn.aggr.AttentionalAggregation' instead\n",
      "  warnings.warn(out)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_3_16_onehot_4\\attention\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 704   \n",
      "2  | head        | Sequential       | 154   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 17    \n",
      "--------------------------------------------------\n",
      "875       Trainable params\n",
      "0         Non-trainable params\n",
      "875       Total params\n",
      "0.003     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▂▂▂▂▂▂▂▃▄▃▄▅▆▆▆▆▆▆▇▆▆▆▇▇▆▆█▇▇▇█▆▆▆▆▇▇▆</td></tr><tr><td>train_auc</td><td>▂▁▂▂▂▁▂▁▂▄▆▅▆▇▇█▇▇▇▇██▇▇▇████████▇▇█████</td></tr><tr><td>train_f1</td><td>▄▁▁▁▁▁▁▁▁▂▆▃▄▆▆▆▆▇▇▇▆▆█▆▆▇▇▆▇█▇▆██▆▆▇▇▇▇</td></tr><tr><td>train_loss_epoch</td><td>▇█▆▇▇▇▇▇▆▆▅▅▅▄▃▃▄▃▃▃▃▃▄▃▃▂▂▂▂▂▂▁▂▄▂▂▁▂▂▂</td></tr><tr><td>train_loss_step</td><td>▆▆█▆▆▇▅▆▆▆▇▄▆▆▅▄▆▃▁▄▄▇▅▃▃▃▅▄▂▁▃█▆▂▄▂▃▃▅▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▂▄▅▄▃▄▆▆▆▄▆▇▆▆▆▇▅▇▇▆▆▇▆▇█▇█</td></tr><tr><td>val_auc</td><td>▁▄▅▆▇▇██████████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▂▁▃▄▅▄▃▅▆▆▆▅▇▇▆▇▇▇▆▇▇▇▇█▇██▇█</td></tr><tr><td>val_loss_epoch</td><td>▆▆▇▇▇▇▆▇▆▅▆▇▅▅▅▅█▆▄▄▅▄▅▅▆▄▁▂▄▅▄▅▂▃▅▅▄▄▁▅</td></tr><tr><td>val_loss_step</td><td>▅▅▆▆▆▆▅▆▅▄▆▆▄▅▅▅█▆▄▄▅▄▅▅▇▄▁▂▄▅▄▅▂▃▅▅▄▄▁▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.66575</td></tr><tr><td>train_auc</td><td>0.71584</td></tr><tr><td>train_f1</td><td>0.48305</td></tr><tr><td>train_loss_epoch</td><td>0.59991</td></tr><tr><td>train_loss_step</td><td>0.58304</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.7033</td></tr><tr><td>val_auc</td><td>0.72669</td></tr><tr><td>val_f1</td><td>0.61972</td></tr><tr><td>val_loss_epoch</td><td>0.63294</td></tr><tr><td>val_loss_step</td><td>0.66163</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_16_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/oeyrgs19' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/oeyrgs19</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240102_080223-oeyrgs19\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240102_082019-3y5jdm6z</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/3y5jdm6z' target=\"_blank\">MLP_3_16_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/3y5jdm6z' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/3y5jdm6z</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_3_16_onehot_4\\attention2\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 704   \n",
      "2  | head        | Sequential       | 154   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 281   \n",
      "--------------------------------------------------\n",
      "1.1 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.1 K     Total params\n",
      "0.005     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▂▂▂▂▂▂▃▃▅▆▅▆▆▅▇▇▅▇▇▇▇▇▇▆▇▇▆█▇▇▇▇▇▆▆██▇</td></tr><tr><td>train_auc</td><td>▁▂▁▁▂▁▂▂▃▆▇▆▇▇▇▇▇▇▇▇▇▇██▇▇▇▇▇█▇▇▇██▇▇▇▇▇</td></tr><tr><td>train_f1</td><td>▃▁▁▁▁▁▁▁▃▄▆▆▆█▇▅▇█▆▇▇▇▇▇▇▇▇▇▆█▇▇▇██▇▆█▇▇</td></tr><tr><td>train_loss_epoch</td><td>█▇███▇▇▇▆▅▅▄▃▄▃▃▂▄▄▃▃▂▁▂▂▂▂▃▃▂▃▂▃▁▂▁▃▂▂▁</td></tr><tr><td>train_loss_step</td><td>▆▇▆▆▆▆▅▆▆▆▃▃▆▄▅▃▇▄▅▅▆▆▄▄▆▅▃▁▄▃▂▂▄▅▅▄▃▄█▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▃▂▄▅▄▇▇▇▇▆▇▇▇▇▇▇█▇█▆▇▇▇▇▇▇▇▅██▇</td></tr><tr><td>val_auc</td><td>▁▅▆▆▇▇██████████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▂▃▃▅▅▅█▇▇█▇▇█▇██████▆▇▇▇▇███▅███</td></tr><tr><td>val_loss_epoch</td><td>▇███▇▆▅▆▆▅▆▆▅▁▄▄▅▃▄▅▄▄▅▃▃▄▄▂▃▃▅▃▂▃▃▅▅▄▂▄</td></tr><tr><td>val_loss_step</td><td>▇█▇█▇▆▅▆▆▅▆▇▅▁▆▆▇▄▆▆▅▅▆▄▄▅▅▃▄▃▆▄▃▄▅▆▆▅▃▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.67397</td></tr><tr><td>train_auc</td><td>0.71993</td></tr><tr><td>train_f1</td><td>0.47577</td></tr><tr><td>train_loss_epoch</td><td>0.59503</td></tr><tr><td>train_loss_step</td><td>0.52395</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.68864</td></tr><tr><td>val_auc</td><td>0.72756</td></tr><tr><td>val_f1</td><td>0.57286</td></tr><tr><td>val_loss_epoch</td><td>0.6049</td></tr><tr><td>val_loss_step</td><td>0.60195</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_16_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/3y5jdm6z' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/3y5jdm6z</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240102_082019-3y5jdm6z\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240102_083825-nhnqkb9p</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/nhnqkb9p' target=\"_blank\">MLP_3_32_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/nhnqkb9p' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/nhnqkb9p</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_3_32_onehot_4\\mean\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 2.4 K \n",
      "2 | head        | Sequential       | 562   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "3.0 K     Trainable params\n",
      "0         Non-trainable params\n",
      "3.0 K     Total params\n",
      "0.012     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▁▁▁▁▁▁▄▆▆▆▆▇▆▇▇▇▇▆▅▇▆██▆█▇▆▇█▇▇██▇▇▇▇▇</td></tr><tr><td>train_auc</td><td>▂▁▁▂▂▂▂▁▃▆▆▆▆▆▆▇▇▇▇▆▇▆▆▇▇▇█▇▇▇▇▇▇▇█▇▇███</td></tr><tr><td>train_f1</td><td>▃▁▁▁▁▁▁▁▅▇▆▇▇▇▇▇█▇█▇▆▇▇█▇███▇▇█████▇▇▇▇█</td></tr><tr><td>train_loss_epoch</td><td>█▇▇▇▇▇▇▆▆▄▄▂▄▃▃▃▃▃▃▃▃▂▂▂▁▂▂▂▄▂▂▂▃▁▂▃▂▂▂▂</td></tr><tr><td>train_loss_step</td><td>█▇▆▆▇▇▆▆▅▅▅▄▅▁▅▅▄▇▄▁▅▄▃▄▆▃▄▂▄▄▇█▅▅▇▆▄▂▆▅</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▃▆▇▇█▇▇█▇█▇▇█▇██▇█▇██▇█▇█▇█▆▇▇██</td></tr><tr><td>val_auc</td><td>▆▁▁▁▁▁▁█████████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▄▇█▇█▇██▇████▇███████▇█▇███▇▇███</td></tr><tr><td>val_loss_epoch</td><td>▆█▇▇▇▇▇█▅▆▄▄▄▆▅▄▆▅▅▄▅▄▄▅▅▂▅▆▄▇▅▅▄▅▃▄▄▁▆▅</td></tr><tr><td>val_loss_step</td><td>▆█▇▇▇▇▇█▅█▅▅▅█▆▅█▆▆▅▆▄▅▆▆▃▅▇▅█▆▅▅▆▃▅▅▁▆▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.69954</td></tr><tr><td>train_auc</td><td>0.71979</td></tr><tr><td>train_f1</td><td>0.57875</td></tr><tr><td>train_loss_epoch</td><td>0.59578</td></tr><tr><td>train_loss_step</td><td>0.63337</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.69597</td></tr><tr><td>val_auc</td><td>0.72239</td></tr><tr><td>val_f1</td><td>0.59512</td></tr><tr><td>val_loss_epoch</td><td>0.61255</td></tr><tr><td>val_loss_step</td><td>0.61611</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_32_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/nhnqkb9p' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/nhnqkb9p</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240102_083825-nhnqkb9p\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240102_085701-hjelgg9u</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/hjelgg9u' target=\"_blank\">MLP_3_32_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/hjelgg9u' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/hjelgg9u</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_3_32_onehot_4\\max\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 2.4 K \n",
      "2 | head        | Sequential       | 562   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "3.0 K     Trainable params\n",
      "0         Non-trainable params\n",
      "3.0 K     Total params\n",
      "0.012     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\torch_geometric\\warnings.py:17: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▃▃▃▄▃▃▃▅▅▆▄▆▅▅▆▅▅▆▅▅▅█▇▇▇▆█▅▆▃▆▆▄▆▇▇▆▇▇</td></tr><tr><td>train_auc</td><td>▂▁▃▂▆▄▅▅▄▄▄▄▄▅▃▆▂▄▄▁▇▅█▇▅▇▆▄▄▄▄▃▄▅▇█▆█▅▆</td></tr><tr><td>train_f1</td><td>▅▂▁▁▂▂▂▃▆▄▇▅▅▅▅▅▅▄▄▄▄▆▆▇▇▇▇█▅▇▅▆▆▆▆▆█▅▇█</td></tr><tr><td>train_loss_epoch</td><td>█▅▅▅▄▄▃▃▄▄▃▃▄▂▄▂▂▂▃▃▁▁▂▁▁▁▄▃▄▃▂▂▁▂▃▄▂▂▂▃</td></tr><tr><td>train_loss_step</td><td>▅▅▄▄▅▆▅▅▄▅▃▃▃▂▅█▇▇▇▃▅▃▂▂▅▅▇▅▃▅▇▅▄▅▅▅▁▄▃▆</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>████▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>▄▄▁▃▅▇▄▇▇█▅▄▅█▆▇▇█▆█▅▄▆▃▆▂▁▅▄▃▃▄▆▅▃▃▂▂▄▁</td></tr><tr><td>val_f1</td><td>▁▁▁▁████████████████████████████████████</td></tr><tr><td>val_loss_epoch</td><td>▁▂▂▂▂▂▂▂▃▂▂▃▂▃▃▂▂▃▂▃▂▄▄▅▄▆▅▃▃▃▄▄▄▄▃▅▇█▇▅</td></tr><tr><td>val_loss_step</td><td>▁▂▂▂▂▂▂▂▃▂▂▂▂▂▂▁▂▃▂▃▂▃▃▄▃▅▄▂▃▃▃▄▃▃▂▄▇█▆▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.61826</td></tr><tr><td>train_auc</td><td>0.54015</td></tr><tr><td>train_f1</td><td>0.44855</td></tr><tr><td>train_loss_epoch</td><td>0.66295</td></tr><tr><td>train_loss_step</td><td>0.68573</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.43956</td></tr><tr><td>val_auc</td><td>0.39208</td></tr><tr><td>val_f1</td><td>0.61069</td></tr><tr><td>val_loss_epoch</td><td>0.79234</td></tr><tr><td>val_loss_step</td><td>0.78214</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_32_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/hjelgg9u' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/hjelgg9u</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240102_085701-hjelgg9u\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240102_091601-9j4gbcqg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/9j4gbcqg' target=\"_blank\">MLP_3_32_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/9j4gbcqg' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/9j4gbcqg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_3_32_onehot_4\\sum\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 2.4 K \n",
      "2 | head        | Sequential       | 562   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "3.0 K     Trainable params\n",
      "0         Non-trainable params\n",
      "3.0 K     Total params\n",
      "0.012     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▂▂▁▂▄▃▃▄▄▄▅▄▅▅▅▅▆▆▄▇▇▇▇█▇▇▇▅▇▆▇▇▇▇█▇▇▇██</td></tr><tr><td>train_auc</td><td>█▇▄█▆▆▆▅▆▅▅▅▅▄▆▆▆▅▂▄▄▇▃▂▃▄▂▂▂▃▁▂▂▂▄▂▂▂▂▂</td></tr><tr><td>train_f1</td><td>▄▃▄▃▃▃▂▂▄▁▃▃▂▅▆▃▅▆▇▆▅▇▇▇▇▇▇█▇▆▇████▇█▇▇█</td></tr><tr><td>train_loss_epoch</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▃▁▁▂▂▂▂▂▁▂▁▂▁▁▂▁▂▁▁▂▁▁▁▂▁▂▁▁▁▁▂▂▂▂▂▁▁▁▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▄▄▄▃▄▄▁▄█▄▄█▆▅▇▅█▇███▇█▇▆████▇███▇██████</td></tr><tr><td>val_auc</td><td>█▂▇▇▂▂▂▂▁▁▁▁▁▁▃▃▅▁▁▂▃▃▁▂▃▃▁▁▁▂▁▁▁▂▁▁▁▁▂▂</td></tr><tr><td>val_f1</td><td>▁▁▁▇▁▇▇▁█▁▂██▄███▇███▇████▇▇████▇▇██▇███</td></tr><tr><td>val_loss_epoch</td><td>▄▇▆█▇██▇▇▇▆▇▇▇▆▆▇▅▆▅▅▃▅▆▆▃▄▆▄█▄▄▄▄▃▄▄▁▆▇</td></tr><tr><td>val_loss_step</td><td>▃▆▅▆▆▇▇▆▆▆▆▆▆▇▅▅▇▅▆▄▅▃▅▅▅▃▄▇▄█▅▄▅▄▃▄▅▁▆▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.69589</td></tr><tr><td>train_auc</td><td>0.3692</td></tr><tr><td>train_f1</td><td>0.60024</td></tr><tr><td>train_loss_epoch</td><td>0.61766</td></tr><tr><td>train_loss_step</td><td>0.69203</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.72527</td></tr><tr><td>val_auc</td><td>0.27631</td></tr><tr><td>val_f1</td><td>0.67811</td></tr><tr><td>val_loss_epoch</td><td>0.66825</td></tr><tr><td>val_loss_step</td><td>0.72568</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_32_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/9j4gbcqg' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/9j4gbcqg</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240102_091601-9j4gbcqg\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240102_093453-gt9tqd6q</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/gt9tqd6q' target=\"_blank\">MLP_3_32_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/gt9tqd6q' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/gt9tqd6q</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\torch_geometric\\deprecation.py:22: UserWarning: 'nn.glob.GlobalAttention' is deprecated, use 'nn.aggr.AttentionalAggregation' instead\n",
      "  warnings.warn(out)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_3_32_onehot_4\\attention\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 2.4 K \n",
      "2  | head        | Sequential       | 562   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 33    \n",
      "--------------------------------------------------\n",
      "3.0 K     Trainable params\n",
      "0         Non-trainable params\n",
      "3.0 K     Total params\n",
      "0.012     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▃▃▃▃▃▃▄▇▅▅▇▇▇▆▆█▇▇██▇▇▇▇▇▆█▆▇▇▇█▇▇▆▆▇▇▇</td></tr><tr><td>train_auc</td><td>▂▄▁▂▂▁▁▃▇▆▄▆▇▇▇▆▆▇▇▇█▇▇█▇█▇▇▇▇███▇█▇▇███</td></tr><tr><td>train_f1</td><td>▃▁▁▁▁▁▁▃▇▇▅▇▇▇▆▆▇▇▇██▇▇█▇▇▆▇▇▆▇██▇▇▅▇▇▇▇</td></tr><tr><td>train_loss_epoch</td><td>█▇█▇███▆▃▄▄▃▂▄▃▄▃▃▂▂▃▃▃▂▂▂▂▃▃▂▁▂▂▄▂▄▂▁▃▁</td></tr><tr><td>train_loss_step</td><td>▆▅▆▆▇▆▆▅▇▅▄▃▆▅▅▆▄▄▄▃▅▅▆▄▅▆▅▇▄▅▄▅▅▅▄▅▃█▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▆▅▅▄▆▆▅▃▄▄▄▄▄▄▄▆▅▄▄▆▅▆▆▄▅▅▇▅▄▅▆█</td></tr><tr><td>val_auc</td><td>▂▁▅▆▆▇▇█████████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▇▆▅▅▆▆▅▄▄▅▅▅▅▅▅▆▆▅▅▇▅▆▆▅▅▅▇▅▅▆▆█</td></tr><tr><td>val_loss_epoch</td><td>▇▇▇█▇▇▆▅▅▅▅▃▄▅▅▃▅▅▅▁▅▆▆█▃▅▅▄▄█▃██▇▃▆▅▇▄▄</td></tr><tr><td>val_loss_step</td><td>▆▇▆▇▆▇▆▄▅▅▅▃▄▅▅▃▄▅▅▁▅▆▆█▃▅▅▅▄█▃▇▇▇▄▆▅▇▄▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.68676</td></tr><tr><td>train_auc</td><td>0.7198</td></tr><tr><td>train_f1</td><td>0.52028</td></tr><tr><td>train_loss_epoch</td><td>0.58714</td></tr><tr><td>train_loss_step</td><td>0.50323</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.663</td></tr><tr><td>val_auc</td><td>0.73415</td></tr><tr><td>val_f1</td><td>0.45882</td></tr><tr><td>val_loss_epoch</td><td>0.58773</td></tr><tr><td>val_loss_step</td><td>0.53866</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_32_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/gt9tqd6q' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/gt9tqd6q</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240102_093453-gt9tqd6q\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240102_095345-y4clhg8t</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/y4clhg8t' target=\"_blank\">MLP_3_32_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/y4clhg8t' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/y4clhg8t</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_3_32_onehot_4\\attention2\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 2.4 K \n",
      "2  | head        | Sequential       | 562   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 1.1 K \n",
      "--------------------------------------------------\n",
      "4.1 K     Trainable params\n",
      "0         Non-trainable params\n",
      "4.1 K     Total params\n",
      "0.016     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▂▂▂▂▂▂▂▅▆▅▆▆▇▆▆▆█▇▆▆▆▇▆▆▇▇▇▇▇▇▇▇▇█▇▇▇▆</td></tr><tr><td>train_auc</td><td>▂▁▂▂▂▂▁▁▃▆▆▅▆▇▆▇▇▇▇▇▇▇▇██▇▇█████████████</td></tr><tr><td>train_f1</td><td>▃▁▁▁▁▁▁▁▂▆▇▆▆▇▇▇▆▆▇▇▇▇▇▇█▇▇▇▇▇▇█▇▇▇█▇▇█▇</td></tr><tr><td>train_loss_epoch</td><td>██▇███▇▇▇▄▅▅▃▃▃▂▄▄▃▃▃▃▃▂▃▂▃▂▁▂▂▃▃▂▂▂▃▃▂▃</td></tr><tr><td>train_loss_step</td><td>▆▇▇▆▆▆▆▆▆▇▅▅▃▅▄▂▄▂▄▅▁▅▃█▆▄▂█▅▂▄▂▃▄▄▂▅▇▄▆</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▂▇▆▆▅▆▆▇▇▇▇▇▇▇▆▇▇██▇██▆▇█▇▇██▇█▇</td></tr><tr><td>val_auc</td><td>▅▁▅█████████████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▂▇▆▆▅▆▇▇█▇▇▇▇▇▇▇▇█████▇███▇████▇</td></tr><tr><td>val_loss_epoch</td><td>▆▇▆▇▅▅▆▆▅▅█▆▅▄▃▅▅▄▄▅▄▃▆▆▂▅▁▅▄▅▆▅▃▆▃▄▄▇▄▅</td></tr><tr><td>val_loss_step</td><td>▄▅▅▆▄▄▅▅▄▅█▆▅▄▃▅▆▄▄▅▄▃▆▆▂▅▁▅▄▅▅▅▃▆▃▄▄▇▄▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.68219</td></tr><tr><td>train_auc</td><td>0.72231</td></tr><tr><td>train_f1</td><td>0.536</td></tr><tr><td>train_loss_epoch</td><td>0.60436</td></tr><tr><td>train_loss_step</td><td>0.66394</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.67399</td></tr><tr><td>val_auc</td><td>0.72647</td></tr><tr><td>val_f1</td><td>0.54822</td></tr><tr><td>val_loss_epoch</td><td>0.63749</td></tr><tr><td>val_loss_step</td><td>0.66635</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_32_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/y4clhg8t' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/y4clhg8t</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240102_095345-y4clhg8t\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240102_101137-qwcyhwug</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/qwcyhwug' target=\"_blank\">MLP_3_64_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/qwcyhwug' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/qwcyhwug</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_3_64_onehot_4\\mean\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 9.0 K \n",
      "2 | head        | Sequential       | 2.1 K \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "11.1 K    Trainable params\n",
      "0         Non-trainable params\n",
      "11.1 K    Total params\n",
      "0.044     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▂▂▂▅▇▇▆▇▇▇▇▇▇▇▇▇▇▆██▇▇█▇▇▇██▇▇▇▇▇▇█▇█▇</td></tr><tr><td>train_auc</td><td>▁▁▂▂▄▆▇▇▇▇▇▇▇▇██▇█▇▇█████▇▇█████████████</td></tr><tr><td>train_f1</td><td>▃▁▁▁▁▄▇▇▆▇█▇▇▇▇▇▇▇▇▆▇▇▇▇███▇▇██▇▇███▇▇▇▇</td></tr><tr><td>train_loss_epoch</td><td>███▇▇▅▄▄▃▃▂▄▃▃▂▂▂▂▂▃▃▂▃▂▂▃▃▁▂▂▂▂▁▂▂▁▂▃▁▂</td></tr><tr><td>train_loss_step</td><td>█▇▆▆▅▄▇▅▅▃▆▅▄▂█▄▅▃▃▅▅▄▆▃▄▆▅▁▆▄▃▆▆▅▆▄▃▅▅▄</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁█▇▇▇█▇▇█▆▆▇██▇████▇▆▇▇█▇▇▇██▆█▇▆█▇▆</td></tr><tr><td>val_auc</td><td>▁▄▅▆▇█████▇▇▇▇█▇▆▆▆▆▆▆▇▆▆▆▆▆▆▆▆▆▆▆▆▇▇▆▆▆</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁█▇▇▇█▇▇▇▇▇▇██▇████▇▆▇███▇▇██▆████▇▇</td></tr><tr><td>val_loss_epoch</td><td>▇▇█▅▄▅▆▄▄▃▁▇▃▄█▅▂▄▄▅▅▅▄▄▃▃▅▂▆▄▅▃▃▃▄▅▃▅▄▄</td></tr><tr><td>val_loss_step</td><td>▅▆▆▄▃▅▆▄▄▃▁▇▃▄█▅▃▄▄▅▅▅▅▄▃▃▅▂▆▄▅▃▃▃▄▅▃▅▃▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.69132</td></tr><tr><td>train_auc</td><td>0.7443</td></tr><tr><td>train_f1</td><td>0.50585</td></tr><tr><td>train_loss_epoch</td><td>0.58618</td></tr><tr><td>train_loss_step</td><td>0.58668</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.66667</td></tr><tr><td>val_auc</td><td>0.72407</td></tr><tr><td>val_f1</td><td>0.53807</td></tr><tr><td>val_loss_epoch</td><td>0.61231</td></tr><tr><td>val_loss_step</td><td>0.61317</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_64_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/qwcyhwug' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/qwcyhwug</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240102_101137-qwcyhwug\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240102_103140-ikbmkh69</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/ikbmkh69' target=\"_blank\">MLP_3_64_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/ikbmkh69' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/ikbmkh69</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_3_64_onehot_4\\max\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 9.0 K \n",
      "2 | head        | Sequential       | 2.1 K \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "11.1 K    Trainable params\n",
      "0         Non-trainable params\n",
      "11.1 K    Total params\n",
      "0.044     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\torch_geometric\\warnings.py:17: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▂▁▂▂▂▂▃▃▄▅▆█▆▆▆▇▆▇▆▇▆▇▇▇▆▆▇▇▇██▆▇▇▇▆▆██▇</td></tr><tr><td>train_auc</td><td>▂▁▃▃▄▄▄▄▅▆▆█▇▇▇▇▇█▇█▇█▇▇▇▇█▇███▇███▇▇▇█▇</td></tr><tr><td>train_f1</td><td>▃▂▁▂▃▃▄▆▅▆▇█▇▆▆▇▇▇▇▇▇▆▇█▇▆▇▇▇█▇▆▇▇▇▇▆▇▇▇</td></tr><tr><td>train_loss_epoch</td><td>██▇▆▆▆▆▇▅▄▅▃▃▄▂▃▂▂▄▂▃▃▂▃▄▃▃▃▂▂▂▃▂▂▂▂▄▂▁▂</td></tr><tr><td>train_loss_step</td><td>█▅▆▆▄▄▅▆▇▄▃▄▅▂▇▅▅▃▃▆▅▄▆▇▃▅▅▂█▄▁▆▄▆▆▅▂▄▃▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>▄▄▁▂▅▅▃▃▅▇▆▆█▇█████████▇▇█▇▇▇▆▇▇▇▇▇▇▇▇▆▇</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_epoch</td><td>▁▁▁▂▂▂▂▂▂▃▂▃▁▂▃▁▄▆▄▄▅▂▆▄▄▄▂▃▃▅▅▃▄▇▆▆▅█▅▅</td></tr><tr><td>val_loss_step</td><td>▂▂▂▃▃▃▃▃▃▃▃▄▁▂▄▁▅▇▄▄▅▁▆▄▄▄▁▃▃▆▅▃▄▇▆▆▄█▄▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.66849</td></tr><tr><td>train_auc</td><td>0.70016</td></tr><tr><td>train_f1</td><td>0.48069</td></tr><tr><td>train_loss_epoch</td><td>0.60714</td></tr><tr><td>train_loss_step</td><td>0.56498</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.43956</td></tr><tr><td>val_auc</td><td>0.62557</td></tr><tr><td>val_f1</td><td>0.61069</td></tr><tr><td>val_loss_epoch</td><td>0.81315</td></tr><tr><td>val_loss_step</td><td>0.80846</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_64_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/ikbmkh69' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/ikbmkh69</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240102_103140-ikbmkh69\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240102_105105-k75x6cwc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/k75x6cwc' target=\"_blank\">MLP_3_64_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/k75x6cwc' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/k75x6cwc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_3_64_onehot_4\\sum\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 9.0 K \n",
      "2 | head        | Sequential       | 2.1 K \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "11.1 K    Trainable params\n",
      "0         Non-trainable params\n",
      "11.1 K    Total params\n",
      "0.044     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▃▄▄▄▅▄▅▆▆▆▇▇▇▇▇▇███▇▇▇▇▇▆█▇█▇▇█▇▆▇▆███</td></tr><tr><td>train_auc</td><td>▅▂▂▄▅▃▃▄▄▆▆█▇▅▆█▆▄▅▅▅▅▄▄▄▄▃▂▁▂▃▄▂▂▄▄▄▄▂▂</td></tr><tr><td>train_f1</td><td>▄▃▃▂▁▁▂▆▄▇▇▇▆▆▅▆▇▇▇▇▇▆▆▇▇▇█▇▇█▇▆▇▇▇█▆▆▇█</td></tr><tr><td>train_loss_epoch</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▃▃▂▁▁▂▂▃▁▂▂▂▁▂▂▂▁▁▂▂▂▃▁▁▃▂▁▂▁▁▂▄▁▂▂▂▂▁▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▂▃▃▄▃▅▄▄▄▄▄▄▄▄▄▃▄▅▄▄▇▆▄▃▃█▆▄▅</td></tr><tr><td>val_auc</td><td>▇▄████████████████████████▇▇▆▅█▇▇▄██▆▇▁▁</td></tr><tr><td>val_f1</td><td>▁▁▂▁▁▁▁▁▁▁▂▂▃▃▄▄▅▄▄▄▄▄▅▄▄▄▃▄▆▅▅▇▆▄▄▃█▇▅▅</td></tr><tr><td>val_loss_epoch</td><td>█▂▂▁▁▁▂▂▂▂▂▃▂▂▃▃▁▁▂▂▂▂▁▁▁▁▃▂▃▂▂▂▂▁▂▃▂▁▂▁</td></tr><tr><td>val_loss_step</td><td>█▃▃▂▁▂▃▄▃▃▃▅▃▃▄▄▂▁▃▃▂▄▁▂▂▂▄▂▄▂▂▃▃▂▃▄▃▂▃▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.6895</td></tr><tr><td>train_auc</td><td>0.48909</td></tr><tr><td>train_f1</td><td>0.6028</td></tr><tr><td>train_loss_epoch</td><td>0.59535</td></tr><tr><td>train_loss_step</td><td>0.66173</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.61538</td></tr><tr><td>val_auc</td><td>0.26465</td></tr><tr><td>val_f1</td><td>0.27586</td></tr><tr><td>val_loss_epoch</td><td>0.64569</td></tr><tr><td>val_loss_step</td><td>0.60475</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_64_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/k75x6cwc' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/k75x6cwc</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240102_105105-k75x6cwc\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240102_111154-oif808w3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/oif808w3' target=\"_blank\">MLP_3_64_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/oif808w3' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/oif808w3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\torch_geometric\\deprecation.py:22: UserWarning: 'nn.glob.GlobalAttention' is deprecated, use 'nn.aggr.AttentionalAggregation' instead\n",
      "  warnings.warn(out)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_3_64_onehot_4\\attention\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 9.0 K \n",
      "2  | head        | Sequential       | 2.1 K \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 65    \n",
      "--------------------------------------------------\n",
      "11.2 K    Trainable params\n",
      "0         Non-trainable params\n",
      "11.2 K    Total params\n",
      "0.045     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▂▂▃▆▆▅▅▅▆▇▆▇▆█▇▆▆▇▇▇▇▇█▆██▇▇▇▆▇██▇▇▇▇▇</td></tr><tr><td>train_auc</td><td>▂▁▂▃▅▆▆▇▇▇▇▇▇▇▇█▇▇█▇▇████▇████▇▇████████</td></tr><tr><td>train_f1</td><td>▃▁▁▁▃▆▇▅▅▄▆▇▆█▇▇▇▆▆▇█▇█▇▇█▇█▇▇██▇█▇▇█▇▇▇</td></tr><tr><td>train_loss_epoch</td><td>███▇▇▅▅▄▄▄▄▂▄▃▃▂▄▄▃▃▄▂▂▃▂▂▂▂▂▁▃▃▂▁▂▁▃▂▃▂</td></tr><tr><td>train_loss_step</td><td>▇█▆▆▆▆▇▃▆▅▃▆▅▃▅▆▅▄▇▇▅▃█▄▇▁▁▅▅▂▃▇▅▆▄▃▅▂▇▄</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▂▆▅▆▇▅▅▆▇▅▆▇▅▇█▅▇▆▇█▇▄▇█▇█▆▆▇▆█▇▆▇██</td></tr><tr><td>val_auc</td><td>▁▃▄▆▇▇███████████▇▇▇▇▇▇▇▆▆▆▆▆▆▅▆▆▆▆▆▆▆▆▆</td></tr><tr><td>val_f1</td><td>▁▁▁▁▂▆▅▆▇▅▅▆▇▅▇▆▅▇█▅▇▆▇█▇▅▇█▇█▆▆▇▆██▆▇██</td></tr><tr><td>val_loss_epoch</td><td>▅▆▇█▆▅▄▃▄▅▄▁▄▂▃▃▇▄▅▃▅▆▄▃▆▅▄▅▂▄▅▃▅▆▄▄█▃▄▆</td></tr><tr><td>val_loss_step</td><td>▄▅▆▇▆▅▄▄▄▆▄▁▅▂▄▃▇▅▅▃▆▇▄▄▇▄▅▅▂▄▅▃▅▆▅▅█▃▄▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.68584</td></tr><tr><td>train_auc</td><td>0.74252</td></tr><tr><td>train_f1</td><td>0.50575</td></tr><tr><td>train_loss_epoch</td><td>0.59223</td></tr><tr><td>train_loss_step</td><td>0.60925</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.69231</td></tr><tr><td>val_auc</td><td>0.7238</td></tr><tr><td>val_f1</td><td>0.61111</td></tr><tr><td>val_loss_epoch</td><td>0.66797</td></tr><tr><td>val_loss_step</td><td>0.73558</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_64_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/oif808w3' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/oif808w3</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240102_111154-oif808w3\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240102_113126-8vffewac</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/8vffewac' target=\"_blank\">MLP_3_64_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/8vffewac' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/8vffewac</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_3_64_onehot_4\\attention2\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 9.0 K \n",
      "2  | head        | Sequential       | 2.1 K \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 4.2 K \n",
      "--------------------------------------------------\n",
      "15.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "15.3 K    Total params\n",
      "0.061     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▂▂▃▅▇▆▆▇▆▇▇▇▇▇▆▇▇▇▇█▇▇▆▇▇▇▇▇▇▇▇▆▇▆▇▇▇▇</td></tr><tr><td>train_auc</td><td>▁▂▂▂▄▅▇▇▇▇▇█▇▇▇▇▇▇██▇█▇███▇████████▇████</td></tr><tr><td>train_f1</td><td>▃▁▁▁▁▆▇▆▇▇█▇█▇██▇▇▇█▇█▇▆▇██▇▇▇▇▇█▇█▇██▇▇</td></tr><tr><td>train_loss_epoch</td><td>█▇▇▇▇▅▃▃▃▃▄▃▃▂▂▃▃▂▂▂▂▁▄▃▂▂▂▂▂▂▁▃▂▃▁▂▃▂▂▂</td></tr><tr><td>train_loss_step</td><td>▇█▇▆▅▆▅▇▃▇▄▄▄▃▄▄▅▁▆▃▂▂▃▃▄▂▂▄▂▁▂▅▃▃▇▅▄▃▃▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▂▆▆▇█▇▅▇▇█▇▇▇█▇▇▇█▆█▇▆▇████▇▆▇▆▇▇███</td></tr><tr><td>val_auc</td><td>▁▃▄▆▇▇███▇▇▇▇▇▇▇▇▇▇▇▆▆▆▆▆▆▆▆▆▆▇▇▇▆▆▆▅▆▆▆</td></tr><tr><td>val_f1</td><td>▁▁▁▁▂▆▆▇█▇▅▇▇█▇▇▇██▇████▇▇▇████▇▇█▇▇▇███</td></tr><tr><td>val_loss_epoch</td><td>███▇▆▆▄▄▄▂▃▆▇▇▅▃▄▇▄▅▂▂▁▅▄█▄▆▅▃▂▅▆▅▄▄▆▄▃▅</td></tr><tr><td>val_loss_step</td><td>▇▇▇▆▅▆▄▅▅▂▂▇█▇▅▃▄▇▄▅▃▃▁▅▄█▄▆▅▃▂▅▆▆▄▅▆▄▃▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.69498</td></tr><tr><td>train_auc</td><td>0.74054</td></tr><tr><td>train_f1</td><td>0.50445</td></tr><tr><td>train_loss_epoch</td><td>0.58776</td></tr><tr><td>train_loss_step</td><td>0.56855</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.69231</td></tr><tr><td>val_auc</td><td>0.72086</td></tr><tr><td>val_f1</td><td>0.60377</td></tr><tr><td>val_loss_epoch</td><td>0.63139</td></tr><tr><td>val_loss_step</td><td>0.65543</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_64_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/8vffewac' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/8vffewac</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240102_113126-8vffewac\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240102_115100-gl6thsxp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/gl6thsxp' target=\"_blank\">MLP_4_16_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/gl6thsxp' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/gl6thsxp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_4_16_onehot_4\\mean\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 976   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "1.1 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.1 K     Total params\n",
      "0.005     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▄▇▆▆▇█▇█▅▇▇▇█▇▇█▇▇</td></tr><tr><td>train_auc</td><td>▁▁▂▁▂▂▂▂▂▂▂▂▂▂▂▁▁▂▁▃▄▄▆▇▇█▇███▇█▇█████▇█</td></tr><tr><td>train_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▅▇▆▆▇█▇▇▆▇▇▇█▇▇█▇▇</td></tr><tr><td>train_loss_epoch</td><td>▇█▇▇▇▇▇▇▇▇█▇▇▇▇▇▇▇▇▇▇▇▅▁▄▂▂▃▃▂▃▂▂▁▂▁▁▁▃▂</td></tr><tr><td>train_loss_step</td><td>▇▆▆▅▆▅▆▆▇█▇▇▇▆▇▇▇▇▆█▆▆▇▄▄▇▄█▁▂▅▆▄▃▆▃▅▃▅▆</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▄▆█▆▆▇██▇▄▇▆█▆▆▇▇▇</td></tr><tr><td>val_auc</td><td>▁▇▇▇▇▇▇▇▇▇▇█████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▄▇█▆▆▇██▇▄▇▇█▇▇▇▇▇</td></tr><tr><td>val_loss_epoch</td><td>▆▆▆▆▇▆▇▆▅█▇▆▆▇▆▅▆▇▇▆▆▆▄▃▃▅▄▃▂▂▅▆▆▁▅▄▁▄▂▃</td></tr><tr><td>val_loss_step</td><td>▅▆▅▅▇▅▇▅▄█▇▅▅▇▆▄▆▇▇▅▅▆▄▄▄▆▄▄▂▂▆▇█▁▆▄▁▅▃▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.6758</td></tr><tr><td>train_auc</td><td>0.71773</td></tr><tr><td>train_f1</td><td>0.48476</td></tr><tr><td>train_loss_epoch</td><td>0.61633</td></tr><tr><td>train_loss_step</td><td>0.66249</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.68498</td></tr><tr><td>val_auc</td><td>0.73175</td></tr><tr><td>val_f1</td><td>0.57426</td></tr><tr><td>val_loss_epoch</td><td>0.60239</td></tr><tr><td>val_loss_step</td><td>0.59701</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_16_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/gl6thsxp' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/gl6thsxp</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240102_115100-gl6thsxp\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240102_121210-9zfp6n9l</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/9zfp6n9l' target=\"_blank\">MLP_4_16_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/9zfp6n9l' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/9zfp6n9l</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_4_16_onehot_4\\max\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 976   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "1.1 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.1 K     Total params\n",
      "0.005     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\torch_geometric\\warnings.py:17: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▅▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇▆▆▇▇▇▆▆▇▇▇▇█▇▇</td></tr><tr><td>train_auc</td><td>▁▂▂▁▂▃▃▅▄▃▃▂▃▃▂▃▂▄▄▅▇▆▆▆▇▆▆▆▇▅▅▅▅▅▆█▅▇▅▅</td></tr><tr><td>train_f1</td><td>█▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▅▂▅▂▃▄▃▄▂▁▁▃▂▅▂▃</td></tr><tr><td>train_loss_epoch</td><td>█▅▃▃▂▃▃▁▂▂▅▄▃▂▂▂▃▃▂▂▁▃▁▁▂▂▃▂▂▂▂▂▂▁▃▁▂▁▃▂</td></tr><tr><td>train_loss_step</td><td>▅▃▄▂▂▂▂▃▅▅▄▄▅▃▅▅▄▅▄█▄▅▄▆▂▄▅▅▃▁▃▃▄▅▄▂▇▂▄▄</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>████████████████████▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>▇▃▁▂▃▃▃▆▄▆█▂▃▃▃▃▂▃▂▂▃▃▅▆▆▇▇▅▅▅▅▅▃▃▂▂▂▃▂▃</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁████████████████████</td></tr><tr><td>val_loss_epoch</td><td>▂▃▂▂▄▂▄▂▁▅▄▃▂▄▃▃▄▄▄▄▄▄▅▆▆▅▅▅▇▆█▄▄▅▄▅▆▄▆▄</td></tr><tr><td>val_loss_step</td><td>▃▄▂▃▅▃▅▃▁▆▅▃▃▄▄▃▄▅▅▄▅▄▅▆▆▅▄▅▇▆█▄▄▅▄▅▆▄▆▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.59178</td></tr><tr><td>train_auc</td><td>0.56502</td></tr><tr><td>train_f1</td><td>0.07069</td></tr><tr><td>train_loss_epoch</td><td>0.66852</td></tr><tr><td>train_loss_step</td><td>0.66966</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.43956</td></tr><tr><td>val_auc</td><td>0.43734</td></tr><tr><td>val_f1</td><td>0.61069</td></tr><tr><td>val_loss_epoch</td><td>0.69912</td></tr><tr><td>val_loss_step</td><td>0.69107</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_16_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/9zfp6n9l' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/9zfp6n9l</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240102_121210-9zfp6n9l\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240102_123242-sc7lrmge</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/sc7lrmge' target=\"_blank\">MLP_4_16_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/sc7lrmge' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/sc7lrmge</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_4_16_onehot_4\\sum\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 976   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "1.1 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.1 K     Total params\n",
      "0.005     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▂▂▄▃▄▄▄▅▂▄▅▄▅▄▅▅▅▆▆▅▅▅▆▆▆▅▆▅▆▇▇▇▆█▇█▇▇</td></tr><tr><td>train_auc</td><td>▅▅▇▆▇▇▆█▆▅▅▃▃▄▄▄▆▅▃▇▄▃▄▅▅▄▂▄▂▄▁▁▃▁▃▄▂▂▃▂</td></tr><tr><td>train_f1</td><td>▃▁▂▃▄▄▃▃▂▁▂▃▃▅▁▃▂▄▆▃▃▃▃▅▃▃▆▄▂▄▄▄▄█▅▅█▇▇▆</td></tr><tr><td>train_loss_epoch</td><td>█▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▂▂▁▁▂▂▂▂▂▂▂█▂▃▃▇▅</td></tr><tr><td>val_auc</td><td>▇████████████████▆▇▇▂▂▂▇▃▁▁▁▂▁▁▁▁▁▁▁▁▂▁▁</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▂▂▁▁▂▂▂▂▂▂▂█▂▃▄▇▅</td></tr><tr><td>val_loss_epoch</td><td>█▄▁▂▃▂▂▁▁▂▂▁▁▂▂▁▁▂▂▁▁▂▁▂▂▂▂▁▁▂▂▂▂▁▂▁▁▂▁▁</td></tr><tr><td>val_loss_step</td><td>██▁▂▆▄▄▃▁▅▄▃▂▄▄▂▃▄▃▂▂▃▃▄▄▄▄▃▂▄▄▅▄▃▄▂▂▄▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.63288</td></tr><tr><td>train_auc</td><td>0.46636</td></tr><tr><td>train_f1</td><td>0.47244</td></tr><tr><td>train_loss_epoch</td><td>0.63544</td></tr><tr><td>train_loss_step</td><td>0.65487</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.61538</td></tr><tr><td>val_auc</td><td>0.24395</td></tr><tr><td>val_f1</td><td>0.2953</td></tr><tr><td>val_loss_epoch</td><td>0.62583</td></tr><tr><td>val_loss_step</td><td>0.62253</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_16_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/sc7lrmge' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/sc7lrmge</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240102_123242-sc7lrmge\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240102_125125-oi1v454f</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/oi1v454f' target=\"_blank\">MLP_4_16_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/oi1v454f' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/oi1v454f</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\torch_geometric\\deprecation.py:22: UserWarning: 'nn.glob.GlobalAttention' is deprecated, use 'nn.aggr.AttentionalAggregation' instead\n",
      "  warnings.warn(out)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_4_16_onehot_4\\attention\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 976   \n",
      "2  | head        | Sequential       | 154   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 17    \n",
      "--------------------------------------------------\n",
      "1.1 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.1 K     Total params\n",
      "0.005     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▆▁█▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃</td></tr><tr><td>train_auc</td><td>▇▆▆▆█▂▆▄▆▁▇▆▂▁▃▄▂▄▄▄▆▅▂▅▃▃▂▂▄▆▃▇▆▅▇▄▇▂▃▄</td></tr><tr><td>train_f1</td><td>█▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_epoch</td><td>▅▆▅▅▅▄▅▇▆▇▁▇▄▇▃▂▅▅▁▃▇▆▆▅▅▆▆▆▇▇█▅▅▆▄▆▄▄▄▄</td></tr><tr><td>train_loss_step</td><td>▂▃▄▃█▄▄▆▃▂▃█▁▄▇▇▂▃▄▄▃▃▂▇▆▅▄▂▃▄▅▄▃▂▅▅▄▄▂▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>▂███████████████████▇▇▇▇▆▇▄▂▁▁▁▁▂▂▂▂▂▂▂▃</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_epoch</td><td>▆▄▆▄▂▄▅▆▅▅▃▄▃▅▆▂▆▅▇▅▅▃▆▃▄▅▁▄▂▁▄▄▅█▅▅▄▆▄▇</td></tr><tr><td>val_loss_step</td><td>▆▄▆▄▂▄▅▆▅▅▃▄▃▅▆▂▆▅▇▅▅▃▆▃▄▅▁▄▂▁▄▄▅█▅▅▄▆▄▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.58904</td></tr><tr><td>train_auc</td><td>0.49636</td></tr><tr><td>train_f1</td><td>0.0</td></tr><tr><td>train_loss_epoch</td><td>0.67493</td></tr><tr><td>train_loss_step</td><td>0.66005</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.56044</td></tr><tr><td>val_auc</td><td>0.53701</td></tr><tr><td>val_f1</td><td>0.0</td></tr><tr><td>val_loss_epoch</td><td>0.71591</td></tr><tr><td>val_loss_step</td><td>0.74744</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_16_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/oi1v454f' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/oi1v454f</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240102_125125-oi1v454f\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240102_131255-1hyymxjn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/1hyymxjn' target=\"_blank\">MLP_4_16_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/1hyymxjn' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/1hyymxjn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_4_16_onehot_4\\attention2\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 976   \n",
      "2  | head        | Sequential       | 154   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 281   \n",
      "--------------------------------------------------\n",
      "1.4 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.4 K     Total params\n",
      "0.006     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▅▁██████████████████████████████████████</td></tr><tr><td>train_auc</td><td>▇▁▃█▅▄▃▇▃▄▇█▄▅▅▅▅▄▃▅▄▄▆▆▆▂▄▆▅▅▅▄▃█▂▅█▂▄▄</td></tr><tr><td>train_f1</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_epoch</td><td>▄█▅▂▄▄▅▄▃▇▇▃▄▄▃▃▃▄▁▄▇▆▃▆▆▃▅▃█▅▃▄▁▃▅▄▅▃▃▃</td></tr><tr><td>train_loss_step</td><td>▇▆▅▅██▅▇█▇▆▆▃▅▃▇▇▆▁▅▆▆▃▆▇▆▆▅▄▆▅▅▆▇▆▇▄▆▅▅</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>▂▁▁▂▄▆█████████████▇▇█▇▆▆▆▆▅▄▃▅▂▄▂▃▃▂▃▃▂</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_epoch</td><td>▅▃▅▇▅▆▇▇▃▆▂▅▂█▆▃▄▅▇▆▅▆▆▆▆▆▂▁▁▃▂▆▅▃▆▃▆▆▆▅</td></tr><tr><td>val_loss_step</td><td>▅▃▅▇▅▇▇▇▃▆▂▅▂█▆▃▄▅▇▆▅▆▆▆▆▆▂▁▁▃▂▆▅▃▆▃▆▆▆▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.58904</td></tr><tr><td>train_auc</td><td>0.49303</td></tr><tr><td>train_f1</td><td>0.0</td></tr><tr><td>train_loss_epoch</td><td>0.67562</td></tr><tr><td>train_loss_step</td><td>0.66659</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.56044</td></tr><tr><td>val_auc</td><td>0.49436</td></tr><tr><td>val_f1</td><td>0.0</td></tr><tr><td>val_loss_epoch</td><td>0.69299</td></tr><tr><td>val_loss_step</td><td>0.69844</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_16_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/1hyymxjn' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/1hyymxjn</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240102_131255-1hyymxjn\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240102_133400-v8ypfd0f</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/v8ypfd0f' target=\"_blank\">MLP_4_32_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/v8ypfd0f' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/v8ypfd0f</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_4_32_onehot_4\\mean\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 3.5 K \n",
      "2 | head        | Sequential       | 562   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "4.0 K     Trainable params\n",
      "0         Non-trainable params\n",
      "4.0 K     Total params\n",
      "0.016     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▅▄▅▅▅▅▅▅▅▅▅▅▅▆▇▇▇█▇▇██▇▇█▇█▇▇█▇▇██▇█▇██</td></tr><tr><td>train_auc</td><td>▂▁▁▁▂▂▂▂▂▁▂▂▃▃▆▇▇▆▇▇▇▇██▇█▇▇▇▇▇█▇█▇█▇███</td></tr><tr><td>train_f1</td><td>▇▂▃▂▁▁▁▁▁▁▁▁▁▁▄██▆█▇▇▇▇▇▆▇▇▇▇▇▇▆▇▇▇▇█▇▇█</td></tr><tr><td>train_loss_epoch</td><td>█▇▇▆▇▆▇▇▆▆▆▇▆▆▄▂▄▃▃▃▃▂▂▂▂▂▂▂▁▂▂▂▂▁▂▁▃▂▁▂</td></tr><tr><td>train_loss_step</td><td>▆█▆█▇█▇▇▇▆▇▆▅▇▆▅▅▆▅▅▅▅▄▆▅▇▄▅▅▃▃▄▁▃▇▅▄▃▅▄</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▃▇▇▇▇▆▆█▇▇▇▇█▇█▇▇▇██▇▇▇▆▇▇▆</td></tr><tr><td>val_auc</td><td>▆▄▁▆▅▅▅▅▅▆▆▆▇▇▇████████▇▇▇▇▆▆▆▆▆▆▆▆▆▆▆▆▆</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▃██▇▇▆▇█▇██▇███▇████▇▇▇▇▇▇▆</td></tr><tr><td>val_loss_epoch</td><td>▆▇▇▇██▇█▇▇▆▇▆▇▅▄▄▅▃▄▃▅▆▆▄▆▆▂▃▄▆▄▇▃▃▅▁▅▄▄</td></tr><tr><td>val_loss_step</td><td>▅▆▇▆▇█▇▇▇▆▆▇▆▇▅▄▅▆▃▅▄▆▇▇▅█▇▂▃▄▆▄█▄▄▆▁▆▅▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.70228</td></tr><tr><td>train_auc</td><td>0.72822</td></tr><tr><td>train_f1</td><td>0.59045</td></tr><tr><td>train_loss_epoch</td><td>0.59816</td></tr><tr><td>train_loss_step</td><td>0.58817</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.66667</td></tr><tr><td>val_auc</td><td>0.72358</td></tr><tr><td>val_f1</td><td>0.48</td></tr><tr><td>val_loss_epoch</td><td>0.60717</td></tr><tr><td>val_loss_step</td><td>0.5941</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_32_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/v8ypfd0f' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/v8ypfd0f</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240102_133400-v8ypfd0f\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240102_135358-rspbcrt0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/rspbcrt0' target=\"_blank\">MLP_4_32_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/rspbcrt0' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/rspbcrt0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_4_32_onehot_4\\max\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 3.5 K \n",
      "2 | head        | Sequential       | 562   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "4.0 K     Trainable params\n",
      "0         Non-trainable params\n",
      "4.0 K     Total params\n",
      "0.016     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\torch_geometric\\warnings.py:17: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▂▁▄▄▅▅▅▅▅▅▅▅▅▅▆▇▆▅▇▆▆▆▇▆▇▆▆▇█▇▇▇▇▇▆▇▆▇▇█</td></tr><tr><td>train_auc</td><td>▁▁▂▁▁▂▄▃▃▂▄▃▅▄▅▅▄▃▄▄▆▅▆▄▆▅▅▅▇▆▇▆▆▇▅▆▆▅▅█</td></tr><tr><td>train_f1</td><td>▇▃▃▂▃▂▃▂▃▁▁▁▃▂▄▅▅▅▆▄▄▇▆▅▇▇▄▆▆▅▅▅█▆▅▆▇▅▆▇</td></tr><tr><td>train_loss_epoch</td><td>█▇▆▅▆▅▆▆▆▅▅▆▃▅▄▄▄▆▆▄▃▄▃▅▄▄▄▅▂▃▃▃▃▃▄▄▅▄▄▁</td></tr><tr><td>train_loss_step</td><td>▃▆▃▅▄█▅▅▄▃▄▃▃▄▃▄▂▅▄▃▅▃▃▄▄▁▅▄▄▂▄▆▃▄▅▄▄▃▄▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>▄▃▂▁▄▅▂█▃▅▄▃▁▅▅▃▃▄▅▄▃▃▆▅▄▆▅▇▄▅▃▂▅▄▅▂▃▃█▅</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_epoch</td><td>▂▂▁▁▁▁▁▁▁▂▂▁▂▂▂▂▃▄▄▃▂▁▄▂▂▄▃▃▃▃▃▅▃▄▆▄█▅▃▆</td></tr><tr><td>val_loss_step</td><td>▂▂▂▂▂▁▂▂▂▂▂▂▃▂▂▂▃▅▄▄▂▁▄▂▁▄▃▃▃▃▂▄▂▃▅▃█▄▂▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.63653</td></tr><tr><td>train_auc</td><td>0.66283</td></tr><tr><td>train_f1</td><td>0.41298</td></tr><tr><td>train_loss_epoch</td><td>0.63394</td></tr><tr><td>train_loss_step</td><td>0.63041</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.43956</td></tr><tr><td>val_auc</td><td>0.50398</td></tr><tr><td>val_f1</td><td>0.61069</td></tr><tr><td>val_loss_epoch</td><td>0.89976</td></tr><tr><td>val_loss_step</td><td>0.92099</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_32_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/rspbcrt0' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/rspbcrt0</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240102_135358-rspbcrt0\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240102_141453-a01fodwl</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/a01fodwl' target=\"_blank\">MLP_4_32_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/a01fodwl' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/a01fodwl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_4_32_onehot_4\\sum\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 3.5 K \n",
      "2 | head        | Sequential       | 562   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "4.0 K     Trainable params\n",
      "0         Non-trainable params\n",
      "4.0 K     Total params\n",
      "0.016     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▁▂▃▂▃▃▃▄▅▆▅▆▆▇▆▇▆█▇▇▇▆▇▆████▇▇▇█▇██▇██</td></tr><tr><td>train_auc</td><td>▅▁▂▂▂▁▄▁▁▁▄▃▅▇▂▅▅▅▂▃▅█▅▅▆▆▅▅▆▄▃▅▇▄▅▄▄▄▅▅</td></tr><tr><td>train_f1</td><td>▁▂▄▁▃▄▃▂▄▃▂▄▄▃▅▇▇▆▆▆▅▇▆▄▅▆▇▇▇▅▄▆█▆▆▆▇▄▆█</td></tr><tr><td>train_loss_epoch</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▄▄▃▄▄▂▂▂▂▂▂▂▂▂▂▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▃▂▂▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▅▁▆▅▅▆▇▅▄▇▅▅▄▇▆▅▅▇█▇▇▇█▆█▇██▇█▇███▆████▇</td></tr><tr><td>val_auc</td><td>▃▁▃▃▃▄▄▄▅▅▅▆▅▅█▇██▇█▆▇█▇███████████▇████</td></tr><tr><td>val_f1</td><td>▄▇▇▄▄▆▇▂▂▆▃▃▁▅▄▄▄▆▇▇▇▇███▇██▇█▆▇███████▇</td></tr><tr><td>val_loss_epoch</td><td>▅█▆▄▅▆▆▆▆▅▃▆▄▅▄▅▄▄▃▃▃▄▅▆▃▆▆▂▃▄▇▄▇▃▄▅▁▆▃▄</td></tr><tr><td>val_loss_step</td><td>▅█▇▄▅▆▆▆▆▅▃▇▄▅▅▆▄▅▃▃▄▄▆▇▄▇▇▂▃▅█▄█▄▄▆▁▇▃▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.68311</td></tr><tr><td>train_auc</td><td>0.56408</td></tr><tr><td>train_f1</td><td>0.61993</td></tr><tr><td>train_loss_epoch</td><td>0.5953</td></tr><tr><td>train_loss_step</td><td>0.57276</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.71062</td></tr><tr><td>val_auc</td><td>0.75583</td></tr><tr><td>val_f1</td><td>0.60697</td></tr><tr><td>val_loss_epoch</td><td>0.60636</td></tr><tr><td>val_loss_step</td><td>0.60169</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_32_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/a01fodwl' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/a01fodwl</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240102_141453-a01fodwl\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240102_143422-l8zxmeru</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/l8zxmeru' target=\"_blank\">MLP_4_32_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/l8zxmeru' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/l8zxmeru</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\torch_geometric\\deprecation.py:22: UserWarning: 'nn.glob.GlobalAttention' is deprecated, use 'nn.aggr.AttentionalAggregation' instead\n",
      "  warnings.warn(out)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_4_32_onehot_4\\attention\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 3.5 K \n",
      "2  | head        | Sequential       | 562   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 33    \n",
      "--------------------------------------------------\n",
      "4.1 K     Trainable params\n",
      "0         Non-trainable params\n",
      "4.1 K     Total params\n",
      "0.016     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▅▅▅▅▅▅▅▅▅▅▅▅▇▇▇▇▇▇▇▇▇▇███▇████▇▇███████</td></tr><tr><td>train_auc</td><td>▁▂▁▁▁▁▁▂▂▂▂▂▄▆▆▇▇▆▇▇▇▇▇▇██▇█▇██▇█▇█▇████</td></tr><tr><td>train_f1</td><td>▇▂▂▁▁▁▁▁▁▁▁▁▁▆▇▇▆▆▆▆▆▆▅▇▇▇▇▇████▇▇█▇▇██▇</td></tr><tr><td>train_loss_epoch</td><td>█▆▆▆▆▆▆▆▆▆▆▆▅▄▃▃▂▃▃▂▃▃▂▃▂▁▂▂▂▁▁▃▁▂▂▁▂▂▁▁</td></tr><tr><td>train_loss_step</td><td>▆▆▇▇▇▆▆█▇▆▇▆▅▅▄▂▁▆▄▁▄▂▃▅▅▅▃▇▃▅▃▆▂▆▁▂▇▆▄▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▅▆▅▆▆▆▄▆▄▇▅▆▆▆▇▆▆█▇███▆▆</td></tr><tr><td>val_auc</td><td>▇▁▅▆▆▆▆▆▇▇▇▇████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▄▅▅▅▆▅▄▅▄▆▅▅▅▅▇▅▆▇▇▇█▇▆█</td></tr><tr><td>val_loss_epoch</td><td>▅▄▆▅▄▅▅▆▅▅▅▅▅█▆▅▅▄▂▄▄▄▂▃▃▃▁▅▃▅▄▃▄▃▅▂▃▃▃▃</td></tr><tr><td>val_loss_step</td><td>▅▅▆▆▅▆▅▆▅▅▆▆▅█▇▅▆▆▃▆▅▅▂▄▄▄▁▆▄▆▅▄▅▄▇▃▄▅▄▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.68493</td></tr><tr><td>train_auc</td><td>0.7374</td></tr><tr><td>train_f1</td><td>0.48584</td></tr><tr><td>train_loss_epoch</td><td>0.59982</td></tr><tr><td>train_loss_step</td><td>0.5781</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.65201</td></tr><tr><td>val_auc</td><td>0.73475</td></tr><tr><td>val_f1</td><td>0.67128</td></tr><tr><td>val_loss_epoch</td><td>0.59943</td></tr><tr><td>val_loss_step</td><td>0.56366</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_32_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/l8zxmeru' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/l8zxmeru</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240102_143422-l8zxmeru\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240102_145335-ry2kt0l2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/ry2kt0l2' target=\"_blank\">MLP_4_32_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/ry2kt0l2' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/ry2kt0l2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_4_32_onehot_4\\attention2\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 3.5 K \n",
      "2  | head        | Sequential       | 562   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 1.1 K \n",
      "--------------------------------------------------\n",
      "5.1 K     Trainable params\n",
      "0         Non-trainable params\n",
      "5.1 K     Total params\n",
      "0.020     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▄▄▄▅▅▅▅▅▅▅▅▅▇█▇▇▇▇██▇▇▇▇██▇██████▇█████</td></tr><tr><td>train_auc</td><td>▃▂▂▁▂▁▂▂▂▁▂▂▅▆▇▇▇▇▇█▇▇▇▇██████▇█████████</td></tr><tr><td>train_f1</td><td>▇▂▂▁▁▁▁▁▁▁▁▁▂▆█▇▇▇█▇▇██▆▆▇▇█▇▇▇▇█▇▇▇▇▇▇█</td></tr><tr><td>train_loss_epoch</td><td>█▆▆▇▆▆▆▆▆▇▆▆▅▄▃▂▃▃▃▂▂▂▂▃▂▂▁▄▃▁▂▁▂▁▂▂▂▂▃▂</td></tr><tr><td>train_loss_step</td><td>█▆█▆▇▇▇▇▆▇▆▇▆▇▆▅▂▄▃▅▅▅▆▄▅▄▇▇▄▄▆▇▅▃▅█▅▁▅▅</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▇▄▅▄▄▇▇█▇▇▇▇▇▇▇▇▇▇███▆▇▆█▇█</td></tr><tr><td>val_auc</td><td>▃▁▃▂▂▁▁▁▂▂▃▅▆▇▇██████▇▇▇▇▇▇██▇▇▆▆▆▆▆▅▅▅▅</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁█▅▆▅▄▇██▇▇██▇▇█████████▇███</td></tr><tr><td>val_loss_epoch</td><td>█▇▆▇█▇█▇▇▆▆█▇▅▆▆▅▅▆▃▅▃▆▆▃▆▆▄▆▄▃▅▆▁▅▆▇▆▆▅</td></tr><tr><td>val_loss_step</td><td>▇▇▆▆▇▆█▆▇▅▅▇▇▅▇▆▅▄▇▃▆▃▇▇▄▆▆▅▆▅▄▆▆▁▆▇█▇▇▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.69863</td></tr><tr><td>train_auc</td><td>0.72909</td></tr><tr><td>train_f1</td><td>0.57474</td></tr><tr><td>train_loss_epoch</td><td>0.59844</td></tr><tr><td>train_loss_step</td><td>0.61325</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.69597</td></tr><tr><td>val_auc</td><td>0.72625</td></tr><tr><td>val_f1</td><td>0.59903</td></tr><tr><td>val_loss_epoch</td><td>0.62464</td></tr><tr><td>val_loss_step</td><td>0.64521</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_32_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/ry2kt0l2' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/ry2kt0l2</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240102_145335-ry2kt0l2\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240102_151320-i8ddetu9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/i8ddetu9' target=\"_blank\">MLP_4_64_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/i8ddetu9' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/i8ddetu9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_4_64_onehot_4\\mean\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 13.1 K\n",
      "2 | head        | Sequential       | 2.1 K \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "15.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "15.3 K    Total params\n",
      "0.061     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▄▄▄▄▄▄▄▄▅▇▇▆▇▇▇▇▇▇██▇▇███▇████▇▇███████</td></tr><tr><td>train_auc</td><td>▃▁▂▃▂▂▂▃▃▅▇▇▇▇▇▇▇▇▇██▇▇███▇█████▇███████</td></tr><tr><td>train_f1</td><td>▅▂▁▁▁▁▁▁▁▃▇▇▅▆▇▆▇▆▆███▇█▇▇▇███▇▇▆███▇███</td></tr><tr><td>train_loss_epoch</td><td>█▇▇▆▇▇▆▆▆▆▅▄▃▃▂▃▂▃▄▂▂▃▃▂▂▂▂▂▂▂▂▄▃▂▂▃▃▂▁▂</td></tr><tr><td>train_loss_step</td><td>▇▇█▇▇▆▆▇▆▄▇▇▅▇▄▄▅▅▅▅▄▅▃▄█▅▅▆▃▅▅▄▇▆▂▁▄▆▅▄</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▇▄▇▇▆██▇█▆▆▆▇▇▇█▇███▇██▇▆▇█▇██▇</td></tr><tr><td>val_auc</td><td>▁▃▃▂▃▃▅▅▇████████████▇▇▇▇▇▆▆▆▆▆▆▆▆▇▇▆▇▇▇</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▇██▇███▇▇▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▆▇▇▇▇▇</td></tr><tr><td>val_loss_epoch</td><td>▆▅▆▆▇▆▆▅▆▅▅▃▄▃▃▃▆▃▆▄▇█▅▂▃▃▇▂▂▁▄▄▃▄▅▂▅▅▃▅</td></tr><tr><td>val_loss_step</td><td>▅▄▄▅▅▅▅▄▄▄▄▃▃▃▃▃▆▃▆▄▇█▅▂▃▃▇▂▃▁▄▄▃▄▅▂▅▅▄▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.69772</td></tr><tr><td>train_auc</td><td>0.74087</td></tr><tr><td>train_f1</td><td>0.59387</td></tr><tr><td>train_loss_epoch</td><td>0.58416</td></tr><tr><td>train_loss_step</td><td>0.59053</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.67033</td></tr><tr><td>val_auc</td><td>0.72533</td></tr><tr><td>val_f1</td><td>0.52632</td></tr><tr><td>val_loss_epoch</td><td>0.64459</td></tr><tr><td>val_loss_step</td><td>0.67568</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_64_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/i8ddetu9' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/i8ddetu9</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240102_151320-i8ddetu9\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240102_153344-16j7k8el</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/16j7k8el' target=\"_blank\">MLP_4_64_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/16j7k8el' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/16j7k8el</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_4_64_onehot_4\\max\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 13.1 K\n",
      "2 | head        | Sequential       | 2.1 K \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "15.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "15.3 K    Total params\n",
      "0.061     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\torch_geometric\\warnings.py:17: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▅▄▅▅▆▇▆▇▆▆▇▇█▆▇▇▇▇█▆▆▇▇█▆▇█▆▆▇▆▆▇▇▆▇▆▇</td></tr><tr><td>train_auc</td><td>▁▁▄▃▃▄▅▅▄▅▄▇▇▇█▆▆▆▆█▆▆▆▇██▇▆▇▆▇▇▇█▇▇▇█▆▇</td></tr><tr><td>train_f1</td><td>▅▅▁▂▁▁▂▄▅█▃▅▆▆▇▆▇▆▅█▇▆▅▆▇▇▇▆▇▇▅▇█▆▆▆▅▇▇▇</td></tr><tr><td>train_loss_epoch</td><td>█▅▄▄▄▃▂▃▂▄▃▂▂▂▁▃▁▃▄▂▂▃▂▂▁▁▂▃▁▃▃▄▃▂▂▂▂▂▃▂</td></tr><tr><td>train_loss_step</td><td>▆▅▇▄▅▂▃▅▅▄▇▃▃▇▂▄▁▃▅▅▆▄▂▄▆▆▃▅▄▂▅▆█▅▄▆▆▆▄▄</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>▂▁▂▃▄▂▅▅▅▆▅▄▂▄▂▅▃▃▂▃▅▂▃▄▂▂▅▆▄█▃▅▃▄▄▄▁▅▅▄</td></tr><tr><td>val_f1</td><td>██▁█████████████████████████████████████</td></tr><tr><td>val_loss_epoch</td><td>▂▂▁▁▁▁▂▃▂▄▂▃▂▂▄▆▄▅▂▄▂▂▄▅▄▇▄▄▆█▄▁▂▅▄▄▂▄▄▄</td></tr><tr><td>val_loss_step</td><td>▃▄▃▃▃▃▃▄▃▅▃▄▃▃▄▇▄▅▂▅▂▂▅▅▄▇▄▄▆█▄▁▂▅▄▅▂▄▄▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.61735</td></tr><tr><td>train_auc</td><td>0.60311</td></tr><tr><td>train_f1</td><td>0.40228</td></tr><tr><td>train_loss_epoch</td><td>0.65437</td></tr><tr><td>train_loss_step</td><td>0.64356</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.43956</td></tr><tr><td>val_auc</td><td>0.49943</td></tr><tr><td>val_f1</td><td>0.61069</td></tr><tr><td>val_loss_epoch</td><td>0.73778</td></tr><tr><td>val_loss_step</td><td>0.73141</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_64_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/16j7k8el' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/16j7k8el</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240102_153344-16j7k8el\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240102_155517-r2y3pghu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/r2y3pghu' target=\"_blank\">MLP_4_64_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/r2y3pghu' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/r2y3pghu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_4_64_onehot_4\\sum\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 13.1 K\n",
      "2 | head        | Sequential       | 2.1 K \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "15.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "15.3 K    Total params\n",
      "0.061     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▂▃▃▁▄▂▄▄▅▄▄▃▅▅▇▆▇▇▇▇▆▇▇▇▇███▇███▇▇██████</td></tr><tr><td>train_auc</td><td>▃▁▃▁▅▄▆▂▃▂▂▁▃▃▆▇▇▇▆▄▄▃▄▇█▅▇▆▃▅▅▅▆▄▅▆▆▂▅▇</td></tr><tr><td>train_f1</td><td>▂▁▂▆▂▃▃▂▄▃▂▄▂▃▅▃▆▆▅▇▇▇▅▆▆▇▇▇▇▇▇▆▆▇██▇▇██</td></tr><tr><td>train_loss_epoch</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▃▂▂▂▁▁▂▂▂▂▁▂▂▁▂▁▁▂▂▂▁▁▁▂▁▁▁▁▁▁▁▂▁▁▁▁▁▁▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▄▆▄▄▄▄▄▅▄▄▄▄▅▇▅██▇████████▇▇████▇▇███▇</td></tr><tr><td>val_auc</td><td>▁▁▁▂███▃█▁▄▄▃█████▄▅██████▇▆▆▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>val_f1</td><td>██▁▄▁▁▁▁▁▄▁▂▁▁▂▆▃▇█▆██████▇█▇▇██▇▇▇▇███▇</td></tr><tr><td>val_loss_epoch</td><td>█▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▂▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_step</td><td>█▂▂▂▂▂▂▂▂▂▂▁▂▂▂▁▂▂▂▂▂▂▂▁▁▁▃▁▁▁▂▂▁▂▂▁▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.68037</td></tr><tr><td>train_auc</td><td>0.57885</td></tr><tr><td>train_f1</td><td>0.63693</td></tr><tr><td>train_loss_epoch</td><td>0.61131</td></tr><tr><td>train_loss_step</td><td>0.65965</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.69597</td></tr><tr><td>val_auc</td><td>0.70594</td></tr><tr><td>val_f1</td><td>0.56085</td></tr><tr><td>val_loss_epoch</td><td>0.63442</td></tr><tr><td>val_loss_step</td><td>0.65122</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_64_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/r2y3pghu' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/r2y3pghu</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240102_155517-r2y3pghu\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240102_161510-89fh69cy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/89fh69cy' target=\"_blank\">MLP_4_64_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/89fh69cy' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/89fh69cy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\torch_geometric\\deprecation.py:22: UserWarning: 'nn.glob.GlobalAttention' is deprecated, use 'nn.aggr.AttentionalAggregation' instead\n",
      "  warnings.warn(out)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_4_64_onehot_4\\attention\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 13.1 K\n",
      "2  | head        | Sequential       | 2.1 K \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 65    \n",
      "--------------------------------------------------\n",
      "15.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "15.3 K    Total params\n",
      "0.061     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▄▄▄▄▄▄▄▄▄▆▆▇▇█▇█▇▇▇▇▇▇█▇▇▇▇█▇█▇▆▇▇███▇▇</td></tr><tr><td>train_auc</td><td>▁▂▂▂▂▂▁▂▃▄▆▆▆▇▇▇▇▇▇▇▇▇▇█▇▇▇▇▇████▇▇███▇█</td></tr><tr><td>train_f1</td><td>▆▂▁▂▁▁▁▁▁▆▆▆▆▇▇▇▇▇█▇▇█▇▇▇▆▇▇█▇█▇▅▅▇▇█▇▆█</td></tr><tr><td>train_loss_epoch</td><td>█▇▇▆▇▆▇▇▇▆▃▄▃▃▃▂▂▃▃▃▂▄▂▁▂▂▃▃▂▂▁▂▄▂▃▂▂▁▂▃</td></tr><tr><td>train_loss_step</td><td>▆▇▇▆▇▇▇▆▇▆█▄▁▅▇▃▄▇▆▃▄▆▃█▄▆▃▅▄▅▆▃▂▅▇▄▆▇▆▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▅▄▇▆▆▆▆▆▄▇▇▄▅▇▇███▅▇▄▇▇▅▇▆▅▇▇▅</td></tr><tr><td>val_auc</td><td>▁▂▂▂▂▂▃▂▆█████████████████████▇████▇▇▇▇▇</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▄▄▆▅▆▅▆▆▄▆▆▄▅▆▆█▇▇▅▆▄▇▇▅▆▆▅▆█▅</td></tr><tr><td>val_loss_epoch</td><td>▅▆▅▅▅▆▆▅▆▆▃▃▃▂▄▆▆▅▄▂▅▄▅▅▆▄▃▁▇▃▇▃▄▅▅▆▂▃▇█</td></tr><tr><td>val_loss_step</td><td>▄▆▄▄▄▅▅▄▅▆▃▂▃▂▄▆▆▅▃▂▅▄▄▅▆▄▃▁▇▃▆▃▄▅▅▆▂▃▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.67397</td></tr><tr><td>train_auc</td><td>0.73435</td></tr><tr><td>train_f1</td><td>0.59933</td></tr><tr><td>train_loss_epoch</td><td>0.60857</td></tr><tr><td>train_loss_step</td><td>0.57992</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.62637</td></tr><tr><td>val_auc</td><td>0.72805</td></tr><tr><td>val_f1</td><td>0.33766</td></tr><tr><td>val_loss_epoch</td><td>0.77234</td></tr><tr><td>val_loss_step</td><td>0.9297</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_64_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/89fh69cy' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/89fh69cy</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240102_161510-89fh69cy\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240102_163432-3ytyec1f</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/3ytyec1f' target=\"_blank\">MLP_4_64_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/3ytyec1f' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/3ytyec1f</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:639: Checkpoint directory \\\\bme-retromaster.ad.gatech.edu\\labs5\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\saved_models\\9PPI_v3\\Graph_GNNs_Kfold\\MLP_4_64_onehot_4\\attention2\\GraphLevelMLP exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 13.1 K\n",
      "2  | head        | Sequential       | 2.1 K \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 4.2 K \n",
      "--------------------------------------------------\n",
      "19.5 K    Trainable params\n",
      "0         Non-trainable params\n",
      "19.5 K    Total params\n",
      "0.078     Total estimated model params size (MB)\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:492: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\pathml\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▃▄▄▄▄▄▄▄▆▇▇▇▆▆▆▇▇▇▇▇▇▇▇▇▇█▇▇█▇▇█▇██▆▇▇█</td></tr><tr><td>train_auc</td><td>▃▂▂▃▂▁▂▂▂▅▆▇▇▇▇▇▇▇█▇▇▇▇▇███▇▇▇▇▇███▇▇▇█▇</td></tr><tr><td>train_f1</td><td>▆▃▁▁▁▁▁▁▁▅▇█▇▅▅▆▇▆▇█▇▆▇▇▇▇████▇▇█▇▇▇▅▆▇▇</td></tr><tr><td>train_loss_epoch</td><td>█▇▇▇▇▇▇▇▇▆▅▄▄▄▄▄▃▄▃▄▃▃▃▃▃▂▃▃▃▂▃▂▂▁▃▂▄▃▂▃</td></tr><tr><td>train_loss_step</td><td>██▇▇█▆▇▇█▇▄▇▅▇▆▆▄▅▆▅▅▄▃▇▃▅▅▂▃▇▃▇▆▁▆▃▅▅▃▄</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▁▁▂▃▂▂▃▃▃▂▄▄▄▄▅▅▅▅▃▅▆▆▃▃▆▇▃▃▇▇▄▄██</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▃▆▄▇▆▆▇▆▇▇▆██▇▇▇█▆▆▆▆█▇▇▇█▆▆▇▇█</td></tr><tr><td>val_auc</td><td>▁▂▃▂▄▁▅▄▆▇███████████▇▇▇▇▇▆▆▆▆▆▆▆▆▆▇▇▇▇▆</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▃▆▄▇█▆▇█▇▆▇▇▇▇▇▇▇▆▆▆▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>val_loss_epoch</td><td>▇▆█▆▆▇▇▆█▄▃▁▅▃▅▂▅▃▆▅▅▁▄▆▃▃▂▃▂█▃▇▅▁▃▁▄▃▄▂</td></tr><tr><td>val_loss_step</td><td>▅▅▆▅▄▅▅▅▇▃▄▁▆▃▅▃▅▃▆▅▅▂▅▆▄▃▂▃▂█▄▇▅▂▃▁▄▄▄▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.69132</td></tr><tr><td>train_auc</td><td>0.72428</td></tr><tr><td>train_f1</td><td>0.53951</td></tr><tr><td>train_loss_epoch</td><td>0.59658</td></tr><tr><td>train_loss_step</td><td>0.57161</td></tr><tr><td>trainer/global_step</td><td>399</td></tr><tr><td>val_acc</td><td>0.69597</td></tr><tr><td>val_auc</td><td>0.72337</td></tr><tr><td>val_f1</td><td>0.59903</td></tr><tr><td>val_loss_epoch</td><td>0.58613</td></tr><tr><td>val_loss_step</td><td>0.56046</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_64_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/3ytyec1f' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_9PPI_v3_Kfold/runs/3ytyec1f</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240102_163432-3ytyec1f\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "k_folds = 5\n",
    "kfold = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "for fold, (train_ids, valid_ids) in enumerate(kfold.split(dataset_filtered)):\n",
    "    train_subset = dataset_filtered.index_select(train_ids.tolist())\n",
    "    val_subset = dataset_filtered.index_select(valid_ids.tolist())\n",
    "    for NUM_LAYERS, HIDDEN_CHANNELS, pool, in list(itertools.product(*[num_layers, hiddens, pools])):\n",
    "            # Path to the folder where the pretrained models are saved\n",
    "        CHECKPOINT_PATH = checkpoint_folder / f'MLP_{NUM_LAYERS}_{HIDDEN_CHANNELS}_onehot_{fold}' / pool\n",
    "        CHECKPOINT_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Skip already trained kfold and pool\n",
    "        checkpoint = CHECKPOINT_PATH / f\"GraphLevel{model}\" / f\"GraphLevel{model}.ckpt\" \n",
    "        if checkpoint.exists():\n",
    "            print(checkpoint)\n",
    "            continue\n",
    "\n",
    "        # Run training\n",
    "        run = wandb.init(project=project_name, name=f'{model}_{NUM_LAYERS}_{HIDDEN_CHANNELS}_onehot_{fold}', \n",
    "                        group=f'{model}_{pool}', \n",
    "                        # mode=\"disabled\"\n",
    "                        )\n",
    "        PPIGraph.train_graph_classifier_kfold(model, \n",
    "                                                train_subset, \n",
    "                                                val_subset, \n",
    "                                                dataset, \n",
    "                                                CHECKPOINT_PATH, \n",
    "                                                AVAIL_GPUS, \n",
    "                                                in_channels=9,\n",
    "                                                hidden_channels=HIDDEN_CHANNELS, \n",
    "                                                out_channels = HIDDEN_CHANNELS,\n",
    "                                                num_layers=NUM_LAYERS, \n",
    "                                                epochs=epochs,\n",
    "                                                embedding=False,\n",
    "                                                batch_size=258,\n",
    "                                                graph_pooling=pool)\n",
    "        run.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snowflake",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
