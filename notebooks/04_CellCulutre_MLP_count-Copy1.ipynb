{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "13991570-26bf-40c4-9627-052a4f5e5902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import itertools\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import skimage.io\n",
    "\n",
    "from collections import defaultdict\n",
    "from tqdm.auto import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "import re\n",
    "import h5py\n",
    "import napari\n",
    "import tifffile as tiff\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "\n",
    "import networkx as nx\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "168e90e1-55cd-4282-a75c-0399e4de6438",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric.utils\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fca65fa1-4d10-4de6-a995-e1937f0a4ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dbe8b2d3-9d8e-4d81-ba5d-610db915d86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_dir = (Path().cwd().parents[0]).absolute()\n",
    "\n",
    "module_path = str(p_dir / \"src\")\n",
    "\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dccef510-2824-49b7-9a19-29ddb58dc1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = (Path().cwd().parents[0] / 'data').absolute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d6b94541-cf3f-4d37-a41c-4c7b5bd2bbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning.pytorch as pl\n",
    "import PPIGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3995536b-837b-46dd-8c96-ebf0b43f38e0",
   "metadata": {},
   "source": [
    "# Test custom torch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dd9148b9-7c73-4de7-9f87-87af9ab447e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "12d592d3-f44c-4c08-8a03-e57243948de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_mapping = {'HCC827Ctrl': 0, 'HCC827Osim': 1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f21327a8-0c52-4236-a27f-0167f94d2397",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_path = data_dir / 'OCT Cell Culture' / 'Whole' / 'graphs' \n",
    "\n",
    "dataset = PPIGraph.GraphDatasetMLP(graph_path, 'raw', 'pt', condition_mapping=condition_mapping, n_c=2)\n",
    "train_set, val_set, test_set = PPIGraph.train_test_val_split(dataset)\n",
    "\n",
    "# Create Dataloader\n",
    "train_loader = DataLoader(train_set, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "da422d2e-2e57-4029-902b-a81b2d8a731c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: GraphDatasetMLP(2064):\n",
      "======================\n",
      "Number of graphs: 2064\n",
      "Number of features: 5\n",
      "Number of classes: 2\n"
     ]
    }
   ],
   "source": [
    "print(f'Dataset: {dataset}:')\n",
    "print('======================')\n",
    "print(f'Number of graphs: {len(dataset)}')\n",
    "print(f'Number of features: {dataset.num_features}')\n",
    "print(f'Number of classes: {dataset.num_classes}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bb3f2143-27fa-4dbc-b082-8a76e35942f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 992, test set: 825, val set: 247\n"
     ]
    }
   ],
   "source": [
    "print(f'Train set: {len(train_set)}, test set: {len(test_set)}, val set: {len(val_set)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6f5f342f-06aa-45bc-8a79-e980e88c20b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1:\n",
      "=======\n",
      "Number of graphs in the current batch: 32\n",
      "DataBatch(x=[32, 5], y=[32], name=[32], batch=[32], ptr=[33])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for step, data in enumerate(train_loader):\n",
    "    print(f'Step {step + 1}:')\n",
    "    print('=======')\n",
    "    print(f'Number of graphs in the current batch: {data.num_graphs}')\n",
    "    print(data)\n",
    "    print()\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543bbc34-99ce-4318-81fa-836bb233c468",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Lightning training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "91c39ab3-454b-4892-8bbe-4cf855f99166",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from lightning.pytorch.accelerators import find_usable_cuda_devices\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8954d479-e810-4d58-8be5-d11a82b475b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# condition = 'base'\n",
    "# checkpoint_folder = (Path().cwd().parents[0]).absolute() / 'data' / \"saved_models\" / f'MLP_{condition}' \n",
    "# project_name = f'PLA_041623_{condition}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c342b874-8e99-4c08-8bd2-6f24685c3b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AVAIL_GPUS = [1]\n",
    "# BATCH_SIZE = 64 if AVAIL_GPUS else 32\n",
    "\n",
    "# # Setting the seed\n",
    "# pl.seed_everything(42)\n",
    "\n",
    "# num_layers = [2,3,4]\n",
    "# hiddens = [8, 16, 32, 64]\n",
    "# pool = 'mean'\n",
    "\n",
    "# epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "673781fb-d568-4cd3-9196-089b3aecfbf7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for NUM_LAYERS, HIDDEN_CHANNELS in list(itertools.product(*[num_layers, hiddens])):\n",
    "    \n",
    "#     # Path to the folder where the pretrained models are saved\n",
    "#     CHECKPOINT_PATH = checkpoint_folder / f'_{NUM_LAYERS}_{HIDDEN_CHANNELS}_count_cyto'\n",
    "#     CHECKPOINT_PATH.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "#     # Run training\n",
    "#     models = ['MLP']\n",
    "#     for model_name in models:\n",
    "#         run = wandb.init(project=project_name, name=model_name+f'_{NUM_LAYERS}_{HIDDEN_CHANNELS}_count_cyto')\n",
    "#         model, result, trainer = PPIGraph.train_graph_classifier(model_name, train_set, val_set, test_set, \n",
    "#                                                                  dataset, CHECKPOINT_PATH, AVAIL_GPUS, \n",
    "#                                                                  in_channels=HIDDEN_CHANNELS,\n",
    "#                                                                  hidden_channels=HIDDEN_CHANNELS, \n",
    "#                                                                  out_channels=HIDDEN_CHANNELS, \n",
    "#                                                                  num_layers=NUM_LAYERS, \n",
    "#                                                                  epochs=epochs,\n",
    "#                                                                  embedding=True,\n",
    "#                                                                  graph_pooling=pool)\n",
    "#         run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "66e000ad-5463-4762-a40d-dbf2e7dd4d5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for NUM_LAYERS, HIDDEN_CHANNELS in list(itertools.product(*[num_layers, hiddens])):\n",
    "    \n",
    "#     # Path to the folder where the pretrained models are saved\n",
    "#     CHECKPOINT_PATH = checkpoint_folder / f'_{NUM_LAYERS}_{HIDDEN_CHANNELS}_count_cell'\n",
    "#     CHECKPOINT_PATH.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "#     # Run training\n",
    "#     models = ['MLP']\n",
    "#     for model_name in models:\n",
    "#         run = wandb.init(project=project_name, name=model_name+f'_{NUM_LAYERS}_{HIDDEN_CHANNELS}_count_cell')\n",
    "#         model, result, trainer = PPIGraph.train_graph_classifier(model_name, train_set, val_set, test_set, \n",
    "#                                                                  dataset, CHECKPOINT_PATH, AVAIL_GPUS, \n",
    "#                                                                  in_channels=HIDDEN_CHANNELS,\n",
    "#                                                                  hidden_channels=HIDDEN_CHANNELS, \n",
    "#                                                                  out_channels=HIDDEN_CHANNELS, \n",
    "#                                                                  num_layers=NUM_LAYERS, \n",
    "#                                                                  epochs=epochs,\n",
    "#                                                                  embedding=True,\n",
    "#                                                                  graph_pooling=pool)\n",
    "#         run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d93c15-00a5-461b-967b-d3cf4077e58c",
   "metadata": {},
   "source": [
    "# K Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ae285e19-3a23-4e0f-b62c-d249f6aa1eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from lightning.pytorch.accelerators import find_usable_cuda_devices\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a9a37e69-6088-47a1-8805-367a392ed6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from torch.utils.data import SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dc6e6629-c3a0-49a6-9cde-6b025373b88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out by maximum number of counts per cell\n",
    "min_count = 20\n",
    "max_count = 70\n",
    "\n",
    "graph_path = data_dir / 'OCT Cell Culture' / 'Whole' / 'graphs' \n",
    "\n",
    "dataset_subcellular = PPIGraph.GraphDataset(graph_path, 'raw', 'pt', condition_mapping=condition_mapping, n_c=2)\n",
    "\n",
    "# Create Dataloader\n",
    "loader = DataLoader(dataset_subcellular, batch_size=1, shuffle=False)\n",
    "\n",
    "# Get Indices\n",
    "indices = []\n",
    "for step, data in enumerate(loader):\n",
    "    if len(data.x) <= min_count:\n",
    "        continue \n",
    "    \n",
    "    if (data.x.sum(axis=0) >= max_count).any():\n",
    "        continue\n",
    "    indices.append(step)\n",
    "    \n",
    "# Get subset dataset\n",
    "dataset_filtered = dataset.index_select(indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b8ac7509-15c8-4c19-820b-e6678f6a0315",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1522"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "541d8502-ca80-414e-a0a6-12b6a540d934",
   "metadata": {},
   "outputs": [],
   "source": [
    "condition = 'Kfold'\n",
    "checkpoint_folder = (Path().cwd().parents[0]).absolute() / 'data' / \"saved_models\" / f\"MLP_count_{condition}\" \n",
    "project_name = f'PLA_050823_{condition}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5c1ec9b0-030b-43d5-9890-96ce08bb3b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    }
   ],
   "source": [
    "AVAIL_GPUS = [1]\n",
    "\n",
    "# Setting the seed\n",
    "pl.seed_everything(42)\n",
    "\n",
    "NUM_LAYERS = 2\n",
    "HIDDEN_CHANNELS = 16\n",
    "\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c899cb13-6eb1-49c5-abd5-047dfd8dcf7e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8494c628832a4b96b23a56b8a3711bd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01691666666495924, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Thomas\\15_PLA\\notebooks\\wandb\\run-20230509_000308-sr5h9yjq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_050823_Kfold/runs/sr5h9yjq' target=\"_blank\">MLP_2_16_cell_0</a></strong> to <a href='https://wandb.ai/thoomas/PLA_050823_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_050823_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_050823_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_050823_Kfold/runs/sr5h9yjq' target=\"_blank\">https://wandb.ai/thoomas/PLA_050823_Kfold/runs/sr5h9yjq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 368   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "522       Trainable params\n",
      "0         Non-trainable params\n",
      "522       Total params\n",
      "0.002     Total estimated model params size (MB)\n",
      "C:\\Users\\thu71\\AppData\\Roaming\\Python\\Python39\\site-packages\\lightning\\pytorch\\utilities\\data.py:77: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 128. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  warning_cache.warn(\n",
      "C:\\Users\\thu71\\AppData\\Roaming\\Python\\Python39\\site-packages\\lightning\\pytorch\\utilities\\data.py:77: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 65. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  warning_cache.warn(\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15e1c5226a7e4f8eafefe690cdb8b004",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▂▂▅▅▅▅▅▅▅▆▆▆▆▆▆▇▇▆▇▇▇▇▇▇▇▇█▇▇▇█▇▇██▇▇▇</td></tr><tr><td>train_auc</td><td>▂▁▂▁▃▂▂▂▂▂▃▃▃▃▄▅▅▅▆▅▅▆▆▆▇▆▆▇█▇▇▆█▇▆▇▇▇▇▇</td></tr><tr><td>train_f1</td><td>▇▆▆▅▄▁▁▁▁▁▂▃▃▃▄▅▅▅▆▅▅▇▆▆▇▇▇▇█▇▇▇█▇▆▇▇▇▇▇</td></tr><tr><td>train_loss_epoch</td><td>█▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▄▃▃▃▂▂▂▂▃▂▂▂▂▃▂▃▂▂▂▂▂▃▂▂▂▂▂▂▁▂▂▂▁▂▂▂▁▃▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▃▁▃▃▂▂▂▂▂▂▄▄▂▅▅▅▆▆▆▆▃▃▇▃▃▃▇▃▃██▃</td></tr><tr><td>val_acc</td><td>▂▁▂▂▂▂▂▂▂▂▂▂▂▃▄▅▆▆▆▅▆▇▇▆█▇█▇▇█▇█▇██▇████</td></tr><tr><td>val_auc</td><td>▁▁▁▁▁▁▁▁▁▁▂▁▁▃▄▄▅▆▆▄▆▇▇▆█▇████▇▇▇███████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▃▄▅▆▆▇▅▆▇▇▇█▇██████████████</td></tr><tr><td>val_loss_epoch</td><td>█▇▇▇▆▆▆▆▆▆▆▅▅▅▄▄▄▃▃▄▃▂▃▃▂▂▂▁▂▁▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_step</td><td>█▆▆▆▆▆▆▅▆▆▅▅▅▅▄▄▄▄▅▄▃▃▅▂▁▃▂▂▂▄▂▂▁▃▂▃▃▁▃▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>49</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.69844</td></tr><tr><td>train_auc</td><td>0.62704</td></tr><tr><td>train_f1</td><td>0.46109</td></tr><tr><td>train_loss_epoch</td><td>0.58712</td></tr><tr><td>train_loss_step</td><td>0.5241</td></tr><tr><td>trainer/global_step</td><td>499</td></tr><tr><td>val_acc</td><td>0.7377</td></tr><tr><td>val_auc</td><td>0.68478</td></tr><tr><td>val_f1</td><td>0.57447</td></tr><tr><td>val_loss_epoch</td><td>0.5325</td></tr><tr><td>val_loss_step</td><td>0.49924</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_16_cell_0</strong> at: <a href='https://wandb.ai/thoomas/PLA_050823_Kfold/runs/sr5h9yjq' target=\"_blank\">https://wandb.ai/thoomas/PLA_050823_Kfold/runs/sr5h9yjq</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230509_000308-sr5h9yjq\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed024ce71465414d98f4e48a153550b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01693333333435779, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Thomas\\15_PLA\\notebooks\\wandb\\run-20230509_002255-u71c2xwd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_050823_Kfold/runs/u71c2xwd' target=\"_blank\">MLP_2_16_cell_1</a></strong> to <a href='https://wandb.ai/thoomas/PLA_050823_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_050823_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_050823_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_050823_Kfold/runs/u71c2xwd' target=\"_blank\">https://wandb.ai/thoomas/PLA_050823_Kfold/runs/u71c2xwd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 368   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "522       Trainable params\n",
      "0         Non-trainable params\n",
      "522       Total params\n",
      "0.002     Total estimated model params size (MB)\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8ddb11b5086473abefb518f1210d78c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▁▂▅▅▄▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇▇▇▇█▇██▇█▇█▇▇█████</td></tr><tr><td>train_auc</td><td>▁▁▁▁▂▂▁▂▂▁▂▃▄▄▅▆▆▆▇▅▆▇▆▇▆█▇██▇▇▇█▇▆▇▇█▇▇</td></tr><tr><td>train_f1</td><td>▆▆▆▆▄▂▁▁▁▁▁▃▄▄▅▇▆▆▇▆▇▇▇▇▆█▇██▇▇▇█▇▆▇▇█▇▇</td></tr><tr><td>train_loss_epoch</td><td>█▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▅▃▂▂▂▂▂▃▂▂▂▂▂▂▂▂▂▁▁▂▂▃▂▂▂▂▂▂▁▁▂▂▁▂▁▁▁▂▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▃▁▃▃▂▂▂▂▂▂▄▄▂▅▅▅▆▆▆▆▃▃▇▃▃▃▇▃▃██▃</td></tr><tr><td>val_acc</td><td>▂▁▃▃▃▃▃▃▃▃▃▃▄▃▇▇▇██▇██▇█▇▇▇▇▇▇▇▇█▇▇▇▇▇▇▇</td></tr><tr><td>val_auc</td><td>▁▁▂▂▂▂▂▂▂▂▂▂▃▃▆▇▇▇▇▇▇▇▇█▇▇▇▇▇▇▇▇█▆▇▇█▇▇▇</td></tr><tr><td>val_f1</td><td>▁▂▁▁▁▁▁▁▁▁▁▁▃▂▆▇▇▇▇▇▇▇▇█▇█▇██▇▇▇█▇▇▇█▇▇█</td></tr><tr><td>val_loss_epoch</td><td>█▅▅▅▅▅▅▄▄▄▄▄▃▃▃▂▂▂▂▂▂▂▂▁▂▁▂▁▁▂▁▂▁▁▁▂▁▁▁▁</td></tr><tr><td>val_loss_step</td><td>█▆▆▅▅▆▆▄▅▅▄▄▄▅▃▄▄▄▃▄▄▃▃▃▄▁▃▃▄▄▂▂▂▃▃▃▃▁▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>49</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.71569</td></tr><tr><td>train_auc</td><td>0.64079</td></tr><tr><td>train_f1</td><td>0.47734</td></tr><tr><td>train_loss_epoch</td><td>0.56692</td></tr><tr><td>train_loss_step</td><td>0.5058</td></tr><tr><td>trainer/global_step</td><td>499</td></tr><tr><td>val_acc</td><td>0.70492</td></tr><tr><td>val_auc</td><td>0.65412</td></tr><tr><td>val_f1</td><td>0.53125</td></tr><tr><td>val_loss_epoch</td><td>0.54548</td></tr><tr><td>val_loss_step</td><td>0.54024</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_16_cell_1</strong> at: <a href='https://wandb.ai/thoomas/PLA_050823_Kfold/runs/u71c2xwd' target=\"_blank\">https://wandb.ai/thoomas/PLA_050823_Kfold/runs/u71c2xwd</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230509_002255-u71c2xwd\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b8274b44deb43b6864f0b81520f520c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01693333333435779, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Thomas\\15_PLA\\notebooks\\wandb\\run-20230509_004117-gqvq61il</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_050823_Kfold/runs/gqvq61il' target=\"_blank\">MLP_2_16_cell_2</a></strong> to <a href='https://wandb.ai/thoomas/PLA_050823_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_050823_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_050823_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_050823_Kfold/runs/gqvq61il' target=\"_blank\">https://wandb.ai/thoomas/PLA_050823_Kfold/runs/gqvq61il</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 368   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "522       Trainable params\n",
      "0         Non-trainable params\n",
      "522       Total params\n",
      "0.002     Total estimated model params size (MB)\n",
      "C:\\Users\\thu71\\AppData\\Roaming\\Python\\Python39\\site-packages\\lightning\\pytorch\\utilities\\data.py:77: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 66. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  warning_cache.warn(\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2cd03e26a1c463991666f25fde68b99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▂▃▅▄▅▅▅▅▅▅▅▆▅▆▆▆▆▆▆▆▆▇▆▇▇▇▇▇▇▇▇▇▇▇▇██▇</td></tr><tr><td>train_auc</td><td>▁▁▂▁▂▁▁▁▂▃▂▂▃▃▃▄▄▄▅▅▄▅▅▆▅▆▆▅▆▆▇▆▇▆▆▆▆▇█▇</td></tr><tr><td>train_f1</td><td>▆▇▇▅▄▂▂▁▂▃▃▂▃▃▃▄▄▅▅▆▄▆▆▆▅▆▇▆▆▆▇▇▇▆▇▆▆▇█▇</td></tr><tr><td>train_loss_epoch</td><td>█▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▁▁▂▂▁▂▂▂▁▁▁▁▁▁▁▂▂▂▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▃▁▃▃▂▂▂▂▂▂▄▄▂▅▅▅▆▆▆▆▃▃▇▃▃▃▇▃▃██▃</td></tr><tr><td>val_acc</td><td>▁▁▂▃▃▃▃▃▃▃▃▃▃▃▃▃▄▄▅▄▅▇▆▆▅██▅▇▇▇█▇▇█▆████</td></tr><tr><td>val_auc</td><td>▁▂▁▁▁▁▁▁▁▁▁▁▁▂▂▂▃▃▅▃▄▇▅▅▄██▅▆▇▇█▇▇▇▆████</td></tr><tr><td>val_f1</td><td>▃▄▁▁▁▁▁▁▁▁▁▁▁▂▂▂▄▄▆▄▅█▆▆▅██▆▇█▇█▇██▆████</td></tr><tr><td>val_loss_epoch</td><td>███▇▇▇▇▇▆▆▆▆▆▆▆▅▅▅▄▄▄▃▃▃▃▂▂▃▃▂▂▂▂▁▂▂▂▁▂▁</td></tr><tr><td>val_loss_step</td><td>██▇▇▇▇▆▇▇▆▆▇▇▆▇▆▆▄▅▆▅▅▃▅▅▆▅▅▅▃▅▄▄▃▅▅▅▄▅▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>49</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.70197</td></tr><tr><td>train_auc</td><td>0.64116</td></tr><tr><td>train_f1</td><td>0.49653</td></tr><tr><td>train_loss_epoch</td><td>0.5768</td></tr><tr><td>train_loss_step</td><td>0.50339</td></tr><tr><td>trainer/global_step</td><td>499</td></tr><tr><td>val_acc</td><td>0.79276</td></tr><tr><td>val_auc</td><td>0.73599</td></tr><tr><td>val_f1</td><td>0.64804</td></tr><tr><td>val_loss_epoch</td><td>0.49214</td></tr><tr><td>val_loss_step</td><td>0.39731</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_16_cell_2</strong> at: <a href='https://wandb.ai/thoomas/PLA_050823_Kfold/runs/gqvq61il' target=\"_blank\">https://wandb.ai/thoomas/PLA_050823_Kfold/runs/gqvq61il</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230509_004117-gqvq61il\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e535b340f96947f1b371be1367395db7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01691666666495924, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Thomas\\15_PLA\\notebooks\\wandb\\run-20230509_011714-s3b3ex4h</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_050823_Kfold/runs/s3b3ex4h' target=\"_blank\">MLP_2_16_cell_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_050823_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_050823_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_050823_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_050823_Kfold/runs/s3b3ex4h' target=\"_blank\">https://wandb.ai/thoomas/PLA_050823_Kfold/runs/s3b3ex4h</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 368   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "522       Trainable params\n",
      "0         Non-trainable params\n",
      "522       Total params\n",
      "0.002     Total estimated model params size (MB)\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e628099cfac8402bb3af66a3a936ab81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▂▂▅▄▅▅▅▆▅▅▆▅▆▆▇▇▆▆▇█▇▇▇▇█▇▇█████▇▇███▇</td></tr><tr><td>train_auc</td><td>▁▁▂▁▃▁▂▁▂▂▂▂▃▃▃▄▅▅▅▄▅▆▅▆▅▆▇▆▇▇▇▇▇▇▇▆▇▇█▆</td></tr><tr><td>train_f1</td><td>▇▇▇▅▄▂▂▁▂▂▂▃▄▃▄▅▅▅▅▄▆▆▆▆▅▆▇▆▇▇▇▇▇▇▇▇▇▇█▆</td></tr><tr><td>train_loss_epoch</td><td>█▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▂▂▂▂▁▂▁▂▁▁▁▁▁▁▁▂▂▂▂▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▃▁▃▃▂▂▂▂▂▂▄▄▂▅▅▅▆▆▆▆▃▃▇▃▃▃▇▃▃██▃</td></tr><tr><td>val_acc</td><td>▁▂▂▂▂▂▂▂▂▂▂▂▃▃▃▄▅▅▅▄▆▇▇▇▆▇▇▇▇▇▇█▇██▇▇▇▇▇</td></tr><tr><td>val_auc</td><td>▂▃▁▁▁▁▁▁▁▁▂▂▂▂▃▃▄▅▅▄▆▆▇▆▆▇▆▆▇▇▇█▆██▇▇█▇▇</td></tr><tr><td>val_f1</td><td>▄▅▁▁▁▁▁▁▁▁▁▁▂▂▃▄▅▅▅▄▆▇▇▇▆▇▇▇▇█▇█▇██▇▇█▇█</td></tr><tr><td>val_loss_epoch</td><td>▇██▇▇▇▇▆▆▆▆▆▆▅▅▄▄▄▃▄▃▃▃▂▃▂▂▂▂▂▂▁▂▁▁▂▂▁▂▁</td></tr><tr><td>val_loss_step</td><td>▇██▇▇▇▆▇▇▆▆▆▇▆▆▅▅▃▄▄▄▄▂▅▄▃▄▃▇▃▃▃▃▄▂▄▁▃▄▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>49</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.67734</td></tr><tr><td>train_auc</td><td>0.60852</td></tr><tr><td>train_f1</td><td>0.42795</td></tr><tr><td>train_loss_epoch</td><td>0.58912</td></tr><tr><td>train_loss_step</td><td>0.57149</td></tr><tr><td>trainer/global_step</td><td>499</td></tr><tr><td>val_acc</td><td>0.79934</td></tr><tr><td>val_auc</td><td>0.73465</td></tr><tr><td>val_f1</td><td>0.64327</td></tr><tr><td>val_loss_epoch</td><td>0.49355</td></tr><tr><td>val_loss_step</td><td>0.5054</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_16_cell_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_050823_Kfold/runs/s3b3ex4h' target=\"_blank\">https://wandb.ai/thoomas/PLA_050823_Kfold/runs/s3b3ex4h</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230509_011714-s3b3ex4h\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da48cd74657747f799880f933f9413d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Thomas\\15_PLA\\notebooks\\wandb\\run-20230509_015402-6lhpg5y4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_050823_Kfold/runs/6lhpg5y4' target=\"_blank\">MLP_2_16_cell_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_050823_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_050823_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_050823_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_050823_Kfold/runs/6lhpg5y4' target=\"_blank\">https://wandb.ai/thoomas/PLA_050823_Kfold/runs/6lhpg5y4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 368   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "522       Trainable params\n",
      "0         Non-trainable params\n",
      "522       Total params\n",
      "0.002     Total estimated model params size (MB)\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d9f66c21d0f46e6a7b569c303c334e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▃▃▅▅▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇██▇▇██▇████████▇██</td></tr><tr><td>train_auc</td><td>▂▁▃▂▃▂▃▂▃▃▃▄▄▄▅▆▅▆▅▆▆▆█▇▆▇██▇▇▇▇██▇█▇▇▇█</td></tr><tr><td>train_f1</td><td>▇▆▇▅▄▁▁▁▂▂▃▃▄▄▄▆▅▆▆▆▆▆█▇▆▆██▇▇█▇██▇█▇▇▇█</td></tr><tr><td>train_loss_epoch</td><td>█▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▄▃▂▂▂▂▂▂▂▂▃▂▂▂▂▂▂▃▂▁▂▂▂▁▁▁▂▁▁▂▂▁▁▂▂▂▂▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▃▁▃▃▂▂▂▂▂▂▄▄▂▅▅▅▆▆▆▆▃▃▇▃▃▃▇▃▃██▃</td></tr><tr><td>val_acc</td><td>▁▃▁▁▁▁▁▁▁▁▁▁▂▃▃▄▇▅▆▅▆▇█▆▆▆█▇▇▇▆▆▇▇▇▇▆▆▆▇</td></tr><tr><td>val_auc</td><td>▁▃▁▁▁▁▁▁▁▁▁▁▂▂▂▄▆▅▅▅▅▇█▆▅▆█▇▇▇▆▆▇▇▇▇▆▆▇▇</td></tr><tr><td>val_f1</td><td>▂▄▁▁▁▁▁▁▁▁▁▁▂▃▃▄▆▅▆▅▆▇█▆▆▇█▇▇▇▇▇▇▇▇█▇▇▇█</td></tr><tr><td>val_loss_epoch</td><td>█▆▆▆▆▆▆▆▅▅▅▅▅▄▄▄▃▃▃▃▃▂▂▂▃▂▁▂▂▂▂▂▁▂▁▁▂▂▂▁</td></tr><tr><td>val_loss_step</td><td>█▆▇▇▇▆▅▆▆▇▆▅▆▅▇▅▄▄▅▄▅▂▁▄▅▄▃▃▆▄▂▃▂▃▂▃▃▂▃▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>49</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.71675</td></tr><tr><td>train_auc</td><td>0.63264</td></tr><tr><td>train_f1</td><td>0.45669</td></tr><tr><td>train_loss_epoch</td><td>0.56503</td></tr><tr><td>train_loss_step</td><td>0.47363</td></tr><tr><td>trainer/global_step</td><td>499</td></tr><tr><td>val_acc</td><td>0.70724</td></tr><tr><td>val_auc</td><td>0.67236</td></tr><tr><td>val_f1</td><td>0.56158</td></tr><tr><td>val_loss_epoch</td><td>0.54672</td></tr><tr><td>val_loss_step</td><td>0.51575</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_16_cell_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_050823_Kfold/runs/6lhpg5y4' target=\"_blank\">https://wandb.ai/thoomas/PLA_050823_Kfold/runs/6lhpg5y4</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230509_015402-6lhpg5y4\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# K fold nuclei cyto seperated\n",
    "k_folds = 5\n",
    "kfold = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "for fold, (train_ids, valid_ids) in enumerate(kfold.split(dataset_filtered)):\n",
    "    train_subset = dataset_filtered.index_select(train_ids.tolist())\n",
    "    val_subset = dataset_filtered.index_select(valid_ids.tolist())\n",
    "    \n",
    "    # Path to the folder where the pretrained models are saved\n",
    "    CHECKPOINT_PATH = checkpoint_folder / f'{NUM_LAYERS}_{HIDDEN_CHANNELS}_cell_{fold}' \n",
    "    CHECKPOINT_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Skip already trained kfold and pool\n",
    "    checkpoint = CHECKPOINT_PATH / \"GraphLevelMLP\" / \"GraphLevelMLP.ckpt\" \n",
    "    if checkpoint.exists():\n",
    "        print(checkpoint)\n",
    "        continue\n",
    "\n",
    "    # Run training\n",
    "    run = wandb.init(project=project_name, name=f'MLP_{NUM_LAYERS}_{HIDDEN_CHANNELS}_cell_{fold}', \n",
    "                    group=f'MLP_cell')\n",
    "    PPIGraph.train_graph_classifier_kfold('MLP', \n",
    "                                         train_subset, \n",
    "                                         val_subset, \n",
    "                                         dataset, \n",
    "                                         CHECKPOINT_PATH, \n",
    "                                         AVAIL_GPUS, \n",
    "                                         in_channels=5,\n",
    "                                         hidden_channels=HIDDEN_CHANNELS, \n",
    "                                         out_channels = HIDDEN_CHANNELS,\n",
    "                                         num_layers=NUM_LAYERS, \n",
    "                                         epochs=epochs,\n",
    "                                         embedding=False,\n",
    "                                         batch_size=128,\n",
    "                                         graph_pooling='mean')\n",
    "    run.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aaeeecb-beed-431c-892f-6992d0ed850f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:snowflake]",
   "language": "python",
   "name": "conda-env-snowflake-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
