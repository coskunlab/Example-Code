{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import skimage.io\n",
    "\n",
    "from collections import defaultdict\n",
    "from tqdm.notebook import trange, tqdm, tqdm_notebook\n",
    "from joblib import Parallel, delayed\n",
    "import re\n",
    "import h5py\n",
    "import tifffile as tiff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = (Path().cwd().parents[0] / 'data').absolute()\n",
    "data_processed = data_dir / 'processed'\n",
    "data_raw = r'Y:\\coskun-lab\\Shuangyi\\ERK, YAP project_2022\\PLA\\indirect PLA_24Jan24'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_dir = (Path().cwd().parents[0]).absolute()\n",
    "\n",
    "module_path = str(p_dir / \"src\")\n",
    " \n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "markers_map = {\n",
    "    'cycle1': {\n",
    "        1: 'DNA', \n",
    "        4: '5 Pairs',\n",
    "    },\n",
    "    'cycle3': {\n",
    "        1: 'DNA', \n",
    "        4: 'cdc25c/p38',\n",
    "    },\n",
    "    'cycle4': {\n",
    "        1: 'DNA', \n",
    "        3: 'Phalloidin',\n",
    "    },\n",
    "}\n",
    "\n",
    "def get_info(data_raw, marker_dict):\n",
    "    conditions = []\n",
    "    fovs = []\n",
    "    cycles = []\n",
    "    channels = []\n",
    "    markers = []\n",
    "    paths = [] \n",
    "    \n",
    "    # Loop through image folder\n",
    "    for (dirpath, dirnames, filenames) in os.walk(data_raw):\n",
    "        if 'after nuclease' in dirpath.lower() or 'Test' in dirpath or 'Dont use' in dirpath:\n",
    "            continue\n",
    "        \n",
    "        for name in sorted(filenames):\n",
    "            if \"tif\" in name and \"sti\" in name \\\n",
    "            and 'overlay' not in name \\\n",
    "            and 'Composite' not in name:\n",
    "                # Get information from image name\n",
    "                \n",
    "                d_split = dirpath.split('\\\\')\n",
    "                condition = d_split[-1].split('_')[1].lower()\n",
    "                n_split = name.split('_')\n",
    "                ch = int(n_split[-1][-5])\n",
    "\n",
    "                cycle = 'cycle' + d_split[-1].split('_')[2][-1]\n",
    "                try: marker = marker_dict[cycle][ch] \n",
    "                except: continue\n",
    "        \n",
    "                conditions.append(condition)\n",
    "                fovs.append('FW1')\n",
    "                cycles.append(cycle)\n",
    "                channels.append(ch)\n",
    "                markers.append(marker)\n",
    "                paths.append(os.path.join(dirpath, name))\n",
    "                \n",
    "    info = {\n",
    "            \"Condition\": conditions,\n",
    "            'FOV': fovs, \n",
    "            \"Cycle\": cycles,\n",
    "            \"Channels\": channels,\n",
    "            \"Markers\": markers,\n",
    "            \"Path\": paths\n",
    "        }\n",
    "\n",
    "    df = pd.DataFrame(info)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created df\n"
     ]
    }
   ],
   "source": [
    "df_meta_path = data_dir / 'indirect' / 'metadata' / 'info.csv'\n",
    "df_meta_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "df_exist = df_meta_path.is_file()\n",
    "\n",
    "if not df_exist:\n",
    "    print('Created df')\n",
    "    df = get_info(data_raw, markers_map)\n",
    "    df.to_csv(df_meta_path, index=False)\n",
    "else:\n",
    "    print('Loaded df')\n",
    "    df = pd.read_csv(df_meta_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Condition\n",
       "ctrl    6\n",
       "o       6\n",
       "dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('Condition').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Condition</th>\n",
       "      <th>FOV</th>\n",
       "      <th>Cycle</th>\n",
       "      <th>Channels</th>\n",
       "      <th>Markers</th>\n",
       "      <th>Path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ctrl</td>\n",
       "      <td>FW1</td>\n",
       "      <td>cycle1</td>\n",
       "      <td>1</td>\n",
       "      <td>DNA</td>\n",
       "      <td>Y:\\coskun-lab\\Shuangyi\\ERK, YAP project_2022\\P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ctrl</td>\n",
       "      <td>FW1</td>\n",
       "      <td>cycle1</td>\n",
       "      <td>4</td>\n",
       "      <td>5 Pairs</td>\n",
       "      <td>Y:\\coskun-lab\\Shuangyi\\ERK, YAP project_2022\\P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ctrl</td>\n",
       "      <td>FW1</td>\n",
       "      <td>cycle3</td>\n",
       "      <td>1</td>\n",
       "      <td>DNA</td>\n",
       "      <td>Y:\\coskun-lab\\Shuangyi\\ERK, YAP project_2022\\P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ctrl</td>\n",
       "      <td>FW1</td>\n",
       "      <td>cycle3</td>\n",
       "      <td>4</td>\n",
       "      <td>cdc25c/p38</td>\n",
       "      <td>Y:\\coskun-lab\\Shuangyi\\ERK, YAP project_2022\\P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ctrl</td>\n",
       "      <td>FW1</td>\n",
       "      <td>cycle4</td>\n",
       "      <td>1</td>\n",
       "      <td>DNA</td>\n",
       "      <td>Y:\\coskun-lab\\Shuangyi\\ERK, YAP project_2022\\P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ctrl</td>\n",
       "      <td>FW1</td>\n",
       "      <td>cycle4</td>\n",
       "      <td>3</td>\n",
       "      <td>Phalloidin</td>\n",
       "      <td>Y:\\coskun-lab\\Shuangyi\\ERK, YAP project_2022\\P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>o</td>\n",
       "      <td>FW1</td>\n",
       "      <td>cycle1</td>\n",
       "      <td>1</td>\n",
       "      <td>DNA</td>\n",
       "      <td>Y:\\coskun-lab\\Shuangyi\\ERK, YAP project_2022\\P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>o</td>\n",
       "      <td>FW1</td>\n",
       "      <td>cycle1</td>\n",
       "      <td>4</td>\n",
       "      <td>5 Pairs</td>\n",
       "      <td>Y:\\coskun-lab\\Shuangyi\\ERK, YAP project_2022\\P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>o</td>\n",
       "      <td>FW1</td>\n",
       "      <td>cycle3</td>\n",
       "      <td>1</td>\n",
       "      <td>DNA</td>\n",
       "      <td>Y:\\coskun-lab\\Shuangyi\\ERK, YAP project_2022\\P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>o</td>\n",
       "      <td>FW1</td>\n",
       "      <td>cycle3</td>\n",
       "      <td>4</td>\n",
       "      <td>cdc25c/p38</td>\n",
       "      <td>Y:\\coskun-lab\\Shuangyi\\ERK, YAP project_2022\\P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>o</td>\n",
       "      <td>FW1</td>\n",
       "      <td>cycle4</td>\n",
       "      <td>1</td>\n",
       "      <td>DNA</td>\n",
       "      <td>Y:\\coskun-lab\\Shuangyi\\ERK, YAP project_2022\\P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>o</td>\n",
       "      <td>FW1</td>\n",
       "      <td>cycle4</td>\n",
       "      <td>3</td>\n",
       "      <td>Phalloidin</td>\n",
       "      <td>Y:\\coskun-lab\\Shuangyi\\ERK, YAP project_2022\\P...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Condition  FOV   Cycle  Channels     Markers  \\\n",
       "0       ctrl  FW1  cycle1         1         DNA   \n",
       "1       ctrl  FW1  cycle1         4     5 Pairs   \n",
       "2       ctrl  FW1  cycle3         1         DNA   \n",
       "3       ctrl  FW1  cycle3         4  cdc25c/p38   \n",
       "4       ctrl  FW1  cycle4         1         DNA   \n",
       "5       ctrl  FW1  cycle4         3  Phalloidin   \n",
       "6          o  FW1  cycle1         1         DNA   \n",
       "7          o  FW1  cycle1         4     5 Pairs   \n",
       "8          o  FW1  cycle3         1         DNA   \n",
       "9          o  FW1  cycle3         4  cdc25c/p38   \n",
       "10         o  FW1  cycle4         1         DNA   \n",
       "11         o  FW1  cycle4         3  Phalloidin   \n",
       "\n",
       "                                                 Path  \n",
       "0   Y:\\coskun-lab\\Shuangyi\\ERK, YAP project_2022\\P...  \n",
       "1   Y:\\coskun-lab\\Shuangyi\\ERK, YAP project_2022\\P...  \n",
       "2   Y:\\coskun-lab\\Shuangyi\\ERK, YAP project_2022\\P...  \n",
       "3   Y:\\coskun-lab\\Shuangyi\\ERK, YAP project_2022\\P...  \n",
       "4   Y:\\coskun-lab\\Shuangyi\\ERK, YAP project_2022\\P...  \n",
       "5   Y:\\coskun-lab\\Shuangyi\\ERK, YAP project_2022\\P...  \n",
       "6   Y:\\coskun-lab\\Shuangyi\\ERK, YAP project_2022\\P...  \n",
       "7   Y:\\coskun-lab\\Shuangyi\\ERK, YAP project_2022\\P...  \n",
       "8   Y:\\coskun-lab\\Shuangyi\\ERK, YAP project_2022\\P...  \n",
       "9   Y:\\coskun-lab\\Shuangyi\\ERK, YAP project_2022\\P...  \n",
       "10  Y:\\coskun-lab\\Shuangyi\\ERK, YAP project_2022\\P...  \n",
       "11  Y:\\coskun-lab\\Shuangyi\\ERK, YAP project_2022\\P...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save hdf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "def save_hdf5(\n",
    "    path: str, name: str, data: np.ndarray, attr_dict=None, mode: str = \"a\"\n",
    ") -> None:\n",
    "    # Read h5 file\n",
    "    hf = h5py.File(path, mode)\n",
    "    # Create z_stack_dataset\n",
    "    if hf.get(name) is None:\n",
    "        data_shape = data.shape\n",
    "        data_type = data.dtype\n",
    "        max_shape = (data_shape[0],) + data_shape[1:]\n",
    "        dset = hf.create_dataset(\n",
    "            name,\n",
    "            shape=data_shape,\n",
    "            maxshape=max_shape,\n",
    "            chunks=True,\n",
    "            dtype=data_type,\n",
    "            compression=\"gzip\",\n",
    "        )\n",
    "        dset[:] = data\n",
    "        if attr_dict is not None:\n",
    "            for attr_key, attr_val in attr_dict.items():\n",
    "                dset.attrs[attr_key] = attr_val\n",
    "    else:\n",
    "        print(f\"Dataset {name} exists\")\n",
    "\n",
    "    hf.close()\n",
    "\n",
    "def joblib_loop(task, pics):\n",
    "    return Parallel(n_jobs=20)(delayed(task)(i) for i in pics)\n",
    "\n",
    "def read_img(path):\n",
    "    return skimage.io.imread(path, as_gray=True)\n",
    "\n",
    "def get_min(imgs):\n",
    "    shapes = np.array([np.array(img.shape) for img in imgs])\n",
    "    return np.min(shapes, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created df\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed06caa44705428a91b4700b2f1217fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\Anaconda3\\envs\\PLA2\\lib\\site-packages\\tqdm\\std.py:1178: FutureWarning: In a future version of pandas, a length 1 tuple will be returned when iterating over a groupby with a grouper equal to a list of length 1. Don't supply a list with a single grouper to avoid this warning.\n",
      "  for obj in iterable:\n"
     ]
    }
   ],
   "source": [
    "df_imgs_path = data_dir /  'indirect'   / 'metadata' / 'imgs.csv'\n",
    "df_imgs_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "temp_path = data_dir  / 'indirect'   /  'hdf5' / 'raw'\n",
    "temp_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "df_exist = df_imgs_path.is_file()\n",
    "\n",
    "\n",
    "if not df_exist:\n",
    "    print('Created df')\n",
    "    \n",
    "    group = df.groupby(['Condition'])\n",
    "    rows = []\n",
    "\n",
    "    for name, df_group in tqdm(group, total=len(group)):\n",
    "        file_name = name + '.hdf5'\n",
    "        file_path = temp_path / file_name\n",
    "        rows.append([name]+[file_path])\n",
    "        \n",
    "        channels = df_group.Channels.to_list()\n",
    "        cycles = df_group.Cycle.to_list()\n",
    "        markers = df_group.Markers.to_list()\n",
    "        paths = df_group.Path.to_numpy()\n",
    "            \n",
    "        imgs = joblib_loop(read_img, paths)\n",
    "        min_shape = get_min(imgs)\n",
    "        imgs_cropped = np.array([img[:min_shape[0], :min_shape[1]] for img in imgs])\n",
    "        info = {\"Cycle\": cycles, \"Channel\": channels, \"Marker\": markers}\n",
    "            \n",
    "            # hdf5 as Channel -> Z mapping\n",
    "        save_hdf5(file_path, 'imgs', imgs_cropped, info)\n",
    "    df_imgs = pd.DataFrame(rows, columns=['Condition', 'Path'])        \n",
    "    df_imgs.to_csv(df_imgs_path, index=False)\n",
    "else:\n",
    "    print('Loaded df')\n",
    "    df_imgs = pd.read_csv(df_imgs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Condition</th>\n",
       "      <th>Path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ctrl</td>\n",
       "      <td>y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\indi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>o</td>\n",
       "      <td>y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\indi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Condition                                               Path\n",
       "0      ctrl  y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\indi...\n",
       "1         o  y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\indi..."
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_imgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Registration ImageJ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Registration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tifffile as tf\n",
    "from PIL import Image\n",
    "import PIL.Image\n",
    "PIL.Image.MAX_IMAGE_PIXELS = 933120000\n",
    "import shutil\n",
    "from datetime import date, datetime\n",
    "import skimage.io \n",
    "from skimage import util\n",
    "from skimage.transform import resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import exposure, util\n",
    "\n",
    "def contrast_str(img, n_min=0.1, n_max=100):\n",
    "    p2, p98 = np.percentile(img, (n_min, n_max))\n",
    "    img_rescale = exposure.rescale_intensity(img, in_range=(p2, p98))\n",
    "    img_rescale = util.img_as_ubyte(img_rescale)\n",
    "    return img_rescale\n",
    "\n",
    "def make_imgs_same_dim(imgs):\n",
    "    # Get max dimensions\n",
    "    shapes = np.array([img.shape[1:] for img in imgs])\n",
    "    min_x, min_y = shapes.min(axis=0)\n",
    "    imgs_cropped = [img[:, :min_x, :min_y] for img in imgs]\n",
    "    # imgs_cropped[0] = contrast_str(imgs_cropped[0])\n",
    "    return imgs_cropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thu71\\AppData\\Local\\Temp\\ipykernel_63484\\2197527561.py:9: FutureWarning: In a future version of pandas, a length 1 tuple will be returned when iterating over a groupby with a grouper equal to a list of length 1. Don't supply a list with a single grouper to avoid this warning.\n",
      "  for cycle, df_cycle in df_group.groupby(['Cycle']):\n",
      "C:\\Users\\thu71\\AppData\\Local\\Temp\\ipykernel_63484\\2197527561.py:9: FutureWarning: In a future version of pandas, a length 1 tuple will be returned when iterating over a groupby with a grouper equal to a list of length 1. Don't supply a list with a single grouper to avoid this warning.\n",
      "  for cycle, df_cycle in df_group.groupby(['Cycle']):\n"
     ]
    }
   ],
   "source": [
    "regSavePath = data_dir / 'indirect' /'imgs' / 'registered_imagej'\n",
    "regSavePath.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "chs = [1, 2, 3, 4]\n",
    "group = df.groupby(['Condition', 'FOV'])\n",
    "for name, df_group in group:\n",
    "    path = df_group.iloc[0].Path\n",
    "    \n",
    "    for cycle, df_cycle in df_group.groupby(['Cycle']):\n",
    "        cycle = cycle[-1]\n",
    "        channel = df_cycle.Channels.tolist()\n",
    "        imgs = [cv2.imread(f, cv2.IMREAD_GRAYSCALE) for f in df_cycle.Path.tolist()]\n",
    "        \n",
    "        for ch in chs: \n",
    "            # Save path per Channel\n",
    "            folderPath = os.path.join(regSavePath, '_'.join(name), 'Original', 'CH' + str(ch)) # 1 index\n",
    "            if not os.path.exists(folderPath):\n",
    "                os.makedirs(folderPath, exist_ok = True)\n",
    "            \n",
    "            fileOut = 'CH' + str(ch) + '_Cycle' + str(cycle).zfill(2) + '.tif'\n",
    "            fileOut = os.path.join(folderPath, fileOut)\n",
    "            if os.path.exists(fileOut):\n",
    "                continue\n",
    "\n",
    "            if ch in channel:\n",
    "                if ch == 1:\n",
    "                    img = contrast_str(imgs[list(channel).index(ch)], n_min=0.1, n_max=99.9)\n",
    "                else:\n",
    "                    img = imgs[list(channel).index(ch)]\n",
    "                tf.imwrite(fileOut, img, photometric = 'minisblack', bigtiff = True)\n",
    "\n",
    "            else:\n",
    "                emptyImage = np.zeros(imgs[0].shape, np.uint8)\n",
    "                # print('Dont exist create empty image', cycle, ch)\n",
    "                tf.imwrite(fileOut, emptyImage, photometric = 'minisblack', bigtiff = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runMacro(\"y:/coskun-lab/Thomas/23_PLA_revision/data/indirect/imgs/registered_imagej/ctrl_FW1/Original/CH1/26Jan2024_register_transforms.ijm\");\n",
      "runMacro(\"y:/coskun-lab/Thomas/23_PLA_revision/data/indirect/imgs/registered_imagej/o_FW1/Original/CH1/26Jan2024_register_transforms.ijm\");\n"
     ]
    }
   ],
   "source": [
    "group = df.groupby(['Condition', 'FOV'])\n",
    "chs = [1, 2, 3, 4]\n",
    "\n",
    "for name, channels in group:\n",
    "    name = '_'.join(name)\n",
    "    '''\n",
    "    run(\"Register Virtual Stack Slices\", \"source=[Y:/coskun-lab/Nicky/07 Temp/register large stitch] output=[Y:/coskun-lab/Nicky/07 Temp/register output] feature=Rigid registration=[Rigid                -- translate + rotate                  ] advanced shrinkage save save_dir=[Y:/coskun-lab/Nicky/07 Temp/register output] initial_gaussian_blur=1.60 steps_per_scale_octave=3 minimum_image_size=64 maximum_image_size=1024 feature_descriptor_size=8 feature_descriptor_orientation_bins=8 closest/next_closest_ratio=0.92 maximal_alignment_error=25 inlier_ratio=0.05 feature_extraction_model=Rigid registration_model=[Rigid                -- translate + rotate                  ] interpolate\");\n",
    "    run(\"Transform Virtual Stack Slices\", \"source = [Y:/coskun-lab/Nicky/07 Temp/other channels/original] output = [Y:/coskun-lab/Nicky/07 Temp/other channels/original] transforms = [Y:/coskun-lab/Nicky/07 Temp/register output] interpolate\");\n",
    "    '''\n",
    "    # folder to save registered images separated by channel to apply transforms\n",
    "    # create all folder\n",
    "    for ii, ch in enumerate(chs): # all channels\n",
    "        os.makedirs(os.path.join(regSavePath, name, 'Original', 'CH' + str(ch)), exist_ok = True)\n",
    "        os.makedirs(os.path.join(regSavePath, name, 'Registered', 'CH' + str(ch)), exist_ok = True)\n",
    "    \n",
    "    os.chdir(os.path.join(regSavePath, name, 'Original', 'CH1'))\n",
    "    now = datetime.now() # current date and time\n",
    "    date_time = now.strftime(\"%d%b%Y\")\n",
    "    macro = open(date_time + '_register_transforms.ijm', 'w')\n",
    "    \n",
    "    # register cycles on CH1\n",
    "    macro.write('run(\"Register Virtual Stack Slices\", \"source=[')\n",
    "    # original files\n",
    "    macro.write(os.path.join(regSavePath, name, 'Original', 'CH1').replace('\\\\', '/'))\n",
    "    macro.write('] output=[')\n",
    "    # registered output files\n",
    "    macro.write(os.path.join(regSavePath, name, 'Registered', 'CH1').replace('\\\\', '/'))\n",
    "    \n",
    "    # Rigid registration: translation + rotation\n",
    "    macro.write('] feature=Rigid registration=[Rigid                -- translate + rotate                  ] advanced shrinkage save save_dir=[')\n",
    "    # folder to save recorded transformations \n",
    "    macro.write('] initial_gaussian_blur=1.60 steps_per_scale_octave=3 minimum_image_size=64 maximum_image_size=1024 feature_descriptor_size=8 feature_descriptor_orientation_bins=8 closest/next_closest_ratio=0.92 maximal_alignment_error=25 inlier_ratio=0.05 feature_extraction_model=Rigid registration_model=[Rigid                -- translate + rotate                  ] interpolate\"); \\n')\n",
    "    \n",
    "    # # bigwrap registration\n",
    "    # macro.write('] feature=Similarity registration=[Elastic              -- bUnwarpJ splines                    ] advanced shrinkage save save_dir=[')\n",
    "    # # folder to save recorded transformations \n",
    "    # macro.write('] initial_gaussian_blur=1.60 steps_per_scale_octave=3 minimum_image_size=64 maximum_image_size=1024 feature_descriptor_size=8 feature_descriptor_orientation_bins=8 closest/next_closest_ratio=0.92 maximal_alignment_error=25 inlier_ratio=0.05 feature_extraction_model=Similarity registration_model=[[Elastic              -- bUnwarpJ splines                    ] interpolate registration=Mono image_subsample_factor=0 initial_deformation=[Very Coarse] final_deformation=Fine divergence_weight=0.1 curl_weight=0.1 landmark_weight=1 image_weight=0 consistency_weight=0 stop_threshold=0.01 shear=0.95 scale=0.95 isotropy=1\"); \\n')\n",
    "    \n",
    "    # Or use similarity: translation + rotation + isotropic scale\n",
    "    # macro.write('] feature=Similarity registration=[Similarity           -- translate + rotate + isotropic scale] advanced shrinkage save save_dir=[')\n",
    "    # # folder to save recorded transformations \n",
    "    # macro.write('] initial_gaussian_blur=1.60 steps_per_scale_octave=3 minimum_image_size=64 maximum_image_size=1024 feature_descriptor_size=25 feature_descriptor_orientation_bins=8 closest/next_closest_ratio=0.92 maximal_alignment_error=50 inlier_ratio=0.05 feature_extraction_model=Similarity registration_model=[Similarity           -- translate + rotate + isotropic scale] interpolate\"); \\n')\n",
    "    \n",
    "    macro.write('run(\"Close All\"); \\n\\n')\n",
    "    \n",
    "    # now apply transform to other channels\n",
    "    for ii, ch in enumerate([1,2,3,4]): # each other channel\n",
    "        \n",
    "        macro.write('run(\"Transform Virtual Stack Slices\", \"source=[')\n",
    "        # unregsitered folder\n",
    "        macro.write(os.path.join(regSavePath, name, 'Original', 'CH' + str(ch)).replace('\\\\', '/'))\n",
    "        macro.write('] output=[')\n",
    "        # registered folder\n",
    "        macro.write(os.path.join(regSavePath, name, 'Registered', 'CH' + str(ch)).replace('\\\\', '/'))\n",
    "        macro.write('] transforms=[')\n",
    "        macro.write(os.path.join(regSavePath, name, 'Original', 'CH1').replace('\\\\', '/')) # stored in original registration folder\n",
    "        macro.write('] interpolate\"); \\n')\n",
    "        macro.write('run(\"Close All\"); \\n\\n')\n",
    "    \n",
    "    macro.close()\n",
    "    \n",
    "    # print command to run macro\n",
    "    print('runMacro(\"' + os.path.join(regSavePath, name, 'Original', 'CH1', macro.name).replace('\\\\', '/') + '\");')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine all registered images into single folder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "regSavePath = data_dir / 'indirect' /'imgs' / 'registered_imagej'\n",
    "\n",
    "regSaveFinalPath = data_dir /'indirect' / 'imgs' / 'registered_imagej_final'\n",
    "regSaveFinalPath.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "regSaveCropPath = data_dir / 'indirect' / 'imgs' /  'registered_crop'\n",
    "regSaveCropPath.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d98a9ef123ae49e898932fe33a2ea703",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0aecf9663504e629d1b21278462b26a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dfgroup = df.groupby(['Condition', 'FOV'])\n",
    "\n",
    "for name, channels in group:\n",
    "    name = '_'.join(name)\n",
    "    for ii, cycle in enumerate(tqdm(channels['Cycle'].unique())): # each cycle\n",
    "    \n",
    "        dfCycle = channels.loc[channels['Cycle'] == cycle]\n",
    "        dfCycle.reset_index(drop = True, inplace = True) # index is channel - 1\n",
    "        cycle = cycle[5:]\n",
    "        for jj, ch in enumerate(dfCycle.Channels): # each channel\n",
    "            \n",
    "            # find registered file\n",
    "            tifPath = os.path.join(regSavePath, name, 'Registered', 'CH' + str(ch), 'CH' + str(ch)+ '_Cycle' + str(cycle).zfill(2) + '.tif')\n",
    "\n",
    "            # File out\n",
    "            fileOut = 'Cycle' + str(cycle).zfill(2) + \\\n",
    "            '_' + 'CH' + str(ch) + '.tif'\n",
    "            folder = regSaveFinalPath / name\n",
    "            folder.mkdir(parents=True, exist_ok=True)\n",
    "            fileOut = os.path.join(regSaveFinalPath, name, fileOut)\n",
    "            # print(tifPath)\n",
    "            # Copy\n",
    "            if os.path.exists(tifPath):\n",
    "                shutil.copyfile(tifPath, fileOut)\n",
    "            else:\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Cropped image to smallest bounding box of non black region\n",
    "\n",
    "# Get channel list\n",
    "group = df.groupby(['Condition', 'FOV'])\n",
    "\n",
    "for name, df_group in group:\n",
    "    channels = df_group.Channels.tolist()\n",
    "    break\n",
    "\n",
    "# Crop\n",
    "for dir in os.listdir(regSaveFinalPath):\n",
    "\n",
    "    # Read imgs\n",
    "    imgs = []\n",
    "    paths = []\n",
    "    for file in os.listdir(regSaveFinalPath / dir):\n",
    "        if 'tif' in file:\n",
    "            path = regSaveFinalPath / dir/ file\n",
    "            imgs.append(tiff.imread(path))\n",
    "            paths.append(file)\n",
    "\n",
    "    # Get bboxs\n",
    "    bboxs = []\n",
    "    for i, img in enumerate(imgs):\n",
    "        if channels[i] != 1:\n",
    "            continue\n",
    "        bbox = skimage.measure.regionprops((img>0).astype(np.uint8))[0]['bbox']\n",
    "        bboxs.append(np.array(bbox))\n",
    "    bboxs = np.stack(bboxs)\n",
    "\n",
    "    bbox_final = [np.max(bboxs[:,0]),\n",
    "                np.max(bboxs[:,1]),\n",
    "                np.min(bboxs[:,2]),\n",
    "                np.min(bboxs[:,3])]\n",
    "\n",
    "    min_row, min_col, max_row, max_col = bbox_final\n",
    "\n",
    "    # Save cropped images\n",
    "    save_dir = regSaveCropPath / dir\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "    for i, img in enumerate(imgs):\n",
    "        save_path = save_dir / paths[i]\n",
    "        tiff.imwrite(save_path, img[min_row:max_row, min_col:max_col], bigtiff = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import util\n",
    "import h5py\n",
    "\n",
    "def get_info(data_raw, marker_dict):\n",
    "    conditions = []\n",
    "    fovs = []\n",
    "    cycles = []\n",
    "    channels = []\n",
    "    markers = []\n",
    "    paths = [] \n",
    "\n",
    "    # Loop through image folder\n",
    "    for (dirpath, dirnames, filenames) in os.walk(data_raw):\n",
    "        for name in sorted(filenames):\n",
    "            if \"tif\" in name:\n",
    "                # Get information from image name\n",
    "                n_split = name.split('_')\n",
    "                                \n",
    "                cond=dirpath.split('\\\\')[-1].split('_')[0]\n",
    "                fov=dirpath.split('\\\\')[-1].split('_')[1]\n",
    "                cycle='cycle'+str(int(n_split[0][-2:]))\n",
    "                ch = int(n_split[1][2])\n",
    "                try:\n",
    "                    marker = marker_dict[cycle][ch]\n",
    "                except:\n",
    "                    continue \n",
    "                    \n",
    "                conditions.append(cond)\n",
    "                fovs.append(fov)\n",
    "                cycles.append(cycle)\n",
    "                channels.append(ch)\n",
    "                markers.append(marker)\n",
    "                paths.append(os.path.join(dirpath, name))\n",
    "                \n",
    "    info = {\n",
    "            \"Condition\": conditions,\n",
    "            \"FOV\": fovs,\n",
    "            \"Cycle\": cycles,\n",
    "            \"Channels\": channels,\n",
    "            \"Markers\": markers,\n",
    "            \"Path\": paths\n",
    "        }\n",
    "\n",
    "    df = pd.DataFrame(info)\n",
    "    return df\n",
    "\n",
    "def save_hdf5(\n",
    "    path: str, name: str, data: np.ndarray, attr_dict=None, mode: str = \"a\"\n",
    ") -> None:\n",
    "    # Read h5 file\n",
    "    hf = h5py.File(path, mode)\n",
    "    # Create z_stack_dataset\n",
    "    if hf.get(name) is None:\n",
    "        data_shape = data.shape\n",
    "        data_type = data.dtype\n",
    "        max_shape = (data_shape[0],) + data_shape[1:]\n",
    "        dset = hf.create_dataset(\n",
    "            name,\n",
    "            shape=data_shape,\n",
    "            maxshape=max_shape,\n",
    "            chunks=True,\n",
    "            dtype=data_type,\n",
    "            compression=\"gzip\",\n",
    "        )\n",
    "        dset[:] = data\n",
    "        if attr_dict is not None:\n",
    "            for attr_key, attr_val in attr_dict.items():\n",
    "                dset.attrs[attr_key] = attr_val\n",
    "    else:\n",
    "        print(f\"Dataset {name} exists\")\n",
    "\n",
    "    hf.close()\n",
    "\n",
    "def read_img(path):\n",
    "    return skimage.io.imread(path, as_gray=True)\n",
    "\n",
    "def joblib_loop(task, pics):\n",
    "    return Parallel(n_jobs=20)(delayed(task)(i) for i in pics)\n",
    "\n",
    "def read_img(path):\n",
    "    return skimage.io.imread(path, as_gray=True)\n",
    "\n",
    "def get_min(imgs):\n",
    "    shapes = np.array([np.array(img.shape) for img in imgs])\n",
    "    return np.min(shapes, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder is already there\n",
      "Created df\n"
     ]
    }
   ],
   "source": [
    "data_raw = data_dir /'indirect'/ 'imgs' /  'registered_crop'\n",
    "df_meta_path = data_dir / 'indirect'/ 'metadata' / 'info_sti.csv'\n",
    "\n",
    "try:\n",
    "    df_meta_path.parent.mkdir(parents=True, exist_ok=False)\n",
    "except FileExistsError:\n",
    "    print(\"Folder is already there\")\n",
    "\n",
    "df_exist = df_meta_path.is_file()\n",
    "\n",
    "if not df_exist:\n",
    "    print('Created df')\n",
    "    df = get_info(data_raw, markers_map)\n",
    "    df.to_csv(df_meta_path, index=False)\n",
    "else:\n",
    "    print('Loaded df')\n",
    "    df = pd.read_csv(df_meta_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Condition\n",
       "ctrl    6\n",
       "o       6\n",
       "dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('Condition').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created df\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd419ee39428451fa504525ee718a53e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_imgs_path = data_dir /'indirect'/ 'metadata' / 'imgs_reg.csv'\n",
    "temp_path =data_dir /'indirect' / 'hdf5' / 'registered'\n",
    "try:\n",
    "    temp_path.mkdir(parents=True, exist_ok=False)\n",
    "except FileExistsError:\n",
    "    print(\"Folder is already there\")\n",
    "\n",
    "df_exist = df_imgs_path.is_file()\n",
    "\n",
    "if not df_exist:\n",
    "    print('Created df')\n",
    "    group = df.groupby(['Condition','FOV'])\n",
    "    rows = []\n",
    "\n",
    "    for name, df_group in tqdm(group, total=len(group)):\n",
    "        file_name = '_'.join(np.array(name).astype(str)) + '.hdf5'\n",
    "        file_path = temp_path / file_name\n",
    "        rows.append(list(name)+[file_path])\n",
    "        \n",
    "        # if file_path.exists():\n",
    "        #     continue\n",
    "        \n",
    "        channels = df_group.Channels.to_list()\n",
    "        cycles = df_group.Cycle.to_list()\n",
    "        markers = df_group.Markers.to_list()\n",
    "        paths = df_group.Path.to_numpy()\n",
    "            \n",
    "        imgs = joblib_loop(read_img, paths)\n",
    "        min_shape = get_min(imgs)\n",
    "        imgs_cropped = np.array([img[:min_shape[0], :min_shape[1]] for img in imgs])\n",
    "        info = {\"Cycle\": cycles, \"Channel\": channels, \"Marker\": markers}\n",
    "\n",
    "        imgs_cropped = util.img_as_ubyte(imgs_cropped)\n",
    "        \n",
    "        # hdf5 as Channel -> Z mapping\n",
    "        save_hdf5(file_path, 'imgs', imgs_cropped, info)\n",
    "    df_imgs = pd.DataFrame(rows, columns=['Condition', 'FOV', 'Path'])        \n",
    "    df_imgs.to_csv(df_imgs_path, index=False)\n",
    "else:\n",
    "    print('Loaded df')\n",
    "    df_imgs = pd.read_csv(df_imgs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Condition</th>\n",
       "      <th>FOV</th>\n",
       "      <th>Path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ctrl</td>\n",
       "      <td>FW1</td>\n",
       "      <td>y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\indi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>o</td>\n",
       "      <td>FW1</td>\n",
       "      <td>y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\indi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Condition  FOV                                               Path\n",
       "0      ctrl  FW1  y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\indi...\n",
       "1         o  FW1  y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\indi..."
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_imgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import napari \n",
    "from skimage import exposure, util\n",
    "\n",
    "def contrast_str(img, n_min=0.1, n_max=99.9):\n",
    "    p2, p98 = np.percentile(img, (n_min, n_max))\n",
    "    img_rescale = exposure.rescale_intensity(img, in_range=(p2, p98))\n",
    "    img_rescale = util.img_as_ubyte(img_rescale)\n",
    "    return img_rescale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imgs_path = data_dir /'indirect' /'metadata' / 'imgs_reg.csv'\n",
    "df_imgs = pd.read_csv(df_imgs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save combined images\n",
    "for row in df_imgs.itertuples():\n",
    "    # Read image\n",
    "    path = row.Path\n",
    "    with h5py.File(path, \"r\") as f:\n",
    "        imgs = f['imgs'][:]\n",
    "        markers = f['imgs'].attrs['Marker']\n",
    "\n",
    "    napari.view_image(imgs, name=markers, channel_axis=0, visible=False)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['DNA', '5 Pairs', 'DNA', 'cdc25c/p38', 'DNA', 'Phalloidin'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "markers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "cyto_markers = ['Phalloidin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_seg_path = data_dir / 'indirect'/ 'imgs' / 'segmentation'\n",
    "whole_seg_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "# Save combined images\n",
    "for row in df_imgs.itertuples():\n",
    "    # Read image\n",
    "    path = row.Path\n",
    "    with h5py.File(path, \"r\") as f:\n",
    "        imgs = f['imgs'][:]\n",
    "        markers = f['imgs'].attrs['Marker']\n",
    "\n",
    "    # Get dapi and cyto imgaes\n",
    "    indices = np.isin(markers, cyto_markers)\n",
    "    img_dapi = imgs[0]\n",
    "    imgs_cyto = imgs[indices,:]\n",
    "    \n",
    "    # Contrast streching and combine to rgb image\n",
    "    img_dapi = contrast_str(img_dapi, n_max=99.9)\n",
    "    imgs_cyto_scaled = [contrast_str(imgs_cyto[0], n_max=99.5)]\n",
    "    img_cyto = np.max(np.array(imgs_cyto_scaled), axis=0)\n",
    "    img_rgb = np.stack([np.zeros(img_dapi.shape),img_cyto, img_dapi], axis=0).astype(np.uint8)\n",
    "    \n",
    "    # Crop and save\n",
    "    file_name = f'{\"_\".join(row[1:3])}.tif'\n",
    "    file_path = whole_seg_path / file_name\n",
    "    tiff.imwrite(file_path, img_rgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QC masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import morphology, measure\n",
    "from skimage.segmentation import clear_border\n",
    "from collections import defaultdict\n",
    "   \n",
    "def count_pixel_label_mask(regionmask, intensity_image):\n",
    "    v,c = np.unique(intensity_image[regionmask], return_counts=True)\n",
    "    return dict(zip(v,c))\n",
    "    \n",
    "# Quality control of mask\n",
    "def qc_nuclei(mask_cyto, mask_nuclei, small_size=10000):\n",
    "    '''\n",
    "    Function to check if cell masks contain nuclei\n",
    "    '''\n",
    "    # Dictionnary storing nuclei and cyto label to cell id \n",
    "    nuclei2cell = {}\n",
    "    cyto2cell = {}\n",
    "    \n",
    "    # Filter out small objects\n",
    "    mask_cyto = morphology.remove_small_objects(mask_cyto,  min_size=small_size)\n",
    "    \n",
    "    # Filter out mask touching border\n",
    "    mask_cyto = clear_border(mask_cyto)\n",
    "    \n",
    "    # Filtered only cell mask region\n",
    "    cell_mask = np.where(mask_cyto > 0, 1, 0)\n",
    "    mask_nuclei_filtered = mask_nuclei * cell_mask\n",
    "    mask_nuclei_filtered =  morphology.remove_small_objects(mask_nuclei_filtered,  min_size=2000)\n",
    "    \n",
    "    nuclei_mask = np.where(mask_nuclei>0, 1, 0)\n",
    "    cyto = (mask_cyto - mask_cyto*nuclei_mask).astype(np.uint16)\n",
    "    \n",
    "    # Count pixel cell label in each nuclei region to assign each nuclei to cell\n",
    "    props = measure.regionprops(mask_nuclei_filtered, intensity_image=mask_cyto, \n",
    "                    extra_properties=(count_pixel_label_mask,))\n",
    "    nuclei_labels = []\n",
    "    cell_labels = []\n",
    "    for prop in props:\n",
    "        df = pd.DataFrame.from_dict(prop['count_pixel_label_mask'], orient='index').reset_index()\n",
    "        df.columns = ['Label', 'Count']\n",
    "        corresponding_label = df.iloc[df['Count'].argmax(axis=0)]['Label']\n",
    "        nuclei_labels.append(prop['Label'])\n",
    "        cell_labels.append(corresponding_label)\n",
    "    \n",
    "    df = pd.DataFrame({'Nuclei': nuclei_labels, 'Cyto': cell_labels})\n",
    "    return mask_cyto, mask_nuclei_filtered, cyto, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read masks paths \n",
    "mask_dir = data_dir /'indirect'/ 'imgs' / 'masks'\n",
    "mask_filt_dir = data_dir /'indirect' / 'imgs' / 'masks_filtered'\n",
    "mask_filt_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "masks_path = defaultdict(dict) \n",
    "for path in os.listdir(mask_dir):\n",
    "    name = path.split('.')[0]\n",
    "    if 'Nuclei' in name:\n",
    "        masks_path[name[7:]]['nuclei'] = mask_dir / path\n",
    "    else:\n",
    "        masks_path[name]['cyto'] = mask_dir / path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in df_imgs.itertuples():\n",
    "    # Read image\n",
    "    path = row.Path\n",
    "    name = '_'.join([row.Condition, row.FOV])\n",
    "    \n",
    "    # Read masks\n",
    "    mask_cyto_path = masks_path[name]['cyto']\n",
    "    mask_nuclei_path = masks_path[name]['nuclei']\n",
    "    \n",
    "    mask_cyto = skimage.io.imread(mask_cyto_path)\n",
    "    mask_nuclei = skimage.io.imread(mask_nuclei_path)\n",
    "    mask_nuclei = mask_nuclei[:mask_cyto.shape[0], :mask_cyto.shape[1]]\n",
    "    cell, nuclei, cyto, df = qc_nuclei(mask_cyto, mask_nuclei)\n",
    "    \n",
    "    file_path =  mask_filt_dir / f'Nuclei_{name}.tif'\n",
    "    tiff.imwrite(file_path, nuclei)\n",
    "    file_path =  mask_filt_dir / f'Cell_{name}.tif'\n",
    "    tiff.imwrite(file_path, cell)\n",
    "    file_path =  mask_filt_dir / f'Cyto_{name}.tif'\n",
    "    tiff.imwrite(file_path, cyto)\n",
    "    file_path =  mask_filt_dir / f'df_{name}.csv'\n",
    "    df.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quanfitication PPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PLA\n",
    "\n",
    "PPI_save_path =  data_dir /  'indirect' / 'PPI'\n",
    "PPI_save_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "PPI_imgs_path =  data_dir /  'indirect'  / 'PPI_imgs'\n",
    "PPI_imgs_path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imgs_path = data_dir / 'indirect' /'metadata' / 'imgs_reg.csv'\n",
    "df_imgs = pd.read_csv(df_imgs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DNA' '5 Pairs' 'DNA' 'cdc25c/p38' 'DNA' 'Phalloidin']\n",
      "['DNA' '5 Pairs' 'DNA' 'cdc25c/p38' 'DNA' 'Phalloidin']\n"
     ]
    }
   ],
   "source": [
    "for row in df_imgs.itertuples():\n",
    "    path = row.Path\n",
    "    with h5py.File(path, 'r') as f:\n",
    "        markers = f['imgs'].attrs['Marker']\n",
    "    print(markers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Condition</th>\n",
       "      <th>FOV</th>\n",
       "      <th>Path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ctrl</td>\n",
       "      <td>FW1</td>\n",
       "      <td>y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\indi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>o</td>\n",
       "      <td>FW1</td>\n",
       "      <td>y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\indi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Condition  FOV                                               Path\n",
       "0      ctrl  FW1  y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\indi...\n",
       "1         o  FW1  y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\indi..."
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "PPI_markers = ['5 Pairs', 'cdc25c/p38']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading image 5 Pairs\n",
      "Processing image 5 Pairs\n",
      "(array([0, 1], dtype=uint8), array([27455115,  4912885], dtype=int64))\n",
      "Reading image cdc25c/p38\n",
      "Processing image cdc25c/p38\n",
      "(array([0, 1], dtype=uint8), array([30428821,  1939179], dtype=int64))\n",
      "File exist. Deleted\n",
      "Reading image 5 Pairs\n",
      "Processing image 5 Pairs\n",
      "(array([0, 1], dtype=uint8), array([26943485,  5280497], dtype=int64))\n",
      "Reading image cdc25c/p38\n",
      "Processing image cdc25c/p38\n",
      "(array([0, 1], dtype=uint8), array([31287750,   936232], dtype=int64))\n",
      "File exist. Deleted\n"
     ]
    }
   ],
   "source": [
    "thresholds = [0.01, 0.005, 0.01, 0.01, 0.01]\n",
    "max_rad = [6, 10, 10, 10, 10]\n",
    "# m_rad = [4, 10, 10, 10, 10]\n",
    "\n",
    "for row in df_imgs.itertuples():\n",
    "    # Read image\n",
    "    path = row.Path\n",
    "    pla_detect = PLA.PLA_detection(path, name='imgs', m='Marker')\n",
    "    \n",
    "    with h5py.File(path, 'r') as f:\n",
    "        markers = f['imgs'].attrs['Marker']\n",
    "        \n",
    "    imgs_spots = []\n",
    "    imgs_wths = []\n",
    "    imgs_raw = []\n",
    "    for i,RNA in enumerate(PPI_markers): \n",
    "        if RNA in markers:\n",
    "            img_spot, img_wth, _, img = pla_detect.detect_spot(RNA, thres=thresholds[i], min_radius=2, max_radius=max_rad[i])\n",
    "            imgs_spots.append(img_spot)\n",
    "            imgs_wths.append(img_wth)\n",
    "            imgs_raw.append(img)\n",
    "\n",
    "    # Save imgs\n",
    "    file_path = PPI_imgs_path / (row[1] + '_raw.tiff')\n",
    "    tiff.imwrite(file_path, imgs_raw)\n",
    "    file_path = PPI_imgs_path / (row[1] + '_processed.tiff')\n",
    "    tiff.imwrite(file_path, imgs_wths)\n",
    "    file_path = PPI_imgs_path / (row[1] + '_detected.tiff')\n",
    "    tiff.imwrite(file_path, imgs_spots)\n",
    "\n",
    "    # Save PPI dict\n",
    "    name = row[1] +'.pkl'\n",
    "    pla_detect.save_pickle(PPI_save_path / name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract per cell PPI count "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "def read_PPI(path):\n",
    "    with open(path, 'rb') as file:\n",
    "        PPI_dict = pickle.load(file)\n",
    "\n",
    "    return PPI_dict\n",
    "\n",
    "def create_PPI_df(PPI_labels, PPI_loc, name, cyto=True):\n",
    "    if cyto:\n",
    "        columns_name = ['Cyto', 'x', 'y']\n",
    "    else:\n",
    "        columns_name = ['Nuclei', 'x', 'y']\n",
    "    df = pd.DataFrame(np.hstack([PPI_labels[:,np.newaxis], PPI_loc]), \n",
    "                      columns=columns_name)\n",
    "    df['PPI'] = name\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imgs_path = data_dir / 'indirect' /'metadata' / 'imgs_reg.csv'\n",
    "df_imgs = pd.read_csv(df_imgs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_filt_dir = data_dir / 'indirect' / 'imgs' / 'masks_filtered'\n",
    "PPI_save_path =  data_dir / 'indirect' / 'PPI'\n",
    "\n",
    "masks_path = defaultdict(dict) \n",
    "for path in os.listdir(mask_filt_dir):\n",
    "    name = path.split('.')[0]\n",
    "    if 'Nuclei' in name:\n",
    "        masks_path[name[7:]]['nuclei'] = mask_filt_dir / path\n",
    "    elif 'Cyto' in name:\n",
    "        masks_path[name[5:]]['cyto'] =mask_filt_dir / path\n",
    "    elif 'Cell' in name:\n",
    "        masks_path[name[5:]]['cell'] =mask_filt_dir / path    \n",
    "    elif 'df' in name:\n",
    "        masks_path[name[3:]]['df'] =mask_filt_dir / path\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in df_imgs.itertuples():\n",
    "    name = '_'.join(row[1:3])\n",
    "    \n",
    "    # Read masks\n",
    "    mask_cyto_path = masks_path[name]['cell']\n",
    "    mask_nuclei_path = masks_path[name]['nuclei']\n",
    "    df_path =  masks_path[name]['df']\n",
    "    \n",
    "    mask_cyto = skimage.io.imread(mask_cyto_path)\n",
    "    mask_nuclei = skimage.io.imread(mask_nuclei_path)\n",
    "    df_cell_info = pd.read_csv(df_path)\n",
    "    nuclei2cell = dict(zip(df_cell_info.iloc[:,0], df_cell_info.iloc[:,1]))   \n",
    "    \n",
    "    # Read PPi\n",
    "    PPI_dict = read_PPI(PPI_save_path / f'{row[1]}.pkl')\n",
    "    dfs_PPI_cyto = []\n",
    "    dfs_PPI_nuclei = []\n",
    "    for k in PPI_dict.keys():\n",
    "        PPI_loc = PPI_dict[k][:, 1:3].astype(np.uint32)\n",
    "        \n",
    "        # Cyto\n",
    "        PPI_labels = mask_cyto[PPI_loc[:,0], PPI_loc[:,1]]\n",
    "        df_PPI = create_PPI_df(PPI_labels, PPI_loc, k)\n",
    "        dfs_PPI_cyto.append(df_PPI)\n",
    "        \n",
    "        # Nuclei\n",
    "        PPI_labels = mask_nuclei[PPI_loc[:,0], PPI_loc[:,1]]\n",
    "        df_PPI = create_PPI_df(PPI_labels, PPI_loc, k, cyto=False)\n",
    "        dfs_PPI_nuclei.append(df_PPI)\n",
    "    \n",
    "    # Combined DFs\n",
    "    df_PPI_cyto = pd.concat(dfs_PPI_cyto)\n",
    "    df_PPI_nuclei = pd.concat(dfs_PPI_nuclei)\n",
    "    df_PPI_nuclei['Nuclei_Cell'] = df_PPI_nuclei['Nuclei'].apply(lambda x: nuclei2cell.get(x,x))   \n",
    "    df_merged = df_PPI_cyto.merge(df_PPI_nuclei)\n",
    "    df_merged['Condition'] = row[1]\n",
    "    df_merged['FOV'] = row[2]\n",
    "    \n",
    "    # Save dataframe\n",
    "    path = PPI_save_path / f'{name}.csv'\n",
    "    df_merged.to_csv(path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract intensity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "def read_PPI(path):\n",
    "    with open(path, 'rb') as file:\n",
    "        PPI_dict = pickle.load(file)\n",
    "\n",
    "    return PPI_dict\n",
    "\n",
    "def create_PPI_df(PPI_labels, PPI_loc, intensity, name):\n",
    "    columns_name = ['Cyto', 'x', 'y', 'Intensity']\n",
    "    \n",
    "    df = pd.DataFrame(np.hstack([PPI_labels[:,np.newaxis], PPI_loc, intensity[:,np.newaxis]]), \n",
    "                      columns=columns_name)\n",
    "    df['PPI'] = name\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imgs_path = data_dir / 'indirect' /'metadata' / 'imgs_reg.csv'\n",
    "df_imgs = pd.read_csv(df_imgs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_filt_dir = data_dir / 'indirect' / 'imgs' / 'masks_filtered'\n",
    "PPI_save_path =  data_dir / 'indirect' / 'PPI'\n",
    "\n",
    "masks_path = defaultdict(dict) \n",
    "for path in os.listdir(mask_filt_dir):\n",
    "    name = path.split('.')[0]\n",
    "    if 'Nuclei' in name:\n",
    "        masks_path[name[7:]]['nuclei'] = mask_filt_dir / path\n",
    "    elif 'Cyto' in name:\n",
    "        masks_path[name[5:]]['cyto'] =mask_filt_dir / path\n",
    "    elif 'Cell' in name:\n",
    "        masks_path[name[5:]]['cell'] =mask_filt_dir / path    \n",
    "    elif 'df' in name:\n",
    "        masks_path[name[3:]]['df'] =mask_filt_dir / path\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in df_imgs.itertuples():\n",
    "    name = '_'.join(row[1:3])\n",
    "    \n",
    "    # Read imgs\n",
    "    path = row.Path    \n",
    "    with h5py.File(path, 'r') as f:\n",
    "        markers = f['imgs'].attrs['Marker']\n",
    "        imgs = f['imgs'][:]\n",
    "\n",
    "    # Read masks\n",
    "    mask_cyto_path = masks_path[name]['cell']\n",
    "    mask_cyto = skimage.io.imread(mask_cyto_path)\n",
    "    \n",
    "    # Read PPi\n",
    "    PPI_dict = read_PPI(PPI_save_path / f'{row[1]}.pkl')\n",
    "    dfs_PPI_cyto = []\n",
    "    dfs_PPI_nuclei = []\n",
    "    for k in PPI_dict.keys():\n",
    "        PPI_loc = PPI_dict[k][:, 1:3].astype(np.uint32)\n",
    "\n",
    "        # Cyto\n",
    "        PPI_labels = mask_cyto[PPI_loc[:,0], PPI_loc[:,1]]\n",
    "        PPI_intensity = imgs[1][PPI_loc[:,0], PPI_loc[:,1]]\n",
    "        df_PPI = create_PPI_df(PPI_labels, PPI_loc, PPI_intensity, k)\n",
    "        dfs_PPI_cyto.append(df_PPI)\n",
    "        \n",
    "    # Combined DFs\n",
    "    df_merged = pd.concat(dfs_PPI_cyto)\n",
    "    df_merged['Condition'] = row[1]\n",
    "    df_merged['FOV'] = row[2]\n",
    "    \n",
    "    # Save dataframe\n",
    "    path = PPI_save_path / 'expression' /f'{name}_intensity.csv'\n",
    "    df_merged.to_csv(path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PLA2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
