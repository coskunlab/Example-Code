{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2e094bf-8350-4dc1-baad-1d2b43506d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import skimage.io\n",
    "\n",
    "from collections import defaultdict\n",
    "from tqdm.auto import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "import re\n",
    "import h5py\n",
    "import napari\n",
    "import tifffile as tiff\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "\n",
    "import networkx as nx\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80d3988c-196c-4fca-b035-0b1af24e0106",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric.utils\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25b12c61-09ad-41b2-9a8c-ae50d190fd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90aabf57-0607-48a3-9c05-742269b7cf2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_dir = (Path().cwd().parents[0]).absolute()\n",
    "\n",
    "module_path = str(p_dir / \"src\")\n",
    "\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d2a6df8-d5c2-4dee-ba53-119a7c084424",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = (Path().cwd().parents[0] / 'data').absolute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7fff6962-d83d-41f3-9b9a-0c431155704f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning.pytorch as pl\n",
    "import PPIGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc9bd74-4390-4c75-99b3-5750f9bc68c9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Test custom torch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd4ef3d7-ddbb-4a8f-8836-a97b012d5cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8cdc6fcf-bfe0-4ff6-99a9-afc7a241e71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_mapping = {'HCC827Ctrl': 0, 'HCC827Osim': 1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1fc72493-4e7b-42f4-adb4-619e7e97be0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_path = data_dir / 'OCT Cell Culture' / 'Whole' / 'graphs' \n",
    "\n",
    "dataset = PPIGraph.GraphDataset(graph_path, 'raw', 'pt', condition_mapping=condition_mapping, n_c=2)\n",
    "train_set, val_set, test_set = PPIGraph.train_test_val_split(dataset)\n",
    "\n",
    "# Create Dataloader\n",
    "train_loader = DataLoader(train_set, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7001e783-5bd6-4442-af42-7c1fa389fd90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: GraphDataset(2064):\n",
      "======================\n",
      "Number of graphs: 2064\n",
      "Number of features: 5\n",
      "Number of classes: 2\n"
     ]
    }
   ],
   "source": [
    "print(f'Dataset: {dataset}:')\n",
    "print('======================')\n",
    "print(f'Number of graphs: {len(dataset)}')\n",
    "print(f'Number of features: {dataset.num_features}')\n",
    "print(f'Number of classes: {dataset.num_classes}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ba2ee90-059c-44f7-b572-29053bc21270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 992, test set: 825, val set: 247\n"
     ]
    }
   ],
   "source": [
    "print(f'Train set: {len(train_set)}, test set: {len(test_set)}, val set: {len(val_set)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "92762d01-a537-47f6-8009-6f3d5faf4237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1:\n",
      "=======\n",
      "Number of graphs in the current batch: 32\n",
      "DataBatch(edge_index=[2, 14760], pos=[2787, 2], labels=[2787, 5], nuclei=[2787], weight=[14760], condition=[32], fov=[32], id=[32], train_mask=[2787], test_mask=[2787], x=[2787, 5], y=[32], edge_weight=[14760], name=[32], batch=[2787], ptr=[33])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for step, data in enumerate(train_loader):\n",
    "    print(f'Step {step + 1}:')\n",
    "    print('=======')\n",
    "    print(f'Number of graphs in the current batch: {data.num_graphs}')\n",
    "    print(data)\n",
    "    print()\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5aa852c-4c8c-45b4-b6a2-4622c562ef6a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Test model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76010aee-b2a3-47d6-8e85-2ad695e595a5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Manual train GCN compoenents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2bd37f9-1a54-408c-adc4-0e68e8aaf8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, epochs, criterion, optimizer, device, mlp=True):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for step, data in enumerate(loader):\n",
    "            data = data.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            if mlp:\n",
    "                input_data = data.x.to(device)\n",
    "                out = model(input_data)\n",
    "            else:\n",
    "                x, edge_index, edge_weight = data.x, data.edge_index, data.edge_weight\n",
    "                out = model(x, edge_index, edge_weight=edge_weight)\n",
    "            loss = criterion(out, data.nuclei)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        if epoch%10==0:\n",
    "            print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')\n",
    "            \n",
    "def test(model, loader, device, mlp=True):\n",
    "    model.eval()\n",
    "    test_accs = [] \n",
    "\n",
    "    for step, data in enumerate(test_loader):\n",
    "        data = data.to(device)\n",
    "        if mlp:\n",
    "            input_data = data.x.to(device)\n",
    "            out = model(input_data)\n",
    "        else:\n",
    "            x, edge_index, edge_weight = data.x, data.edge_index, data.edge_weight\n",
    "            out = model(x, edge_index, edge_weight=edge_weight)\n",
    "        pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "        test_correct = pred == data.nuclei  # Check against ground-truth labels.\n",
    "        test_acc = int(test_correct.sum()) / len(pred)\n",
    "        test_accs.append(test_acc)\n",
    "        print(test_acc)\n",
    "        \n",
    "    return test_accs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6535cac1-f491-4b8e-bdaf-a9480612acb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "AVAIL_GPUS = [1]\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd19908d-2b54-4a02-a0a5-8fd348cee0a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %time\n",
    "# # Create model, criterion and optimizer\n",
    "# model_gcn = PPIGraph.GNNModel(\"GCN\", 5, 16, 2).to(device)\n",
    "# criterion = torch.nn.CrossEntropyLoss()  # Define loss criterion.\n",
    "# optimizer = torch.optim.Adam(model_gcn.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "\n",
    "# # Train Loop\n",
    "# train(model_gcn, train_loader, 10, criterion, optimizer, device, mlp=False)\n",
    "# test_accs = test(model_gcn, test_loader, device, mlp=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf68066-a828-4079-bcd8-5921a8dabe1a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Lightning train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1ce5c68d-b020-4e7d-b28b-a7f1ae8d9ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from lightning.pytorch.accelerators import find_usable_cuda_devices\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ae4853b5-9932-4e59-aa45-9a1331a8f986",
   "metadata": {},
   "outputs": [],
   "source": [
    "condition = 'base'\n",
    "checkpoint_folder = (Path().cwd().parents[0]).absolute() / 'data' / \"saved_models\" / f\"Graph_GNNs_{condition}\" \n",
    "project_name = f'PLA_042623_{condition}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7474adc2-b101-49c7-a293-88e2fe1633f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AVAIL_GPUS = [1]\n",
    "# BATCH_SIZE = 64 if AVAIL_GPUS else 32\n",
    "\n",
    "# # Setting the seed\n",
    "# pl.seed_everything(42)\n",
    "\n",
    "# num_layers = [2,3,4]\n",
    "# hiddens = [16, 32, 64]\n",
    "# pools = ['mean', 'max', 'sum', 'attention', 'attention2']\n",
    "\n",
    "# epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3248cbc2-c017-4384-998d-2cd906758c97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for NUM_LAYERS, HIDDEN_CHANNELS, pool in list(itertools.product(*[num_layers, hiddens, pools])):\n",
    "    \n",
    "#     # Path to the folder where the pretrained models are saved\n",
    "#     CHECKPOINT_PATH = checkpoint_folder / f'{NUM_LAYERS}_{HIDDEN_CHANNELS}_onehot' / pool\n",
    "#     CHECKPOINT_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#     # Run training\n",
    "#     models = ['GCN']\n",
    "#     for model_name in models:\n",
    "#         run = wandb.init(project=project_name, name=model_name+f'_{NUM_LAYERS}_{HIDDEN_CHANNELS}_onehot')\n",
    "#         model, result, trainer = PPIGraph.train_graph_classifier(model_name, \n",
    "#                                                                  train_set, \n",
    "#                                                                  val_set, \n",
    "#                                                                  test_set, \n",
    "#                                                                  dataset, \n",
    "#                                                                  CHECKPOINT_PATH, \n",
    "#                                                                  AVAIL_GPUS, \n",
    "#                                                                  in_channels=5,\n",
    "#                                                                  hidden_channels=HIDDEN_CHANNELS, \n",
    "#                                                                  out_channels = HIDDEN_CHANNELS,\n",
    "#                                                                  num_layers=NUM_LAYERS, epochs=epochs,\n",
    "#                                                                  embedding=False,\n",
    "#                                                                  graph_pooling=pool)\n",
    "#         run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6499ccee-a900-445f-b2f5-2ab0471430ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for NUM_LAYERS, HIDDEN_CHANNELS, pool in list(itertools.product(*[num_layers, hiddens, pools])):\n",
    "    \n",
    "#     # Path to the folder where the pretrained models are saved\n",
    "#     CHECKPOINT_PATH = checkpoint_folder / f'{NUM_LAYERS}_{HIDDEN_CHANNELS}_emb' / pool\n",
    "#     CHECKPOINT_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#     # Run training\n",
    "#     models = ['GCN']\n",
    "#     for model_name in models:\n",
    "#         run = wandb.init(project=project_name, name=model_name+f'_{NUM_LAYERS}_{HIDDEN_CHANNELS}_emb')\n",
    "#         model, result, trainer = PPIGraph.train_graph_classifier(model_name, \n",
    "#                                                                  train_set, \n",
    "#                                                                  val_set, \n",
    "#                                                                  test_set, \n",
    "#                                                                  dataset, \n",
    "#                                                                  CHECKPOINT_PATH, \n",
    "#                                                                  AVAIL_GPUS, \n",
    "#                                                                  in_channels=HIDDEN_CHANNELS,\n",
    "#                                                                  hidden_channels=HIDDEN_CHANNELS, \n",
    "#                                                                  out_channels = HIDDEN_CHANNELS,\n",
    "#                                                                  num_layers=NUM_LAYERS, epochs=epochs,\n",
    "#                                                                  embedding=True,\n",
    "#                                                                  graph_pooling=pool)\n",
    "#         run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f547c07-499f-4979-a023-83ad03e5094b",
   "metadata": {},
   "source": [
    "# K Fold on  filter dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d008e853-c4c2-4d2f-8c35-0da6f206ccbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from torch.utils.data import SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c86dd364-b7cb-4910-9961-b288741c5cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out by maximum number of counts per cell\n",
    "min_count = 20\n",
    "max_count = 70\n",
    "\n",
    "graph_path = data_dir / 'OCT Cell Culture' / 'Whole' / 'graphs' \n",
    "\n",
    "dataset = PPIGraph.GraphDataset(graph_path, 'raw', 'pt', condition_mapping=condition_mapping, n_c=2)\n",
    "\n",
    "# Create Dataloader\n",
    "loader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# Get Indices\n",
    "indices = []\n",
    "for step, data in enumerate(loader):\n",
    "    if len(data.x) <= min_count:\n",
    "        continue \n",
    "    \n",
    "    if (data.x.sum(axis=0) >= max_count).any():\n",
    "        continue\n",
    "    indices.append(step)\n",
    "    \n",
    "# Get subset dataset\n",
    "dataset_filtered = dataset.index_select(indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1696b9fc-9437-4349-adc9-f97102ff9806",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1522"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "25d9cc11-9602-4626-9d57-ef66ca80e684",
   "metadata": {},
   "outputs": [],
   "source": [
    "condition = 'Kfold'\n",
    "checkpoint_folder = (Path().cwd().parents[0]).absolute() / 'data' / \"saved_models\" / f\"Graph_GNNs_{condition}\" \n",
    "project_name = f'PLA_042923_{condition}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "de1f4bfd-cae7-4515-8808-9c623124b621",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    }
   ],
   "source": [
    "AVAIL_GPUS = [1]\n",
    "\n",
    "# Setting the seed\n",
    "pl.seed_everything(42)\n",
    "\n",
    "NUM_LAYERS = 2\n",
    "HIDDEN_CHANNELS = 16\n",
    "pools = ['mean', 'max', 'sum', 'attention', 'attention2']\n",
    "\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ee100db6-fd70-4fa1-b100-85feeb144653",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# k_folds = 5\n",
    "# kfold = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# for fold, (train_ids, valid_ids) in enumerate(kfold.split(dataset_filtered)):\n",
    "#     train_subset = dataset_filtered.index_select(train_ids.tolist())\n",
    "#     val_subset = dataset_filtered.index_select(valid_ids.tolist())\n",
    "    \n",
    "#     for pool in pools:\n",
    "#         # Path to the folder where the pretrained models are saved\n",
    "#         CHECKPOINT_PATH = checkpoint_folder / f'{NUM_LAYERS}_{HIDDEN_CHANNELS}_onehot_{fold}' / pool\n",
    "#         CHECKPOINT_PATH.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "#         checkpoint = CHECKPOINT_PATH / \"GraphLevelGCN\" / \"GraphLevelGCN.ckpt\" \n",
    "#         if checkpoint.exists():\n",
    "#             print(checkpoint)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "64c99c78-bb9f-4f01-8f0d-3771363ed437",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### GCN \n",
    "\n",
    "# k_folds = 5\n",
    "# kfold = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# for fold, (train_ids, valid_ids) in enumerate(kfold.split(dataset_filtered)):\n",
    "#     train_subset = dataset_filtered.index_select(train_ids.tolist())\n",
    "#     val_subset = dataset_filtered.index_select(valid_ids.tolist())\n",
    "    \n",
    "#     for pool in pools:\n",
    "#         # Path to the folder where the pretrained models are saved\n",
    "#         CHECKPOINT_PATH = checkpoint_folder / f'{NUM_LAYERS}_{HIDDEN_CHANNELS}_onehot_{fold}' / pool\n",
    "#         CHECKPOINT_PATH.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "#         # Skip already trained kfold and pool\n",
    "#         checkpoint = CHECKPOINT_PATH / \"GraphLevelGCN\" / \"GraphLevelGCN.ckpt\" \n",
    "#         if checkpoint.exists():\n",
    "#             print(checkpoint)\n",
    "#             continue\n",
    "        \n",
    "#         # Run training\n",
    "#         run = wandb.init(project=project_name, name=f'GCN_{NUM_LAYERS}_{HIDDEN_CHANNELS}_onehot_{fold}', \n",
    "#                         group=f'GCN_{pool}')\n",
    "#         PPIGraph.train_graph_classifier_kfold('GCN', \n",
    "#                                              train_subset, \n",
    "#                                              val_subset, \n",
    "#                                              dataset, \n",
    "#                                              CHECKPOINT_PATH, \n",
    "#                                              AVAIL_GPUS, \n",
    "#                                              in_channels=5,\n",
    "#                                              hidden_channels=HIDDEN_CHANNELS, \n",
    "#                                              out_channels = HIDDEN_CHANNELS,\n",
    "#                                              num_layers=NUM_LAYERS, \n",
    "#                                              epochs=epochs,\n",
    "#                                              embedding=False,\n",
    "#                                              graph_pooling=pool)\n",
    "#         run.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "644ad90d-6d1f-4939-98af-6e4ced448b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "condition = 'Kfold'\n",
    "checkpoint_folder = (Path().cwd().parents[0]).absolute() / 'data' / \"saved_models\" / f\"MLP_{condition}\" \n",
    "project_name = f'PLA_042923_{condition}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6b60220c-cf5c-40ea-9b55-fd57a50d905a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    }
   ],
   "source": [
    "AVAIL_GPUS = [1]\n",
    "\n",
    "# Setting the seed\n",
    "pl.seed_everything(42)\n",
    "\n",
    "NUM_LAYERS = 2\n",
    "HIDDEN_CHANNELS = 16\n",
    "pools = ['mean', 'max', 'sum', 'attention', 'attention2']\n",
    "\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "16f35fa8-bee7-4267-92e5-bc9bce650822",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mthoomas\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d21a6d4652c4a7699cc25b80dc4d7c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01691666666883975, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Thomas\\15_PLA\\notebooks\\wandb\\run-20230509_110346-a2s7zwa7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/a2s7zwa7' target=\"_blank\">MLP_2_16_onehot_0</a></strong> to <a href='https://wandb.ai/thoomas/PLA_042923_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_042923_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/a2s7zwa7' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold/runs/a2s7zwa7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "C:\\Users\\thu71\\Anaconda3\\envs\\snowflake\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:395: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 368   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "522       Trainable params\n",
      "0         Non-trainable params\n",
      "522       Total params\n",
      "0.002     Total estimated model params size (MB)\n",
      "C:\\Users\\thu71\\AppData\\Roaming\\Python\\Python39\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\thu71\\AppData\\Roaming\\Python\\Python39\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\thu71\\AppData\\Roaming\\Python\\Python39\\site-packages\\lightning\\pytorch\\utilities\\data.py:77: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 2. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  warning_cache.warn(\n",
      "C:\\Users\\thu71\\AppData\\Roaming\\Python\\Python39\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▅▅▆▆▇▇██▇█▇█▇▇█████▇▇█▇▇███▇██▇▇▇█▇▇▇██</td></tr><tr><td>train_auc</td><td>▁▁▂▂▃▆▆▇▇▆▇▇▇▇▇▇▇█▇▇▆▆█▅▆▇▇▇▇▇▇▆▆▆▇▇▆▇▇▇</td></tr><tr><td>train_f1</td><td>▆▂▁▁▄▆▇▇▇▆█▇▇█▇▇▇█▇▇▇▇█▆▇▇▇▇▇▇▇▇▇▆▇▇▆█▇█</td></tr><tr><td>train_loss_epoch</td><td>█▆▆▆▄▃▂▂▂▂▂▃▂▂▂▁▃▂▂▂▂▂▂▂▂▁▂▂▂▂▂▂▂▂▁▂▂▃▁▂</td></tr><tr><td>train_loss_step</td><td>█▇▇▅▄▃▄▄▆▃▆▇▄▄▄▂▃▄▅▃▄▂▄▃▄▁▃▃▅▄▃▃▇▁▃█▂▄▃▅</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▁▂▂▁▃▃▂▃▃▂▄▄▂▂▄▂▂▅▅▂▆▆▂▆▆▃▇▇▃▃▇▃▃█▃</td></tr><tr><td>val_acc</td><td>▁▁▁▁▄▇▇▇▇▇█▇▇▇▇█▇▇▇▇▇█▇██▇▇▇█▇▇█▇█▇▇▇▇▇▇</td></tr><tr><td>val_auc</td><td>▁▁▁▁▃██▇█▇███▇███████▇█▇▇████████▇██▇█▇█</td></tr><tr><td>val_f1</td><td>▁▁▁▁▄████▇███████████▇█▇▇████████▇██▇███</td></tr><tr><td>val_loss_epoch</td><td>███▇▄▃▂▂▂▂▃▃▂▂▂▂▃▂▂▂▂▂▂▂▃▁▂▂▁▂▂▂▁▃▃▂▂▂▁▂</td></tr><tr><td>val_loss_step</td><td>████▆▅▃▅▃▁▄▅▂▄▃▆▄▅▅▆▃▄▄▅▄▃▅▃▂▃▆▄▄▇▅▄▄▄▅▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.70419</td></tr><tr><td>train_auc</td><td>0.65348</td></tr><tr><td>train_f1</td><td>0.53368</td></tr><tr><td>train_loss_epoch</td><td>0.56902</td></tr><tr><td>train_loss_step</td><td>0.60403</td></tr><tr><td>trainer/global_step</td><td>999</td></tr><tr><td>val_acc</td><td>0.70164</td></tr><tr><td>val_auc</td><td>0.65755</td></tr><tr><td>val_f1</td><td>0.54726</td></tr><tr><td>val_loss_epoch</td><td>0.54592</td></tr><tr><td>val_loss_step</td><td>0.52767</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_16_onehot_0</strong> at: <a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/a2s7zwa7' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold/runs/a2s7zwa7</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230509_110346-a2s7zwa7\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae9c0ffb9c114355989238c47ff1e3fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01693333333435779, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Thomas\\15_PLA\\notebooks\\wandb\\run-20230509_120311-v99v4nsc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/v99v4nsc' target=\"_blank\">MLP_2_16_onehot_0</a></strong> to <a href='https://wandb.ai/thoomas/PLA_042923_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_042923_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/v99v4nsc' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold/runs/v99v4nsc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 368   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "522       Trainable params\n",
      "0         Non-trainable params\n",
      "522       Total params\n",
      "0.002     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a40b591c93d4f95bbc313a23c7c913a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▅▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇██▇▇██▇▇▇█▇█▇█████▇▇▇█</td></tr><tr><td>train_auc</td><td>▂▁▄▄▄▄▄▄▄▄▄▄▄▄▄▄▅▅▆▇▆▅▇▅▄▄▄▆▅▆▅▅▅▆▇▇▅█▆▇</td></tr><tr><td>train_f1</td><td>█▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▃▃▃▂▄▂▁▂▂▂▂▃▃▂▂▂▃▃▂▅▃▃</td></tr><tr><td>train_loss_epoch</td><td>█▄▄▃▃▃▂▃▃▂▂▃▃▂▂▂▂▂▂▁▂▁▂▂▂▁▂▁▂▂▂▁▂▂▁▂▁▁▂▁</td></tr><tr><td>train_loss_step</td><td>▆▄▅▂▅▃▂▄▆▂▂█▄▄▆▃▅▂▃▂▂▂▂▄▆▁▃▁▃▆▃▄▅▃▃▅▃▁▂▅</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▁▂▂▁▃▃▂▃▃▂▄▄▂▂▄▂▂▅▅▂▆▆▂▆▆▃▇▇▃▃▇▃▃█▃</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_epoch</td><td>██▆▅▄▅▂▃▅▃▆▄▃▄▄▃▅▅▅▄▆▄▃▂▆▂▃▅▂▄▃▃▂▆▂▄▆▃▁▄</td></tr><tr><td>val_loss_step</td><td>▅▅▅▅▄▅▂▅▂▂▄▅▃▄▅▇▂▆▄▅▃▂▄▅▄▂▆▁▂▄▆▂▆█▄▄▇▄▂▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.63763</td></tr><tr><td>train_auc</td><td>0.52502</td></tr><tr><td>train_f1</td><td>0.14369</td></tr><tr><td>train_loss_epoch</td><td>0.64806</td></tr><tr><td>train_loss_step</td><td>0.69317</td></tr><tr><td>trainer/global_step</td><td>999</td></tr><tr><td>val_acc</td><td>0.62295</td></tr><tr><td>val_auc</td><td>0.5</td></tr><tr><td>val_f1</td><td>0.0</td></tr><tr><td>val_loss_epoch</td><td>0.66623</td></tr><tr><td>val_loss_step</td><td>0.67917</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_16_onehot_0</strong> at: <a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/v99v4nsc' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold/runs/v99v4nsc</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230509_120311-v99v4nsc\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47a31372196d4ddaa6815d26c2db6217",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Thomas\\15_PLA\\notebooks\\wandb\\run-20230509_125419-l4hs7go9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/l4hs7go9' target=\"_blank\">MLP_2_16_onehot_0</a></strong> to <a href='https://wandb.ai/thoomas/PLA_042923_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_042923_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/l4hs7go9' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold/runs/l4hs7go9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 368   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "522       Trainable params\n",
      "0         Non-trainable params\n",
      "522       Total params\n",
      "0.002     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5765da92f5e44b28b3f9524e75103345",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▄▅▅▆▆▆▇▇▇▇▇▇▇▇▇▇██▇▇█▇▇███▇▇█▇██▇█▇██▇</td></tr><tr><td>train_auc</td><td>▁▂▃▃▃▅▄▅▆▅▇▆▆▇▇▇▆▇▇▇▇▆█▆▇▇▇▇▇▇█▇▇▇██▇██▇</td></tr><tr><td>train_f1</td><td>▃▃▂▂▁▄▃▄▅▄▇▆▅▆▆▇▅▇▇▆▇▅█▅▇▆▆▇▇▇▇▆▇▆█▇▆█▇▇</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▃▃▂▃▂▂▂▂▂▂▃▃▂▂▂▃▂▂▂▂▂▂▂▂▁▂▁▂▃▂▂▂▁▁▃▂▂▂▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▁▂▂▁▃▃▂▃▃▂▄▄▂▂▄▂▂▅▅▂▆▆▂▆▆▃▇▇▃▃▇▃▃█▃</td></tr><tr><td>val_acc</td><td>▁▅▅▅▅▇▇▇▇▇▇█▇▇█▇▇██████▇▇█▇███▇▇█▇▇██▇▇█</td></tr><tr><td>val_auc</td><td>▁▄▄▃▃▆▆▆▇▆▆▇▇▆▇▆▆▇▇█▇██▇█████▇▆▇█▇▆██▇▇█</td></tr><tr><td>val_f1</td><td>▂▂▂▁▁▅▅▆█▅▅▇█▅▇▅▅▇▇▇▇█▇██████▇▆▆██▅██▇██</td></tr><tr><td>val_loss_epoch</td><td>█▇▆▆▅▅▄▃▄▃▄▃▃▃▂▃▃▂▃▂▂▂▂▂▂▂▂▂▁▂▂▂▁▂▃▂▁▂▁▁</td></tr><tr><td>val_loss_step</td><td>█▇▇█▆▆▄▆▃▂▄▆▂▃▃▆▄▃▄▅▂▂▃▄▃▂▃▃▁▃▆▃▃▃▅▃▂▃▄▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.70994</td></tr><tr><td>train_auc</td><td>0.65583</td></tr><tr><td>train_f1</td><td>0.53245</td></tr><tr><td>train_loss_epoch</td><td>0.56173</td></tr><tr><td>train_loss_step</td><td>0.68452</td></tr><tr><td>trainer/global_step</td><td>999</td></tr><tr><td>val_acc</td><td>0.74098</td></tr><tr><td>val_auc</td><td>0.69771</td></tr><tr><td>val_f1</td><td>0.60302</td></tr><tr><td>val_loss_epoch</td><td>0.52647</td></tr><tr><td>val_loss_step</td><td>0.52804</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_16_onehot_0</strong> at: <a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/l4hs7go9' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold/runs/l4hs7go9</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230509_125419-l4hs7go9\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2306d738c06e4faf839b29caaae14c1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016933333330477276, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Thomas\\15_PLA\\notebooks\\wandb\\run-20230509_133815-vsrk6nv8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/vsrk6nv8' target=\"_blank\">MLP_2_16_onehot_0</a></strong> to <a href='https://wandb.ai/thoomas/PLA_042923_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_042923_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/vsrk6nv8' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold/runs/vsrk6nv8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "C:\\Users\\thu71\\Anaconda3\\envs\\snowflake\\lib\\site-packages\\torch_geometric\\deprecation.py:12: UserWarning: 'nn.glob.GlobalAttention' is deprecated, use 'nn.aggr.AttentionalAggregation' instead\n",
      "  warnings.warn(out)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 368   \n",
      "2  | head        | Sequential       | 154   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 17    \n",
      "--------------------------------------------------\n",
      "539       Trainable params\n",
      "0         Non-trainable params\n",
      "539       Total params\n",
      "0.002     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31d341078e86411e8c48167fb1aa0c42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▅▅▅▆▇▇█▇▇███▇█▇██▇█▇▇█▇▇▇██▇▇██▇▇███▇██</td></tr><tr><td>train_auc</td><td>▁▁▁▁▅▅▆▇▇▇▇██▆▇▇▇█▆▇▇▇▇▅▇▇▇▇▆▆██▇▆▇██▇█▇</td></tr><tr><td>train_f1</td><td>▇▃▁▁▆▆▇█▇▇███▇▇▇▇█▇█▇█▇▆▇███▇▇███▇▇██▇█▇</td></tr><tr><td>train_loss_epoch</td><td>█▇▆▆▄▃▂▂▂▂▂▂▂▃▂▂▂▂▂▁▂▂▂▂▂▂▂▁▂▂▂▁▁▂▂▂▁▂▂▁</td></tr><tr><td>train_loss_step</td><td>▆▅▅▅▄▅▃▄▃▂▃▃▂▅▃▄▃▃▂▁▃▂█▄▄▁▃▃▂▃▅▂▃▃▄▅▂▅▄▅</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▁▂▂▁▃▃▂▃▃▂▄▄▂▂▄▂▂▅▅▂▆▆▂▆▆▃▇▇▃▃▇▃▃█▃</td></tr><tr><td>val_acc</td><td>▁▁▁▁▇▆▇▇█▆▇▆▇▅▅█▇▇▇▇▇▅▇▅▇█▆▇▇█▇▆█▆▇██▅▇▆</td></tr><tr><td>val_auc</td><td>▁▁▁▁███▇▇▇▇▆▆▄▅▇▇▇▇▆▆▅▇▄█▇▆▇▇▇▇▅▇▇▇▇▇▄▆▆</td></tr><tr><td>val_f1</td><td>▁▁▁▁███▇▇▇▇▆▇▅▅▇█▇▇▇▆▅▇▅█▇▆▇▇▇█▆▇▇█▇▇▅▇▆</td></tr><tr><td>val_loss_epoch</td><td>█▇█▇▅▄▂▂▂▃▂▂▁▄▃▃▃▃▂▁▃▃▂▃▂▃▂▁▂▂▂▃▃▃▃▃▂▂▂▃</td></tr><tr><td>val_loss_step</td><td>██▇█▅▄▂▄▂▂▄▁▃▄▇▃▄▄▃▂▂▃▂▃▃▇▆▃▃▃▄▄▄▆▂▄▃▄▄▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.69433</td></tr><tr><td>train_auc</td><td>0.63357</td></tr><tr><td>train_f1</td><td>0.49041</td></tr><tr><td>train_loss_epoch</td><td>0.56159</td></tr><tr><td>train_loss_step</td><td>0.63089</td></tr><tr><td>trainer/global_step</td><td>999</td></tr><tr><td>val_acc</td><td>0.69508</td></tr><tr><td>val_auc</td><td>0.62311</td></tr><tr><td>val_f1</td><td>0.4497</td></tr><tr><td>val_loss_epoch</td><td>0.56118</td></tr><tr><td>val_loss_step</td><td>0.54405</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_16_onehot_0</strong> at: <a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/vsrk6nv8' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold/runs/vsrk6nv8</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230509_133815-vsrk6nv8\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "409b73f25eef42dca9f0f4e88ba83420",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Thomas\\15_PLA\\notebooks\\wandb\\run-20230509_141747-xm4u0f7g</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/xm4u0f7g' target=\"_blank\">MLP_2_16_onehot_0</a></strong> to <a href='https://wandb.ai/thoomas/PLA_042923_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_042923_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/xm4u0f7g' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold/runs/xm4u0f7g</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 368   \n",
      "2  | head        | Sequential       | 154   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 281   \n",
      "--------------------------------------------------\n",
      "803       Trainable params\n",
      "0         Non-trainable params\n",
      "803       Total params\n",
      "0.003     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68951114b49643bab6f2d4ea7bcd3cab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▄▅▅▆█▇█▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇██▇▇▇▇▇██▇▇▇▇▇▇▇▇</td></tr><tr><td>train_auc</td><td>▂▁▁▁▄▇▇▇▅▆▆▆▇▆▇▆▇▇▇▆▇▆▅▇█▇▇▇▆▆▇▇▇▆▇▇▆▅▅▇</td></tr><tr><td>train_f1</td><td>▇▂▁▁▅█▇█▆▇▇▇▇▇█▇▇▇▇▇▇▇▆▇███▇▇▇▇▇▇▇▇▇▇▆▆▇</td></tr><tr><td>train_loss_epoch</td><td>█▇▆▆▄▃▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▂▁▁▁▂▂▂▁▂▂▂▂▁▂▂▂▂▂</td></tr><tr><td>train_loss_step</td><td>▆▆▇█▆▅▃▅▆▃▄▆▆▂▃▃▇▅▁█▅▂▃▅▄▄▅▂▄▄▅▅▃▃▄▄█▄▃▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▁▂▂▁▃▃▂▃▃▂▄▄▂▂▄▂▂▅▅▂▆▆▂▆▆▃▇▇▃▃▇▃▃█▃</td></tr><tr><td>val_acc</td><td>▁▁▁▁▆█▆▆▇▇▇▇▇▇█▇▇▇▇▇▇▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>val_auc</td><td>▁▁▁▁▄█▅▅▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆</td></tr><tr><td>val_f1</td><td>▁▁▁▁▅█▆▅▇▇▇▇▇▇▇▇▇▇▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▆</td></tr><tr><td>val_loss_epoch</td><td>██▇▇▄▄▂▂▃▂▃▂▂▂▃▂▂▂▂▂▁▂▂▂▂▃▄▂▃▂▃▃▁▂▂▃▂▂▂▂</td></tr><tr><td>val_loss_step</td><td>▇██▆▅▄▁▃▃▃▃▃▄▄▅▄▃▃▄▃▃▅▂▃▂▆▃▃▅▄▂▃▃▄▃▃▃▂▅▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.69269</td></tr><tr><td>train_auc</td><td>0.64564</td></tr><tr><td>train_f1</td><td>0.52778</td></tr><tr><td>train_loss_epoch</td><td>0.5659</td></tr><tr><td>train_loss_step</td><td>0.53029</td></tr><tr><td>trainer/global_step</td><td>999</td></tr><tr><td>val_acc</td><td>0.70492</td></tr><tr><td>val_auc</td><td>0.64645</td></tr><tr><td>val_f1</td><td>0.51087</td></tr><tr><td>val_loss_epoch</td><td>0.5496</td></tr><tr><td>val_loss_step</td><td>0.52742</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_16_onehot_0</strong> at: <a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/xm4u0f7g' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold/runs/xm4u0f7g</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230509_141747-xm4u0f7g\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb839f8e2b5d4000a995d397db69ceef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016933333330477276, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Thomas\\15_PLA\\notebooks\\wandb\\run-20230509_150135-ej4dnu94</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/ej4dnu94' target=\"_blank\">MLP_2_16_onehot_1</a></strong> to <a href='https://wandb.ai/thoomas/PLA_042923_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_042923_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/ej4dnu94' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold/runs/ej4dnu94</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 368   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "522       Trainable params\n",
      "0         Non-trainable params\n",
      "522       Total params\n",
      "0.002     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b797be19858a4dda9df3fdb0d0a24941",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▅▅▅▆▇▇▇█▇██████▇▇██▇██▇▇█▇██████▇██▇███</td></tr><tr><td>train_auc</td><td>▁▁▂▂▄▇▆▆▇▆█▇▇▇▇▇▇▆▇▇▆▇▇▆▆▇▇█▇▇▇▇▇▆▇▇▆▇▇▇</td></tr><tr><td>train_f1</td><td>▆▂▁▁▄▇▇▇█▇██▇███▇▇█▇▇▇█▇▇▇▇█▇▇▇▇▇▇▇▇▇██▇</td></tr><tr><td>train_loss_epoch</td><td>█▆▆▆▄▃▂▁▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▂▂▂▁▂▂▁▂▂▂▁▂</td></tr><tr><td>train_loss_step</td><td>█▇▇▅▆▅▂▃▆▄▂█▃▄▃▂▅▂▂▃▁▃▃▃▄▃▂▃▇▇▄▄▅▂▅▆▅▃▅▅</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▁▂▂▁▃▃▂▃▃▂▄▄▂▂▄▂▂▅▅▂▆▆▂▆▆▃▇▇▃▃▇▃▃█▃</td></tr><tr><td>val_acc</td><td>▁▁▁▁▆▇█▆█▇▇▇██▇█▇▇▇▆█▇▇▇▇█▇▇▇██▇▆█▇▆▇▇▇▇</td></tr><tr><td>val_auc</td><td>▁▁▁▁▅▇▇▇▇▆▇▇█▇█▇▇▇▇▇▇▇▇▆▇█▇▇███▇▇▇█▇▆▇▇▇</td></tr><tr><td>val_f1</td><td>▁▁▁▁▅█▇▇▇▇▇██▇█▇█▇▇▇▇▇█▇▇█▇▇███▇▇▇██▆▇█▇</td></tr><tr><td>val_loss_epoch</td><td>███▇▅▃▃▂▄▃▃▃▁▃▃▂▄▄▃▃▃▃▃▃▂▂▄▃▂▂▂▂▂▃▃▂▄▂▂▃</td></tr><tr><td>val_loss_step</td><td>▇▇▇▇▅▄▃▃▂▁▄▄▄▄▆▅▂█▂▅▃▄▄▄▅▃▅▃▃▂▁▂▃▅▂▄▅▄▃▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.70501</td></tr><tr><td>train_auc</td><td>0.65266</td></tr><tr><td>train_f1</td><td>0.53072</td></tr><tr><td>train_loss_epoch</td><td>0.56432</td></tr><tr><td>train_loss_step</td><td>0.62391</td></tr><tr><td>trainer/global_step</td><td>999</td></tr><tr><td>val_acc</td><td>0.65246</td></tr><tr><td>val_auc</td><td>0.6035</td></tr><tr><td>val_f1</td><td>0.46465</td></tr><tr><td>val_loss_epoch</td><td>0.58678</td></tr><tr><td>val_loss_step</td><td>0.64502</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_16_onehot_1</strong> at: <a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/ej4dnu94' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold/runs/ej4dnu94</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230509_150135-ej4dnu94\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d1366e42f2c438ab1e610c486c4876e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Thomas\\15_PLA\\notebooks\\wandb\\run-20230509_153921-nqbrs0li</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/nqbrs0li' target=\"_blank\">MLP_2_16_onehot_1</a></strong> to <a href='https://wandb.ai/thoomas/PLA_042923_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_042923_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/nqbrs0li' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold/runs/nqbrs0li</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 368   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "522       Trainable params\n",
      "0         Non-trainable params\n",
      "522       Total params\n",
      "0.002     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c947bdca2e240bd977ec16e87df7f1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▅▇█████████▇████▇██████▇▇███▇██████▇███</td></tr><tr><td>train_auc</td><td>▂▁▅▄▄▄▄▄▄▄▄▄▄▄▄▄▅▄▇▆▄▆▇▅▄▄▆▆▄▅▅▆▇▅▆▆▅█▅█</td></tr><tr><td>train_f1</td><td>█▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▃▃▁▂▃▂▁▂▃▃▁▂▂▂▃▂▃▂▃▄▂▃</td></tr><tr><td>train_loss_epoch</td><td>█▄▄▃▃▃▂▃▃▂▂▃▂▂▂▂▂▁▁▂▁▂▂▂▂▁▂▁▁▂▂▁▂▂▂▂▂▁▁▂</td></tr><tr><td>train_loss_step</td><td>▆▄▅▂▅▃▂▅▅▂▂█▅▄▅▂▇▃▂▂▂▂▂▄▆▁▄▂▂▇▃▄▆▃▃▄▇▂▁▆</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▁▂▂▁▃▃▂▃▃▂▄▄▂▂▄▂▂▅▅▂▆▆▂▆▆▃▇▇▃▃▇▃▃█▃</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_epoch</td><td>▇█▆▄▃▅▁▃▅▂▆▄▂▄▄▃▅▅▅▄▅▄▃▂▅▁▃▄▁▄▂▃▂▆▂▃▅▃▁▄</td></tr><tr><td>val_loss_step</td><td>▅▅▅▅▄▄▂▅▂▁▄▅▃▄▅▆▂▆▄▄▃▂▄▅▄▂▅▂▂▄▆▂▅█▅▅▇▄▃▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.63599</td></tr><tr><td>train_auc</td><td>0.52127</td></tr><tr><td>train_f1</td><td>0.13307</td></tr><tr><td>train_loss_epoch</td><td>0.65248</td></tr><tr><td>train_loss_step</td><td>0.69221</td></tr><tr><td>trainer/global_step</td><td>999</td></tr><tr><td>val_acc</td><td>0.61639</td></tr><tr><td>val_auc</td><td>0.5</td></tr><tr><td>val_f1</td><td>0.0</td></tr><tr><td>val_loss_epoch</td><td>0.67224</td></tr><tr><td>val_loss_step</td><td>0.69112</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_16_onehot_1</strong> at: <a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/nqbrs0li' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold/runs/nqbrs0li</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230509_153921-nqbrs0li\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4ebbc202ba04d7cae19cbb025d45a97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01693333333435779, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Thomas\\15_PLA\\notebooks\\wandb\\run-20230509_161910-hw0cxu7b</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/hw0cxu7b' target=\"_blank\">MLP_2_16_onehot_1</a></strong> to <a href='https://wandb.ai/thoomas/PLA_042923_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_042923_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/hw0cxu7b' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold/runs/hw0cxu7b</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 368   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "522       Trainable params\n",
      "0         Non-trainable params\n",
      "522       Total params\n",
      "0.002     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e68b8392623415a8cedab152cb58001",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▃▄▅▆▆▆▆▆▇▇▇▇▇▆█▆▇▇█▇█▇▇█▇█▇████▇█▇▇███</td></tr><tr><td>train_auc</td><td>▁▁▂▃▄▅▅▅▅▅▇▅▆▆▇▆▇▆▇▆▇▆█▆▆▇▆▇▆▇▇▇▇▆▇▇▆██▇</td></tr><tr><td>train_f1</td><td>▃▂▁▁▃▅▄▄▅▄▇▄▆▅▆▆▆▅▇▅▇▅█▆▅▆▅▇▆▇▇▇▇▅▇▇▅██▇</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▃▃▃▃▃▂▂▃▃▂▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▃▂▂▂▂▁▂▃▂▁▂▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▁▂▂▁▃▃▂▃▃▂▄▄▂▂▄▂▂▅▅▂▆▆▂▆▆▃▇▇▃▃▇▃▃█▃</td></tr><tr><td>val_acc</td><td>▁▄▄▅▄▆▅▆▆▆▆▇█▇▇▆▇▆▆▇▇▇▆▇▇▇▇▇▇█▇▇▇█▇▆▇▆▇▇</td></tr><tr><td>val_auc</td><td>▁▃▃▄▃▅▄▅▅▆▅▆█▆▆▆▆▅▅▇▆▆▅▆▇▆▆▆▇█▆▆▆█▆▅▆▅▆▆</td></tr><tr><td>val_f1</td><td>▁▁▂▃▁▅▄▅▅▆▅▆█▆▆▆▇▆▅▇▆▆▅▆▇▇▇▆██▇▆▇█▆▅▆▅▇▇</td></tr><tr><td>val_loss_epoch</td><td>█▇▆▆▅▄▄▃▄▃▄▂▁▃▂▂▂▃▃▂▂▂▂▂▁▂▂▂▂▁▁▂▁▁▂▂▂▂▁▂</td></tr><tr><td>val_loss_step</td><td>██▇▇▆▆▅▅▃▂▅▄▄▃▄▄▂▅▂▄▃▃▃▃▄▃▄▃▃▂▁▂▃▃▂▃▄▃▂▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.71323</td></tr><tr><td>train_auc</td><td>0.65694</td></tr><tr><td>train_f1</td><td>0.53154</td></tr><tr><td>train_loss_epoch</td><td>0.56072</td></tr><tr><td>train_loss_step</td><td>0.61758</td></tr><tr><td>trainer/global_step</td><td>999</td></tr><tr><td>val_acc</td><td>0.71475</td></tr><tr><td>val_auc</td><td>0.66694</td></tr><tr><td>val_f1</td><td>0.55385</td></tr><tr><td>val_loss_epoch</td><td>0.55089</td></tr><tr><td>val_loss_step</td><td>0.59809</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_16_onehot_1</strong> at: <a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/hw0cxu7b' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold/runs/hw0cxu7b</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230509_161910-hw0cxu7b\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d184a2632293455a9fca774cd9ed7668",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Thomas\\15_PLA\\notebooks\\wandb\\run-20230509_165743-1lpmrywn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/1lpmrywn' target=\"_blank\">MLP_2_16_onehot_1</a></strong> to <a href='https://wandb.ai/thoomas/PLA_042923_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_042923_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/1lpmrywn' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold/runs/1lpmrywn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 368   \n",
      "2  | head        | Sequential       | 154   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 17    \n",
      "--------------------------------------------------\n",
      "539       Trainable params\n",
      "0         Non-trainable params\n",
      "539       Total params\n",
      "0.002     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7109066b8f548469c4c85ef0bfc773d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▅▅▅▆▆▇▇▇▇███▇▇▇█▇▇▇▇▇▇▇▇▇▇▇██▇▇██▇██▇██</td></tr><tr><td>train_auc</td><td>▂▁▁▁▅▅▆▆▆▆█▇█▇▆▆█▇▇▇▇▇▆▅▅▇▇▇▇▇▇▇▇▇▇▇█▇▇█</td></tr><tr><td>train_f1</td><td>▇▃▁▁▆▆▇▇▇▇███▇▇▇█▇▇█▇█▇▆▆█▇▇▇▇▇▇██▇▇████</td></tr><tr><td>train_loss_epoch</td><td>█▇▇▆▄▃▃▂▂▂▂▁▂▂▂▂▂▂▂▁▂▁▂▂▂▂▁▁▁▁▁▁▂▁▂▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▇▇▆▅▆▄▅▄▃▃▁▆▄▃▅▅▄▄▁▃▅█▄▄▂▆▂▂▅▆▄▄▄▅▄▃▆▅▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▁▂▂▁▃▃▂▃▃▂▄▄▂▂▄▂▂▅▅▂▆▆▂▆▆▃▇▇▃▃▇▃▃█▃</td></tr><tr><td>val_acc</td><td>▁▁▁▁▆▆█▆▅▆▅▅▅▅▆▆█▆▆▆▆▆▆▄▆▆▅▆▅▅█▆▆▅▆▆▆▅▅▆</td></tr><tr><td>val_auc</td><td>▁▁▁▁█▅▇▅▅▅▆▄▄▅▅▆▇▆▅▅▅▄▆▃▅▅▅▄▆▅█▅▅▅▅▅▅▄▄▅</td></tr><tr><td>val_f1</td><td>▁▁▁▁█▆█▆▆▆▇▅▅▆▆▇▇▆▆▆▆▅▆▄▆▅▆▅▇▆█▆▆▆▆▆▆▅▅▅</td></tr><tr><td>val_loss_epoch</td><td>█▇█▇▆▄▃▂▂▂▂▂▂▃▂▃▃▂▁▂▂▄▂▄▃▄▂▂▂▂▂▂▃▂▃▂▃▃▅▁</td></tr><tr><td>val_loss_step</td><td>▇█▇█▆▅▅▆▃▅▅▃▅▅▅▄▄▆▄▃▄▅▄▅▅█▅▅▅▅▂▅▃▄▄▄▆▅▄▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.71241</td></tr><tr><td>train_auc</td><td>0.66035</td></tr><tr><td>train_f1</td><td>0.54188</td></tr><tr><td>train_loss_epoch</td><td>0.55575</td></tr><tr><td>train_loss_step</td><td>0.50259</td></tr><tr><td>trainer/global_step</td><td>999</td></tr><tr><td>val_acc</td><td>0.65902</td></tr><tr><td>val_auc</td><td>0.58945</td></tr><tr><td>val_f1</td><td>0.39535</td></tr><tr><td>val_loss_epoch</td><td>0.55619</td></tr><tr><td>val_loss_step</td><td>0.45765</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_16_onehot_1</strong> at: <a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/1lpmrywn' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold/runs/1lpmrywn</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230509_165743-1lpmrywn\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a97b0c009b75446a880e1bb89b45fe13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01693333333435779, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Thomas\\15_PLA\\notebooks\\wandb\\run-20230509_174103-uthbcyiu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/uthbcyiu' target=\"_blank\">MLP_2_16_onehot_1</a></strong> to <a href='https://wandb.ai/thoomas/PLA_042923_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_042923_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/uthbcyiu' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold/runs/uthbcyiu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 368   \n",
      "2  | head        | Sequential       | 154   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 281   \n",
      "--------------------------------------------------\n",
      "803       Trainable params\n",
      "0         Non-trainable params\n",
      "803       Total params\n",
      "0.003     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a1d95dc4ad7499ba944ed681d9f142a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▅▅▅▇▇██▇▇▇▇▇▇█▇███▇█▇▇████▇▇█▇▇███▇█▇▇▇</td></tr><tr><td>train_auc</td><td>▁▁▁▁▅▇█▇▇▇▇▇▇▇█▆▇▇█▆█▇▇██▇▇▇▇▇▆▆▇██▇▇▆▅▇</td></tr><tr><td>train_f1</td><td>▇▂▁▁▆▇██▇▇▇▇▇██▇█▇█▇█▇▇███▇▇▇▇▇▇█████▇▆█</td></tr><tr><td>train_loss_epoch</td><td>█▇▆▆▃▃▂▂▂▂▂▂▃▂▁▂▂▂▂▂▁▂▂▂▂▂▁▂▂▁▂▂▁▁▁▂▂▂▂▂</td></tr><tr><td>train_loss_step</td><td>▇▇██▇▅▄▅▆▄▅▆▆▆▃▄▅▅▄▇▃▄▅▇▃▅▃▆▇▃▇▆▄▂▁▅▅▃▆▅</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▁▂▂▁▃▃▂▃▃▂▄▄▂▂▄▂▂▅▅▂▆▆▂▆▆▃▇▇▃▃▇▃▃█▃</td></tr><tr><td>val_acc</td><td>▁▁▁▁▇█▅▅▅▆▅▅▅▅▅▅▅▅▅▆▅▇▅▅▆▆▆▅▆█▅▅▅▇▅▅▆▅▅▅</td></tr><tr><td>val_auc</td><td>▁▁▁▁██▅▅▆▆▅▆▅▆▅▆▆▆▆▅▅▇▆▅▆▆▆▆▆█▆▆▆█▆▆▆▆▆▅</td></tr><tr><td>val_f1</td><td>▁▁▁▁██▆▆▇▆▆▆▆▇▆▇▇▇▇▆▆▇▇▆▆▆▇▇▇█▇▇▇█▇▇▇▇▇▆</td></tr><tr><td>val_loss_epoch</td><td>██▇▇▄▃▂▂▃▂▄▂▂▁▂▁▃▃▄▃▃▃▃▂▂▃▃▂▂▄▂▃▂▁▃▄▂▃▃▂</td></tr><tr><td>val_loss_step</td><td>▇██▆▄▃▃▅▄▂▄▂▆▅▁▅▃▆▆▃▄▄▅▅▅▅▄▄▄▄▂▅▄▁▃▄▃▅▄▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.70008</td></tr><tr><td>train_auc</td><td>0.65643</td></tr><tr><td>train_f1</td><td>0.54658</td></tr><tr><td>train_loss_epoch</td><td>0.57465</td></tr><tr><td>train_loss_step</td><td>0.56421</td></tr><tr><td>trainer/global_step</td><td>999</td></tr><tr><td>val_acc</td><td>0.65574</td></tr><tr><td>val_auc</td><td>0.5997</td></tr><tr><td>val_f1</td><td>0.44444</td></tr><tr><td>val_loss_epoch</td><td>0.56996</td></tr><tr><td>val_loss_step</td><td>0.55573</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_16_onehot_1</strong> at: <a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/uthbcyiu' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold/runs/uthbcyiu</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230509_174103-uthbcyiu\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a30c49730d74638b00948f29c039229",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Thomas\\15_PLA\\notebooks\\wandb\\run-20230509_182428-djke794w</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/djke794w' target=\"_blank\">MLP_2_16_onehot_2</a></strong> to <a href='https://wandb.ai/thoomas/PLA_042923_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_042923_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/djke794w' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold/runs/djke794w</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 368   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "522       Trainable params\n",
      "0         Non-trainable params\n",
      "522       Total params\n",
      "0.002     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4a5c586af7e43168a8b5f98abf844ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▅▅▅▆▇▇█▇▇▇▇▇▇▇▇█████▇▇▇██▇▇▇▇▇██▇▇▇█▇▇▇</td></tr><tr><td>train_auc</td><td>▁▂▁▂▃▆▇▇▇▇▇▇▆▆▇▇▇▇▇▇▇▆▇▇▇█▇▆▆▆▇▇▇▆▆▆▇▆▆▆</td></tr><tr><td>train_f1</td><td>▆▂▁▁▄▆▇▇▇█▇▇▇▇▇▇█▇▇▇█▆▇▇▇█▇▆▆▇▇▇▇▇▆▇▇▇▇▇</td></tr><tr><td>train_loss_epoch</td><td>█▆▆▆▄▂▂▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▃▂▂▁▂▁▂▂▂▂▂▂▂▁</td></tr><tr><td>train_loss_step</td><td>█▇▇▇▇▅▅▄▃▅▃▅▅▅▇▅▃▇▅▂▇▅▇▄▃▄▅▇▃▃▇▁▅▃▆▅▄▂▄▄</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▁▂▂▁▃▃▂▃▃▂▄▄▂▂▄▂▂▅▅▂▆▆▂▆▆▃▇▇▃▃▇▃▃█▃</td></tr><tr><td>val_acc</td><td>▁▁▁▁▃▄▇▇█▅▆▅▅▅▆▄▆▅▅▅▇▅▅▇▇██▅▅▆▇▆▇▅▇▇█▅▇▅</td></tr><tr><td>val_auc</td><td>▁▁▁▁▃▄▇▇█▅▆▅▅▅▆▄▆▅▅▆▇▆▅▇▇██▅▅▆▇▆▇▅▇▇█▅▇▆</td></tr><tr><td>val_f1</td><td>▁▁▁▁▄▅▇██▆▇▆▆▆▇▅▇▆▆▆▇▆▆▇███▆▆▇█▇█▆███▆▇▇</td></tr><tr><td>val_loss_epoch</td><td>███▇▄▃▂▃▂▃▁▂▂▃▂▂▁▂▃▁▂▂▃▂▁▂▂▂▂▂▂▁▂▂▂▂▂▂▂▂</td></tr><tr><td>val_loss_step</td><td>▇██▆▆▃▃▅▂▁▅▂▄▅▆▃▅▃▄▃▂▃█▂▄▄▃▅▃▄▁▄▃▄▅▂▅▃▅▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.67488</td></tr><tr><td>train_auc</td><td>0.61679</td></tr><tr><td>train_f1</td><td>0.46631</td></tr><tr><td>train_loss_epoch</td><td>0.56997</td></tr><tr><td>train_loss_step</td><td>0.55983</td></tr><tr><td>trainer/global_step</td><td>999</td></tr><tr><td>val_acc</td><td>0.73026</td></tr><tr><td>val_auc</td><td>0.65001</td></tr><tr><td>val_f1</td><td>0.5</td></tr><tr><td>val_loss_epoch</td><td>0.51795</td></tr><tr><td>val_loss_step</td><td>0.49402</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_16_onehot_2</strong> at: <a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/djke794w' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold/runs/djke794w</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230509_182428-djke794w\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9ea2e6c3f27427fbb2d2f3b3f20260e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01691666666883975, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Thomas\\15_PLA\\notebooks\\wandb\\run-20230509_190156-kas25x8h</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/kas25x8h' target=\"_blank\">MLP_2_16_onehot_2</a></strong> to <a href='https://wandb.ai/thoomas/PLA_042923_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_042923_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/kas25x8h' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold/runs/kas25x8h</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 368   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "522       Trainable params\n",
      "0         Non-trainable params\n",
      "522       Total params\n",
      "0.002     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12354737a8824b0a8112bb48bc69ccba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▇▇████████████████▇▇██▇██▇▇███▇████▇▇██</td></tr><tr><td>train_auc</td><td>▁▄▃▄▄▄▄▄▄▄▄▄▄▄▅▆▅▄▅▄▅▄▅▃▅▆▅▃▆▆▇▄▆█▅█▄▄▅▆</td></tr><tr><td>train_f1</td><td>█▃▂▁▁▁▁▁▁▁▁▁▁▁▂▃▂▁▁▂▃▂▂▂▂▂▃▁▃▃▃▂▃▄▂▄▂▂▂▂</td></tr><tr><td>train_loss_epoch</td><td>█▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▂▂▂▂▂▂▂▂▂▁▂▂▁▂▂▂▁▂▂▁▂▂▂▁</td></tr><tr><td>train_loss_step</td><td>▇▄▅▅▇▄▄▄▁▄▃▂▂▆▄▄▅█▅▂▄▅▆▁▄▄▄▆▁▃▅▂▇▇▆▄▃▂▅▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▁▂▂▁▃▃▂▃▃▂▄▄▂▂▄▂▂▅▅▂▆▆▂▆▆▃▇▇▃▃▇▃▃█▃</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_epoch</td><td>██▆▄▄▄▃▄▂▅▄▂▄▃▄▃▁▃▃▃▃▃▃▄▂▂▃▂▂▂▃▁▁▃▁▂▂▂▃▄</td></tr><tr><td>val_loss_step</td><td>▇▇▇▄▆▅▅▇▆▄▅▃▆▆█▅▆▆▆▆▄▅▆▅▆▄▅▆▄▆▃▇▅▇▅▁▅▆▆█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.62397</td></tr><tr><td>train_auc</td><td>0.51227</td></tr><tr><td>train_f1</td><td>0.08032</td></tr><tr><td>train_loss_epoch</td><td>0.65095</td></tr><tr><td>train_loss_step</td><td>0.63275</td></tr><tr><td>trainer/global_step</td><td>999</td></tr><tr><td>val_acc</td><td>0.65461</td></tr><tr><td>val_auc</td><td>0.5</td></tr><tr><td>val_f1</td><td>0.0</td></tr><tr><td>val_loss_epoch</td><td>0.65709</td></tr><tr><td>val_loss_step</td><td>0.69073</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_16_onehot_2</strong> at: <a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/kas25x8h' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold/runs/kas25x8h</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230509_190156-kas25x8h\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33e22f4f0ac440889c8c7c0e124e5945",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01693333333435779, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Thomas\\15_PLA\\notebooks\\wandb\\run-20230509_193849-mlqtxj6m</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/mlqtxj6m' target=\"_blank\">MLP_2_16_onehot_2</a></strong> to <a href='https://wandb.ai/thoomas/PLA_042923_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_042923_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/mlqtxj6m' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold/runs/mlqtxj6m</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 368   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "522       Trainable params\n",
      "0         Non-trainable params\n",
      "522       Total params\n",
      "0.002     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d82eb9339e8d4e0ba2e8813adff3993c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▅▅▅▆▆▇▆▆▇▇▇▆▇▇█▇▇▇▇▇▇▇▇█▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>train_auc</td><td>▁▁▃▃▄▄▅▆▆▅▇▆▆▅▅▆▇▇▆▇▇▆▇▇▇█▆▇▆▇▇▇▆▆▇▆▇▆▆▇</td></tr><tr><td>train_f1</td><td>▄▃▂▁▃▃▅▆▆▆▇▅▆▄▄▆▇▆▆▇▇▆▆▇▇█▆▆▆▇▆▇▆▆▇▅▇▅▆▇</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▄▄▄▄▄▄▄▂▃▂▃▂▄▃▄▂▄▃▁▄▃▅▃▃▂▃▄▃▂▅▁▃▁▄▂▂▂▃▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▁▂▂▁▃▃▂▃▃▂▄▄▂▂▄▂▂▅▅▂▆▆▂▆▆▃▇▇▃▃▇▃▃█▃</td></tr><tr><td>val_acc</td><td>▁▁▂▂▂▃▆▆▅▂▇▆▅▄▇▄▆▆▇▅▇█▆▇▇▇▇█▇█▇▇█▇▇▇▇▇▅▇</td></tr><tr><td>val_auc</td><td>▁▂▂▂▂▂▆▆▅▂▆▆▅▄█▄▆▇▇▅▇█▆▇▇▇▇█▇█▇▇█▇██▇▇▅▇</td></tr><tr><td>val_f1</td><td>▁▃▃▂▃▃▇▇▆▃▇▆▆▅█▅▇▇█▆▇█▇▇▇▇████▇▇████▇█▆█</td></tr><tr><td>val_loss_epoch</td><td>█▇▇▆▅▅▃▃▃▄▂▂▂▃▃▃▂▂▃▂▂▂▃▂▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁</td></tr><tr><td>val_loss_step</td><td>▇██▆▇▅▅▅▄▃▄▂▄▅▅▄▄▃▄▄▃▃▅▃▄▅▄▄▃▄▁▃▃▄▄▁▄▃▃▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.70608</td></tr><tr><td>train_auc</td><td>0.65194</td></tr><tr><td>train_f1</td><td>0.52394</td></tr><tr><td>train_loss_epoch</td><td>0.55522</td></tr><tr><td>train_loss_step</td><td>0.57687</td></tr><tr><td>trainer/global_step</td><td>999</td></tr><tr><td>val_acc</td><td>0.79605</td></tr><tr><td>val_auc</td><td>0.73625</td></tr><tr><td>val_f1</td><td>0.64773</td></tr><tr><td>val_loss_epoch</td><td>0.47648</td></tr><tr><td>val_loss_step</td><td>0.42175</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_16_onehot_2</strong> at: <a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/mlqtxj6m' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold/runs/mlqtxj6m</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230509_193849-mlqtxj6m\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdab24713c8f42ebaaa061b185f70af7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Thomas\\15_PLA\\notebooks\\wandb\\run-20230509_201601-m3d6m3eq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/m3d6m3eq' target=\"_blank\">MLP_2_16_onehot_2</a></strong> to <a href='https://wandb.ai/thoomas/PLA_042923_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_042923_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/m3d6m3eq' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold/runs/m3d6m3eq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 368   \n",
      "2  | head        | Sequential       | 154   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 17    \n",
      "--------------------------------------------------\n",
      "539       Trainable params\n",
      "0         Non-trainable params\n",
      "539       Total params\n",
      "0.002     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e44f6139b97b4773be39914b06196be4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▅▅▅▆▆▇▆▇██▇▇▇█▇▇█▇▇▇▇▇▇█▇▇▇█▇▇▇▇▇▇▇█▇▇▇</td></tr><tr><td>train_auc</td><td>▁▂▂▂▄▅▇▄▆█▇▅▆▆▇▆▆▇▇▆▇▆▇▇█▇▇▆▇▇▇▇▇▇▇▆█▆▇▆</td></tr><tr><td>train_f1</td><td>▆▃▁▁▅▆▇▅▆██▆▇▇▇▇▇▇▇▇▇▇█▇█▇▇▇█▇█▇▇▇█▇█▇▇▇</td></tr><tr><td>train_loss_epoch</td><td>█▆▆▆▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▂▂▂▁▁▁▂▂▂▂</td></tr><tr><td>train_loss_step</td><td>▇▇█▄▅▅▆▅▃▄▆▃▃▄▃▄▄▄▅▃▅▂▂▃▃▄▂▃▃▅▂▄▃▂▁▂▆▅▄▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▁▂▂▁▃▃▂▃▃▂▄▄▂▂▄▂▂▅▅▂▆▆▂▆▆▃▇▇▃▃▇▃▃█▃</td></tr><tr><td>val_acc</td><td>▁▁▁▁▅▆▇▇▇▅▆█▆▇▇▇██▅█▅▇▃▄▄▆▆▆▄▄▄▄▄▄▄▄▆▄▆▅</td></tr><tr><td>val_auc</td><td>▁▁▁▁▅▆▇██▅▆█▆▇▇▆██▅▇▅▆▃▄▄▆▆▆▄▄▄▄▄▄▃▄▆▄▆▄</td></tr><tr><td>val_f1</td><td>▁▁▁▁▆▇▇██▆▇█▇▇▇▇██▆█▆▇▄▅▅▆▆▇▅▅▅▅▅▅▄▅▇▅▇▅</td></tr><tr><td>val_loss_epoch</td><td>███▇▅▄▂▄▃▂▁▃▂▂▂▁▂▂▂▂▂▂▂▂▂▁▂▂▂▂▂▂▃▂▂▂▁▂▁▂</td></tr><tr><td>val_loss_step</td><td>██▇▇▆▅▂▄▂▄▆▆▃▃▃▃▂▄▂▂▁▃▄▅▄▂▄▃▆▃▅▄▁▄▂▁▂▂▃▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.67077</td></tr><tr><td>train_auc</td><td>0.61306</td></tr><tr><td>train_f1</td><td>0.46174</td></tr><tr><td>train_loss_epoch</td><td>0.57785</td></tr><tr><td>train_loss_step</td><td>0.5638</td></tr><tr><td>trainer/global_step</td><td>999</td></tr><tr><td>val_acc</td><td>0.71382</td></tr><tr><td>val_auc</td><td>0.61046</td></tr><tr><td>val_f1</td><td>0.4</td></tr><tr><td>val_loss_epoch</td><td>0.53266</td></tr><tr><td>val_loss_step</td><td>0.57665</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_16_onehot_2</strong> at: <a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/m3d6m3eq' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold/runs/m3d6m3eq</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230509_201601-m3d6m3eq\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec8954c51cb4478299f42ac6a6d9c283",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Thomas\\15_PLA\\notebooks\\wandb\\run-20230509_205330-hw35oifq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/hw35oifq' target=\"_blank\">MLP_2_16_onehot_2</a></strong> to <a href='https://wandb.ai/thoomas/PLA_042923_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_042923_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/hw35oifq' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold/runs/hw35oifq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 368   \n",
      "2  | head        | Sequential       | 154   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 281   \n",
      "--------------------------------------------------\n",
      "803       Trainable params\n",
      "0         Non-trainable params\n",
      "803       Total params\n",
      "0.003     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▅▅▅▇█▇▇█▇██▇▇██▇█▇███▇█▇▇████▇████▇▇█▇█</td></tr><tr><td>train_auc</td><td>▁▁▁▂▆▇▇▇▇▆█▇▆▇█▇▇▇▆▇▇█▇▇▆▆█▇▇█▇▇▇▇▇▇▇▇▇█</td></tr><tr><td>train_f1</td><td>▇▃▁▁▆█▇█▇▇█▇▇▇██▇▇▇▇███▇▇▆██▇██▇▇▇█▇▇▇██</td></tr><tr><td>train_loss_epoch</td><td>█▇▆▆▃▃▃▂▂▃▂▂▃▂▂▂▂▂▁▂▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁</td></tr><tr><td>train_loss_step</td><td>█▇▆▇▃▅▆▅▅▅▃▂▆▃▄▃▃▄▁▅▃▅▃▄▃▃▅▄▂▄▄▄▃█▄▅▄▄▄▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▁▂▂▁▃▃▂▃▃▂▄▄▂▂▄▂▂▅▅▂▆▆▂▆▆▃▇▇▃▃▇▃▃█▃</td></tr><tr><td>val_acc</td><td>▁▁▁▁▄▄█▆█▇▆█▇▄▆▆▇▇▆▆▇▇▇▆▆▇▇█▆▆█▇▅█▇██▇▇▆</td></tr><tr><td>val_auc</td><td>▁▁▁▁▄▄█▆██▆█▇▄▇▆▇▇▆▇▇█▇▆▆█▇█▆▆██▅█████▇▆</td></tr><tr><td>val_f1</td><td>▁▁▁▁▅▅█▇██▇██▅▇▇▇▇▇▇███▇▇█▇█▇▇██▆█████▇▇</td></tr><tr><td>val_loss_epoch</td><td>██▇▇▄▃▂▁▃▃▂▃▂▃▂▂▂▂▂▃▂▂▂▂▂▂▂▁▂▂▂▂▂▂▃▁▂▃▂▂</td></tr><tr><td>val_loss_step</td><td>▇█▇▇▅▄▄▃▄▂▃▅▃▃▅▄▄▅▂▂▃▃▅▄▂▁▃▃▄▄▃▃▅▁▂▂▃▃▄▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.69212</td></tr><tr><td>train_auc</td><td>0.6502</td></tr><tr><td>train_f1</td><td>0.53988</td></tr><tr><td>train_loss_epoch</td><td>0.5619</td></tr><tr><td>train_loss_step</td><td>0.5069</td></tr><tr><td>trainer/global_step</td><td>999</td></tr><tr><td>val_acc</td><td>0.74671</td></tr><tr><td>val_auc</td><td>0.67832</td></tr><tr><td>val_f1</td><td>0.55491</td></tr><tr><td>val_loss_epoch</td><td>0.51688</td></tr><tr><td>val_loss_step</td><td>0.49458</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_16_onehot_2</strong> at: <a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/hw35oifq' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold/runs/hw35oifq</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230509_205330-hw35oifq\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fa6515b86604ef780c18076aec0d708",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Thomas\\15_PLA\\notebooks\\wandb\\run-20230509_213140-ky2ilifa</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/ky2ilifa' target=\"_blank\">MLP_2_16_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_042923_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_042923_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/ky2ilifa' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold/runs/ky2ilifa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 368   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "522       Trainable params\n",
      "0         Non-trainable params\n",
      "522       Total params\n",
      "0.002     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f7971782db84dee8f63cb2291556fa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▅▅▅▆▇██▇█▇▇▇▇█▇▇▇▇▇█▇▆▇▇█▇▇▇▇▇▇▇▇▇██▇▇▇</td></tr><tr><td>train_auc</td><td>▁▂▂▂▃▇██▇█▇▇▆▇▇▇▇▇▆▇█▆▆▇▆▇▇▆▆▆▇▇▇▇▇▇▇▆▆▆</td></tr><tr><td>train_f1</td><td>▆▂▁▁▄▇██▇█▇▇▇▇█▇█▇▇▇█▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>train_loss_epoch</td><td>█▆▆▆▄▃▂▂▂▂▁▂▂▂▂▂▁▂▂▁▁▁▃▂▂▁▂▂▂▂▂▂▂▂▂▂▁▁▂▂</td></tr><tr><td>train_loss_step</td><td>▇▆▇▆▆▄▄▃▄▄▂▄▁▆▂▃▁█▅▃▂▄█▃▄▃▅█▃▁▅▆▅▅▅▇▂▁▅▄</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▁▂▂▁▃▃▂▃▃▂▄▄▂▂▄▂▂▅▅▂▆▆▂▆▆▃▇▇▃▃▇▃▃█▃</td></tr><tr><td>val_acc</td><td>▁▁▁▁▆▇▇▇▇▇▇▇▇▇▆▆▇▆▆▆▆▇▆▇▇▇▇▆▇▆▇▇▇▇█▇█▆▆▇</td></tr><tr><td>val_auc</td><td>▁▁▁▁▅▆▇▇▇▇▇▆▇▆▆▆▇▆▆▇▇▇▆▇▇▇▆▆▇▇▇▇▇▆█▇█▆▆▆</td></tr><tr><td>val_f1</td><td>▁▁▁▁▆▇███▇▇▇▇▇▇▇█▇▇▇▇▇▇▇█▇▇▇▇▇▇▇█▇█▇█▇▇▇</td></tr><tr><td>val_loss_epoch</td><td>██▇▆▃▂▂▂▁▂▂▁▂▂▂▃▂▃▁▂▁▂▂▂▂▁▂▁▁▂▂▂▂▂▂▂▃▁▂▂</td></tr><tr><td>val_loss_step</td><td>▆██▆▅▄▄▄▃▂▃▁▅▄▆▂▄▇▃▅▃▅▃▅▄▁▃▄▂▄▃▃▃▄▅▂▇▄▃▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.67077</td></tr><tr><td>train_auc</td><td>0.61055</td></tr><tr><td>train_f1</td><td>0.45144</td></tr><tr><td>train_loss_epoch</td><td>0.58594</td></tr><tr><td>train_loss_step</td><td>0.58686</td></tr><tr><td>trainer/global_step</td><td>999</td></tr><tr><td>val_acc</td><td>0.73355</td></tr><tr><td>val_auc</td><td>0.6636</td></tr><tr><td>val_f1</td><td>0.53179</td></tr><tr><td>val_loss_epoch</td><td>0.52255</td></tr><tr><td>val_loss_step</td><td>0.52928</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_16_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/ky2ilifa' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold/runs/ky2ilifa</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230509_213140-ky2ilifa\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a21a8e7e09dc4587992b378b8deb8b6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Thomas\\15_PLA\\notebooks\\wandb\\run-20230509_225730-8tbbirvt</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/8tbbirvt' target=\"_blank\">MLP_2_16_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_042923_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_042923_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/8tbbirvt' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold/runs/8tbbirvt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 368   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "522       Trainable params\n",
      "0         Non-trainable params\n",
      "522       Total params\n",
      "0.002     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37ee7c3f3c1d489faa78e6bc13b4566d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇█▇██▇▇▇▇▇▇█▇▇▇▇▇▇██▇▇▇</td></tr><tr><td>train_auc</td><td>▁▁▁▂▂▂▂▂▂▂▂▂▂▂▁▃▆▂▅▂█▅▆▄▄▄▄▃▅▄▅▅▆▅▃█▅▄▂▅</td></tr><tr><td>train_f1</td><td>█▃▂▁▁▁▁▁▁▁▁▁▁▁▁▂▂▁▂▁▃▂▃▂▂▂▂▂▂▃▂▂▃▃▂▃▂▂▁▂</td></tr><tr><td>train_loss_epoch</td><td>█▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▁▁▂▁▁▁▁▁▂▃▂▂▂▂▁</td></tr><tr><td>train_loss_step</td><td>▆▄▅▅▇▅▅▅▂▄▄▃▃▆▄▄▅█▅▄▄▅▇▁▅▆▄▇▂▃▅▂▆▅█▅▃▃▅▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▁▂▂▁▃▃▂▃▃▂▄▄▂▂▄▂▂▅▅▂▆▆▂▆▆▃▇▇▃▃▇▃▃█▃</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_epoch</td><td>██▆▄▄▄▃▄▂▅▄▂▃▃▄▃▁▂▂▃▂▂▃▄▁▂▃▃▃▃▃▂▂▄▂▃▃▂▄▄</td></tr><tr><td>val_loss_step</td><td>▇▇▇▄▆▅▅▇▅▄▅▂▆▆█▄▅▅▅▆▃▅▆▅▅▃▅▆▅▆▄▇▅▇▆▁▅▆▆█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.61741</td></tr><tr><td>train_auc</td><td>0.50857</td></tr><tr><td>train_f1</td><td>0.08268</td></tr><tr><td>train_loss_epoch</td><td>0.65194</td></tr><tr><td>train_loss_step</td><td>0.63157</td></tr><tr><td>trainer/global_step</td><td>999</td></tr><tr><td>val_acc</td><td>0.66118</td></tr><tr><td>val_auc</td><td>0.5</td></tr><tr><td>val_f1</td><td>0.0</td></tr><tr><td>val_loss_epoch</td><td>0.6579</td></tr><tr><td>val_loss_step</td><td>0.68703</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_16_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/8tbbirvt' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold/runs/8tbbirvt</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230509_225730-8tbbirvt\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb3c18aafdff476bbe5a61b94835f3c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Thomas\\15_PLA\\notebooks\\wandb\\run-20230510_003359-v8w81f91</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/v8w81f91' target=\"_blank\">MLP_2_16_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_042923_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_042923_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/v8w81f91' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold/runs/v8w81f91</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 368   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "522       Trainable params\n",
      "0         Non-trainable params\n",
      "522       Total params\n",
      "0.002     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b65b64dc28d42998d64cf91a42d0d4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▃▄▅▆▆▇▇▇▇▇▇▇▆▇▇█▇▇▇█▇▆███▇▇▇▇█▇█▇▇█▇▇█▇</td></tr><tr><td>train_auc</td><td>▁▂▃▃▄▅▆▆▆▆▇▇▆▅▆▇█▅▇▇█▆▆█▇█▆▇▇▇█▇█▇▇█▇▇▇▆</td></tr><tr><td>train_f1</td><td>▄▄▂▁▄▅▆▆▇▇▇▇▆▄▅▇█▅▇▇█▆▅█▇█▆▇▆▇█▆█▇▇██▆▇▆</td></tr><tr><td>train_loss_epoch</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▄▃▃▄▃▃▂▃▂▂▃▁▃▂▂▁▃▂▂▁▂▃▁▃▂▂▄▂▁▃▃▃▂▃▃▂▂▂▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▁▂▂▁▃▃▂▃▃▂▄▄▂▂▄▂▂▅▅▂▆▆▂▆▆▃▇▇▃▃▇▃▃█▃</td></tr><tr><td>val_acc</td><td>▁▄▃▄▄▄▇█▇▄█▇█▆▇▇██▇██▇▇██████████▇▇██▇▇▇</td></tr><tr><td>val_auc</td><td>▁▃▂▃▃▄▆▇▇▃▇▆█▅▇▇████▇█▆████▇███▇█▇███▇█▇</td></tr><tr><td>val_f1</td><td>▁▃▁▃▃▄▇█▇▂▇▇█▅▇▇████▇█▇████▇███▇█▇███▇█▇</td></tr><tr><td>val_loss_epoch</td><td>█▇▆▆▄▄▃▃▃▃▂▂▃▃▃▃▁▂▂▂▂▂▂▂▂▁▂▁▂▂▂▂▁▂▂▂▂▂▂▂</td></tr><tr><td>val_loss_step</td><td>▇█▇▆▆▅▄▄▃▄▂▁▄▄▇▂▃▄▃▄▂▄▃▃▃▁▃▄▂▂▁▂▂▃▃▂▆▃▂▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.69048</td></tr><tr><td>train_auc</td><td>0.62773</td></tr><tr><td>train_f1</td><td>0.47125</td></tr><tr><td>train_loss_epoch</td><td>0.57478</td></tr><tr><td>train_loss_step</td><td>0.59802</td></tr><tr><td>trainer/global_step</td><td>999</td></tr><tr><td>val_acc</td><td>0.79605</td></tr><tr><td>val_auc</td><td>0.72506</td></tr><tr><td>val_f1</td><td>0.62651</td></tr><tr><td>val_loss_epoch</td><td>0.50313</td></tr><tr><td>val_loss_step</td><td>0.51959</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_16_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/v8w81f91' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold/runs/v8w81f91</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230510_003359-v8w81f91\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "602056b5c6eb48f1b05bab3fc30b28db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Thomas\\15_PLA\\notebooks\\wandb\\run-20230510_014910-1p3jb262</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/1p3jb262' target=\"_blank\">MLP_2_16_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_042923_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_042923_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/1p3jb262' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold/runs/1p3jb262</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 368   \n",
      "2  | head        | Sequential       | 154   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 17    \n",
      "--------------------------------------------------\n",
      "539       Trainable params\n",
      "0         Non-trainable params\n",
      "539       Total params\n",
      "0.002     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8920462d5a54714ba974b26ec36f979",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▅▆▆▆▇██▇▇▇██▇▇▇███████▇█████▇███▇██████</td></tr><tr><td>train_auc</td><td>▁▂▂▂▃▅▇▇▆▇▇▇▇▇▆▇▇▇██▇▇█▇███▇▇▇██▇▇█▇█▇▇▇</td></tr><tr><td>train_f1</td><td>▆▃▁▁▃▆▇▇▆▇▇▇▇▇▇▇▇▇███▇█████▇▇▇██▇▇█▇███▇</td></tr><tr><td>train_loss_epoch</td><td>█▆▆▆▄▄▂▄▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▂▁▂▁▁▂▁▂▁▂▂▁▁▁▂▂</td></tr><tr><td>train_loss_step</td><td>▇▇█▄▆▄▄█▅▃▅▃▆▄▄▅▃▁▃▃▄▃▃▃▄▁▃▁▂▄▃▆▂▆▂▂▄▃▃▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▁▂▂▁▃▃▂▃▃▂▄▄▂▂▄▂▂▅▅▂▆▆▂▆▆▃▇▇▃▃▇▃▃█▃</td></tr><tr><td>val_acc</td><td>▁▁▁▁▅█▇▅▇▇▇▇▇▆▇▆█▆▆▇▆▆▄▄▅▆▆▆▅▇▆▇▆▇▅▅▆▅█▅</td></tr><tr><td>val_auc</td><td>▁▁▁▁▄█▆█▆▆▆▇▇▅▇▆▇▆▆▆▅▅▃▄▄▆▅▆▅▆▆▆▆▆▄▅▅▄▇▅</td></tr><tr><td>val_f1</td><td>▁▁▁▁▅█▇█▇▇▇█▇▆▇▇█▇▇▇▆▆▄▅▆▆▆▇▆▇▇▇▇▇▅▆▆▅█▆</td></tr><tr><td>val_loss_epoch</td><td>███▇▅▅▂▆▂▃▂▃▂▂▃▁▂▂▂▁▃▁▂▂▁▁▂▂▃▂▁▁▂▂▁▂▁▂▂▁</td></tr><tr><td>val_loss_step</td><td>██▇▇▆▅▄▄▄▃▆▄▄▄▅▄▃▅▅▄▃▅▄▅▄▃▄▄▇▄▄▅▄▅▄▄▃▄▄▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.68227</td></tr><tr><td>train_auc</td><td>0.62108</td></tr><tr><td>train_f1</td><td>0.46473</td></tr><tr><td>train_loss_epoch</td><td>0.58311</td></tr><tr><td>train_loss_step</td><td>0.57799</td></tr><tr><td>trainer/global_step</td><td>999</td></tr><tr><td>val_acc</td><td>0.71382</td></tr><tr><td>val_auc</td><td>0.62737</td></tr><tr><td>val_f1</td><td>0.45963</td></tr><tr><td>val_loss_epoch</td><td>0.50032</td></tr><tr><td>val_loss_step</td><td>0.41968</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_16_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/1p3jb262' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold/runs/1p3jb262</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230510_014910-1p3jb262\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0afcd2c7257349ec89129782f7465d11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Thomas\\15_PLA\\notebooks\\wandb\\run-20230510_025006-7u15209b</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/7u15209b' target=\"_blank\">MLP_2_16_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_042923_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_042923_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/7u15209b' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold/runs/7u15209b</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 368   \n",
      "2  | head        | Sequential       | 154   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 281   \n",
      "--------------------------------------------------\n",
      "803       Trainable params\n",
      "0         Non-trainable params\n",
      "803       Total params\n",
      "0.003     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4822ab631e424080babe9a3481d473f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▄▅▅▇▇▇▇▇▇██▇▇▇▇▇▇▇▇▇███▇▇██▇▇█▇▇▇█▇▇▇▇▇</td></tr><tr><td>train_auc</td><td>▁▁▁▁▅▇▇▇▆▆▇▇▆▆▇▆▆▆▆▆▇▇█▇▆▆▇█▇▇▇▆▇▇▇▆▇▇▇▇</td></tr><tr><td>train_f1</td><td>▆▂▁▁▆▇▇▇▇▇▇▇▆▇█▇▇▆▆▇▇▇█▇▇▆▇█▇▇█▇▇▇█▇▇▇▇▇</td></tr><tr><td>train_loss_epoch</td><td>█▇▆▆▄▂▃▂▃▂▂▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▃▂▂▂▂▂▂▂▂▂▂</td></tr><tr><td>train_loss_step</td><td>█▇▅▆▃▃▅▅▆▅▂▄▆▅▄▁▄█▄▅▃▃▃▄▅▄▆▄▃▇▃▂▂▅▅▄▃▄▄▄</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▁▂▂▁▃▃▂▃▃▂▄▄▂▂▄▂▂▅▅▂▆▆▂▆▆▃▇▇▃▃▇▃▃█▃</td></tr><tr><td>val_acc</td><td>▁▁▁▁▅▆▆▆▇▇▆▇▇▆▆▇▇▆▆▇▇▇▆▇▇▇▇█▆▇██▇▇▇▇▇▇▇▆</td></tr><tr><td>val_auc</td><td>▁▁▁▁▅▆▆▆▇▇▆▇▇▆▆▆▇▆▆▇▇▇▇▇▇▇▇█▆▆██▇▇▇▇▇▇▇▇</td></tr><tr><td>val_f1</td><td>▁▁▁▁▆▆▇▇██▇▇█▇▇▇▇▇▇█▇▇▇▇██▇█▇▇██▇███▇██▇</td></tr><tr><td>val_loss_epoch</td><td>██▇▇▄▃▃▁▃▂▂▂▂▂▂▂▂▂▁▂▂▂▁▂▂▃▁▂▁▂▂▂▂▂▃▂▁▂▂▃</td></tr><tr><td>val_loss_step</td><td>██▇▇▅▄▄▅▃▅▃▂▄▅▄▄▃▃▄▃▆▄▃▅▄▆▄▅▂▅▃▃▅▃▂▄▁▃▄▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.68555</td></tr><tr><td>train_auc</td><td>0.63762</td></tr><tr><td>train_f1</td><td>0.51334</td></tr><tr><td>train_loss_epoch</td><td>0.5795</td></tr><tr><td>train_loss_step</td><td>0.57631</td></tr><tr><td>trainer/global_step</td><td>999</td></tr><tr><td>val_acc</td><td>0.73026</td></tr><tr><td>val_auc</td><td>0.68005</td></tr><tr><td>val_f1</td><td>0.56842</td></tr><tr><td>val_loss_epoch</td><td>0.54074</td></tr><tr><td>val_loss_step</td><td>0.61182</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_16_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/7u15209b' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold/runs/7u15209b</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230510_025006-7u15209b\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d3af07206b241d582e8ff436ed5edec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016933333330477276, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Thomas\\15_PLA\\notebooks\\wandb\\run-20230510_033925-bjuqdnn3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/bjuqdnn3' target=\"_blank\">MLP_2_16_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_042923_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_042923_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/bjuqdnn3' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold/runs/bjuqdnn3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 368   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "522       Trainable params\n",
      "0         Non-trainable params\n",
      "522       Total params\n",
      "0.002     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75d6e6c4e73846f49261f90c3d58c112",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▆▆▆▇██▇▇█▇█▇▇▇██▇▇▇██████▇▇▇▇██▇▇▇▇▇▇▇█</td></tr><tr><td>train_auc</td><td>▁▂▁▁▄▇▇▇▇█▇▇▆▆▇▇▇▇▇▆█▇▇▇▇█▇▆▆▇▇▇▅▇▆▅▆▆▇▇</td></tr><tr><td>train_f1</td><td>▇▂▁▁▅▇▇▇▇█▇█▇▇▇██▇▇▇█▇▇▇▇█▇▇▇▇▇▇▆▇▇▆▇▇▇▇</td></tr><tr><td>train_loss_epoch</td><td>█▆▆▆▃▂▂▂▂▃▂▂▂▂▂▂▂▂▂▂▁▂▂▂▂▁▁▁▁▂▂▂▂▂▂▂▁▂▂▁</td></tr><tr><td>train_loss_step</td><td>█▆▇▇▇▅▇▇▂▇▁▃▃▃▄▅▂▅▅▂▃▅▄▄▄▃▃▃▃▆▇▁▇▅▄█▅▃▆▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▁▂▂▁▃▃▂▃▃▂▄▄▂▂▄▂▂▅▅▂▆▆▂▆▆▃▇▇▃▃▇▃▃█▃</td></tr><tr><td>val_acc</td><td>▁▁▁▁▅▆█▇█▇▆▇▇▆▆▆█▇▇█▇▇▇███▇▇█▇█▇▆▆▇▆█▆▇▆</td></tr><tr><td>val_auc</td><td>▁▁▁▁▅▆█▇█▇▇▇▇▆▆▆█▇▇█▇▇▇███▇▇█▇█▇▆▆▇▆█▆▇▆</td></tr><tr><td>val_f1</td><td>▁▁▁▁▆▇███▇▇██▇▇▇█▇▇█▇▇▇███▇▇█▇█▇▇▇█▇█▇▇▇</td></tr><tr><td>val_loss_epoch</td><td>▇██▆▃▂▂▂▂▂▁▁▁▃▃▂▂▂▁▂▁▁▃▂▁▁▂▂▁▂▃▂▂▂▂▃▁▂▂▂</td></tr><tr><td>val_loss_step</td><td>▆▇▇▆▄▄▅▅▂▂▄▁▄▃█▅▄▄▂▄▂▅▆▄▃▃▃▄▃▃▁▃▁▅▃▂▂▅▃▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.69704</td></tr><tr><td>train_auc</td><td>0.61771</td></tr><tr><td>train_f1</td><td>0.44175</td></tr><tr><td>train_loss_epoch</td><td>0.5572</td></tr><tr><td>train_loss_step</td><td>0.54258</td></tr><tr><td>trainer/global_step</td><td>999</td></tr><tr><td>val_acc</td><td>0.6875</td></tr><tr><td>val_auc</td><td>0.64503</td></tr><tr><td>val_f1</td><td>0.49735</td></tr><tr><td>val_loss_epoch</td><td>0.58365</td></tr><tr><td>val_loss_step</td><td>0.60139</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_16_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/bjuqdnn3' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold/runs/bjuqdnn3</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230510_033925-bjuqdnn3\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a2be2ad54024066ae8af40f37ae3bb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Thomas\\15_PLA\\notebooks\\wandb\\run-20230510_043110-se2l6r6o</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/se2l6r6o' target=\"_blank\">MLP_2_16_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_042923_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_042923_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/se2l6r6o' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold/runs/se2l6r6o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 368   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "522       Trainable params\n",
      "0         Non-trainable params\n",
      "522       Total params\n",
      "0.002     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1ec5ac4983b478b9aaec0e3d4e2cf6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▇██████████████████████████████████████</td></tr><tr><td>train_auc</td><td>▁▅▅▅▅▅▅▅▅▅▅▅▅▄▄▅▄▅▅▅█▄▆▆▅▇▅▆▅▄▅▆▅▅▅▇▅▅▅▅</td></tr><tr><td>train_f1</td><td>█▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▃▁▂▂▁▂▁▂▁▁▁▁▁▁▁▂▂▂▁▁</td></tr><tr><td>train_loss_epoch</td><td>█▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▂▂▂▁▂▂▁▂▂▁▁▁▁▂▂▁▂▁▂▁▁▂▁▂</td></tr><tr><td>train_loss_step</td><td>▇▄▅▅▇▃▄▅▁▄▃▃▁▇▄▄▅█▅▂▃▆▆▁▄▅▅▅▂▃▆▁█▆▇▆▁▂▆▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▁▂▂▁▃▃▂▃▃▂▄▄▂▂▄▂▂▅▅▂▆▆▂▆▆▃▇▇▃▃▇▃▃█▃</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_epoch</td><td>▅▅▅▂▃▄▄▆▂▅▄▁▆▅▆▅▃▅▄▄▂▄▅█▃▄▅▅▄▅▆▅▄▇▄▇▇▆█▇</td></tr><tr><td>val_loss_step</td><td>▅▅▅▃▄▅▅▆▄▃▅▁▅▅█▆▆▅▄▅▄▅▅▅▅▄▅▅▄▆▄▇▄█▆▁▇▇▆▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.63711</td></tr><tr><td>train_auc</td><td>0.5002</td></tr><tr><td>train_f1</td><td>0.01339</td></tr><tr><td>train_loss_epoch</td><td>0.65005</td></tr><tr><td>train_loss_step</td><td>0.61402</td></tr><tr><td>trainer/global_step</td><td>999</td></tr><tr><td>val_acc</td><td>0.57566</td></tr><tr><td>val_auc</td><td>0.5</td></tr><tr><td>val_f1</td><td>0.0</td></tr><tr><td>val_loss_epoch</td><td>0.69703</td></tr><tr><td>val_loss_step</td><td>0.72224</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_16_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/se2l6r6o' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold/runs/se2l6r6o</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230510_043110-se2l6r6o\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2bf6bca046f434fb5d51205dc3865e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Thomas\\15_PLA\\notebooks\\wandb\\run-20230510_051825-hsydj261</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/hsydj261' target=\"_blank\">MLP_2_16_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_042923_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_042923_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/hsydj261' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold/runs/hsydj261</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 368   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "522       Trainable params\n",
      "0         Non-trainable params\n",
      "522       Total params\n",
      "0.002     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6182fb6ddea2497eb8f54a8a0b6b9344",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▃▅▆▆▆▆▇▆▇▇▇▇▇▇█▇▇█▇█▇██▇█▇▇▇██▇▇█▇▇█▇▇█</td></tr><tr><td>train_auc</td><td>▁▂▃▃▄▅▅▆▅▆▆▆▇▆▆▇▆▆▇▆▇▆▇▇▇█▇▆▅▇▇▇▆█▆▆▇▆▆▇</td></tr><tr><td>train_f1</td><td>▄▂▁▁▂▄▅▅▅▆▅▆▆▅▆▇▆▅▇▆▇▅▇▇▇█▇▅▄▇▇▇▅█▅▆▇▆▆▆</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▅▄▃▄▄▃▃▃▃▃▃▂▃▂▃▂▂▄▂▂▃▃▃▂▂▃▂▂▃▃▁▄▃▃▄▃▂▃▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▁▂▂▁▃▃▂▃▃▂▄▄▂▂▄▂▂▅▅▂▆▆▂▆▆▃▇▇▃▃▇▃▃█▃</td></tr><tr><td>val_acc</td><td>▁▃▃▃▅▅█▆▅▃█▆▆▇▇▆▇██▆▆▇▆█▆█▆███▆▆▇▇██████</td></tr><tr><td>val_auc</td><td>▁▃▂▂▄▄█▅▄▂█▅▆▆▇▅▆▇█▅▆▇▆█▆█▆██▇▆▆▇▇███▇██</td></tr><tr><td>val_f1</td><td>▃▃▂▁▄▄█▆▄▁█▅▆▇▇▅▆▇█▅▆▇▆█▆█▆██▇▆▆▇▇███▇██</td></tr><tr><td>val_loss_epoch</td><td>█▇▆▆▅▄▃▄▄▆▂▃▂▃▃▃▂▂▂▃▂▁▃▂▂▁▂▁▁▂▃▃▂▂▂▂▁▁▂▂</td></tr><tr><td>val_loss_step</td><td>▇█▇▆▅▆▅▅▃▂▄▁▄▃▇▅▄▄▃▃▂▄▅▄▃▁▄▄▃▃▂▄▁▅▂▁▂▆▃▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.71921</td></tr><tr><td>train_auc</td><td>0.63852</td></tr><tr><td>train_f1</td><td>0.47222</td></tr><tr><td>train_loss_epoch</td><td>0.54628</td></tr><tr><td>train_loss_step</td><td>0.51565</td></tr><tr><td>trainer/global_step</td><td>999</td></tr><tr><td>val_acc</td><td>0.73026</td></tr><tr><td>val_auc</td><td>0.70458</td></tr><tr><td>val_f1</td><td>0.62727</td></tr><tr><td>val_loss_epoch</td><td>0.55928</td></tr><tr><td>val_loss_step</td><td>0.60176</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_16_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/hsydj261' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold/runs/hsydj261</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230510_051825-hsydj261\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8676a5c55dd45f5bba7dc66e85058a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01691666666495924, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Thomas\\15_PLA\\notebooks\\wandb\\run-20230510_060935-yaxem0h3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/yaxem0h3' target=\"_blank\">MLP_2_16_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_042923_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_042923_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/yaxem0h3' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold/runs/yaxem0h3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 368   \n",
      "2  | head        | Sequential       | 154   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 17    \n",
      "--------------------------------------------------\n",
      "539       Trainable params\n",
      "0         Non-trainable params\n",
      "539       Total params\n",
      "0.002     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d76e73feff7642d28714f3f058178bf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▅▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇▇█▇█▇▇▇▇▇█▇▇█▇</td></tr><tr><td>train_auc</td><td>▁▂▂▂▄▅▆▆▆▆▆▇▆▆▇▆▆▆▇▆▇▇▇▇█▇▆▇▆█▇▇▆▆▇▇▇▇▇▆</td></tr><tr><td>train_f1</td><td>▆▂▁▁▄▆▇▆▇▇▇█▇▇█▇▆▆█▆▇▇▇▇█▇▆▇▇█▇▇▇▇██▇▇█▇</td></tr><tr><td>train_loss_epoch</td><td>█▆▆▅▃▃▂▂▂▂▂▂▂▁▂▂▂▁▂▂▂▁▁▂▁▂▂▂▁▂▂▂▂▂▁▂▂▁▁▂</td></tr><tr><td>train_loss_step</td><td>█▇█▅▅▅▆▄▅▅▇▅▄▅▅▅▆▄▅▇▄▄▃▅▁▅▄▇▄▆▅▆▄▅▅▅▆▄▃▆</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▁▂▂▁▃▃▂▃▃▂▄▄▂▂▄▂▂▅▅▂▆▆▂▆▆▃▇▇▃▃▇▃▃█▃</td></tr><tr><td>val_acc</td><td>▁▁▁▁▅▆▇██▆██▆▇▇▇▇█▆▇▆▆▄▆▆▆▇▆▆▄▆▆█▆▆▆▆▄▆▆</td></tr><tr><td>val_auc</td><td>▁▁▁▁▅▆▆█▇▆▇▇▆▆▇▆▇▇▅▆▆▆▃▆▆▆▆▆▆▄▆▆▇▆▆▆▆▄▆▆</td></tr><tr><td>val_f1</td><td>▁▁▁▁▅▇▆█▇▆▇▇▆▇▇▇▇▇▆▇▆▆▄▆▆▆▇▆▆▅▆▆▇▆▆▆▆▅▆▆</td></tr><tr><td>val_loss_epoch</td><td>▇▇█▇▅▄▃▂▃▃▁▂▃▁▃▃▂▂▃▃▅▃▅▃▃▃▂▃▄▄▃▁▃▃▂▃▃▄▂▁</td></tr><tr><td>val_loss_step</td><td>▇▇▆▆▅▄▅▄▅▄▅▅▄▅▆▃▄▆▅▅▅▅▆▅▄▄▄▄█▄▄▆▄▆▄▄▅▃▅▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.67898</td></tr><tr><td>train_auc</td><td>0.60604</td></tr><tr><td>train_f1</td><td>0.43579</td></tr><tr><td>train_loss_epoch</td><td>0.57045</td></tr><tr><td>train_loss_step</td><td>0.62669</td></tr><tr><td>trainer/global_step</td><td>999</td></tr><tr><td>val_acc</td><td>0.68421</td></tr><tr><td>val_auc</td><td>0.63911</td></tr><tr><td>val_f1</td><td>0.47826</td></tr><tr><td>val_loss_epoch</td><td>0.55839</td></tr><tr><td>val_loss_step</td><td>0.44765</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_16_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/yaxem0h3' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold/runs/yaxem0h3</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230510_060935-yaxem0h3\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38c69f8c118743f7a29350214ab265bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01693333333435779, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Thomas\\15_PLA\\notebooks\\wandb\\run-20230510_065448-tesujlrx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/tesujlrx' target=\"_blank\">MLP_2_16_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_042923_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_042923_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/tesujlrx' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold/runs/tesujlrx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 368   \n",
      "2  | head        | Sequential       | 154   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 281   \n",
      "--------------------------------------------------\n",
      "803       Trainable params\n",
      "0         Non-trainable params\n",
      "803       Total params\n",
      "0.003     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94671dde0f4b4659a433b386ba25d236",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▅▆▆▇▇███▇▇▇█▇█▇█▇▇█▇▇███▇▇█████████▇███</td></tr><tr><td>train_auc</td><td>▁▁▁▂▅▇▇▇▇▇▆▇▇▇█▇▇▅▆▆▇▇█▇▇▆▆▇▇██▇▇▇██▆▇██</td></tr><tr><td>train_f1</td><td>▇▂▁▁▆█▇▇▇▇▇▇▇██▇▇▆▆▇▇▇█▇▇▆▇▇▇███▇▇██▇▇██</td></tr><tr><td>train_loss_epoch</td><td>█▆▆▅▃▂▂▁▁▂▁▂▁▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▂▁▁▂▁▁▁▁▂▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▇▅▆▃▂█▃▁▄▁▃▂▅▄▄▄▅▁▂▃▅▂▃▃▁▃▃▄▆▅▄▂▃▃▅▆▃▃▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▁▂▂▁▃▃▂▃▃▂▄▄▂▂▄▂▂▅▅▂▆▆▂▆▆▃▇▇▃▃▇▃▃█▃</td></tr><tr><td>val_acc</td><td>▁▁▁▁▆▆██▆▇███▆█▇▇▇▇██▇▇██▇▇███▇█▇█▇█▇▆█▆</td></tr><tr><td>val_auc</td><td>▁▁▁▁▆▆██▆▇███▆█▇▇▇▇██▇▇███▇███▇█▇█▇█▇▆█▆</td></tr><tr><td>val_f1</td><td>▁▁▁▁▆▇██▇▇███▇█▇▇▇▇██▇▇███▇███▇█▇█▇█▇▇█▇</td></tr><tr><td>val_loss_epoch</td><td>█▇█▇▄▂▂▃▃▂▂▂▂▂▂▁▂▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▄</td></tr><tr><td>val_loss_step</td><td>▇▇▇▆▄▄▄▄▅▅▃▃▃▄▅▄▃▁▃▅▅▃▄▄▂▅▃▄▅▃▅▄▄▅▄▃▅▅▅█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.70361</td></tr><tr><td>train_auc</td><td>0.65</td></tr><tr><td>train_f1</td><td>0.52687</td></tr><tr><td>train_loss_epoch</td><td>0.56365</td></tr><tr><td>train_loss_step</td><td>0.54942</td></tr><tr><td>trainer/global_step</td><td>999</td></tr><tr><td>val_acc</td><td>0.68421</td></tr><tr><td>val_auc</td><td>0.64319</td></tr><tr><td>val_f1</td><td>0.5</td></tr><tr><td>val_loss_epoch</td><td>0.60199</td></tr><tr><td>val_loss_step</td><td>0.70527</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_16_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/tesujlrx' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold/runs/tesujlrx</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230510_065448-tesujlrx\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### MLP\n",
    "\n",
    "k_folds = 5\n",
    "kfold = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "for fold, (train_ids, valid_ids) in enumerate(kfold.split(dataset_filtered)):\n",
    "    train_subset = dataset_filtered.index_select(train_ids.tolist())\n",
    "    val_subset = dataset_filtered.index_select(valid_ids.tolist())\n",
    "    \n",
    "    for pool in pools:\n",
    "        # Path to the folder where the pretrained models are saved\n",
    "        CHECKPOINT_PATH = checkpoint_folder / f'{NUM_LAYERS}_{HIDDEN_CHANNELS}_onehot_{fold}' / pool\n",
    "        CHECKPOINT_PATH.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Skip already trained kfold and pool\n",
    "        checkpoint = CHECKPOINT_PATH / \"GraphLevelMLP\" / \"GraphLevelMLP.ckpt\" \n",
    "        if checkpoint.exists():\n",
    "            print(checkpoint)\n",
    "            continue\n",
    "        \n",
    "        # Run training\n",
    "        run = wandb.init(project=project_name, name=f'MLP_{NUM_LAYERS}_{HIDDEN_CHANNELS}_onehot_{fold}', \n",
    "                        group=f'MLP_{pool}')\n",
    "        PPIGraph.train_graph_classifier_kfold('MLP', \n",
    "                                             train_subset, \n",
    "                                             val_subset, \n",
    "                                             dataset, \n",
    "                                             CHECKPOINT_PATH, \n",
    "                                             AVAIL_GPUS, \n",
    "                                             in_channels=5,\n",
    "                                             hidden_channels=HIDDEN_CHANNELS, \n",
    "                                             out_channels = HIDDEN_CHANNELS,\n",
    "                                             num_layers=NUM_LAYERS, \n",
    "                                             epochs=epochs,\n",
    "                                             batch_size=128,\n",
    "                                             embedding=False,\n",
    "                                             graph_pooling=pool)\n",
    "        run.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0eacd3-57dd-42e7-b4af-afb66864d4eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:snowflake]",
   "language": "python",
   "name": "conda-env-snowflake-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
