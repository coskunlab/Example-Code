{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import skimage.io\n",
    "\n",
    "from collections import defaultdict\n",
    "from tqdm.auto import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "import re\n",
    "import h5py\n",
    "import napari\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_dir = (Path().cwd().parents[0]).absolute()\n",
    "\n",
    "module_path = str(p_dir / \"src\")\n",
    "\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = (Path().cwd().parents[0] / 'data').absolute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PPIGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read per cell data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define condition mapping\n",
    "condition_mapping = {'1W': 0, '1M': 1}\n",
    "\n",
    "# Load graph dataset and process if neede\n",
    "graph_path = data_dir / 'OCT mouse' / 'graphs' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out by maximum number of counts per cell\n",
    "min_count = 5\n",
    "max_count = 40\n",
    "\n",
    "dataset = PPIGraph.GraphDataset(graph_path, 'raw', 'pt',  condition_mapping=condition_mapping, n_c=2)\n",
    "\n",
    "# Create Dataloader\n",
    "loader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# # Get Indices\n",
    "# indices = []\n",
    "# for step, data in enumerate(loader):\n",
    "#     if len(data.x) <= min_count:\n",
    "#         continue \n",
    "    \n",
    "#     if (data.x.sum(axis=0) >= max_count).any():\n",
    "#         continue\n",
    "#     indices.append(step)\n",
    "    \n",
    "# # Get subset dataset\n",
    "# dataset_filtered = dataset.index_select(indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save(graph_path / 'indices.npy', indices)\n",
    "# indices_test = np.load(graph_path / 'indices.npy')\n",
    "\n",
    "# np.array_equal(indices_test,np.array(indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GraphDataset(49629)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.load(graph_path / 'indices.npy')\n",
    "indices = indices[::15]\n",
    "dataset_filtered = dataset.index_select(list(indices))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GraphDataset(2901)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(edge_index=[2, 100], pos=[20, 2], labels=[20, 5], nuclei=[20], weight=[100], condition=0, fov=1, id=100, train_mask=[20], test_mask=[20], edge_attr=[100, 2], x=[20, 5], y=[1], edge_weight=[100], name='0_1_100.gpt')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_filtered[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import preprocessing, metrics\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# condition = 'ML'\n",
    "# project_name = f'PLA_xeno_11324_{condition}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models = {\n",
    "#     'Adaboost': AdaBoostClassifier(),\n",
    "#     'DecisionTree': DecisionTreeClassifier(),\n",
    "#     'GradientBoosting' : GradientBoostingClassifier(),\n",
    "#     'NaiveBayes': GaussianNB(),\n",
    "#     'RandomForest': RandomForestClassifier(), \n",
    "#     'SVM': SVC(probability =True),\n",
    "#     'LogisticRegression':  LogisticRegression(),\n",
    "#     'MLP': MLPClassifier(random_state=1, max_iter=100, hidden_layer_sizes=[16, 16, 16])\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch \n",
    "\n",
    "# data = torch.empty((0, 5))\n",
    "# label = []\n",
    "\n",
    "# for graph in dataset_filtered:\n",
    "#     new_row =graph.x.sum(axis = 0).unsqueeze(0)\n",
    "#     data = torch.cat((data, new_row), dim=0)\n",
    "#     label.append(graph.condition)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save(graph_path / 'data.npy', data)\n",
    "# np.save(graph_path / 'label.npy', label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler = StandardScaler()\n",
    "\n",
    "# # Run model on cell count\n",
    "# X = data.numpy()\n",
    "# X = scaler.fit_transform(X)\n",
    "# y = np.array(label)\n",
    "\n",
    "# # X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=0)\n",
    "# kfold = KFold(n_splits = 10, shuffle = True, random_state = 0)\n",
    "\n",
    "# for model_name, model in models.items():\n",
    "#     for k, (train_index, test_index) in enumerate(kfold.split(X)):\n",
    "#         # Split the dataset\n",
    "#         X_train, X_test = X[train_index], X[test_index]\n",
    "#         y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "#         model.fit(X_train, y_train)\n",
    "#         y_pred = model.predict(X_test)\n",
    "#         y_probas = model.predict_proba(X_test)\n",
    "\n",
    "#         run = wandb.init(project=project_name, group=model_name+'_cell', name=model_name+f'_cell_{k}')\n",
    "\n",
    "#         accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "#         b_accuracy = metrics.balanced_accuracy_score(y_test, y_pred)\n",
    "#         f1 = metrics.f1_score(y_test, y_pred)\n",
    "#         auc = metrics.roc_auc_score(y_test, y_pred)\n",
    "#         wandb.log({\"accuracy\": accuracy, 'b_accuracy': b_accuracy, 'f1':f1, 'auc': auc})\n",
    "#     run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch \n",
    "\n",
    "# data = torch.empty((0, 10))\n",
    "# label = []\n",
    "\n",
    "# for graph in dataset_filtered:\n",
    "#     cell =graph.x.sum(axis = 0).unsqueeze(0)\n",
    "\n",
    "#     nuclei = torch.tile(graph.nuclei, (5, 1)).T * graph.x\n",
    "#     nuclei = nuclei.sum(axis=0).unsqueeze(0)\n",
    "#     cyto = cell - nuclei\n",
    "#     new_row = torch.cat([cyto, nuclei], axis=1)\n",
    "#     data = torch.cat((data, new_row), dim=0)\n",
    "#     label.append(graph.condition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler = StandardScaler()\n",
    "\n",
    "# # Run model on cell count\n",
    "# X = data.numpy()\n",
    "# X = scaler.fit_transform(X)\n",
    "# y = np.array(label)\n",
    "\n",
    "# # X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=0)\n",
    "# kfold = KFold(n_splits = 5, shuffle = True, random_state = 0)\n",
    "\n",
    "# for model_name, model in models.items():\n",
    "#     for k, (train_index, test_index) in enumerate(kfold.split(X)):\n",
    "#         # Split the dataset\n",
    "#         X_train, X_test = X[train_index], X[test_index]\n",
    "#         y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "#         model.fit(X_train, y_train)\n",
    "#         y_pred = model.predict(X_test)\n",
    "#         y_probas = model.predict_proba(X_test)\n",
    "\n",
    "#         run = wandb.init(project=project_name, group=model_name+'_cytonuclei', name=model_name+f'_cytonuclei_{k}')\n",
    "\n",
    "#         accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "#         b_accuracy = metrics.balanced_accuracy_score(y_test, y_pred)\n",
    "#         f1 = metrics.f1_score(y_test, y_pred)\n",
    "#         auc = metrics.roc_auc_score(y_test, y_pred)\n",
    "#         wandb.log({\"accuracy\": accuracy, 'b_accuracy': b_accuracy, 'f1':f1, 'auc': auc})\n",
    "#     run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GNN training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning.pytorch as pl\n",
    "import PPIGraph\n",
    "from sklearn.model_selection import train_test_split, KFold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from lightning.pytorch.accelerators import find_usable_cuda_devices\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'xenograph'\n",
    "condition = 'Kfold'\n",
    "checkpoint_folder = (Path().cwd().parents[0]).absolute() / 'data' /'OCT mouse' /\"saved_models\" / dataset_name / f\"Graph_GNNs_{condition}\" \n",
    "project_name = f'PLA_01122024_{dataset_name}_{condition}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    }
   ],
   "source": [
    "AVAIL_GPUS = [1]\n",
    "\n",
    "# Setting the seed\n",
    "pl.seed_everything(42)\n",
    "epochs = 20\n",
    "\n",
    "# NUM_LAYERS = 2\n",
    "# HIDDEN_CHANNELS = 16\n",
    "# pools = ['mean', 'max', 'sum', 'attention', 'attention2']\n",
    "# num_layers = [2,3,4]\n",
    "# hiddens = [16, 32, 64]\n",
    "\n",
    "\n",
    "# # model = 'GAT'\n",
    "# model = 'GINConv'\n",
    "params = [[3,32,'mean','MLP'], \n",
    "[3,32,'attention2','GraphConv'],\n",
    "[2,64,'attention2','GCN'],\n",
    "[3,64,'attention','GAT'],\n",
    "[2,16,'attention','GINConv']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.set_float32_matmul_precision('medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT mouse\\saved_models\\xenograph\\Graph_GNNs_Kfold\\MLP_3_32_onehot_0\\mean\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT mouse\\saved_models\\xenograph\\Graph_GNNs_Kfold\\GraphConv_3_32_onehot_0\\attention2\\GraphLevelGraphConv\\GraphLevelGraphConv.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT mouse\\saved_models\\xenograph\\Graph_GNNs_Kfold\\GCN_2_64_onehot_0\\attention2\\GraphLevelGCN\\GraphLevelGCN.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT mouse\\saved_models\\xenograph\\Graph_GNNs_Kfold\\GAT_3_64_onehot_0\\attention\\GraphLevelGAT\\GraphLevelGAT.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT mouse\\saved_models\\xenograph\\Graph_GNNs_Kfold\\GINConv_2_16_onehot_0\\attention\\GraphLevelGINConv\\GraphLevelGINConv.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT mouse\\saved_models\\xenograph\\Graph_GNNs_Kfold\\MLP_3_32_onehot_1\\mean\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT mouse\\saved_models\\xenograph\\Graph_GNNs_Kfold\\GraphConv_3_32_onehot_1\\attention2\\GraphLevelGraphConv\\GraphLevelGraphConv.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT mouse\\saved_models\\xenograph\\Graph_GNNs_Kfold\\GCN_2_64_onehot_1\\attention2\\GraphLevelGCN\\GraphLevelGCN.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT mouse\\saved_models\\xenograph\\Graph_GNNs_Kfold\\GAT_3_64_onehot_1\\attention\\GraphLevelGAT\\GraphLevelGAT.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT mouse\\saved_models\\xenograph\\Graph_GNNs_Kfold\\GINConv_2_16_onehot_1\\attention\\GraphLevelGINConv\\GraphLevelGINConv.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT mouse\\saved_models\\xenograph\\Graph_GNNs_Kfold\\MLP_3_32_onehot_2\\mean\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT mouse\\saved_models\\xenograph\\Graph_GNNs_Kfold\\GraphConv_3_32_onehot_2\\attention2\\GraphLevelGraphConv\\GraphLevelGraphConv.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT mouse\\saved_models\\xenograph\\Graph_GNNs_Kfold\\GCN_2_64_onehot_2\\attention2\\GraphLevelGCN\\GraphLevelGCN.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT mouse\\saved_models\\xenograph\\Graph_GNNs_Kfold\\GAT_3_64_onehot_2\\attention\\GraphLevelGAT\\GraphLevelGAT.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT mouse\\saved_models\\xenograph\\Graph_GNNs_Kfold\\GINConv_2_16_onehot_2\\attention\\GraphLevelGINConv\\GraphLevelGINConv.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT mouse\\saved_models\\xenograph\\Graph_GNNs_Kfold\\MLP_3_32_onehot_3\\mean\\GraphLevelMLP\\GraphLevelMLP.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mthoomas\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f88224e3b064049ac1ef0f4b72dc1fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240122_120800-kdar8zre</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_01122024_xenograph_Kfold/runs/kdar8zre' target=\"_blank\">GraphConv_3_32_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_01122024_xenograph_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_01122024_xenograph_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_01122024_xenograph_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_01122024_xenograph_Kfold/runs/kdar8zre' target=\"_blank\">https://wandb.ai/thoomas/PLA_01122024_xenograph_Kfold/runs/kdar8zre</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "c:\\Users\\thu71\\Anaconda3\\envs\\snowflake\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:395: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GNN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | GNNModel         | 4.6 K \n",
      "2  | head        | Sequential       | 562   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 1.1 K \n",
      "--------------------------------------------------\n",
      "6.3 K     Trainable params\n",
      "0         Non-trainable params\n",
      "6.3 K     Total params\n",
      "0.025     Total estimated model params size (MB)\n",
      "C:\\Users\\thu71\\AppData\\Roaming\\Python\\Python39\\site-packages\\lightning\\pytorch\\utilities\\data.py:77: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 2. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  warning_cache.warn(\n",
      "c:\\Users\\thu71\\Anaconda3\\envs\\snowflake\\lib\\site-packages\\torchmetrics\\utilities\\prints.py:36: UserWarning: No negative samples in targets, false positive value should be meaningless. Returning zero tensor in false positive score\n",
      "  warnings.warn(*args, **kwargs)\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08d56c68cd894508b51e1d156d43842e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▄▇▇▇▇███████▆██████</td></tr><tr><td>train_auc</td><td>▁▄▆▇▇▇▇▇█▇█▇▇▇▇▇████</td></tr><tr><td>train_f1</td><td>▁▅▆▇▇▇▇█▇█▇▇▇▇█▇████</td></tr><tr><td>train_loss_epoch</td><td>█▆▄▃▂▃▃▂▂▂▂▂▃▃▂▂▂▁▁▂</td></tr><tr><td>train_loss_step</td><td>█▆▄▅▃▃▆▁▄▃▂▃█▅▄▂▄▂▂▆</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▁▂▁▃▁▃▂▃▃▃▂▄▄▄▂▅▅▅▂▆▆▂▂▆▆▃▃▇▇▃▃██▃█</td></tr><tr><td>val_acc</td><td>▁▃▆▇██████▇█▆▆█▇▇███</td></tr><tr><td>val_auc</td><td>▁▆▇███▇█████▇▇▇▇█▇▇▇</td></tr><tr><td>val_f1</td><td>▁▃▆▇▇▇████▇█▇▆▇▇▇██▇</td></tr><tr><td>val_loss_epoch</td><td>█▅▃▂▂▂▁▁▁▂▁▂▂▃▂▁▁▁▁▂</td></tr><tr><td>val_loss_step</td><td>█▃█▂▇▂▅▂▅▃▅▂▅▂▅▂▅▂▅▂▆▁▄▂▇▁▆▂▅▃▇▁▅▂▅▂▅▂▅▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>19</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.77208</td></tr><tr><td>train_auc</td><td>0.81305</td></tr><tr><td>train_f1</td><td>0.8151</td></tr><tr><td>train_loss_epoch</td><td>0.52156</td></tr><tr><td>train_loss_step</td><td>0.69539</td></tr><tr><td>trainer/global_step</td><td>199</td></tr><tr><td>val_acc</td><td>0.79655</td></tr><tr><td>val_auc</td><td>0.86796</td></tr><tr><td>val_f1</td><td>0.83791</td></tr><tr><td>val_loss_epoch</td><td>0.44233</td></tr><tr><td>val_loss_step</td><td>0.38218</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">GraphConv_3_32_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_01122024_xenograph_Kfold/runs/kdar8zre' target=\"_blank\">https://wandb.ai/thoomas/PLA_01122024_xenograph_Kfold/runs/kdar8zre</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240122_120800-kdar8zre\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e1ebfab90bf4b33bd60752a98137b81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240122_134416-dwc641xg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_01122024_xenograph_Kfold/runs/dwc641xg' target=\"_blank\">GCN_2_64_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_01122024_xenograph_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_01122024_xenograph_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_01122024_xenograph_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_01122024_xenograph_Kfold/runs/dwc641xg' target=\"_blank\">https://wandb.ai/thoomas/PLA_01122024_xenograph_Kfold/runs/dwc641xg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | GNNModel         | 4.7 K \n",
      "2  | head        | Sequential       | 2.1 K \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 4.2 K \n",
      "--------------------------------------------------\n",
      "11.0 K    Trainable params\n",
      "0         Non-trainable params\n",
      "11.0 K    Total params\n",
      "0.044     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GNN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4317a24cc11f4aaf87c82fc4dce2865c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▆▆▇▇▇▇▇▇▇█▇█▇█▆▇▇█▇</td></tr><tr><td>train_auc</td><td>▁▅▆▇█▇▇██▇█▇███▆█▆█▇</td></tr><tr><td>train_f1</td><td>▁▇▆█▇█▆█▇▇█▇█▇█▇▆█▇▇</td></tr><tr><td>train_loss_epoch</td><td>█▃▅▂▄▄▃▃▂▄▃▄▂▁▂▃▃▄▁▁</td></tr><tr><td>train_loss_step</td><td>▄▁▄▂▇▇▃▅▃▇▇█▅▁▅▅▅▆▃▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▁▂▁▃▁▃▂▃▃▃▂▄▄▄▂▅▅▅▂▆▆▂▂▆▆▃▃▇▇▃▃██▃█</td></tr><tr><td>val_acc</td><td>▁▆▇▇▇▇▆▇▇▇▇▆▇▇▇█▆▇▇▇</td></tr><tr><td>val_auc</td><td>▄▁▅▇█▆▄▇▆▆▃▅█▇▇█▆▇▅█</td></tr><tr><td>val_f1</td><td>▁▆▇▇▇█▆▆▇▇▇▇▇▇▇█▆▇▇▇</td></tr><tr><td>val_loss_epoch</td><td>█▇▆▃▃▂▂▁▁▁▂▂▁▁▂▁▂▂▁▁</td></tr><tr><td>val_loss_step</td><td>█▄▇▄▅▄▆▂▅▃▆▂▆▂▆▂▆▁▆▁▆▂▇▁▆▁▆▁▆▁▅▂▆▂▆▁▅▂▅▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>19</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.76777</td></tr><tr><td>train_auc</td><td>0.8091</td></tr><tr><td>train_f1</td><td>0.81343</td></tr><tr><td>train_loss_epoch</td><td>0.50182</td></tr><tr><td>train_loss_step</td><td>0.49024</td></tr><tr><td>trainer/global_step</td><td>199</td></tr><tr><td>val_acc</td><td>0.8</td></tr><tr><td>val_auc</td><td>0.87577</td></tr><tr><td>val_f1</td><td>0.83934</td></tr><tr><td>val_loss_epoch</td><td>0.42512</td></tr><tr><td>val_loss_step</td><td>0.35618</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">GCN_2_64_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_01122024_xenograph_Kfold/runs/dwc641xg' target=\"_blank\">https://wandb.ai/thoomas/PLA_01122024_xenograph_Kfold/runs/dwc641xg</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240122_134416-dwc641xg\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5579a45f71a46889eb767bdaf526e02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240122_151702-xlbbb551</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_01122024_xenograph_Kfold/runs/xlbbb551' target=\"_blank\">GAT_3_64_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_01122024_xenograph_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_01122024_xenograph_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_01122024_xenograph_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_01122024_xenograph_Kfold/runs/xlbbb551' target=\"_blank\">https://wandb.ai/thoomas/PLA_01122024_xenograph_Kfold/runs/xlbbb551</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "c:\\Users\\thu71\\Anaconda3\\envs\\snowflake\\lib\\site-packages\\torch_geometric\\deprecation.py:12: UserWarning: 'nn.glob.GlobalAttention' is deprecated, use 'nn.aggr.AttentionalAggregation' instead\n",
      "  warnings.warn(out)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | GAT              | 78.0 K\n",
      "2  | head        | Sequential       | 2.1 K \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 65    \n",
      "--------------------------------------------------\n",
      "80.2 K    Trainable params\n",
      "0         Non-trainable params\n",
      "80.2 K    Total params\n",
      "0.321     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GAT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffeabd81bfdc49c2a1f5ab4f43076bae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▆▇▇█▇▇▇▇██▇████▇██▇</td></tr><tr><td>train_auc</td><td>▁▆▇▇▇▇▇▇███████████▇</td></tr><tr><td>train_f1</td><td>▁▆▇▇█▇▇▇▆███▇██████▇</td></tr><tr><td>train_loss_epoch</td><td>█▄▅▄▂▂▂▂▂▁▁▁▂▃▂▂▃▂▂▃</td></tr><tr><td>train_loss_step</td><td>▂▃█▅▂▃▂▄▁▁▃▁▂▄▄▄▇▃▄▅</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▁▂▁▃▁▃▂▃▃▃▂▄▄▄▂▅▅▅▂▆▆▂▂▆▆▃▃▇▇▃▃██▃█</td></tr><tr><td>val_acc</td><td>▁▂▄▅▇▇▇█▇█▇█▇██████▇</td></tr><tr><td>val_auc</td><td>▁▃▇▇▇▇██▇███▇██████▇</td></tr><tr><td>val_f1</td><td>▁▂▄▆▇▇██▇█▇█▇███▇██▇</td></tr><tr><td>val_loss_epoch</td><td>█▇▅▄▂▂▂▂▂▂▂▂▂▁▂▁▂▁▂▂</td></tr><tr><td>val_loss_step</td><td>█▂█▂█▁▇▁▆▁▆▁▅▂▄▂▅▂▄▂▅▁▄▂▆▁▄▂▄▂▅▁▃▃▄▂▄▂▅▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>19</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.75829</td></tr><tr><td>train_auc</td><td>0.80764</td></tr><tr><td>train_f1</td><td>0.80608</td></tr><tr><td>train_loss_epoch</td><td>0.52736</td></tr><tr><td>train_loss_step</td><td>0.59372</td></tr><tr><td>trainer/global_step</td><td>199</td></tr><tr><td>val_acc</td><td>0.7931</td></tr><tr><td>val_auc</td><td>0.86345</td></tr><tr><td>val_f1</td><td>0.83516</td></tr><tr><td>val_loss_epoch</td><td>0.41723</td></tr><tr><td>val_loss_step</td><td>0.27631</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">GAT_3_64_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_01122024_xenograph_Kfold/runs/xlbbb551' target=\"_blank\">https://wandb.ai/thoomas/PLA_01122024_xenograph_Kfold/runs/xlbbb551</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240122_151702-xlbbb551\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bbb44b31bbd4cb083d2b00f13ca605c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240122_164920-cbtzdgn0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_01122024_xenograph_Kfold/runs/cbtzdgn0' target=\"_blank\">GINConv_2_16_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_01122024_xenograph_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_01122024_xenograph_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_01122024_xenograph_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_01122024_xenograph_Kfold/runs/cbtzdgn0' target=\"_blank\">https://wandb.ai/thoomas/PLA_01122024_xenograph_Kfold/runs/cbtzdgn0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | GIN              | 1.0 K \n",
      "2  | head        | Sequential       | 154   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 17    \n",
      "--------------------------------------------------\n",
      "1.2 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.2 K     Total params\n",
      "0.005     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GINConv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e42aeffd8e7f4de9b620d167eb2fec33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▅▇▇▇▇▇▇▇▇▇██▇██████</td></tr><tr><td>train_auc</td><td>▁▆▇▇████████████████</td></tr><tr><td>train_f1</td><td>▁▆▇▇▇▇██▇▇█▇████▇███</td></tr><tr><td>train_loss_epoch</td><td>█▅▄▄▃▃▃▂▂▂▃▂▂▁▂▃▂▁▂▂</td></tr><tr><td>train_loss_step</td><td>▅▇▇▅▆▇▆▄▅▄▇▅▅▃▅█▃▁▅▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▁▂▁▃▁▃▂▃▃▃▂▄▄▄▂▅▅▅▂▆▆▂▂▆▆▃▃▇▇▃▃██▃█</td></tr><tr><td>val_acc</td><td>▁▆▇▇█▇▇▇██████▇█████</td></tr><tr><td>val_auc</td><td>▁▆▇▇▇▇██████████▇▇█▇</td></tr><tr><td>val_f1</td><td>▁▆▅▇█▇▇▇█▇██████▇███</td></tr><tr><td>val_loss_epoch</td><td>█▄▆▃▂▂▂▂▁▁▁▂▂▁▁▂▃▂▁▂</td></tr><tr><td>val_loss_step</td><td>█▂▅▂▂▄▄▂▄▂▄▂▄▂▄▂▄▁▄▂▅▁▄▂▄▂▄▂▅▁▄▂▄▂▄▂▄▂▄▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>19</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.76433</td></tr><tr><td>train_auc</td><td>0.82764</td></tr><tr><td>train_f1</td><td>0.81196</td></tr><tr><td>train_loss_epoch</td><td>0.50599</td></tr><tr><td>train_loss_step</td><td>0.44999</td></tr><tr><td>trainer/global_step</td><td>199</td></tr><tr><td>val_acc</td><td>0.81379</td></tr><tr><td>val_auc</td><td>0.88134</td></tr><tr><td>val_f1</td><td>0.84831</td></tr><tr><td>val_loss_epoch</td><td>0.40458</td></tr><tr><td>val_loss_step</td><td>0.31026</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">GINConv_2_16_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_01122024_xenograph_Kfold/runs/cbtzdgn0' target=\"_blank\">https://wandb.ai/thoomas/PLA_01122024_xenograph_Kfold/runs/cbtzdgn0</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240122_164920-cbtzdgn0\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2297ea24a5bc4cacaa7f526b3afa0caa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016933333333327028, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240122_181804-puua5vzp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_01122024_xenograph_Kfold/runs/puua5vzp' target=\"_blank\">MLP_3_32_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_01122024_xenograph_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_01122024_xenograph_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_01122024_xenograph_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_01122024_xenograph_Kfold/runs/puua5vzp' target=\"_blank\">https://wandb.ai/thoomas/PLA_01122024_xenograph_Kfold/runs/puua5vzp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 2.3 K \n",
      "2 | head        | Sequential       | 562   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "2.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "2.9 K     Total params\n",
      "0.011     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5480ebc82d404f12a753d13502c23a38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▅▆▇█▇▇▇██████████▇</td></tr><tr><td>train_auc</td><td>▁▃▆▇▇███████████████</td></tr><tr><td>train_f1</td><td>▁▁▅▅▅▇▇▇▇█▇█▇▇▇█▇█▇▇</td></tr><tr><td>train_loss_epoch</td><td>██▅▄▄▂▂▄▂▄▁▂▁▂▂▁▁▁▂▂</td></tr><tr><td>train_loss_step</td><td>▅▆▄▃▄▄▄▇▃█▂▃▂▃▆▂▁▁▄▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▁▂▁▃▁▃▂▃▃▃▂▄▄▄▂▅▅▅▂▆▆▂▂▆▆▃▃▇▇▃▃██▃█</td></tr><tr><td>val_acc</td><td>▁▂▇█▇▇█▇▇█▇▇▇▇█▇▇▇▇▇</td></tr><tr><td>val_auc</td><td>█▁██████████████████</td></tr><tr><td>val_f1</td><td>▁▂▅██▇▇█▇█▇▇▇▇█▇█▇▇▇</td></tr><tr><td>val_loss_epoch</td><td>█▆▄▃▂▁▃▁▂▁▂▁▂▁▂▂▁▂▁▃</td></tr><tr><td>val_loss_step</td><td>█▄█▃▅▃▅▃▇▂▇▁▅▃▇▂▆▂▆▂▆▂▆▂▆▂▇▁▅▃▆▂▇▁▅▃▇▁▅▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>19</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.74795</td></tr><tr><td>train_auc</td><td>0.80518</td></tr><tr><td>train_f1</td><td>0.80864</td></tr><tr><td>train_loss_epoch</td><td>0.51633</td></tr><tr><td>train_loss_step</td><td>0.49403</td></tr><tr><td>trainer/global_step</td><td>199</td></tr><tr><td>val_acc</td><td>0.73793</td></tr><tr><td>val_auc</td><td>0.81068</td></tr><tr><td>val_f1</td><td>0.79006</td></tr><tr><td>val_loss_epoch</td><td>0.51766</td></tr><tr><td>val_loss_step</td><td>0.49369</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_32_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_01122024_xenograph_Kfold/runs/puua5vzp' target=\"_blank\">https://wandb.ai/thoomas/PLA_01122024_xenograph_Kfold/runs/puua5vzp</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240122_181804-puua5vzp\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebd9163b279f4517bde962ca2eb95103",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240122_195117-0tsanncf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_01122024_xenograph_Kfold/runs/0tsanncf' target=\"_blank\">GraphConv_3_32_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_01122024_xenograph_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_01122024_xenograph_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_01122024_xenograph_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_01122024_xenograph_Kfold/runs/0tsanncf' target=\"_blank\">https://wandb.ai/thoomas/PLA_01122024_xenograph_Kfold/runs/0tsanncf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | GNNModel         | 4.6 K \n",
      "2  | head        | Sequential       | 562   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 1.1 K \n",
      "--------------------------------------------------\n",
      "6.3 K     Trainable params\n",
      "0         Non-trainable params\n",
      "6.3 K     Total params\n",
      "0.025     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GNN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30eb714409244ac997ad51221d3ae791",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▅▆▇▇▇▇▇██▇██▇████▇█</td></tr><tr><td>train_auc</td><td>▁▆▇▇▇▇█▇██████████▇█</td></tr><tr><td>train_f1</td><td>▁▅▆▇▇▇▇▇▇█▇██▇████▇▇</td></tr><tr><td>train_loss_epoch</td><td>█▅▃▃▃▂▁▁▁▂▁▁▁▂▂▂▁▂▂▁</td></tr><tr><td>train_loss_step</td><td>█▇▂▅▃▇▁▂▂▅▃▂▂▇▆▆▄▆▃▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▁▂▁▃▁▃▂▃▃▃▂▄▄▄▂▅▅▅▂▆▆▂▂▆▆▃▃▇▇▃▃██▃█</td></tr><tr><td>val_acc</td><td>▁▅▆▆▇▇▇████▇▇██▇█▇█▇</td></tr><tr><td>val_auc</td><td>▁▆▇▇▆▇▇▇▇██▇████▇███</td></tr><tr><td>val_f1</td><td>▁▅▇▇▇▆▇▇█▇▇▆███▇▇▇█▇</td></tr><tr><td>val_loss_epoch</td><td>█▅▃▂▃▂▂▂▂▂▂▂▁▁▂▁▃▁▂▁</td></tr><tr><td>val_loss_step</td><td>█▂▇▂▇▂▇▁▅▂▆▁▆▁▅▂▆▂▅▂▅▂▅▂▇▁▅▂▅▂▆▁▅▂▇▁▅▂▆▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>19</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.77897</td></tr><tr><td>train_auc</td><td>0.81524</td></tr><tr><td>train_f1</td><td>0.81943</td></tr><tr><td>train_loss_epoch</td><td>0.49456</td></tr><tr><td>train_loss_step</td><td>0.37371</td></tr><tr><td>trainer/global_step</td><td>199</td></tr><tr><td>val_acc</td><td>0.74655</td></tr><tr><td>val_auc</td><td>0.84078</td></tr><tr><td>val_f1</td><td>0.80632</td></tr><tr><td>val_loss_epoch</td><td>0.45155</td></tr><tr><td>val_loss_step</td><td>0.32465</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">GraphConv_3_32_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_01122024_xenograph_Kfold/runs/0tsanncf' target=\"_blank\">https://wandb.ai/thoomas/PLA_01122024_xenograph_Kfold/runs/0tsanncf</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240122_195117-0tsanncf\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b493dada86b745258e4155facd5a3e48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240122_212338-4ld3f3u4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_01122024_xenograph_Kfold/runs/4ld3f3u4' target=\"_blank\">GCN_2_64_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_01122024_xenograph_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_01122024_xenograph_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_01122024_xenograph_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_01122024_xenograph_Kfold/runs/4ld3f3u4' target=\"_blank\">https://wandb.ai/thoomas/PLA_01122024_xenograph_Kfold/runs/4ld3f3u4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | GNNModel         | 4.7 K \n",
      "2  | head        | Sequential       | 2.1 K \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 4.2 K \n",
      "--------------------------------------------------\n",
      "11.0 K    Trainable params\n",
      "0         Non-trainable params\n",
      "11.0 K    Total params\n",
      "0.044     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GNN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d29a4b3e29594f76aa1346a4062286e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▆▆▆▇▇█▇▇▇▇▇█▆▇▇▇▆▇▇</td></tr><tr><td>train_auc</td><td>▁▅▆▇▇▇██▇▇▇██▇▇▇████</td></tr><tr><td>train_f1</td><td>▁▇▆▆█▇█▇▇▇▇▇█▇▇█▇▇▆▇</td></tr><tr><td>train_loss_epoch</td><td>█▄▃▄▄▂▁▁▃▂▂▂▃▄▃▂▃▄▃▁</td></tr><tr><td>train_loss_step</td><td>▃▅▅▆▇▁▁▁▄▄▃▄▅█▅▄▃█▄▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▁▂▁▃▁▃▂▃▃▃▂▄▄▄▂▅▅▅▂▆▆▂▂▆▆▃▃▇▇▃▃██▃█</td></tr><tr><td>val_acc</td><td>▁▆▆▇▇▇▇▇▆▆▆▇█▆▇▆▇▇▆█</td></tr><tr><td>val_auc</td><td>▁▅▅▆▆▆▇▆▇▇▇█▇▇█▇▇▇▇█</td></tr><tr><td>val_f1</td><td>▁▆▄▆▃▇█▆▄▅▄█▇▅▆▃▆▄▃▇</td></tr><tr><td>val_loss_epoch</td><td>█▇▅▃▄▁▁▂▂▂▂▁▂▂▁▂▂▃▃▂</td></tr><tr><td>val_loss_step</td><td>█▄▇▄▆▄▆▃▅▄▇▁▇▁▆▂▇▂▆▂▆▂▇▁▆▂▆▂▇▂▆▂▆▂▅▃▅▃▆▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>19</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.77553</td></tr><tr><td>train_auc</td><td>0.82859</td></tr><tr><td>train_f1</td><td>0.81828</td></tr><tr><td>train_loss_epoch</td><td>0.48907</td></tr><tr><td>train_loss_step</td><td>0.39555</td></tr><tr><td>trainer/global_step</td><td>199</td></tr><tr><td>val_acc</td><td>0.75862</td></tr><tr><td>val_auc</td><td>0.8324</td></tr><tr><td>val_f1</td><td>0.80609</td></tr><tr><td>val_loss_epoch</td><td>0.49492</td></tr><tr><td>val_loss_step</td><td>0.46914</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">GCN_2_64_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_01122024_xenograph_Kfold/runs/4ld3f3u4' target=\"_blank\">https://wandb.ai/thoomas/PLA_01122024_xenograph_Kfold/runs/4ld3f3u4</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240122_212338-4ld3f3u4\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f20be7c83bf43678fd28536afbac702",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01693333333338766, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240122_225941-au63ihof</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_01122024_xenograph_Kfold/runs/au63ihof' target=\"_blank\">GAT_3_64_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_01122024_xenograph_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_01122024_xenograph_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_01122024_xenograph_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_01122024_xenograph_Kfold/runs/au63ihof' target=\"_blank\">https://wandb.ai/thoomas/PLA_01122024_xenograph_Kfold/runs/au63ihof</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | GAT              | 78.0 K\n",
      "2  | head        | Sequential       | 2.1 K \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 65    \n",
      "--------------------------------------------------\n",
      "80.2 K    Trainable params\n",
      "0         Non-trainable params\n",
      "80.2 K    Total params\n",
      "0.321     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GAT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17b90a53edec4cf999cad74a53a9d2d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▆▇▇▇▇█▇███▇████████</td></tr><tr><td>train_auc</td><td>▁▆▇▇█▇▇███████████▇█</td></tr><tr><td>train_f1</td><td>▁▆▇▇█▇█▇███████▇████</td></tr><tr><td>train_loss_epoch</td><td>██▄▆▂▃▃▂▂▂▂▃▁▂▁▁▂▁▂▂</td></tr><tr><td>train_loss_step</td><td>▂█▄▇▂▄▅▂▂▂▃▃▁▂▃▁▃▂▃▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▁▂▁▃▁▃▂▃▃▃▂▄▄▄▂▅▅▅▂▆▆▂▂▆▆▃▃▇▇▃▃██▃█</td></tr><tr><td>val_acc</td><td>▁▄▄▅▇▇▇▇█▇▇▇▇▇▇▇█▇█▇</td></tr><tr><td>val_auc</td><td>▁▁▅▆▆▅▅▆▆▆▇█▇█▇▇█▄█▆</td></tr><tr><td>val_f1</td><td>▁▅▅▅▇▆▆▇▇▇▇▇▇▇▇▇█▇▇▇</td></tr><tr><td>val_loss_epoch</td><td>██▄▂▂▃▄▃▄▃▂▂▁▂▂▁▂▂▂▃</td></tr><tr><td>val_loss_step</td><td>█▁▇▂█▁▇▁▆▂▅▂▅▂▅▂▅▃▅▂▅▂▅▂▇▁▅▂▆▂▆▁▅▂▆▂▅▂▅▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>19</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.78285</td></tr><tr><td>train_auc</td><td>0.8326</td></tr><tr><td>train_f1</td><td>0.82216</td></tr><tr><td>train_loss_epoch</td><td>0.50378</td></tr><tr><td>train_loss_step</td><td>0.56869</td></tr><tr><td>trainer/global_step</td><td>199</td></tr><tr><td>val_acc</td><td>0.75172</td></tr><tr><td>val_auc</td><td>0.81786</td></tr><tr><td>val_f1</td><td>0.8</td></tr><tr><td>val_loss_epoch</td><td>0.49315</td></tr><tr><td>val_loss_step</td><td>0.41589</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">GAT_3_64_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_01122024_xenograph_Kfold/runs/au63ihof' target=\"_blank\">https://wandb.ai/thoomas/PLA_01122024_xenograph_Kfold/runs/au63ihof</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240122_225941-au63ihof\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1865e85b201044cab02aaad7b338a888",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240123_003731-skc5t9nb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_01122024_xenograph_Kfold/runs/skc5t9nb' target=\"_blank\">GINConv_2_16_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_01122024_xenograph_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_01122024_xenograph_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_01122024_xenograph_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_01122024_xenograph_Kfold/runs/skc5t9nb' target=\"_blank\">https://wandb.ai/thoomas/PLA_01122024_xenograph_Kfold/runs/skc5t9nb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | GIN              | 1.0 K \n",
      "2  | head        | Sequential       | 154   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 17    \n",
      "--------------------------------------------------\n",
      "1.2 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.2 K     Total params\n",
      "0.005     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GINConv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d128970a9ac943eda765145a3fedf021",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▆▇▇▇▇▇██████████▇██</td></tr><tr><td>train_auc</td><td>▁▆▇█▇███████████████</td></tr><tr><td>train_f1</td><td>▁▆▇█▇█▇█████████▇███</td></tr><tr><td>train_loss_epoch</td><td>█▅▄▄▄▂▂▁▃▁▂▄▂▂▁▃▂▂▁▂</td></tr><tr><td>train_loss_step</td><td>▃▄▄▅▄▁▂▂▆▁▃█▄▃▂▇▂▂▂▅</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▁▂▁▃▁▃▂▃▃▃▂▄▄▄▂▅▅▅▂▆▆▂▂▆▆▃▃▇▇▃▃██▃█</td></tr><tr><td>val_acc</td><td>▁▇▇▇▇▇█▇▇▇▇▇█▇▇▇█▇██</td></tr><tr><td>val_auc</td><td>▁▅▇▇▇▇▇▅▆▆▆▄▇▇▇▇█▇█▇</td></tr><tr><td>val_f1</td><td>▁▆▅▇▆▇▇▅▅▅▇▅▇▇▇▆█▇█▇</td></tr><tr><td>val_loss_epoch</td><td>█▃▅▃▄▂▃▅▄▅▃▄▃▂▂▃▂▂▁▂</td></tr><tr><td>val_loss_step</td><td>█▁▅▁▃▃▅▂▄▂▄▂▄▂▄▂▄▂▄▂▅▂▅▂▄▂▅▁▅▁▄▂▄▂▅▂▅▁▄▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>19</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.77941</td></tr><tr><td>train_auc</td><td>0.84038</td></tr><tr><td>train_f1</td><td>0.82296</td></tr><tr><td>train_loss_epoch</td><td>0.51544</td></tr><tr><td>train_loss_step</td><td>0.66729</td></tr><tr><td>trainer/global_step</td><td>199</td></tr><tr><td>val_acc</td><td>0.75862</td></tr><tr><td>val_auc</td><td>0.83126</td></tr><tr><td>val_f1</td><td>0.80057</td></tr><tr><td>val_loss_epoch</td><td>0.48176</td></tr><tr><td>val_loss_step</td><td>0.44362</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">GINConv_2_16_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_01122024_xenograph_Kfold/runs/skc5t9nb' target=\"_blank\">https://wandb.ai/thoomas/PLA_01122024_xenograph_Kfold/runs/skc5t9nb</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240123_003731-skc5t9nb\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "k_folds = 5\n",
    "kfold = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "for fold, (train_ids, valid_ids) in enumerate(kfold.split(dataset_filtered)):\n",
    "    train_subset = dataset_filtered.index_select(train_ids.tolist())\n",
    "    val_subset = dataset_filtered.index_select(valid_ids.tolist())\n",
    "    for NUM_LAYERS, HIDDEN_CHANNELS, pool, model in params:\n",
    "            # Path to the folder where the pretrained models are saved\n",
    "        # CHECKPOINT_PATH = checkpoint_folder / f'GAT_{NUM_LAYERS}_{HIDDEN_CHANNELS}_onehot_{fold}' / pool\n",
    "        CHECKPOINT_PATH = checkpoint_folder / f'{model}_{NUM_LAYERS}_{HIDDEN_CHANNELS}_onehot_{fold}' / pool\n",
    "        CHECKPOINT_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Skip already trained kfold and pool\n",
    "        checkpoint = CHECKPOINT_PATH / f\"GraphLevel{model}\" / f\"GraphLevel{model}.ckpt\" \n",
    "        if checkpoint.exists():\n",
    "            print(checkpoint)\n",
    "            continue\n",
    "\n",
    "        # Run training\n",
    "        run = wandb.init(project=project_name, name=f'{model}_{NUM_LAYERS}_{HIDDEN_CHANNELS}_onehot_{fold}', \n",
    "                        group=f'{model}_{pool}', \n",
    "                        # mode=\"disabled\"\n",
    "                        )\n",
    "        PPIGraph.train_graph_classifier_kfold(model, \n",
    "                                                train_subset, \n",
    "                                                val_subset, \n",
    "                                                dataset_filtered, \n",
    "                                                CHECKPOINT_PATH, \n",
    "                                                AVAIL_GPUS, \n",
    "                                                in_channels=5,\n",
    "                                                hidden_channels=HIDDEN_CHANNELS, \n",
    "                                                out_channels = HIDDEN_CHANNELS,\n",
    "                                                num_layers=NUM_LAYERS, \n",
    "                                                epochs=epochs,\n",
    "                                                embedding=False,\n",
    "                                                batch_size=256,\n",
    "                                                num_workers= 24,\n",
    "                                                graph_pooling=pool)\n",
    "        run.finish()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PLA2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
