{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bb1094b-3ac6-49ae-9c51-9952c8987127",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import skimage.io\n",
    "\n",
    "from collections import defaultdict\n",
    "from tqdm.auto import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "import re\n",
    "import h5py\n",
    "import napari\n",
    "import tifffile as tiff\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "\n",
    "import networkx as nx\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c2dc746-ef6a-4289-b0ea-38adf6a33bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric.utils\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94f7e35b-e960-4fab-84fa-bced8761b99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1549ea57-9b54-4965-8735-107d08afff41",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_dir = (Path().cwd().parents[0]).absolute()\n",
    "\n",
    "module_path = str(p_dir / \"src\")\n",
    "\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c9bcfbd-bd6d-448b-abc7-e49f91ca3b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = (Path().cwd().parents[0] / 'data').absolute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce8d51c1-7532-430e-969e-43a7318225e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning.pytorch as pl\n",
    "import PPIGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e1ef59-fba4-424d-801f-4ca6bcafc417",
   "metadata": {},
   "source": [
    "# Test custom torch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a303e43-e680-42e3-9827-c610578ed813",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62377297-c838-46d3-af70-729f049e1c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_mapping = {'HCC827Ctrl': 0, 'HCC827Osim': 1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be1bfa6f-ca26-43dc-aaea-e90f4931bea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_path = data_dir / 'OCT Cell Culture' / '3D_Whole' / 'graphs' \n",
    "\n",
    "dataset = PPIGraph.GraphDataset(graph_path, 'raw', 'pt', condition_mapping=condition_mapping, n_c=2)\n",
    "train_set, val_set, test_set = PPIGraph.train_test_val_split(dataset)\n",
    "\n",
    "# Create Dataloader\n",
    "train_loader = DataLoader(train_set, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64f3773a-dd67-40ea-8f43-217aa19c21b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: GraphDataset(2305):\n",
      "======================\n",
      "Number of graphs: 2305\n",
      "Number of features: 5\n",
      "Number of classes: 2\n"
     ]
    }
   ],
   "source": [
    "print(f'Dataset: {dataset}:')\n",
    "print('======================')\n",
    "print(f'Number of graphs: {len(dataset)}')\n",
    "print(f'Number of features: {dataset.num_features}')\n",
    "print(f'Number of classes: {dataset.num_classes}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c7beaf5-75c9-4d49-abc6-153fce500148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 1107, test set: 922, val set: 276\n"
     ]
    }
   ],
   "source": [
    "print(f'Train set: {len(train_set)}, test set: {len(test_set)}, val set: {len(val_set)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cac6d207-a878-4acb-b6a9-3caf98cb5577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1:\n",
      "=======\n",
      "Number of graphs in the current batch: 32\n",
      "DataBatch(edge_index=[2, 41282], pos=[3254, 3], labels=[3254, 5], nuclei=[3254], weight=[41282], condition=[32], fov=[32], id=[32], train_mask=[3254], test_mask=[3254], x=[3254, 5], y=[32], edge_weight=[41282], name=[32], batch=[3254], ptr=[33])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for step, data in enumerate(train_loader):\n",
    "    print(f'Step {step + 1}:')\n",
    "    print('=======')\n",
    "    print(f'Number of graphs in the current batch: {data.num_graphs}')\n",
    "    print(data)\n",
    "    print()\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c288bf21-b281-49de-a8f0-946e9a3d8d13",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33c736d8-0f3a-4cf0-b9a3-46b3785b68dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from lightning.pytorch.accelerators import find_usable_cuda_devices\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cbc9f6bb-9704-45a4-8aa4-6ac5f1d10c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "condition = 'Kfold'\n",
    "checkpoint_folder = (Path().cwd().parents[0]).absolute() / 'data' / \"saved_models\" / f\"Graph_GNNs_{condition}\" \n",
    "project_name = f'PLA_042623_{condition}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2721e73a-6503-49b6-880b-671c722230fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AVAIL_GPUS = [1]\n",
    "# BATCH_SIZE = 64 if AVAIL_GPUS else 32\n",
    "\n",
    "# # Setting the seed\n",
    "# pl.seed_everything(42)\n",
    "\n",
    "# NUM_LAYERS = 2\n",
    "# HIDDEN_CHANNELS = 16\n",
    "# pools = ['mean', 'max', 'sum', 'attention', 'attention2']\n",
    "\n",
    "# epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1d330a4f-82b0-428f-a7ae-c07dee9147c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for pool in pools:\n",
    "#     # Path to the folder where the pretrained models are saved\n",
    "#     CHECKPOINT_PATH = checkpoint_folder / f'3D_{NUM_LAYERS}_{HIDDEN_CHANNELS}_onehot' / pool\n",
    "#     CHECKPOINT_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#     # Run training\n",
    "#     models = ['GCN']\n",
    "#     for model_name in models:\n",
    "#         run = wandb.init(project=project_name, name=model_name+f'_{NUM_LAYERS}_{HIDDEN_CHANNELS}_onehot')\n",
    "#         model, result, trainer = PPIGraph.train_graph_classifier(model_name, \n",
    "#                                                                  train_set, \n",
    "#                                                                  val_set, \n",
    "#                                                                  test_set, \n",
    "#                                                                  dataset, \n",
    "#                                                                  CHECKPOINT_PATH, \n",
    "#                                                                  AVAIL_GPUS, \n",
    "#                                                                  in_channels=5,\n",
    "#                                                                  hidden_channels=HIDDEN_CHANNELS, \n",
    "#                                                                  out_channels = HIDDEN_CHANNELS,\n",
    "#                                                                  num_layers=NUM_LAYERS, \n",
    "#                                                                  epochs=epochs,\n",
    "#                                                                  embedding=False,\n",
    "#                                                                  graph_pooling=pool)\n",
    "#         run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac541be-7224-48e5-9b48-8b1ecb31f1ef",
   "metadata": {},
   "source": [
    "# K Fold filter datasaet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a332036c-f465-4414-9bf2-87fb676ee181",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "895cb707-2209-46b1-80cd-1bd1799f43d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out by maximum number of counts per cell\n",
    "min_count = 20\n",
    "max_count = 70\n",
    "\n",
    "graph_path = data_dir / 'OCT Cell Culture' / '3D_Whole' / 'graphs' \n",
    "\n",
    "dataset = PPIGraph.GraphDataset(graph_path, 'raw', 'pt', condition_mapping=condition_mapping, n_c=2)\n",
    "\n",
    "# Create Dataloader\n",
    "loader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# Get Indices\n",
    "indices = []\n",
    "for step, data in enumerate(loader):\n",
    "    if len(data.x) <= min_count:\n",
    "        continue \n",
    "    \n",
    "    if (data.x.sum(axis=0) >= max_count).any():\n",
    "        continue\n",
    "    indices.append(step)\n",
    "    \n",
    "# Get subset dataset\n",
    "dataset_filtered = dataset.index_select(indices)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5940dd43-9fd1-4a1a-8284-4b7348d46872",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1595"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "246afbfb-031f-4731-a594-cbf0372c27de",
   "metadata": {},
   "outputs": [],
   "source": [
    "condition = 'Kfold'\n",
    "checkpoint_folder = (Path().cwd().parents[0]).absolute() / 'data' / \"saved_models\" / f\"Graph_GNNs_{condition}\" \n",
    "project_name = f'PLA_042923_{condition}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "de172714-fbb1-4ecd-aacf-a3b72761833d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    }
   ],
   "source": [
    "AVAIL_GPUS = [1]\n",
    "\n",
    "# Setting the seed\n",
    "pl.seed_everything(42)\n",
    "\n",
    "NUM_LAYERS = 2\n",
    "HIDDEN_CHANNELS = 16\n",
    "pools = ['mean', 'max', 'sum', 'attention', 'attention2']\n",
    "\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "de9c6338-3ea7-4d18-8a33-0cc660701c79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# k_folds = 5\n",
    "# kfold = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# for fold, (train_ids, valid_ids) in enumerate(kfold.split(dataset_filtered)):\n",
    "#     train_subset = dataset_filtered.index_select(train_ids.tolist())\n",
    "#     val_subset = dataset_filtered.index_select(valid_ids.tolist())\n",
    "    \n",
    "#     for pool in pools:\n",
    "#         # Path to the folder where the pretrained models are saved\n",
    "#         CHECKPOINT_PATH = checkpoint_folder / f'3D_{NUM_LAYERS}_{HIDDEN_CHANNELS}_onehot_{fold}' / pool\n",
    "#         CHECKPOINT_PATH.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "#         # Skip already trained kfold and pool\n",
    "#         checkpoint = CHECKPOINT_PATH / \"GraphLevelGCN\" / \"GraphLevelGCN.ckpt\" \n",
    "#         if checkpoint.exists():\n",
    "#             print(checkpoint)\n",
    "#             continue\n",
    "        \n",
    "#         # Run training\n",
    "#         run = wandb.init(project=project_name, name=f'3D_GCN_{NUM_LAYERS}_{HIDDEN_CHANNELS}_onehot_{fold}', \n",
    "#                         group=f'3D_GCN_{pool}')\n",
    "#         PPIGraph.train_graph_classifier_kfold('GCN', \n",
    "#                                              train_subset, \n",
    "#                                              val_subset, \n",
    "#                                              dataset, \n",
    "#                                              CHECKPOINT_PATH, \n",
    "#                                              AVAIL_GPUS, \n",
    "#                                              in_channels=5,\n",
    "#                                              hidden_channels=HIDDEN_CHANNELS, \n",
    "#                                              out_channels = HIDDEN_CHANNELS,\n",
    "#                                              num_layers=NUM_LAYERS, \n",
    "#                                              epochs=epochs,\n",
    "#                                              embedding=False,\n",
    "#                                              graph_pooling=pool)\n",
    "#         run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a71ba203-c955-440c-841d-e37c60826e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "condition = 'Kfold'\n",
    "checkpoint_folder = (Path().cwd().parents[0]).absolute() / 'data' / \"saved_models\" / f\"MLP_{condition}\" \n",
    "project_name = f'PLA_042923_{condition}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a8e725ab-24a3-413e-aaa7-8c8bb2b6b073",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    }
   ],
   "source": [
    "AVAIL_GPUS = [1]\n",
    "\n",
    "# Setting the seed\n",
    "pl.seed_everything(42)\n",
    "\n",
    "NUM_LAYERS = 2\n",
    "HIDDEN_CHANNELS = 16\n",
    "pools = ['mean', 'max', 'sum', 'attention', 'attention2']\n",
    "\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "de6d38e1-408f-4f2f-b753-b475beb5fa05",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mthoomas\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cca4fc3d7cc4cfab3db4f6f3a62e8bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01691666666883975, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Thomas\\15_PLA\\notebooks\\wandb\\run-20230510_231134-6sy5s883</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/6sy5s883' target=\"_blank\">3D_MLP_2_16_onehot_0</a></strong> to <a href='https://wandb.ai/thoomas/PLA_042923_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_042923_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/6sy5s883' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold/runs/6sy5s883</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "C:\\Users\\thu71\\Anaconda3\\envs\\snowflake\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:395: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 368   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "522       Trainable params\n",
      "0         Non-trainable params\n",
      "522       Total params\n",
      "0.002     Total estimated model params size (MB)\n",
      "C:\\Users\\thu71\\AppData\\Roaming\\Python\\Python39\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\thu71\\AppData\\Roaming\\Python\\Python39\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\thu71\\AppData\\Roaming\\Python\\Python39\\site-packages\\lightning\\pytorch\\utilities\\data.py:77: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 2. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  warning_cache.warn(\n",
      "C:\\Users\\thu71\\AppData\\Roaming\\Python\\Python39\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3acf25ed19bb4c55a921bf36475395d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▅▅▅▇▇██▇▇██▇▇█▇██▇█▇██▇▇█████▇▇█▇▇▇▇███</td></tr><tr><td>train_auc</td><td>▁▂▁▂▆▇█▇▇▆██▇▇▇▇█▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇██</td></tr><tr><td>train_f1</td><td>▆▂▁▁▆▇█▇▇▇██▇▇█▇██▇▇▇▇█▇▇▇▇█▇▇▇▇▇█▇▇▇▇██</td></tr><tr><td>train_loss_epoch</td><td>█▆▆▆▃▂▂▂▂▂▂▁▂▁▂▂▂▂▁▂▂▂▂▁▂▁▁▂▁▁▁▂▂▂▁▁▂▁▁▂</td></tr><tr><td>train_loss_step</td><td>██▆▆▅▃▅▃▄▃▄▅▇▂▅▄▄▄▃▄▆▅▅▅▆▅▄▅▄▆▇▄▄▁▆▂▃▃▆▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▁▂▂▁▃▃▂▃▃▂▄▄▂▂▄▂▂▅▅▂▆▆▂▆▆▃▇▇▃▃▇▃▃█▃</td></tr><tr><td>val_acc</td><td>▁▁▁▁▇▆█▇▇▇▇▇▇▇▇▇▇█▇█████▇█▇██▇▇▇█▇▇████▇</td></tr><tr><td>val_auc</td><td>▁▁▁▁▇▇██▇███▇█▇▇████▇███▇████▇███▇███▇██</td></tr><tr><td>val_f1</td><td>▁▁▁▁████████▇███████▇███▇████████▇███▇██</td></tr><tr><td>val_loss_epoch</td><td>██▇▆▃▂▁▂▁▂▂▂▂▂▂▂▂▂▁▂▂▁▂▁▂▂▁▂▂▁▁▂▁▁▂▂▂▂▂▁</td></tr><tr><td>val_loss_step</td><td>▇█▇▅▄▂▂▄▃▃▄▃▂▃▄▃▅▃▁▃▂▄▄▃▄▄▄▄▄▄▂▁▄▃▃▅▃▂▃▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.72884</td></tr><tr><td>train_auc</td><td>0.68463</td></tr><tr><td>train_f1</td><td>0.58413</td></tr><tr><td>train_loss_epoch</td><td>0.55035</td></tr><tr><td>train_loss_step</td><td>0.50137</td></tr><tr><td>trainer/global_step</td><td>999</td></tr><tr><td>val_acc</td><td>0.74922</td></tr><tr><td>val_auc</td><td>0.69921</td></tr><tr><td>val_f1</td><td>0.59596</td></tr><tr><td>val_loss_epoch</td><td>0.48227</td></tr><tr><td>val_loss_step</td><td>0.42939</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">3D_MLP_2_16_onehot_0</strong> at: <a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/6sy5s883' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold/runs/6sy5s883</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230510_231134-6sy5s883\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f955d3f1eb84df390dc4629cc735bac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01693333333435779, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Thomas\\15_PLA\\notebooks\\wandb\\run-20230511_002157-8zfuqlgf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/8zfuqlgf' target=\"_blank\">3D_MLP_2_16_onehot_0</a></strong> to <a href='https://wandb.ai/thoomas/PLA_042923_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_042923_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/8zfuqlgf' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold/runs/8zfuqlgf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 368   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "522       Trainable params\n",
      "0         Non-trainable params\n",
      "522       Total params\n",
      "0.002     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54a02b8f82f04c51958a1584877ef631",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▇▇▇▇▇▇▇▇▇▇█▇▇▇▇▇▇▇▇▇▇██▇▇█▇▇▇█▇▇███▇▇▇▇</td></tr><tr><td>train_auc</td><td>▁▄▂▂▂▂▂▂▂▂▃▅▄▃▃▆▅▅▃▂▃▄▅▆▃▄▅▄▄▆▇▅▄█▆▇▄▅█▄</td></tr><tr><td>train_f1</td><td>█▄▂▁▁▁▁▁▁▁▁▃▂▂▃▃▃▃▂▂▃▂▃▃▃▃▃▃▃▄▄▃▃▅▄▅▄▃▆▃</td></tr><tr><td>train_loss_epoch</td><td>█▃▃▃▃▃▂▃▂▂▂▂▂▁▂▂▂▂▂▂▂▂▁▂▂▂▁▁▂▁▂▂▁▁▁▂▂▂▂▁</td></tr><tr><td>train_loss_step</td><td>▆▆▃▃▄▅▄▅▆▄▄▄▇▂▅▃▄▃▄▅▃█▅▅▅▂▄▃▃▄▃▂▃▁▄▃▂▃▆▄</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▁▂▂▁▃▃▂▃▃▂▄▄▂▂▄▂▂▅▅▂▆▆▂▆▆▃▇▇▃▃▇▃▃█▃</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_epoch</td><td>▅▅▃▁▂▁▁▂▂▃▇▇▆▇█▅▇▄▄▃▃▂▄▂▂▂▄▄▂▄▂▃▃▃▃▂▃▃▅▄</td></tr><tr><td>val_loss_step</td><td>▆▆▄▁▆▃▁▅▄▆▇▇▆▇█▆▆▄▄▆▅▄▇▅▄▄▆▅▄▄▅▃▇▃▅▅▅▆▅▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.62774</td></tr><tr><td>train_auc</td><td>0.51102</td></tr><tr><td>train_f1</td><td>0.10208</td></tr><tr><td>train_loss_epoch</td><td>0.64034</td></tr><tr><td>train_loss_step</td><td>0.65291</td></tr><tr><td>trainer/global_step</td><td>999</td></tr><tr><td>val_acc</td><td>0.65831</td></tr><tr><td>val_auc</td><td>0.5</td></tr><tr><td>val_f1</td><td>0.0</td></tr><tr><td>val_loss_epoch</td><td>0.67526</td></tr><tr><td>val_loss_step</td><td>0.66585</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">3D_MLP_2_16_onehot_0</strong> at: <a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/8zfuqlgf' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold/runs/8zfuqlgf</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230511_002157-8zfuqlgf\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8c2653b746541c69fb84edd718aaef7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Thomas\\15_PLA\\notebooks\\wandb\\run-20230511_012032-431s3b9z</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/431s3b9z' target=\"_blank\">3D_MLP_2_16_onehot_0</a></strong> to <a href='https://wandb.ai/thoomas/PLA_042923_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_042923_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/431s3b9z' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold/runs/431s3b9z</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 368   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "522       Trainable params\n",
      "0         Non-trainable params\n",
      "522       Total params\n",
      "0.002     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19d332efbe8147da87aee09d6a3b6776",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▄▅▅▆▆▆▇▇▇▇▆▇▇▇█▇▇▇▇▇█▇▇▇▇██▇███▇██▇███</td></tr><tr><td>train_auc</td><td>▁▂▃▃▃▄▄▅▆▆▇▇▅▇▇▇█▆▇▇▆▇█▆▇▇▆▇▇▇▇█▇▇█▇▇███</td></tr><tr><td>train_f1</td><td>▃▂▂▁▁▃▃▅▆▆▆▆▅▇▇▆█▆▇▆▆▆▇▅▇▇▅▇▇▆▇█▇▆█▇▆▇██</td></tr><tr><td>train_loss_epoch</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▄▂▃▃▃▂▂▂▂▂▂▃▁▂▂▂▂▂▂▂▂▂▂▃▂▂▂▁▂▃▂▂▁▂▁▁▂▃▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▁▂▂▁▃▃▂▃▃▂▄▄▂▂▄▂▂▅▅▂▆▆▂▆▆▃▇▇▃▃▇▃▃█▃</td></tr><tr><td>val_acc</td><td>▁▁▁▂▃▅▆▆▇█▇▆▆▇▇▇█▇▆▆▆▆█▇▅▇▇█▆████▇▇█▇▇▆▇</td></tr><tr><td>val_auc</td><td>▁▂▁▂▃▄▅▅▆█▇▆▅█▇▆█▆▆▆▆▆█▇▅▇▆█▆▇███▇▇█▇▇▆▆</td></tr><tr><td>val_f1</td><td>▁▃▂▂▄▅▆▆▇██▆▆█▇▇█▇▇▇▇▇█▇▆▇▇█▇████▇███▇▆▇</td></tr><tr><td>val_loss_epoch</td><td>█▇▆▆▅▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▂▁▂▂▂▂▂▂▁▁▁▁▂▂▁▂▂▁</td></tr><tr><td>val_loss_step</td><td>██▇▆▆▅▄▅▄▃▄▄▃▃▄▄▅▃▂▄▃▄▄▃▄▄▄▄▄▃▃▂▄▃▃▅▃▂▃▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.72806</td></tr><tr><td>train_auc</td><td>0.68832</td></tr><tr><td>train_f1</td><td>0.5932</td></tr><tr><td>train_loss_epoch</td><td>0.53487</td></tr><tr><td>train_loss_step</td><td>0.52749</td></tr><tr><td>trainer/global_step</td><td>999</td></tr><tr><td>val_acc</td><td>0.76489</td></tr><tr><td>val_auc</td><td>0.70229</td></tr><tr><td>val_f1</td><td>0.59459</td></tr><tr><td>val_loss_epoch</td><td>0.45361</td></tr><tr><td>val_loss_step</td><td>0.37542</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">3D_MLP_2_16_onehot_0</strong> at: <a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/431s3b9z' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold/runs/431s3b9z</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230511_012032-431s3b9z\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "707a59f4b899407a95a102dc9da735b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016933333330477276, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Thomas\\15_PLA\\notebooks\\wandb\\run-20230511_025257-atv7yoyh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/atv7yoyh' target=\"_blank\">3D_MLP_2_16_onehot_0</a></strong> to <a href='https://wandb.ai/thoomas/PLA_042923_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_042923_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/atv7yoyh' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold/runs/atv7yoyh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "C:\\Users\\thu71\\Anaconda3\\envs\\snowflake\\lib\\site-packages\\torch_geometric\\deprecation.py:12: UserWarning: 'nn.glob.GlobalAttention' is deprecated, use 'nn.aggr.AttentionalAggregation' instead\n",
      "  warnings.warn(out)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 368   \n",
      "2  | head        | Sequential       | 154   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 17    \n",
      "--------------------------------------------------\n",
      "539       Trainable params\n",
      "0         Non-trainable params\n",
      "539       Total params\n",
      "0.002     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a703adc0adf4a93a9603cae68c1df87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▄▅▅▆▇▇▇▇▇▇▇▇▇▇█▇▇▇▇███▇▇▇▇▇█▇▇▇▇▇▇▇██▇█</td></tr><tr><td>train_auc</td><td>▁▁▁▁▅▆▇▆▆▆▇▆▆▆▆▇▇▇▆▇██▇▆▆▆▆▇█▇▇▆▆▆▇▇▇▇▇▇</td></tr><tr><td>train_f1</td><td>▆▂▁▁▆▆▇▇▇▇█▇▇▆▇▇▇▇▇▇███▇▇▇▇▇█▇▇▇▇▇▇█████</td></tr><tr><td>train_loss_epoch</td><td>█▇▆▆▄▃▃▃▂▂▂▂▂▂▁▂▂▁▂▁▁▁▁▂▂▁▁▂▁▁▁▁▁▂▂▂▁▂▁▂</td></tr><tr><td>train_loss_step</td><td>█▇██▄▅▆▅▆▂█▄▅▅▅▄▄▄▅▅▃▁▄▅▂▄▄▄▂▂▂▄▂▄▄▄▅▆▆▄</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▁▂▂▁▃▃▂▃▃▂▄▄▂▂▄▂▂▅▅▂▆▆▂▆▆▃▇▇▃▃▇▃▃█▃</td></tr><tr><td>val_acc</td><td>▁▁▁▁████▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇▇▇▇▇██▇█▇▇█▇▇▇▄▇</td></tr><tr><td>val_auc</td><td>▁▁▁▁██▆█▆▆▆▆▆▆▇▆▆▇▇▆▅▆▆▆▅▆▇▆▆▆▆▆▆▇█▅▆▆▃▅</td></tr><tr><td>val_f1</td><td>▁▁▁▁██▇█▇▇▇▇▇▇▇▇▇▇▇▇▆▇▇▇▆▇▇▆▇▇▆▇▇██▆▇▇▄▆</td></tr><tr><td>val_loss_epoch</td><td>███▇▅▄▂▄▂▂▂▂▂▂▂▂▂▂▂▁▁▂▂▂▂▁▂▂▁▁▂▂▂▁▂▂▂▁▃▂</td></tr><tr><td>val_loss_step</td><td>▇███▆▄▃▃▄▂▃▃▂▂▂▃▃▃▃▃▄▂▄▃▂▂▃▃▂▃▁▃▂▁▃▂▅▃▃▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.71708</td></tr><tr><td>train_auc</td><td>0.66838</td></tr><tr><td>train_f1</td><td>0.55706</td></tr><tr><td>train_loss_epoch</td><td>0.55308</td></tr><tr><td>train_loss_step</td><td>0.56631</td></tr><tr><td>trainer/global_step</td><td>999</td></tr><tr><td>val_acc</td><td>0.74608</td></tr><tr><td>val_auc</td><td>0.64168</td></tr><tr><td>val_f1</td><td>0.45638</td></tr><tr><td>val_loss_epoch</td><td>0.49833</td></tr><tr><td>val_loss_step</td><td>0.46746</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">3D_MLP_2_16_onehot_0</strong> at: <a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/atv7yoyh' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold/runs/atv7yoyh</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230511_025257-atv7yoyh\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b4134c1868842a99b142c61e08fbee6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016933333330477276, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Thomas\\15_PLA\\notebooks\\wandb\\run-20230511_040340-y54jmw50</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/y54jmw50' target=\"_blank\">3D_MLP_2_16_onehot_0</a></strong> to <a href='https://wandb.ai/thoomas/PLA_042923_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_042923_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/y54jmw50' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold/runs/y54jmw50</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 368   \n",
      "2  | head        | Sequential       | 154   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 281   \n",
      "--------------------------------------------------\n",
      "803       Trainable params\n",
      "0         Non-trainable params\n",
      "803       Total params\n",
      "0.003     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19d3496e4d00401796148135f759e3f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▄▅▅▆▇██▇█▇▇▇▇███▇▇█▇▇▇▇█▇▇██▇▇███▇▇▇▇██</td></tr><tr><td>train_auc</td><td>▁▁▂▂▅▇█▇▇▇▇▇▇▆██▇▇▇▇▇▇▆▇▇▆▆▇▇▆▆▇█▇▇▇▇▇██</td></tr><tr><td>train_f1</td><td>▆▁▁▁▅███▇▇▇▇▇▇███▇▇▇▇▇▇██▇▇██▇▇██▇▇▇▇███</td></tr><tr><td>train_loss_epoch</td><td>█▇▆▆▄▃▂▂▁▁▂▂▂▂▂▂▁▂▂▁▂▂▁▂▁▁▂▁▂▂▂▂▁▁▂▂▁▂▁▁</td></tr><tr><td>train_loss_step</td><td>▇▇█▆▆▅▃▄▃▆▅▄▄▅▅▂▃▄▅▄▄▄▄▃▂▆▂▄▂▄▂▃▃▃▆▄▄▅▁▆</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▁▂▂▁▃▃▂▃▃▂▄▄▂▂▄▂▂▅▅▂▆▆▂▆▆▃▇▇▃▃▇▃▃█▃</td></tr><tr><td>val_acc</td><td>▁▁▁▁█▇█▇▇██▇█▇█▇▇▇▇▇▇▇▇██▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>val_auc</td><td>▁▁▁▁█▆▆▅▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▇▆▆▆▆▇▆▇▆▆▇▆▆▆▇</td></tr><tr><td>val_f1</td><td>▁▁▁▁█▇▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>val_loss_epoch</td><td>█▇▇▇▆▂▂▂▂▂▂▂▁▂▁▂▂▁▂▂▂▁▂▂▂▁▂▂▂▂▂▂▁▂▂▁▁▂▂▂</td></tr><tr><td>val_loss_step</td><td>███▇▇▅▃▄▃▃▃▄▄▄▁▃▄▂▄▃▅▂▆▅▄▃▄▃▄▄▂▃▄▅▃▄▃▂▂▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.72492</td></tr><tr><td>train_auc</td><td>0.67763</td></tr><tr><td>train_f1</td><td>0.57143</td></tr><tr><td>train_loss_epoch</td><td>0.54568</td></tr><tr><td>train_loss_step</td><td>0.62632</td></tr><tr><td>trainer/global_step</td><td>999</td></tr><tr><td>val_acc</td><td>0.74608</td></tr><tr><td>val_auc</td><td>0.70345</td></tr><tr><td>val_f1</td><td>0.60488</td></tr><tr><td>val_loss_epoch</td><td>0.50251</td></tr><tr><td>val_loss_step</td><td>0.55344</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">3D_MLP_2_16_onehot_0</strong> at: <a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/y54jmw50' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold/runs/y54jmw50</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230511_040340-y54jmw50\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f52007ed270341368d2d68c4fd525099",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Thomas\\15_PLA\\notebooks\\wandb\\run-20230511_050122-gjvwzwxp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/gjvwzwxp' target=\"_blank\">3D_MLP_2_16_onehot_1</a></strong> to <a href='https://wandb.ai/thoomas/PLA_042923_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_042923_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/gjvwzwxp' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold/runs/gjvwzwxp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 368   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "522       Trainable params\n",
      "0         Non-trainable params\n",
      "522       Total params\n",
      "0.002     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▅▅▅▇▇▇█▇████▇████▇▇▇▇▇▇▇▇▇█▇███▇▇█▇▇███</td></tr><tr><td>train_auc</td><td>▁▁▁▁▅▇▇▇▇█▇█▇▇█▇██▇▇▆▆▇▆▇▇▇▇▇█▇█▇▇█▇▇▇█▇</td></tr><tr><td>train_f1</td><td>▆▂▁▁▅▇█▇████▇▇█▇██▇▇▇▇▇▇▇▇▇▇▇█▇█▇▇█▇███▇</td></tr><tr><td>train_loss_epoch</td><td>█▆▆▆▃▃▂▂▂▂▂▂▂▂▂▂▁▁▂▂▂▂▁▂▂▂▁▂▂▁▁▁▂▂▂▂▂▂▂▁</td></tr><tr><td>train_loss_step</td><td>▆▆▅▄▃▆▃▃▅▂▃▄█▂▅▂▄▃▂▃▁▆▄▃▄▄▂▂▃▂▄▄▄▄▃▄▄▁▅▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▁▂▂▁▃▃▂▃▃▂▄▄▂▂▄▂▂▅▅▂▆▆▂▆▆▃▇▇▃▃▇▃▃█▃</td></tr><tr><td>val_acc</td><td>▁▁▁▁▅▆▆▆▆▆▇▇▆▇▆▆█▆▆▆▆▆▆▆▆▆▇▇▆█▆▇▆▆▆▆▇▆▆▆</td></tr><tr><td>val_auc</td><td>▁▁▁▁▅▆▆▆▆▆▇▇▆▇▆▆█▆▆▆▆▆▆▆▆▆▇▇▆▇▆▇▆▅▆▆▆▆▆▆</td></tr><tr><td>val_f1</td><td>▁▁▁▁▆▇▇▇▇▇▇▇▇▇▇▇█▇▇▇▇▇▇▇▇▇█▇▇█▇▇▇▆▇▇▇▇▇▇</td></tr><tr><td>val_loss_epoch</td><td>███▇▃▂▂▂▂▂▁▁▂▂▂▃▁▂▂▂▂▂▂▂▁▂▁▂▂▁▁▂▂▂▂▃▂▂▂▁</td></tr><tr><td>val_loss_step</td><td>██▇▇▆▂▂▅▃▃▅▂▃▃▄▂▃▅▄▄▂▄▃▃▄▂▄▃▃▂▂▃▅▂▃▅▅▃▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.71787</td></tr><tr><td>train_auc</td><td>0.65642</td></tr><tr><td>train_f1</td><td>0.52756</td></tr><tr><td>train_loss_epoch</td><td>0.53195</td></tr><tr><td>train_loss_step</td><td>0.54699</td></tr><tr><td>trainer/global_step</td><td>999</td></tr><tr><td>val_acc</td><td>0.73354</td></tr><tr><td>val_auc</td><td>0.69438</td></tr><tr><td>val_f1</td><td>0.60094</td></tr><tr><td>val_loss_epoch</td><td>0.49302</td></tr><tr><td>val_loss_step</td><td>0.42631</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">3D_MLP_2_16_onehot_1</strong> at: <a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/gjvwzwxp' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold/runs/gjvwzwxp</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230511_050122-gjvwzwxp\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f39a631b3a524769963e29d49ccb82c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01691666666883975, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Thomas\\15_PLA\\notebooks\\wandb\\run-20230511_055605-0qir5hwa</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/0qir5hwa' target=\"_blank\">3D_MLP_2_16_onehot_1</a></strong> to <a href='https://wandb.ai/thoomas/PLA_042923_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_042923_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/0qir5hwa' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold/runs/0qir5hwa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 368   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "522       Trainable params\n",
      "0         Non-trainable params\n",
      "522       Total params\n",
      "0.002     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be152df6b0294a43b8dfb02f86ffac4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▇▇▇▇▇▇▇▇▇▇██▇█▇█▇▇▇████▇███▇██████▇▇▇██</td></tr><tr><td>train_auc</td><td>▁▄▁▂▂▂▂▂▂▂▂▂▂▂▃▂▂▂▃▂▃▃▄▅▃▃▃▆▃▅▄▆▆█▅▅▂▄▆▅</td></tr><tr><td>train_f1</td><td>█▃▂▁▁▁▁▁▁▁▁▁▁▁▂▁▁▂▂▁▂▂▂▃▂▂▂▄▂▃▂▃▃▄▃▄▂▃▃▃</td></tr><tr><td>train_loss_epoch</td><td>█▃▃▃▃▃▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▂▂▂▂▁▁▂▂▂▁▁</td></tr><tr><td>train_loss_step</td><td>▆▅▃▃▃▅▄▅▆▅▅▄▆▁▄▃▄▁▄▅▂█▅▄▆▃▂▃▃▄▃▂▃▃▅▄▄▃▆▄</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▁▂▂▁▃▃▂▃▃▂▄▄▂▂▄▂▂▅▅▂▆▆▂▆▆▃▇▇▃▃▇▃▃█▃</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_epoch</td><td>▅▄▄▂▂▁▁▃▂▃▅▅▄▅▆▇█▆▆▅▅▆▆▅▅▃▄▆▄▅▃▅▂▂▄▅▅▄▄▂</td></tr><tr><td>val_loss_step</td><td>▆▆▄▃▇▃▁▇▆▆▇▆▅▆▇▇▇▆▅█▆▇█▆▆▅▆▆▅▆▄▅█▂▅▇▇▆▃▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.64577</td></tr><tr><td>train_auc</td><td>0.51756</td></tr><tr><td>train_f1</td><td>0.11719</td></tr><tr><td>train_loss_epoch</td><td>0.63291</td></tr><tr><td>train_loss_step</td><td>0.63621</td></tr><tr><td>trainer/global_step</td><td>999</td></tr><tr><td>val_acc</td><td>0.60502</td></tr><tr><td>val_auc</td><td>0.5</td></tr><tr><td>val_f1</td><td>0.0</td></tr><tr><td>val_loss_epoch</td><td>0.67222</td></tr><tr><td>val_loss_step</td><td>0.65044</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">3D_MLP_2_16_onehot_1</strong> at: <a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/0qir5hwa' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold/runs/0qir5hwa</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230511_055605-0qir5hwa\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bc7eb4d25a14230b363f00b7f95f8ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016933333330477276, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Thomas\\15_PLA\\notebooks\\wandb\\run-20230511_064123-274pa2ul</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/274pa2ul' target=\"_blank\">3D_MLP_2_16_onehot_1</a></strong> to <a href='https://wandb.ai/thoomas/PLA_042923_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_042923_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/274pa2ul' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold/runs/274pa2ul</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 368   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "522       Trainable params\n",
      "0         Non-trainable params\n",
      "522       Total params\n",
      "0.002     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▃▄▅▅▆▆▆▆▇▇▇█▇▇▇█▇▇█▇▇▇▇▇█▇▇██▇▇█▇█▇█▇▇█</td></tr><tr><td>train_auc</td><td>▁▂▂▂▃▄▅▄▅▆▆▇▇▆▇▇▇▆▆█▇▆▇▆▇▇▆▆▇█▇▇▇▆▇▇█▆▇█</td></tr><tr><td>train_f1</td><td>▄▄▃▁▃▅▅▄▆▆▆▇▇▆▇▇▇▆▆█▇▆▇▆▇▇▆▆██▇▇▇▆▇▇█▇▇█</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▄▃▃▃▃▃▂▃▂▂▂▃▁▃▂▂▂▂▂▁▃▂▂▂▃▂▂▂▂▃▂▂▂▂▃▃▁▃▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▁▂▂▁▃▃▂▃▃▂▄▄▂▂▄▂▂▅▅▂▆▆▂▆▆▃▇▇▃▃▇▃▃█▃</td></tr><tr><td>val_acc</td><td>▁▂▁▁▂▁▄▅▅██▆▆▇▆▇█▆▅▆▇▇▇▇▅▇▇▇▆▇▇█▇▇▇▅▆▆▆▆</td></tr><tr><td>val_auc</td><td>▁▁▁▁▂▁▄▅▅██▆▆▇▅▇█▆▅▆▇▇▇▇▅▇▇▇▆█▇█▇▇▇▅▆▆▆▆</td></tr><tr><td>val_f1</td><td>▁▂▁▂▃▁▅▆▆██▇▇▇▆▇█▇▆▇▇▇█▇▆▇▇█▇█▇█▇▇▇▆▇▇▇▆</td></tr><tr><td>val_loss_epoch</td><td>█▇▆▆▅▅▄▄▃▂▃▂▂▂▂▂▁▂▂▂▂▂▁▂▂▂▂▂▂▁▁▁▂▁▂▃▂▂▂▂</td></tr><tr><td>val_loss_step</td><td>██▇▇▇▅▄▆▄▄▅▃▃▃▄▂▃▃▃▄▃▄▂▃▃▃▃▃▂▃▁▃▃▁▃▄▄▃▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.73276</td></tr><tr><td>train_auc</td><td>0.68881</td></tr><tr><td>train_f1</td><td>0.58866</td></tr><tr><td>train_loss_epoch</td><td>0.52356</td></tr><tr><td>train_loss_step</td><td>0.5248</td></tr><tr><td>trainer/global_step</td><td>999</td></tr><tr><td>val_acc</td><td>0.74608</td></tr><tr><td>val_auc</td><td>0.69372</td></tr><tr><td>val_f1</td><td>0.58031</td></tr><tr><td>val_loss_epoch</td><td>0.47985</td></tr><tr><td>val_loss_step</td><td>0.42665</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">3D_MLP_2_16_onehot_1</strong> at: <a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/274pa2ul' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold/runs/274pa2ul</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230511_064123-274pa2ul\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fddfcdabfc6415a833939b1b760df3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Thomas\\15_PLA\\notebooks\\wandb\\run-20230511_072949-8ipxfdu8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/8ipxfdu8' target=\"_blank\">3D_MLP_2_16_onehot_1</a></strong> to <a href='https://wandb.ai/thoomas/PLA_042923_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_042923_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/8ipxfdu8' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold/runs/8ipxfdu8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 368   \n",
      "2  | head        | Sequential       | 154   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 17    \n",
      "--------------------------------------------------\n",
      "539       Trainable params\n",
      "0         Non-trainable params\n",
      "539       Total params\n",
      "0.002     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46aae4b4cfc1462eba7291b7f2a0beab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▅▆▆▇▇█▇▇▇▇▇█▇▇███▇█████▇█▇▇█▇███▇██████</td></tr><tr><td>train_auc</td><td>▁▁▂▂▄▅▇▆▆▇▆▆▇▆▆▇▇▇▆▇▇█▇█▇▇▇▇█▆▇▇▇▇▇█▇▇█▇</td></tr><tr><td>train_f1</td><td>▆▂▁▁▅▆▇▆▇▇▇▇▇▇▇▇█▇▆▇██▇█▇▇▇▇█▇█▇▇▇▇█▇▇██</td></tr><tr><td>train_loss_epoch</td><td>█▇▆▆▄▄▃▄▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▂▂▂▁▂▂▁▂▂▂▂</td></tr><tr><td>train_loss_step</td><td>▇▆▇█▅▆▆▇▄▄▆▃▃▃▆▄▂▅▄▄▆▂▃▃▄▄▄▆▁▃▄▄▁▅▃▄▄▃▅▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▁▂▂▁▃▃▂▃▃▂▄▄▂▂▄▂▂▅▅▂▆▆▂▆▆▃▇▇▃▃▇▃▃█▃</td></tr><tr><td>val_acc</td><td>▁▁▁▁▇▇▇██▇▅▅▇▇▇▇▄▇█▆▆▄▄▆▅▇▅▇▄▆▅▄▅▆▇▅▅▆▃▃</td></tr><tr><td>val_auc</td><td>▁▁▁▁▇▇▇██▇▅▅▆▇▇▆▄▆▇▆▅▃▄▅▅▇▄▆▄▆▅▄▅▆▇▅▅▆▃▃</td></tr><tr><td>val_f1</td><td>▁▁▁▁▇▇▇██▇▆▆▇▇▇▇▅▇█▇▆▅▅▆▆▇▅▇▅▇▆▅▆▇▇▆▆▇▄▄</td></tr><tr><td>val_loss_epoch</td><td>███▇▅▄▂▃▂▁▂▃▂▂▁▂▂▁▁▂▂▃▃▂▃▁▂▂▃▂▃▂▃▂▂▁▂▂▄▃</td></tr><tr><td>val_loss_step</td><td>▇▇▇█▆▄▄▃▄▁▃▅▂▂▂▂▄▂▃▄▅▂▅▂▂▃▆▃▄▁▁▅▂▄▂▂▅▂▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.71708</td></tr><tr><td>train_auc</td><td>0.65823</td></tr><tr><td>train_f1</td><td>0.53299</td></tr><tr><td>train_loss_epoch</td><td>0.54395</td></tr><tr><td>train_loss_step</td><td>0.52046</td></tr><tr><td>trainer/global_step</td><td>999</td></tr><tr><td>val_acc</td><td>0.66144</td></tr><tr><td>val_auc</td><td>0.58796</td></tr><tr><td>val_f1</td><td>0.35714</td></tr><tr><td>val_loss_epoch</td><td>0.54558</td></tr><tr><td>val_loss_step</td><td>0.52444</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">3D_MLP_2_16_onehot_1</strong> at: <a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/8ipxfdu8' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold/runs/8ipxfdu8</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230511_072949-8ipxfdu8\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81f1d5414b654816a62d013671ce933d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Thomas\\15_PLA\\notebooks\\wandb\\run-20230511_081948-0whiv8b2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/0whiv8b2' target=\"_blank\">3D_MLP_2_16_onehot_1</a></strong> to <a href='https://wandb.ai/thoomas/PLA_042923_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_042923_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/0whiv8b2' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold/runs/0whiv8b2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 368   \n",
      "2  | head        | Sequential       | 154   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 281   \n",
      "--------------------------------------------------\n",
      "803       Trainable params\n",
      "0         Non-trainable params\n",
      "803       Total params\n",
      "0.003     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb0d139b2ae64274ab07bfe6798011e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▅▅▅▆▇▇▇▇▇▇▇█▇█▇▇▇▇▇▇▇▇▇▇▇▇██▇▇▇█▇▇▇▇█▇█</td></tr><tr><td>train_auc</td><td>▁▁▂▂▃█▆█▇▆▆▇▇▇█▇▇▆▇▇▇▇▇▇▇▇▇▇█▇▇▇█▇▇▇▇██▇</td></tr><tr><td>train_f1</td><td>▆▂▁▁▄█▇█▇▇▇▇▇▇█▇▇▇▇▇▇▇▇▇▇▇███▇▇▇█▇▇▇████</td></tr><tr><td>train_loss_epoch</td><td>█▇▆▆▄▃▃▃▂▂▂▂▁▂▂▂▁▂▃▂▁▂▂▂▂▂▂▁▁▁▂▂▁▂▂▂▁▁▂▁</td></tr><tr><td>train_loss_step</td><td>▆▇█▆▄▃▃▅▂▂▄▃▂▃▄▁▂▂▃▄▄▁▄▃▄▄▂▄▅▄▁▃▁▃▃▅▃▂▂▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▁▂▂▁▃▃▂▃▃▂▄▄▂▂▄▂▂▅▅▂▆▆▂▆▆▃▇▇▃▃▇▃▃█▃</td></tr><tr><td>val_acc</td><td>▁▁▁▁▇▆▇▆▇█▇█▇▇▇▆▆▇▆▇▇▇▇▇▇▇▇▇▇▇█▆█▆█▇▇▆▆▇</td></tr><tr><td>val_auc</td><td>▁▁▁▁▇▆▇▆▇███▆▇▇▆▆▇▆▇▇▆▇▇▆▇▇▇▇▆█▆▇▆▇▇▇▆▆▇</td></tr><tr><td>val_f1</td><td>▁▁▁▁▇▇▇▇▇███▇▇▇▇▆▇▇▇▇▇▇█▇█▇▇▇▇█▇█▇█▇▇▇▇█</td></tr><tr><td>val_loss_epoch</td><td>█▇█▇▅▂▂▂▂▂▂▁▂▂▂▃▂▂▃▃▂▂▂▃▂▂▂▂▂▂▁▂▁▂▂▁▂▃▁▁</td></tr><tr><td>val_loss_step</td><td>███▇▆▅▅▄▃▄▅▁▄▄▄▃▄▄▄▃▄▃▄▄▅▅▅▄▆▄▄▄▃▄▄▄▅▄▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.72649</td></tr><tr><td>train_auc</td><td>0.66555</td></tr><tr><td>train_f1</td><td>0.54139</td></tr><tr><td>train_loss_epoch</td><td>0.53206</td></tr><tr><td>train_loss_step</td><td>0.48629</td></tr><tr><td>trainer/global_step</td><td>999</td></tr><tr><td>val_acc</td><td>0.76176</td></tr><tr><td>val_auc</td><td>0.73147</td></tr><tr><td>val_f1</td><td>0.66071</td></tr><tr><td>val_loss_epoch</td><td>0.4962</td></tr><tr><td>val_loss_step</td><td>0.47912</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">3D_MLP_2_16_onehot_1</strong> at: <a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/0whiv8b2' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold/runs/0whiv8b2</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230511_081948-0whiv8b2\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8379a063a1cb43208e3d7443f839eb73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Thomas\\15_PLA\\notebooks\\wandb\\run-20230511_090944-rsmi8uab</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/rsmi8uab' target=\"_blank\">3D_MLP_2_16_onehot_2</a></strong> to <a href='https://wandb.ai/thoomas/PLA_042923_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_042923_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/rsmi8uab' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold/runs/rsmi8uab</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 368   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "522       Trainable params\n",
      "0         Non-trainable params\n",
      "522       Total params\n",
      "0.002     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d1f89701ad540a8b98d42f7faa83108",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▅▅▅▆██▇▇██▇▇▇█▇▇████▇▇██████████▇████▇█</td></tr><tr><td>train_auc</td><td>▁▂▁▁▅█▇▇▇▇█▇▇▇▇▇▇██▇█▇▇▇▇▇▇▇▇▇▇▇█▇▇▇▇▇▇█</td></tr><tr><td>train_f1</td><td>▆▂▁▁▆██▇▇▇██▇▇▇▇▇██▇█▇▇▇███▇█████▇▇▇██▇█</td></tr><tr><td>train_loss_epoch</td><td>█▇▆▆▃▂▂▂▃▂▂▂▁▂▂▂▂▂▂▁▁▂▂▂▂▁▁▁▁▁▂▂▁▂▁▁▂▁▂▂</td></tr><tr><td>train_loss_step</td><td>▇▇▆▆▄▄▂▆▆▃▄▃▇▂▂▂▄▅▁▄▃█▄▅▂▂▃▅▅▂▇▃▃▂▃▃▄▅▆▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▁▂▂▁▃▃▂▃▃▂▄▄▂▂▄▂▂▅▅▂▆▆▂▆▆▃▇▇▃▃▇▃▃█▃</td></tr><tr><td>val_acc</td><td>▁▁▁▁▄▇▄▄▆▄▄▃▄▄▄▄█▄▄▅▄▅▃▃▅▃▃▃▃▄▄▄▄▅▄▄▄▅▄▄</td></tr><tr><td>val_auc</td><td>▁▁▁▁▅█▆▆▅▆▆▅▆▆▆▆█▆▆▆▆▆▆▅▆▆▆▅▆▆▆▆▆▆▆▆▆▆▆▆</td></tr><tr><td>val_f1</td><td>▁▁▁▁▆█▇▇▆▇▇▇▇▇▇▇█▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>val_loss_epoch</td><td>█▇▇▆▃▃▂▂▃▂▃▂▂▂▂▂▃▂▂▂▂▂▂▁▁▂▂▂▁▂▂▂▁▂▂▂▂▂▂▂</td></tr><tr><td>val_loss_step</td><td>███▆▆▄▅▄▄▅▄▄▄▅▅▄▅▄▃▄▄▄▆▅▅▄▅▅▁▅▄▄▅▄▄▅▄▄▄▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.72962</td></tr><tr><td>train_auc</td><td>0.6874</td></tr><tr><td>train_f1</td><td>0.58977</td></tr><tr><td>train_loss_epoch</td><td>0.55101</td></tr><tr><td>train_loss_step</td><td>0.53172</td></tr><tr><td>trainer/global_step</td><td>999</td></tr><tr><td>val_acc</td><td>0.71473</td></tr><tr><td>val_auc</td><td>0.66634</td></tr><tr><td>val_f1</td><td>0.53807</td></tr><tr><td>val_loss_epoch</td><td>0.50329</td></tr><tr><td>val_loss_step</td><td>0.46371</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">3D_MLP_2_16_onehot_2</strong> at: <a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/rsmi8uab' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold/runs/rsmi8uab</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230511_090944-rsmi8uab\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87da28a7d5324a7bad874344fa391195",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Thomas\\15_PLA\\notebooks\\wandb\\run-20230511_100105-dl63su83</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/dl63su83' target=\"_blank\">3D_MLP_2_16_onehot_2</a></strong> to <a href='https://wandb.ai/thoomas/PLA_042923_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_042923_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/dl63su83' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold/runs/dl63su83</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 368   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "522       Trainable params\n",
      "0         Non-trainable params\n",
      "522       Total params\n",
      "0.002     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▇▇▇▇▇▇▇▇▇███▇▇▇▇███▇▇██▇██████▇▇▇██████</td></tr><tr><td>train_auc</td><td>▁▄▁▂▂▂▂▂▂▂▃▄▅▃▅▄▃█▆█▃▄▅▅▄▇▄▇▇▆▆▅▅▆▆█▆█▇▅</td></tr><tr><td>train_f1</td><td>█▄▂▁▁▁▁▁▁▁▂▃▃▂▃▄▂▅▄▅▃▃▃▃▃▄▂▅▄▄▄▃▄▄▄▅▄▅▅▃</td></tr><tr><td>train_loss_epoch</td><td>█▄▃▃▃▃▃▃▃▂▂▂▂▂▂▃▂▂▂▂▂▃▂▂▂▂▁▁▂▂▂▁▂▂▁▂▂▂▂▂</td></tr><tr><td>train_loss_step</td><td>▅▅▄▃▄▅▃▅▆▄▄▃▇▁▂▃▃▃▄▄▂█▅▅▄▃▃▄▄▄▃▂▄▃▄▁▁▄█▄</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▁▂▂▁▃▃▂▃▃▂▄▄▂▂▄▂▂▅▅▂▆▆▂▆▆▃▇▇▃▃▇▃▃█▃</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>▁▁▁▁▁▁▁▁▁▁▁▅█▁▁▁█▅▅▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁██▁▁▁███▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_epoch</td><td>▆▆▄▁▂▁▁▂▁▃▇█▇▇▆▆█▇█▅▅▄▅▃▃▄▄▇▆▆▅▆▅▄▄▅▄▃▅▅</td></tr><tr><td>val_loss_step</td><td>▇▇▅▁▆▃▂▅▅▆▇█▇▇█▇█▇▇▆▆▅▇▅▆▆▆▇▇▇▆▇▇▄▅▆▆▆▅▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.63009</td></tr><tr><td>train_auc</td><td>0.52138</td></tr><tr><td>train_f1</td><td>0.12268</td></tr><tr><td>train_loss_epoch</td><td>0.64784</td></tr><tr><td>train_loss_step</td><td>0.65166</td></tr><tr><td>trainer/global_step</td><td>999</td></tr><tr><td>val_acc</td><td>0.69279</td></tr><tr><td>val_auc</td><td>0.5</td></tr><tr><td>val_f1</td><td>0.0</td></tr><tr><td>val_loss_epoch</td><td>0.671</td></tr><tr><td>val_loss_step</td><td>0.66268</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">3D_MLP_2_16_onehot_2</strong> at: <a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/dl63su83' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold/runs/dl63su83</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230511_100105-dl63su83\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3288e1eacfc41ae9a07c1431ea4b4d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016933333330477276, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Thomas\\15_PLA\\notebooks\\wandb\\run-20230511_111018-ppwoaluk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/ppwoaluk' target=\"_blank\">3D_MLP_2_16_onehot_2</a></strong> to <a href='https://wandb.ai/thoomas/PLA_042923_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_042923_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/ppwoaluk' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold/runs/ppwoaluk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 368   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "522       Trainable params\n",
      "0         Non-trainable params\n",
      "522       Total params\n",
      "0.002     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd56236d9675441ca6e8c44a6b6ee0c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▃▄▅▆▆▆▇▇▆▇▇▇▇▇▇▇███▇▇▇▇█████▇▇██▇███▇█</td></tr><tr><td>train_auc</td><td>▁▂▂▂▃▄▅▅▆▆▅▇▇▆▇▇▇▇▇██▆▇▆▇▇██▇█▇▇▇▇▇▇▇▇▇█</td></tr><tr><td>train_f1</td><td>▄▄▂▁▂▄▄▄▆▆▅▇▇▆▇▇▇▇▇▇▇▆▇▆▇▇██▇█▇▇▇▇▇▇▇▇▇█</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▄▃▃▃▂▂▃▂▂▂▂▃▁▁▁▂▂▁▂▂▃▂▂▁▁▃▂▂▂▃▂▁▁▂▁▂▂▃▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▁▂▂▁▃▃▂▃▃▂▄▄▂▂▄▂▂▅▅▂▆▆▂▆▆▃▇▇▃▃▇▃▃█▃</td></tr><tr><td>val_acc</td><td>▁▂▂▃▃▄█▆▇▇█▆▅▇▆▆▇▆▅▅▇▆▇▇▆▇▇▆▅▇▇▆▆▆▆▇▆▅▆▆</td></tr><tr><td>val_auc</td><td>▁▂▂▂▃▄▇▄▆▇▇▅▆▇▆▆▇▆▆▆▆▆█▇▆██▇▆█▇▇▇▆▇█▆▆▆▆</td></tr><tr><td>val_f1</td><td>▁▂▃▃▃▅▇▅▇██▆▇█▇▇▇▇▇▇▇▇█▇▇██▇▇██▇▇▇▇█▇▇▇▇</td></tr><tr><td>val_loss_epoch</td><td>███▇▆▅▅▄▄▃▃▂▂▃▂▂▂▂▂▂▂▂▂▂▁▂▂▁▁▂▁▂▁▁▁▂▂▂▁▁</td></tr><tr><td>val_loss_step</td><td>███▆▇▆▆▅▃▅▄▄▃▅▅▃▄▃▃▃▄▄▅▅▄▄▃▄▁▄▄▃▄▂▃▃▃▃▃▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.73276</td></tr><tr><td>train_auc</td><td>0.69631</td></tr><tr><td>train_f1</td><td>0.60759</td></tr><tr><td>train_loss_epoch</td><td>0.53383</td></tr><tr><td>train_loss_step</td><td>0.5321</td></tr><tr><td>trainer/global_step</td><td>999</td></tr><tr><td>val_acc</td><td>0.75235</td></tr><tr><td>val_auc</td><td>0.67645</td></tr><tr><td>val_f1</td><td>0.54335</td></tr><tr><td>val_loss_epoch</td><td>0.46043</td></tr><tr><td>val_loss_step</td><td>0.40671</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">3D_MLP_2_16_onehot_2</strong> at: <a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/ppwoaluk' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold/runs/ppwoaluk</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230511_111018-ppwoaluk\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "676b274ca8d84229943e071c74e0a23c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Thomas\\15_PLA\\notebooks\\wandb\\run-20230511_131034-qwhctwi5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/qwhctwi5' target=\"_blank\">3D_MLP_2_16_onehot_2</a></strong> to <a href='https://wandb.ai/thoomas/PLA_042923_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_042923_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/qwhctwi5' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold/runs/qwhctwi5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 368   \n",
      "2  | head        | Sequential       | 154   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 17    \n",
      "--------------------------------------------------\n",
      "539       Trainable params\n",
      "0         Non-trainable params\n",
      "539       Total params\n",
      "0.002     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79f23dacf7d645f0b7df9f7d1fe3b007",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▄▅▅▆▇▇▇██▇▇█▇█▇██▇██████▇███████▇██▇███</td></tr><tr><td>train_auc</td><td>▁▁▁▁▅▆▇▇▇▇▇▇▇▅▇▇█▇▇▇▇██▇▇▇███▇█▇█▇██▇███</td></tr><tr><td>train_f1</td><td>▆▂▁▁▆▇▇▇▇█▇▇█▆▇▇█▇▇▇█████▇███████▇██▇███</td></tr><tr><td>train_loss_epoch</td><td>█▇▆▆▄▃▂▂▂▂▂▂▂▂▁▂▂▁▂▁▁▁▁▁▁▁▂▁▂▂▂▁▂▂▂▁▂▁▂▁</td></tr><tr><td>train_loss_step</td><td>█▇███▆▅▆▅▄▄▃▆▇▄▅▄▇▅▆▂▁▆▆▃▅▅▂▄▅▃▂▄▆▄▅▅▄▆▅</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▁▂▂▁▃▃▂▃▃▂▄▄▂▂▄▂▂▅▅▂▆▆▂▆▆▃▇▇▃▃▇▃▃█▃</td></tr><tr><td>val_acc</td><td>▁▁▁▁█▅▄▄▅▃▄▄▄▄▅▃▃▄▅▄▄▄▃▃▄▃▄▄▅▃▄▄▄▃█▄▄▅▆▄</td></tr><tr><td>val_auc</td><td>▁▁▁▁█▇▆▆▇▅▅▅▅▅▆▅▅▆▆▅▄▄▅▅▆▅▅▅▅▅▅▄▅▅█▄▅▅▄▅</td></tr><tr><td>val_f1</td><td>▁▁▁▁█▇▇▇▇▇▇▆▆▇▇▇▆▇▇▇▆▆▆▇▇▇▇▆▆▆▆▆▆▇█▆▇▆▆▆</td></tr><tr><td>val_loss_epoch</td><td>███▇▆▅▃▃▄▂▂▂▂▃▃▂▂▃▃▂▁▂▂▂▁▂▂▂▂▁▂▁▁▁▂▁▂▁▂▁</td></tr><tr><td>val_loss_step</td><td>▇███▆▆▃▅▅▃▅▄▄▄▄▅▃▅▃▃▄▄▅▄▄▆▆▄▅▄▁▅▄▁▄▂▄▄▆▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.72649</td></tr><tr><td>train_auc</td><td>0.69125</td></tr><tr><td>train_f1</td><td>0.60205</td></tr><tr><td>train_loss_epoch</td><td>0.54212</td></tr><tr><td>train_loss_step</td><td>0.54992</td></tr><tr><td>trainer/global_step</td><td>999</td></tr><tr><td>val_acc</td><td>0.71787</td></tr><tr><td>val_auc</td><td>0.62884</td></tr><tr><td>val_f1</td><td>0.46429</td></tr><tr><td>val_loss_epoch</td><td>0.48685</td></tr><tr><td>val_loss_step</td><td>0.48435</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">3D_MLP_2_16_onehot_2</strong> at: <a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/qwhctwi5' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold/runs/qwhctwi5</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230511_131034-qwhctwi5\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bfa276b5f8b4b3fbb4e6f06bee13091",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01693333333435779, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Thomas\\15_PLA\\notebooks\\wandb\\run-20230511_141849-70bljl3l</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/70bljl3l' target=\"_blank\">3D_MLP_2_16_onehot_2</a></strong> to <a href='https://wandb.ai/thoomas/PLA_042923_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_042923_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/70bljl3l' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold/runs/70bljl3l</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 368   \n",
      "2  | head        | Sequential       | 154   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 281   \n",
      "--------------------------------------------------\n",
      "803       Trainable params\n",
      "0         Non-trainable params\n",
      "803       Total params\n",
      "0.003     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "291b68fa0d164cb3939215f5108ce0b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▄▅▅▇▇▇▇▇▇▇█▇███▇█▇▇▇███▇▇███▇█▇████▇▇▇▇</td></tr><tr><td>train_auc</td><td>▁▁▁▁▆█▇▇▇▇▇▇▇███▇▇▇▇▆▇▇█▇▇▇▇█▆▇▆▇▇▇▇▇▇▇▇</td></tr><tr><td>train_f1</td><td>▆▂▁▁▇█▇▇▇▇▇█▇███▇▇▇▇▇████▇███▇▇▇███▇▇█▇▇</td></tr><tr><td>train_loss_epoch</td><td>█▇▇▆▃▂▂▃▂▂▃▁▂▁▂▂▁▂▂▂▂▂▁▂▂▂▂▂▁▂▂▂▂▂▂▂▂▂▂▂</td></tr><tr><td>train_loss_step</td><td>▇██▆▅▃▅▃▃▄▅▁▄▄▄▁▂▅▄▃▄▁▃▅▄▄▂▅▃▃▃▂▃▄▄▅▂▂▂▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▁▂▂▁▃▃▂▃▃▂▄▄▂▂▄▂▂▅▅▂▆▆▂▆▆▃▇▇▃▃▇▃▃█▃</td></tr><tr><td>val_acc</td><td>▁▁▁▁█▆▄▄▅█▄▇▄▄▄▃▃▄▅▇▄▃▄▃▃▅▄▄▄▄▆▇▅▄▅▄▄▃▃▆</td></tr><tr><td>val_auc</td><td>▁▁▁▁█▅▆▆▆█▆█▆▆▆▆▆▆▆▇▆▅▆▅▆▇▆▆▆▆▇█▇▆▇▆▆▆▆▇</td></tr><tr><td>val_f1</td><td>▁▁▁▁█▆▇▇▇█▇█▇▇▇▇▇▇▇█▇▇▇▇▇▇▇▇▇▇██▇▇▇▇▇▇▇█</td></tr><tr><td>val_loss_epoch</td><td>█▇▇▇▄▁▂▂▂▂▂▂▁▁▁▂▁▁▁▂▂▁▂▁▁▁▂▁▂▁▂▂▁▁▁▁▂▁▁▂</td></tr><tr><td>val_loss_step</td><td>███▇▅▄▄▄▁▅▅▅▅▄▃▁▅▂▃▂▄▃▅▄▅▁▄▄▇▆▄▅▄▅▄▄▄▄▄▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.7116</td></tr><tr><td>train_auc</td><td>0.66289</td></tr><tr><td>train_f1</td><td>0.54791</td></tr><tr><td>train_loss_epoch</td><td>0.55076</td></tr><tr><td>train_loss_step</td><td>0.5334</td></tr><tr><td>trainer/global_step</td><td>999</td></tr><tr><td>val_acc</td><td>0.72727</td></tr><tr><td>val_auc</td><td>0.68958</td></tr><tr><td>val_f1</td><td>0.57143</td></tr><tr><td>val_loss_epoch</td><td>0.52197</td></tr><tr><td>val_loss_step</td><td>0.55953</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">3D_MLP_2_16_onehot_2</strong> at: <a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/70bljl3l' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold/runs/70bljl3l</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230511_141849-70bljl3l\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e54ee4a1f3c3423cb23e298af77d5c66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Thomas\\15_PLA\\notebooks\\wandb\\run-20230511_150451-lcaunm28</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/lcaunm28' target=\"_blank\">3D_MLP_2_16_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_042923_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_042923_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/lcaunm28' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold/runs/lcaunm28</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 368   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "522       Trainable params\n",
      "0         Non-trainable params\n",
      "522       Total params\n",
      "0.002     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efc7993ea8594719965d6cc670b79ccb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▅▅▅▇▇██▇▇▇▇█▇██████▇▇██▇▇▇█▇▇███▇█▇████</td></tr><tr><td>train_auc</td><td>▁▂▁▁▅▇█▇▇▇▇█▇▇█▇▇██▇▇▇██▇▇▇█▇▇█▇█▇█▇█▇▇█</td></tr><tr><td>train_f1</td><td>▆▂▁▁▆▇██▇▇▇█▇▇██████▇▇███▇▇█▇▇█▇████████</td></tr><tr><td>train_loss_epoch</td><td>█▆▆▆▃▃▂▂▃▂▁▁▁▂▁▂▁▁▁▁▁▂▁▁▂▁▁▂▁▂▁▂▁▂▂▁▁▂▁▁</td></tr><tr><td>train_loss_step</td><td>██▆▅▄▅▃▃▃▄▃▂▃▂▅▂▅▄▃▆▂▅▅▂▇▄▃▃▁▅▄▃▃▂▂▂▁▃▅▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▁▂▂▁▃▃▂▃▃▂▄▄▂▂▄▂▂▅▅▂▆▆▂▆▆▃▇▇▃▃▇▃▃█▃</td></tr><tr><td>val_acc</td><td>▁▁▁▁▅▇▆▆▆█▇█▅█▇▆▇▇▇▇▆▅█▆▅▇▆▇██▇█▆▇▇▇▇▇▇▇</td></tr><tr><td>val_auc</td><td>▁▁▁▁▅▇▆▆▆█▇█▅█▇▆█▇▇▇▆▆█▆▆▇▆▇██▇█▆▇▇▇▇▇▇▇</td></tr><tr><td>val_f1</td><td>▁▁▁▁▆█▇▇▇███▆██▇███▇▇▆█▇▇▇▇███▇█▇▇█▇█▇██</td></tr><tr><td>val_loss_epoch</td><td>███▆▂▂▁▃▂▂▂▁▂▁▂▂▂▁▁▂▂▂▂▂▂▁▂▃▁▂▂▂▂▁▁▂▂▂▂▂</td></tr><tr><td>val_loss_step</td><td>▇█▇▅▅▂▁▄▅▃▄▃▃▆▄▃▅▁▃▄▄▅▆▄▃▂▃▄▃▃▁▄▅▂▆▅▅▆▂▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.72257</td></tr><tr><td>train_auc</td><td>0.67708</td></tr><tr><td>train_f1</td><td>0.57246</td></tr><tr><td>train_loss_epoch</td><td>0.54245</td></tr><tr><td>train_loss_step</td><td>0.50021</td></tr><tr><td>trainer/global_step</td><td>999</td></tr><tr><td>val_acc</td><td>0.74922</td></tr><tr><td>val_auc</td><td>0.6939</td></tr><tr><td>val_f1</td><td>0.58763</td></tr><tr><td>val_loss_epoch</td><td>0.51339</td></tr><tr><td>val_loss_step</td><td>0.49954</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">3D_MLP_2_16_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/lcaunm28' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold/runs/lcaunm28</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230511_150451-lcaunm28\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d565f47baa24a4fb450231fd5e637b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01693333333435779, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Thomas\\15_PLA\\notebooks\\wandb\\run-20230511_154907-ycpjq3di</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/ycpjq3di' target=\"_blank\">3D_MLP_2_16_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_042923_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_042923_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/ycpjq3di' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold/runs/ycpjq3di</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 368   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "522       Trainable params\n",
      "0         Non-trainable params\n",
      "522       Total params\n",
      "0.002     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7c39db490184f56bcd2a7ca62734ff4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▇▇▇▇▇▇▇▇▇▇█▇▇▇█▇▇▇▇▇▇██▇████▇▇▇▇▇▇▇▇▇█▇</td></tr><tr><td>train_auc</td><td>▂▃▁▂▂▂▂▂▂▁▂▃▂▂▂▅▃▄▃▄▄▄▅▆▃▅▅▆█▄▂▃▄▅▄▄▄▄▇▃</td></tr><tr><td>train_f1</td><td>█▄▂▁▁▁▁▁▁▁▁▁▁▁▃▄▂▃▂▃▃▃▃▃▂▃▃▃▅▃▂▂▃▄▃▃▃▃▄▃</td></tr><tr><td>train_loss_epoch</td><td>█▃▃▃▃▃▂▂▂▂▂▂▂▁▁▂▂▁▂▂▂▂▁▂▂▁▁▁▁▂▂▂▁▂▁▁▁▂▁▁</td></tr><tr><td>train_loss_step</td><td>▅▅▃▃▃▄▃▄▅▅▅▂▅▁▃▃▄▁▄▄▄█▅▄▅▃▄▂▃▄▃▁▃▂▅▃▂▂▆▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▁▂▂▁▃▃▂▃▃▂▄▄▂▂▄▂▂▅▅▂▆▆▂▆▆▃▇▇▃▃▇▃▃█▃</td></tr><tr><td>val_acc</td><td>███████████████████▆██████▁▁▁▁█▁▁██▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>███████████████████▁█████▇██████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▃▁▁▁▁▁▁████▁██▁▁█████</td></tr><tr><td>val_loss_epoch</td><td>▄▄▃▁▂▁▁▂▂▂▃▃▃▄▅▄▄▅▅▆▅▅▅▄▄▆▇███▆▆▆▅▆▇▇█▇▇</td></tr><tr><td>val_loss_step</td><td>▅▅▃▁▅▂▁▅▄▅▅▄▄▅▆▅▅▆▆▆▆▆▆▆▅▆▇▇█▇▆▇▇▆▆▇▇▇▇▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.63009</td></tr><tr><td>train_auc</td><td>0.50975</td></tr><tr><td>train_f1</td><td>0.09924</td></tr><tr><td>train_loss_epoch</td><td>0.63973</td></tr><tr><td>train_loss_step</td><td>0.64054</td></tr><tr><td>trainer/global_step</td><td>999</td></tr><tr><td>val_acc</td><td>0.35737</td></tr><tr><td>val_auc</td><td>0.5</td></tr><tr><td>val_f1</td><td>0.52656</td></tr><tr><td>val_loss_epoch</td><td>0.69897</td></tr><tr><td>val_loss_step</td><td>0.702</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">3D_MLP_2_16_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/ycpjq3di' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold/runs/ycpjq3di</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230511_154907-ycpjq3di\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "653850b9574a4e43a906349dfcb61be7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01693333333435779, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Thomas\\15_PLA\\notebooks\\wandb\\run-20230511_163453-spug8crc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/spug8crc' target=\"_blank\">3D_MLP_2_16_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_042923_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_042923_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/spug8crc' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold/runs/spug8crc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 368   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "522       Trainable params\n",
      "0         Non-trainable params\n",
      "522       Total params\n",
      "0.002     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe6a83ac721a42ab8e87c65ee9420583",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▃▄▅▅▆▅▆▆▆▇▇▇█▇▇██▇█▇██▇▇▇▇██████▇▇█████</td></tr><tr><td>train_auc</td><td>▁▂▂▃▃▄▄▅▅▅▅▇▆▇▇▇█▇▇█▆▇█▇▇▇▆▇▇███▇▆▇███▇█</td></tr><tr><td>train_f1</td><td>▄▄▁▁▂▄▃▅▅▅▅▇▆▇▇▇▇▇▇█▆▇█▆▇▇▆▇▇█▇█▇▆▇▇██▇█</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▃▃▃▂▃▂▂▂▃▂▁▂▂▁▁▁▂▂▂▂▂▂▁▂▃▁▂▁▂▂▁▂▁▁▂▁▁▂▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▁▂▂▁▃▃▂▃▃▂▄▄▂▂▄▂▂▅▅▂▆▆▂▆▆▃▇▇▃▃▇▃▃█▃</td></tr><tr><td>val_acc</td><td>▁▂▂▂▃▄▆▅▆▆▆▆▆▇▆▆▇▇▇▆▆▇▇▇▆▇▇▇▇▇▇█▆▆▆▇▆▆▆▆</td></tr><tr><td>val_auc</td><td>▁▂▂▁▂▃▆▄▅▇▅▆▅▇▆▅▇▇▇▆▅▇█▇▅▇▇▇▇▇▇█▆▅▅▇▆▆▆▆</td></tr><tr><td>val_f1</td><td>▁▁▁▁▂▄▆▅▅█▆▇▆▇▆▆█▇▇▇▆▇█▇▆▇▇█▇█▇█▆▆▆▇▇▇▇▇</td></tr><tr><td>val_loss_epoch</td><td>█▇▇▆▆▅▄▄▃▄▂▂▂▁▂▂▁▁▁▂▂▁▂▁▂▁▂▂▁▂▁▂▂▁▁▂▁▂▁▁</td></tr><tr><td>val_loss_step</td><td>██▇▆█▅▅▆▆▅▅▄▃▅▄▃▄▂▄▄▅▅▅▃▃▃▄▄▄▄▁▄▅▂▅▄▃▆▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.73824</td></tr><tr><td>train_auc</td><td>0.69304</td></tr><tr><td>train_f1</td><td>0.59466</td></tr><tr><td>train_loss_epoch</td><td>0.52695</td></tr><tr><td>train_loss_step</td><td>0.50677</td></tr><tr><td>trainer/global_step</td><td>999</td></tr><tr><td>val_acc</td><td>0.73354</td></tr><tr><td>val_auc</td><td>0.66029</td></tr><tr><td>val_f1</td><td>0.51977</td></tr><tr><td>val_loss_epoch</td><td>0.48809</td></tr><tr><td>val_loss_step</td><td>0.43403</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">3D_MLP_2_16_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/spug8crc' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold/runs/spug8crc</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230511_163453-spug8crc\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a098517e82324eb78d2e7f012881fe1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01691666666883975, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Thomas\\15_PLA\\notebooks\\wandb\\run-20230511_180625-6a67q807</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/6a67q807' target=\"_blank\">3D_MLP_2_16_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_042923_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_042923_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/6a67q807' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold/runs/6a67q807</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 368   \n",
      "2  | head        | Sequential       | 154   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 17    \n",
      "--------------------------------------------------\n",
      "539       Trainable params\n",
      "0         Non-trainable params\n",
      "539       Total params\n",
      "0.002     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▄▅▅▇█▇▇▇▇▇▇▇▇▇▇▇▇█████▇███▇███▇███████▇</td></tr><tr><td>train_auc</td><td>▁▁▂▂▄▇▆▇▆▆▆▇▆▆▇▆▇▆▇▇▇▇▇▇▇▇█▇█▇▇▇▇▇█▇█▇█▇</td></tr><tr><td>train_f1</td><td>▆▂▁▁▅▇▇▇▆▇▇▇▇▆▇▇▇▇▇███▇▇▇▇█▇█▇█▇▇▇█▇█▇█▇</td></tr><tr><td>train_loss_epoch</td><td>█▇▆▆▄▃▃▂▂▂▂▂▂▂▂▂▂▁▂▁▂▂▂▂▂▂▁▂▂▂▂▁▁▁▂▂▁▁▁▂</td></tr><tr><td>train_loss_step</td><td>█▇██▇█▅▇▄▄▄▄▄▄▅▅▄▂▃▂▁▂▆▁▃▄▄▅▃▂▅▅▂▄▃▅▄▁▅▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▁▂▂▁▃▃▂▃▃▂▄▄▂▂▄▂▂▅▅▂▆▆▂▆▆▃▇▇▃▃▇▃▃█▃</td></tr><tr><td>val_acc</td><td>▁▁▁▁▇▇▇▆▇▅▇▆▄█▇▇▄▇▆▇▄▄▅▆▆▆▄▇▅▅▄▄▅▇▇▄▄▅▄▄</td></tr><tr><td>val_auc</td><td>▁▁▁▁▇█▇▆▇▅▇▆▄█▇▆▅▇▆▇▄▄▅▆▆▆▄▆▅▅▄▄▅▆█▄▄▅▄▄</td></tr><tr><td>val_f1</td><td>▁▁▁▁▇█▇▇▇▆▇▇▆█▇▇▆█▇▇▅▅▆▇▇▇▅▇▆▆▆▆▆▇█▅▆▆▅▆</td></tr><tr><td>val_loss_epoch</td><td>███▇▅▄▃▂▂▂▂▁▂▂▂▂▂▂▂▁▂▂▃▂▂▂▂▂▁▁▃▁▂▂▂▂▄▂▃▂</td></tr><tr><td>val_loss_step</td><td>▅▆▅▅▄▄▄▃▃▂▃▁▃▂▃▃▃▂▂▂▄▂▅▂▂▂▆▄▁▃▂▅▄▃▂▂█▄▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.7069</td></tr><tr><td>train_auc</td><td>0.65754</td></tr><tr><td>train_f1</td><td>0.54167</td></tr><tr><td>train_loss_epoch</td><td>0.53911</td></tr><tr><td>train_loss_step</td><td>0.49245</td></tr><tr><td>trainer/global_step</td><td>999</td></tr><tr><td>val_acc</td><td>0.70219</td></tr><tr><td>val_auc</td><td>0.61643</td></tr><tr><td>val_f1</td><td>0.43114</td></tr><tr><td>val_loss_epoch</td><td>0.516</td></tr><tr><td>val_loss_step</td><td>0.43855</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">3D_MLP_2_16_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/6a67q807' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold/runs/6a67q807</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230511_180625-6a67q807\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e235cd3112ff46bf89b5380dbfe1da72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Thomas\\15_PLA\\notebooks\\wandb\\run-20230511_191020-xchunts9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/xchunts9' target=\"_blank\">3D_MLP_2_16_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_042923_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_042923_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/xchunts9' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold/runs/xchunts9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 368   \n",
      "2  | head        | Sequential       | 154   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 281   \n",
      "--------------------------------------------------\n",
      "803       Trainable params\n",
      "0         Non-trainable params\n",
      "803       Total params\n",
      "0.003     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abe2393c9888416c9cbd8b5e2cbf76f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▅▅▅▆▇█▇█▇█▇▇███▇▇▇██▇▇▇█████████▇█▇███▇</td></tr><tr><td>train_auc</td><td>▁▁▁▁▄▇█▇█▇▇▇▆▇▇█▆▆▆▇▇▇▇▇▇▇█▇▇▇▇█▇▇▇▆▇▇█▇</td></tr><tr><td>train_f1</td><td>▆▂▁▁▅██▇█▇▇▇▇▇██▇▇▇██▇▇▇█▇███▇▇██▇█▇███▇</td></tr><tr><td>train_loss_epoch</td><td>█▇▆▆▄▃▃▂▂▂▂▃▂▁▁▂▂▂▂▁▁▁▂▁▂▂▁▂▂▁▁▂▂▂▁▂▂▁▁▂</td></tr><tr><td>train_loss_step</td><td>▇▇█▆▅▄▆▄▄▃▄▄▁▄▄▂▃▄▄▄▃▃▃▃▄▅▄▃▄▄▃▂▄▄▅▆▂▆▂▄</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▁▂▂▁▃▃▂▃▃▂▄▄▂▂▄▂▂▅▅▂▆▆▂▆▆▃▇▇▃▃▇▃▃█▃</td></tr><tr><td>val_acc</td><td>▁▁▁▁▇▄▅▅▅▇▇▇▅▇▇▆▅▇▅▇▇▆▆▇▇▇▇▇▇▇█▇▇▆▇▇▇▇▇█</td></tr><tr><td>val_auc</td><td>▁▁▁▁█▄▅▅▅▇▇▇▅▇▇▆▅▇▅▇▇▆▆▇▇▇▇▇▇▇█▇▇▆▇▇▇▇▇█</td></tr><tr><td>val_f1</td><td>▁▁▁▁█▆▆▆▆█▇▇▆██▇▆▇▆▇▇▇▇▇▇▇▇▇▇▇█▇▇▇▇▇█▇▇█</td></tr><tr><td>val_loss_epoch</td><td>█▇█▇▅▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▂▃▁▂▂▂▁▁▁▂▁▁▁▂▂▂</td></tr><tr><td>val_loss_step</td><td>▇▇█▇▅▂▂▃▂▃▅▂▂▂▂▁▃▄▃▃▃▃▂▁▂█▅▃▄▂▅▄▃▂▃▄▂▂▁▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.71003</td></tr><tr><td>train_auc</td><td>0.65198</td></tr><tr><td>train_f1</td><td>0.5232</td></tr><tr><td>train_loss_epoch</td><td>0.55478</td></tr><tr><td>train_loss_step</td><td>0.57188</td></tr><tr><td>trainer/global_step</td><td>999</td></tr><tr><td>val_acc</td><td>0.77116</td></tr><tr><td>val_auc</td><td>0.74213</td></tr><tr><td>val_f1</td><td>0.66667</td></tr><tr><td>val_loss_epoch</td><td>0.52092</td></tr><tr><td>val_loss_step</td><td>0.53182</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">3D_MLP_2_16_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/xchunts9' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold/runs/xchunts9</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230511_191020-xchunts9\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a28fc2321d143daaecc9074f3b9c5df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Thomas\\15_PLA\\notebooks\\wandb\\run-20230511_200210-sdpuyt5q</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/sdpuyt5q' target=\"_blank\">3D_MLP_2_16_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_042923_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_042923_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/sdpuyt5q' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold/runs/sdpuyt5q</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 368   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "522       Trainable params\n",
      "0         Non-trainable params\n",
      "522       Total params\n",
      "0.002     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28b21a78904c41d9ac18cb1be58efb71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▅▅▅▆▇▇▇▇▇▇█▇▇▇▇█▇▇▇▇▇▇█▇█▇▇█▇▇▇▇▇▇▇▇▇▇█</td></tr><tr><td>train_auc</td><td>▁▂▁▂▄▇▇▇▇▇▇█▇▆▇▇█▇▆▇▆▇▇▇▇▇▆▇▇▆▇▇▇▆▆▇▇▇▇▇</td></tr><tr><td>train_f1</td><td>▆▂▁▁▅▇▇▇▇▇██▇▇▇▇██▇▇▇▇▇██▇▇▇▇▇▇▇█▇▇▇█▇▇█</td></tr><tr><td>train_loss_epoch</td><td>█▆▆▆▃▃▂▃▂▂▂▂▂▂▂▂▁▂▂▂▂▂▁▁▂▂▂▂▂▂▂▁▂▂▂▁▂▂▂▁</td></tr><tr><td>train_loss_step</td><td>▇▇▅▅▂▃▂▂▆▃▃▃█▂▃▂▃▄▄▃▃▄▃▄▃▂▄▄▄▃▄▁▆▂▆▃▁▄▄▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▁▂▂▁▃▃▂▃▃▂▄▄▂▂▄▂▂▅▅▂▆▆▂▆▆▃▇▇▃▃▇▃▃█▃</td></tr><tr><td>val_acc</td><td>▁▁▁▁▇▆▇▇▅▇▇▇▆▇▇▇▇▇▇▆▇▇▇▇▆▇▇▇▇▇█▇▇▆▇▇▇▇▇█</td></tr><tr><td>val_auc</td><td>▁▁▁▁▆▆▇▇▅▇▇▇▆▇▇▇▇▇▇▆▆▇▇▇▆▆▇▇▇▇█▇▇▆▇▇▇▇▇█</td></tr><tr><td>val_f1</td><td>▁▁▁▁▇▆▇▇▆▇▇▇▆▇▇▇▇▇▇▆▇▇▇▇▇▇▇▇▇▇█▇▇▇▇▇▇▇▇█</td></tr><tr><td>val_loss_epoch</td><td>███▇▃▂▂▂▃▂▂▂▂▂▁▂▂▂▂▂▂▂▁▂▂▃▁▂▁▂▂▂▂▁▁▃▂▂▂▂</td></tr><tr><td>val_loss_step</td><td>██▇▇▅▄▃▅▅▄▄▅▄▃▁▅▃▄▃▄▃▄▃▃▅▆▃▄▁▅▄▄▄▁▄▃▅▆▄▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.72884</td></tr><tr><td>train_auc</td><td>0.66503</td></tr><tr><td>train_f1</td><td>0.53867</td></tr><tr><td>train_loss_epoch</td><td>0.52435</td></tr><tr><td>train_loss_step</td><td>0.50928</td></tr><tr><td>trainer/global_step</td><td>999</td></tr><tr><td>val_acc</td><td>0.70533</td></tr><tr><td>val_auc</td><td>0.68652</td></tr><tr><td>val_f1</td><td>0.61789</td></tr><tr><td>val_loss_epoch</td><td>0.54841</td></tr><tr><td>val_loss_step</td><td>0.54209</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">3D_MLP_2_16_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/sdpuyt5q' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold/runs/sdpuyt5q</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230511_200210-sdpuyt5q\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "041d97ff1bd54e7f92412d65a3c43363",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01691666666883975, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Thomas\\15_PLA\\notebooks\\wandb\\run-20230511_211347-mcc6c2ax</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/mcc6c2ax' target=\"_blank\">3D_MLP_2_16_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_042923_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_042923_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/mcc6c2ax' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold/runs/mcc6c2ax</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 368   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "522       Trainable params\n",
      "0         Non-trainable params\n",
      "522       Total params\n",
      "0.002     Total estimated model params size (MB)\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afcf71d8c0bb4df5b37ce65459f69305",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▇▇███████████████▇█▇████▇█████████▇████</td></tr><tr><td>train_auc</td><td>▂▄▁▂▂▂▂▂▂▂▂▂▂▃▂▄▄▅▂▄▂▂▄▅▃▃▅▅▆▄▅▃▄▇▄▅▆▅█▄</td></tr><tr><td>train_f1</td><td>█▃▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▃▂▂▂▁▂▂▂▂▃▃▄▃▃▂▃▄▂▃▃▃▄▂</td></tr><tr><td>train_loss_epoch</td><td>█▄▃▃▃▃▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▂▂▁▂▂▂▁▂▁▁</td></tr><tr><td>train_loss_step</td><td>▆▅▃▃▃▅▃▅▆▅▅▃▇▂▅▃▄▂▅▅▄█▅▄▅▃▃▃▄▃▃▁▃▄▇▃▂▃▆▄</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▁▂▂▁▃▃▂▃▃▂▄▄▂▂▄▂▂▅▅▂▆▆▂▆▆▃▇▇▃▃▇▃▃█▃</td></tr><tr><td>val_acc</td><td>████████████████▃▃█▁██████▇████████▇████</td></tr><tr><td>val_auc</td><td>▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▅▅▇▁▆█▆▆▇▆▅▆▆▆▆▆▆▆▆▅▆▆█▆</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁██▁█▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁</td></tr><tr><td>val_loss_epoch</td><td>▄▄▃▂▄▂▁▃▃▂▃▅▄▆▅▆██▇█▆█▇▆▇▅▇▇▅▆▄▅▄▄▅▇▆▇▇▄</td></tr><tr><td>val_loss_step</td><td>▅▆▅▄█▃▁▇▆▆▆▇▆▆▆▆▇▇▇▇▇▇▇▇▇▆▇▆▆▆▆▆█▄▆▇▇▇▆▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.65517</td></tr><tr><td>train_auc</td><td>0.51041</td></tr><tr><td>train_f1</td><td>0.05172</td></tr><tr><td>train_loss_epoch</td><td>0.62643</td></tr><tr><td>train_loss_step</td><td>0.63263</td></tr><tr><td>trainer/global_step</td><td>999</td></tr><tr><td>val_acc</td><td>0.57367</td></tr><tr><td>val_auc</td><td>0.5</td></tr><tr><td>val_f1</td><td>0.0</td></tr><tr><td>val_loss_epoch</td><td>0.68422</td></tr><tr><td>val_loss_step</td><td>0.67383</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">3D_MLP_2_16_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/mcc6c2ax' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold/runs/mcc6c2ax</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230511_211347-mcc6c2ax\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b09a8ab2c21c4e68809d875593bf4363",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01693333333435779, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Thomas\\15_PLA\\notebooks\\wandb\\run-20230511_221914-ru1j014l</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/ru1j014l' target=\"_blank\">3D_MLP_2_16_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_042923_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_042923_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/ru1j014l' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold/runs/ru1j014l</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 368   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "522       Trainable params\n",
      "0         Non-trainable params\n",
      "522       Total params\n",
      "0.002     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▃▄▅▅▅▅▆▇▇▆▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇▇▇▇▇▇▇█</td></tr><tr><td>train_auc</td><td>▁▂▂▂▃▄▄▄▆▅▅▆▅▆▇▇▆▆▇▇▅▆▆▆▇▆▆▆▇▆▇▇▇▆▇▇▇▇▇█</td></tr><tr><td>train_f1</td><td>▄▄▂▁▂▃▄▄▆▅▅▆▆▇▇▇▇▆▇▇▅▆▇▆▇▆▆▇▇▇▇▇▇▆▇▇▇▇▇█</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▃▃▃▂▂▂▂▂▂▂▂▃▁▂▁▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▂▂▂▂▁▂▂▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▁▂▂▁▃▃▂▃▃▂▄▄▂▂▄▂▂▅▅▂▆▆▂▆▆▃▇▇▃▃▇▃▃█▃</td></tr><tr><td>val_acc</td><td>▁▁▂▁▂▂▄▃▅▇▇▅▅▇▆▆▅▇▆▅█▆█▇▅▆▆▆▆▇█▇▆▅▆▆▆▇▆▇</td></tr><tr><td>val_auc</td><td>▁▁▁▁▁▁▄▃▅▇▇▅▄▆▆▅▅▆▆▅█▆█▆▄▆▆▆▆▇█▇▆▅▅▆▅▇▆▇</td></tr><tr><td>val_f1</td><td>▂▂▁▁▁▂▅▄▅▇▇▆▅▇▇▆▆▇▇▆█▇█▇▅▇▆▆▆██▇▆▆▆▆▆▇▇▇</td></tr><tr><td>val_loss_epoch</td><td>█▇▇▇▆▆▄▄▃▃▂▃▃▂▂▂▂▂▂▂▁▂▁▂▃▂▂▂▁▁▁▁▂▂▂▃▃▂▂▁</td></tr><tr><td>val_loss_step</td><td>██▇▇█▆▅▅▆▅▄▅▄▃▂▅▃▄▃▅▂▄▃▃▄▃▂▂▁▄▂▃▃▁▄▃▆▅▃▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.75313</td></tr><tr><td>train_auc</td><td>0.69868</td></tr><tr><td>train_f1</td><td>0.59459</td></tr><tr><td>train_loss_epoch</td><td>0.5084</td></tr><tr><td>train_loss_step</td><td>0.4447</td></tr><tr><td>trainer/global_step</td><td>999</td></tr><tr><td>val_acc</td><td>0.73354</td></tr><tr><td>val_auc</td><td>0.71016</td></tr><tr><td>val_f1</td><td>0.6383</td></tr><tr><td>val_loss_epoch</td><td>0.51504</td></tr><tr><td>val_loss_step</td><td>0.49364</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">3D_MLP_2_16_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/ru1j014l' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold/runs/ru1j014l</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230511_221914-ru1j014l\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d17caf3803149b4ba3d7b8b1dfd4cbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016933333330477276, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Thomas\\15_PLA\\notebooks\\wandb\\run-20230511_231705-d1qs7fib</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/d1qs7fib' target=\"_blank\">3D_MLP_2_16_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_042923_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_042923_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/d1qs7fib' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold/runs/d1qs7fib</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 368   \n",
      "2  | head        | Sequential       | 154   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 17    \n",
      "--------------------------------------------------\n",
      "539       Trainable params\n",
      "0         Non-trainable params\n",
      "539       Total params\n",
      "0.002     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf75bf695551453abb37761a076cdd72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▅▅▅▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇█▇▇█▇▇▇▇▇█▇▇▇██▇█▇█</td></tr><tr><td>train_auc</td><td>▁▁▂▂▄▆▆▆▆▆▆▇▆▆▆▆▇▇▆▇▇█▇▆▇▇▇▇▇▇▇▆▆▆▇▇▇▇▇▇</td></tr><tr><td>train_f1</td><td>▆▂▁▁▅▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇▇▇▇▇▇▇▇█▇▇▇▇█▇▇▇▇</td></tr><tr><td>train_loss_epoch</td><td>█▇▆▅▄▃▂▃▂▂▁▂▂▁▁▂▂▂▂▁▁▁▂▁▁▁▁▁▂▁▁▂▁▂▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▇██▅▅▄▆▃▄▅▅▃▆▄▅▃▄▃▅▃▂▂▂▅▁▄▂▂▃▃▄▁▄▂▂▃▄▄▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▁▂▂▁▃▃▂▃▃▂▄▄▂▂▄▂▂▅▅▂▆▆▂▆▆▃▇▇▃▃▇▃▃█▃</td></tr><tr><td>val_acc</td><td>▁▁▁▁▇█▆▇▇▆▅▆▆█▆▇▅▇▇▆▆▄▅▆▇▅▆▇▅▆▄▅▅▇█▅▆▆▄▅</td></tr><tr><td>val_auc</td><td>▁▁▁▁▇▇▅▇▆▆▄▅▆█▆▆▅▇▇▆▅▄▅▆▇▅▅▆▅▆▄▄▄▇▇▅▅▆▄▅</td></tr><tr><td>val_f1</td><td>▁▁▁▁▇█▆▇▇▆▅▆▇█▇▇▅▇▇▆▆▅▆▆▇▆▆▇▆▇▅▆▆▇█▆▆▇▅▆</td></tr><tr><td>val_loss_epoch</td><td>███▇▄▃▃▃▂▂▃▂▂▂▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▁▃▂▂▁▁▂▃▂▄▂</td></tr><tr><td>val_loss_step</td><td>▇▇██▄▅▅▄▂▂▆▁▃▄▃▂▂▂▃▂▄▃▂▂▄▄▆▃▄▄▁▄▃▁▂▃▆▄▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.73276</td></tr><tr><td>train_auc</td><td>0.66908</td></tr><tr><td>train_f1</td><td>0.54473</td></tr><tr><td>train_loss_epoch</td><td>0.52963</td></tr><tr><td>train_loss_step</td><td>0.4811</td></tr><tr><td>trainer/global_step</td><td>999</td></tr><tr><td>val_acc</td><td>0.66144</td></tr><tr><td>val_auc</td><td>0.62277</td></tr><tr><td>val_f1</td><td>0.47573</td></tr><tr><td>val_loss_epoch</td><td>0.55746</td></tr><tr><td>val_loss_step</td><td>0.53853</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">3D_MLP_2_16_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/d1qs7fib' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold/runs/d1qs7fib</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230511_231705-d1qs7fib\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c96fdb80c0045568abe0d30ffa6697e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Thomas\\15_PLA\\notebooks\\wandb\\run-20230512_001904-mxgq4uje</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/mxgq4uje' target=\"_blank\">3D_MLP_2_16_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_042923_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_042923_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/mxgq4uje' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold/runs/mxgq4uje</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 368   \n",
      "2  | head        | Sequential       | 154   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 281   \n",
      "--------------------------------------------------\n",
      "803       Trainable params\n",
      "0         Non-trainable params\n",
      "803       Total params\n",
      "0.003     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▅▅▅▆▇▇█▇▇▇██████▇███▇▇█▇█▇███████████▇█</td></tr><tr><td>train_auc</td><td>▁▁▁▁▃▇▆█▇▆▇▇▇█▇█▇▇█▇▇▇▇█▇▇▆██▇▇█▇▇▇▇▇▇▇▇</td></tr><tr><td>train_f1</td><td>▆▁▁▁▄█▇█▇▇▇▇▇███▇▇█▇▇▇▇█▇▇▇██▇██▇▇▇▇▇▇▇█</td></tr><tr><td>train_loss_epoch</td><td>█▆▆▆▄▃▃▂▂▂▂▂▂▁▂▁▁▁▁▂▂▁▁▂▂▂▂▂▁▁▁▂▂▂▁▁▂▁▂▂</td></tr><tr><td>train_loss_step</td><td>▆▇█▆▅▇▃▂▆▄▃▃▂█▃▃▄▄▃▂▃▃▃▃▂▄▁▄▂▂▂▃▃▄▄▃▂▁▄▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▁▂▂▁▃▃▂▃▃▂▄▄▂▂▄▂▂▅▅▂▆▆▂▆▆▃▇▇▃▃▇▃▃█▃</td></tr><tr><td>val_acc</td><td>▁▁▁▁█▅▇▄▆▇▇▇▆▆▆▇▆▇▅▆▇▆▇▆▆▇▇▇▇▆▆▆█▇▇▇▇▇▇▆</td></tr><tr><td>val_auc</td><td>▁▁▁▁█▅▇▄▆▇▇▇▆▆▆▇▆▇▅▆▆▆▇▆▆▇▇▆▇▆▆▆█▇▇▇▇▇▇▆</td></tr><tr><td>val_f1</td><td>▁▁▁▁█▆▇▅▇▇▇▇▇▆▇▇▆▇▆▇▇▆▇▇▆▇▇▇▇▇▇▇█▇█▇█▇▇▇</td></tr><tr><td>val_loss_epoch</td><td>█▇█▇▄▂▂▂▂▂▁▂▂▂▂▁▂▁▃▁▂▂▁▂▂▁▂▂▂▁▂▂▂▂▁▂▁▂▂▂</td></tr><tr><td>val_loss_step</td><td>█▇█▆▆▄▅▂▁▄▅▄▄▄▆▂▅▂▃▃▃▂▁▅▄▃▄▆▅▄▆▄▂▄▃▄▃▃▂▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.73276</td></tr><tr><td>train_auc</td><td>0.66753</td></tr><tr><td>train_f1</td><td>0.54105</td></tr><tr><td>train_loss_epoch</td><td>0.53173</td></tr><tr><td>train_loss_step</td><td>0.4851</td></tr><tr><td>trainer/global_step</td><td>999</td></tr><tr><td>val_acc</td><td>0.68652</td></tr><tr><td>val_auc</td><td>0.65501</td></tr><tr><td>val_f1</td><td>0.54545</td></tr><tr><td>val_loss_epoch</td><td>0.5629</td></tr><tr><td>val_loss_step</td><td>0.59122</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">3D_MLP_2_16_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_042923_Kfold/runs/mxgq4uje' target=\"_blank\">https://wandb.ai/thoomas/PLA_042923_Kfold/runs/mxgq4uje</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230512_001904-mxgq4uje\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### MLP\n",
    "\n",
    "k_folds = 5\n",
    "kfold = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "for fold, (train_ids, valid_ids) in enumerate(kfold.split(dataset_filtered)):\n",
    "    train_subset = dataset_filtered.index_select(train_ids.tolist())\n",
    "    val_subset = dataset_filtered.index_select(valid_ids.tolist())\n",
    "    \n",
    "    for pool in pools:\n",
    "        # Path to the folder where the pretrained models are saved\n",
    "        CHECKPOINT_PATH = checkpoint_folder / f'3D_{NUM_LAYERS}_{HIDDEN_CHANNELS}_onehot_{fold}' / pool\n",
    "        CHECKPOINT_PATH.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Skip already trained kfold and pool\n",
    "        checkpoint = CHECKPOINT_PATH / \"GraphLevelMLP\" / \"GraphLevelMLP.ckpt\" \n",
    "        if checkpoint.exists():\n",
    "            print(checkpoint)\n",
    "            continue\n",
    "        \n",
    "        # Run training\n",
    "        run = wandb.init(project=project_name, name=f'3D_MLP_{NUM_LAYERS}_{HIDDEN_CHANNELS}_onehot_{fold}', \n",
    "                        group=f'3D_MLP_{pool}')\n",
    "        PPIGraph.train_graph_classifier_kfold('MLP', \n",
    "                                             train_subset, \n",
    "                                             val_subset, \n",
    "                                             dataset, \n",
    "                                             CHECKPOINT_PATH, \n",
    "                                             AVAIL_GPUS, \n",
    "                                             in_channels=5,\n",
    "                                             hidden_channels=HIDDEN_CHANNELS, \n",
    "                                             out_channels = HIDDEN_CHANNELS,\n",
    "                                             num_layers=NUM_LAYERS, \n",
    "                                             epochs=epochs,\n",
    "                                             batch_size=128,\n",
    "                                             embedding=False,\n",
    "                                             graph_pooling=pool)\n",
    "        run.finish()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:snowflake]",
   "language": "python",
   "name": "conda-env-snowflake-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
