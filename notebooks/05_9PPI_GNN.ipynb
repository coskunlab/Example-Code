{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2e094bf-8350-4dc1-baad-1d2b43506d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import skimage.io\n",
    "\n",
    "from collections import defaultdict\n",
    "from tqdm.auto import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "import re\n",
    "import h5py\n",
    "import napari\n",
    "import tifffile as tiff\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "\n",
    "import networkx as nx\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80d3988c-196c-4fca-b035-0b1af24e0106",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric.utils\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25b12c61-09ad-41b2-9a8c-ae50d190fd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90aabf57-0607-48a3-9c05-742269b7cf2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_dir = (Path().cwd().parents[0]).absolute()\n",
    "\n",
    "module_path = str(p_dir / \"src\")\n",
    "\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d2a6df8-d5c2-4dee-ba53-119a7c084424",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = (Path().cwd().parents[0] / 'data').absolute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7fff6962-d83d-41f3-9b9a-0c431155704f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning.pytorch as pl\n",
    "import PPIGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc9bd74-4390-4c75-99b3-5750f9bc68c9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Test custom torch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd4ef3d7-ddbb-4a8f-8836-a97b012d5cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8cdc6fcf-bfe0-4ff6-99a9-afc7a241e71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_mapping = {'HCC827Ctrl': 0, 'HCC827Osim': 1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1fc72493-4e7b-42f4-adb4-619e7e97be0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_path = data_dir / '9PPI Cell Culture' / 'Whole' / 'graphs' \n",
    "\n",
    "dataset = PPIGraph.GraphDataset(graph_path, 'raw', 'pt', condition_mapping=condition_mapping, n_c=2)\n",
    "train_set, val_set, test_set = PPIGraph.train_test_val_split(dataset)\n",
    "\n",
    "# Create Dataloader\n",
    "train_loader = DataLoader(train_set, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7001e783-5bd6-4442-af42-7c1fa389fd90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: GraphDataset(1491):\n",
      "======================\n",
      "Number of graphs: 1491\n",
      "Number of features: 9\n",
      "Number of classes: 2\n"
     ]
    }
   ],
   "source": [
    "print(f'Dataset: {dataset}:')\n",
    "print('======================')\n",
    "print(f'Number of graphs: {len(dataset)}')\n",
    "print(f'Number of features: {dataset.num_features}')\n",
    "print(f'Number of classes: {dataset.num_classes}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ba2ee90-059c-44f7-b572-29053bc21270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 716, test set: 596, val set: 179\n"
     ]
    }
   ],
   "source": [
    "print(f'Train set: {len(train_set)}, test set: {len(test_set)}, val set: {len(val_set)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "92762d01-a537-47f6-8009-6f3d5faf4237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1:\n",
      "=======\n",
      "Number of graphs in the current batch: 32\n",
      "DataBatch(edge_index=[2, 62326], pos=[10585, 2], labels=[10585, 9], nuclei=[10585], weight=[62326], condition=[32], fov=[32], id=[32], train_mask=[10585], test_mask=[10585], x=[10585, 9], y=[32], edge_weight=[62326], name=[32], batch=[10585], ptr=[33])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for step, data in enumerate(train_loader):\n",
    "    print(f'Step {step + 1}:')\n",
    "    print('=======')\n",
    "    print(f'Number of graphs in the current batch: {data.num_graphs}')\n",
    "    print(data)\n",
    "    print()\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f547c07-499f-4979-a023-83ad03e5094b",
   "metadata": {},
   "source": [
    "# K Fold on  filter dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "840632fd-828e-4fdb-a636-2829e373ea82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from lightning.pytorch.accelerators import find_usable_cuda_devices\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d008e853-c4c2-4d2f-8c35-0da6f206ccbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from torch.utils.data import SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c86dd364-b7cb-4910-9961-b288741c5cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out by maximum number of counts per cell\n",
    "min_count = 100\n",
    "max_count = 400\n",
    "\n",
    "graph_path = data_dir / '9PPI Cell Culture' / 'Whole' / 'graphs' \n",
    "\n",
    "dataset = PPIGraph.GraphDataset(graph_path, 'raw', 'pt', condition_mapping=condition_mapping, n_c=2)\n",
    "\n",
    "# Create Dataloader\n",
    "loader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# Get Indices\n",
    "indices = []\n",
    "for step, data in enumerate(loader):\n",
    "    if len(data.x) <= min_count:\n",
    "        continue \n",
    "    \n",
    "    if (data.x.sum(axis=0) >= max_count).any():\n",
    "        continue\n",
    "    indices.append(step)\n",
    "    \n",
    "# Get subset dataset\n",
    "dataset_filtered = dataset.index_select(indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1696b9fc-9437-4349-adc9-f97102ff9806",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1368"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "25d9cc11-9602-4626-9d57-ef66ca80e684",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = '9PPI_v2'\n",
    "condition = 'Kfold'\n",
    "checkpoint_folder = (Path().cwd().parents[0]).absolute() / 'data' / \"saved_models\" / dataset_name / f\"Graph_GNNs_{condition}\" \n",
    "project_name = f'PLA_070323_{dataset_name}_{condition}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "de1f4bfd-cae7-4515-8808-9c623124b621",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    }
   ],
   "source": [
    "AVAIL_GPUS = [1]\n",
    "\n",
    "# Setting the seed\n",
    "pl.seed_everything(42)\n",
    "\n",
    "NUM_LAYERS = 2\n",
    "HIDDEN_CHANNELS = 32\n",
    "pools = ['mean', 'max', 'sum', 'attention', 'attention2']\n",
    "\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ee100db6-fd70-4fa1-b100-85feeb144653",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# k_folds = 5\n",
    "# kfold = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# for fold, (train_ids, valid_ids) in enumerate(kfold.split(dataset_filtered)):\n",
    "#     train_subset = dataset_filtered.index_select(train_ids.tolist())\n",
    "#     val_subset = dataset_filtered.index_select(valid_ids.tolist())\n",
    "    \n",
    "#     for pool in pools:\n",
    "#         # Path to the folder where the pretrained models are saved\n",
    "#         CHECKPOINT_PATH = checkpoint_folder / f'{NUM_LAYERS}_{HIDDEN_CHANNELS}_onehot_{fold}' / pool\n",
    "#         CHECKPOINT_PATH.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "#         checkpoint = CHECKPOINT_PATH / \"GraphLevelGCN\" / \"GraphLevelGCN.ckpt\" \n",
    "#         if checkpoint.exists():\n",
    "#             print(checkpoint)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "64c99c78-bb9f-4f01-8f0d-3771363ed437",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ## GCN \n",
    "\n",
    "# k_folds = 5\n",
    "# kfold = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# for fold, (train_ids, valid_ids) in enumerate(kfold.split(dataset_filtered)):\n",
    "#     train_subset = dataset_filtered.index_select(train_ids.tolist())\n",
    "#     val_subset = dataset_filtered.index_select(valid_ids.tolist())\n",
    "    \n",
    "#     for pool in pools:\n",
    "#         # Path to the folder where the pretrained models are saved\n",
    "#         CHECKPOINT_PATH = checkpoint_folder / f'{NUM_LAYERS}_{HIDDEN_CHANNELS}_onehot_{fold}' / pool\n",
    "#         CHECKPOINT_PATH.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "#         # Skip already trained kfold and pool\n",
    "#         checkpoint = CHECKPOINT_PATH / \"GraphLevelGCN\" / \"GraphLevelGCN.ckpt\" \n",
    "#         if checkpoint.exists():\n",
    "#             print(checkpoint)\n",
    "#             continue\n",
    "        \n",
    "#         # Run training\n",
    "#         run = wandb.init(project=project_name, name=f'GCN_{NUM_LAYERS}_{HIDDEN_CHANNELS}_onehot_{fold}', \n",
    "#                         group=f'GCN_{pool}')\n",
    "#         PPIGraph.train_graph_classifier_kfold('GCN', \n",
    "#                                              train_subset, \n",
    "#                                              val_subset, \n",
    "#                                              dataset, \n",
    "#                                              CHECKPOINT_PATH, \n",
    "#                                              AVAIL_GPUS, \n",
    "#                                              in_channels=9,\n",
    "#                                              hidden_channels=HIDDEN_CHANNELS, \n",
    "#                                              out_channels = HIDDEN_CHANNELS,\n",
    "#                                              num_layers=NUM_LAYERS, \n",
    "#                                              epochs=epochs,\n",
    "#                                              embedding=False,\n",
    "#                                              graph_pooling=pool)\n",
    "#         run.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "644ad90d-6d1f-4939-98af-6e4ced448b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_folder = (Path().cwd().parents[0]).absolute() / 'data' / \"saved_models\" / dataset_name /  f\"MLP_{condition}\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "16f35fa8-bee7-4267-92e5-bc9bce650822",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mthoomas\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "139321139a3d43b1b610991952b1413b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01693333333338766, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Thomas\\15_PLA\\notebooks\\wandb\\run-20230704_123754-fw1m0k54</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/fw1m0k54' target=\"_blank\">MLP_2_32_onehot_0</a></strong> to <a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/fw1m0k54' target=\"_blank\">https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/fw1m0k54</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "C:\\Users\\thu71\\Anaconda3\\envs\\snowflake\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:395: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 1.4 K \n",
      "2 | head        | Sequential       | 562   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.008     Total estimated model params size (MB)\n",
      "C:\\Users\\thu71\\AppData\\Roaming\\Python\\Python39\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\thu71\\AppData\\Roaming\\Python\\Python39\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\thu71\\AppData\\Roaming\\Python\\Python39\\site-packages\\lightning\\pytorch\\utilities\\data.py:77: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 2. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  warning_cache.warn(\n",
      "C:\\Users\\thu71\\AppData\\Roaming\\Python\\Python39\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\thu71\\AppData\\Roaming\\Python\\Python39\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:280: PossibleUserWarning: The number of training batches (9) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▁▄▇▆▆▇▇▇▇█▇▇▇▇█▇█▇▇▇█▇▇▇▇▇▇▇▇█▇▇▇▇▇▇▇▇</td></tr><tr><td>train_auc</td><td>▁▁▁▄▇▇▆▇▇▇▇█▇▇▇▇█▇█▇▇▇▇▇▇▇▇█▇▇▇█▇▇█▇▇▇▇▇</td></tr><tr><td>train_f1</td><td>▁▁▁▆▇▇▇██▇▇███▇▇█▇███▇█▇▇███▇▇██▇▇█▇▇▇▇▇</td></tr><tr><td>train_loss_epoch</td><td>██▇▆▃▃▃▁▁▂▂▁▂▁▂▂▂▂▁▂▂▂▂▁▂▁▂▁▁▂▁▂▂▂▁▁▂▁▂▂</td></tr><tr><td>train_loss_step</td><td>███▆▆▃▄▅▆▄▂▂▅▅▄▃▇▃▄▃▅▅▅▅▄▂▂▅▄▁▄▃▃▄▂▆▆▆▃▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▁▂▂▁▂▃▂▃▂▂▄▄▂▄▂▂▅▂▂▅▂▆▆▃▆▆▃▇▇▃▇█▃██▃</td></tr><tr><td>val_acc</td><td>▁▁▂▃▅▆▇▆▇██▇▇██▆▇█▇▇▇▅█▇▆▆▇▇▇█▇▇█▇█▇▇▇▇█</td></tr><tr><td>val_auc</td><td>▁▁▂▃▅▆▇▆▆██▇▇██▇▇█▇▇▆▇█▇▇▆▇▇▇▇▇▇▇▇▇█▇█▇▇</td></tr><tr><td>val_f1</td><td>▁▁▂▃▆▇▇▇▇███▇███▇█▇▇▇██▇█▆▇▇▇▇▇▇▇▇▇█▇█▇▇</td></tr><tr><td>val_loss_epoch</td><td>▇█▇▇▄▆▃▃▅▂▅▆▆▄▃▃▆▅▅▁▃▃▂▃▆▃▅▄▅█▃▄▅▄▆▅▄▂▄▅</td></tr><tr><td>val_loss_step</td><td>▆▇▇█▇▆▃▆▄▆▅█▆▆▃▆▅▆▄▇▇▄▁▇▆▂▆▄▅▆▆▅▄▄▅▄▄▅▅▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.68647</td></tr><tr><td>train_auc</td><td>0.6561</td></tr><tr><td>train_f1</td><td>0.55627</td></tr><tr><td>train_loss_epoch</td><td>0.59541</td></tr><tr><td>train_loss_step</td><td>0.56906</td></tr><tr><td>trainer/global_step</td><td>899</td></tr><tr><td>val_acc</td><td>0.66788</td></tr><tr><td>val_auc</td><td>0.62544</td></tr><tr><td>val_f1</td><td>0.49162</td></tr><tr><td>val_loss_epoch</td><td>0.63597</td></tr><tr><td>val_loss_step</td><td>0.66706</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_32_onehot_0</strong> at: <a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/fw1m0k54' target=\"_blank\">https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/fw1m0k54</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230704_123754-fw1m0k54\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6350210e4a6d4b4daa4138481b4ad13a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016916666666899498, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Thomas\\15_PLA\\notebooks\\wandb\\run-20230704_130157-as6uyzjo</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/as6uyzjo' target=\"_blank\">MLP_2_32_onehot_0</a></strong> to <a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/as6uyzjo' target=\"_blank\">https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/as6uyzjo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 1.4 K \n",
      "2 | head        | Sequential       | 562   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.008     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▃▃▃▅▄▅▆▆▆▅▆▇▆▇▆▆▆▇▆▇▆▇▆▇▇▇█▇▇▇▇▇▆▆▇▆▇▇▇</td></tr><tr><td>train_auc</td><td>▁▂▁▂▄▄▄▆▅▅▄▆▆▆▇▆▆▆▇▆▇▆▇▆▆▇▇█▇▇▇▇▇▆▆▆▆▇▆▇</td></tr><tr><td>train_f1</td><td>▄▂▁▁▄▅▅▆▆▆▆▆▇▇▇▆▇▇▇▇▇▆▇▇▇███▇▇▇▇▇▆▇▇▇▇▇▇</td></tr><tr><td>train_loss_epoch</td><td>█▆▆▆▅▅▄▃▃▃▄▃▃▃▁▃▄▃▂▃▂▃▃▂▁▃▂▂▂▁▂▃▂▄▂▁▄▂▂▃</td></tr><tr><td>train_loss_step</td><td>██▇▆▆▇▆▄▇▅▄▃▄▄▂▅▆▄▆▅▅▇▄▃▃▄▂▅▄▄▄▆▁▅▃▄▅▄▅▅</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▁▂▂▁▂▃▂▃▂▂▄▄▂▄▂▂▅▂▂▅▂▆▆▃▆▆▃▇▇▃▇█▃██▃</td></tr><tr><td>val_acc</td><td>▇▇▇▇▇██▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>▁▁▁▁▁▇█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁███████████████████████████████████</td></tr><tr><td>val_loss_epoch</td><td>▁▁▁▁▁▁▁▂▃▃▃▄▅▆▆▆▅▆▇▆▆▅▆▆▇▅▇▆▆▄█▆▆▅▅▆▅▅▆▅</td></tr><tr><td>val_loss_step</td><td>▁▁▁▁▁▁▁▂▃▃▄▅▄▆▅▆▅█▆▆▆▅▇▅▆▆▆▆▇▆▅▆▅▆▅▇▅▆▇▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.65539</td></tr><tr><td>train_auc</td><td>0.62265</td></tr><tr><td>train_f1</td><td>0.5059</td></tr><tr><td>train_loss_epoch</td><td>0.63715</td></tr><tr><td>train_loss_step</td><td>0.64478</td></tr><tr><td>trainer/global_step</td><td>899</td></tr><tr><td>val_acc</td><td>0.40876</td></tr><tr><td>val_auc</td><td>0.5</td></tr><tr><td>val_f1</td><td>0.58031</td></tr><tr><td>val_loss_epoch</td><td>0.99969</td></tr><tr><td>val_loss_step</td><td>0.85166</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_32_onehot_0</strong> at: <a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/as6uyzjo' target=\"_blank\">https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/as6uyzjo</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230704_130157-as6uyzjo\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82e1d6716382457eb17a6b3e1457bbad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Thomas\\15_PLA\\notebooks\\wandb\\run-20230704_132542-q5v2naj2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/q5v2naj2' target=\"_blank\">MLP_2_32_onehot_0</a></strong> to <a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/q5v2naj2' target=\"_blank\">https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/q5v2naj2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 1.4 K \n",
      "2 | head        | Sequential       | 562   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.008     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▃▃▄▅▅▆▇▇▆▅▇▇▇▇▇█▇█▇█▇█▇██▇▇██▇█▇█▇▇█▇▇</td></tr><tr><td>train_auc</td><td>▁▂▃▃▃▄▅▅▇▆▆▆▇▇▇▇█▇▇██▇██▇██▇▇██▇█▇█▇▇█▇▇</td></tr><tr><td>train_f1</td><td>▁▃▂▁▁▃▅▅▆▅▅▆▇▆▆▆█▇▇▇▇▇██▆██▇▇▇█▇█▆█▇▇█▇▇</td></tr><tr><td>train_loss_epoch</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▂▂▂▁▂▁▂▁▂▁▁▁▁▁▁▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▁▂▂▁▂▃▂▃▂▂▄▄▂▄▂▂▅▂▂▅▂▆▆▃▆▆▃▇▇▃▇█▃██▃</td></tr><tr><td>val_acc</td><td>▁▁▂▂▂▄▅▅▇▅▅▇▇▆▇▄▇▇▇▇█▆▇▇▇██▆▇▇▇▇▆▇▇▅▆▅▇▇</td></tr><tr><td>val_auc</td><td>▁▁▁▂▁▃▄▅▆▇█▇▆█▇█▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇█▇▇▇█▇▇</td></tr><tr><td>val_f1</td><td>▁▁▁▂▁▃▄▆▅▇█▆▆▇▇█▆▇▇▇▆▇▇▇▇▆▆▇▇▇▆▇▇▇▇▇▇█▇▇</td></tr><tr><td>val_loss_epoch</td><td>█▆▅▅▄▃▃▃▂▂▄▄▃▃▂▃▄▃▂▁▂▂▁▂▄▂▅▃▃▅▃▃▃▂▃▃▃▃▂▃</td></tr><tr><td>val_loss_step</td><td>▆▇▇█▇▆▅▅▄▅▆▆▆▆▄▆▅▆▅▇▇▅▁▇▆▁▇▄▅▆▆▅▅▂▅▄▄▆▅▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.68373</td></tr><tr><td>train_auc</td><td>0.66657</td></tr><tr><td>train_f1</td><td>0.59767</td></tr><tr><td>train_loss_epoch</td><td>0.60768</td></tr><tr><td>train_loss_step</td><td>0.61501</td></tr><tr><td>trainer/global_step</td><td>899</td></tr><tr><td>val_acc</td><td>0.65693</td></tr><tr><td>val_auc</td><td>0.62307</td></tr><tr><td>val_f1</td><td>0.51042</td></tr><tr><td>val_loss_epoch</td><td>0.61085</td></tr><tr><td>val_loss_step</td><td>0.63838</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_32_onehot_0</strong> at: <a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/q5v2naj2' target=\"_blank\">https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/q5v2naj2</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230704_132542-q5v2naj2\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1ad625de42d4169aca0bab269ddb38c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01693333333338766, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Thomas\\15_PLA\\notebooks\\wandb\\run-20230704_134929-dd9j3wgh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/dd9j3wgh' target=\"_blank\">MLP_2_32_onehot_0</a></strong> to <a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/dd9j3wgh' target=\"_blank\">https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/dd9j3wgh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "C:\\Users\\thu71\\Anaconda3\\envs\\snowflake\\lib\\site-packages\\torch_geometric\\deprecation.py:12: UserWarning: 'nn.glob.GlobalAttention' is deprecated, use 'nn.aggr.AttentionalAggregation' instead\n",
      "  warnings.warn(out)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 1.4 K \n",
      "2  | head        | Sequential       | 562   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 33    \n",
      "--------------------------------------------------\n",
      "2.0 K     Trainable params\n",
      "0         Non-trainable params\n",
      "2.0 K     Total params\n",
      "0.008     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ce8f3d7a702479bbfcdbc6c4bf5433c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▁▅▅▆▆▇▇▇▇█▇▇█▇▆▇▇▇▇▇█▇█▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>train_auc</td><td>▁▁▁▅▆▇▆▇▇▇▇█▇▇█▇▆▆▇▇▇▇▇▇█▇▇▆▇██▇▇▇█▇█▇▇▇</td></tr><tr><td>train_f1</td><td>▁▁▂▅▇▇▇██▇▇█▇██▇▇▇▇▇██▇▇███▇▇██████▇██▇█</td></tr><tr><td>train_loss_epoch</td><td>█▇▇▄▄▃▂▂▂▂▂▂▂▂▃▂▂▂▂▂▂▁▁▁▁▃▂▂▁▁▂▃▁▁▂▂▂▁▂▂</td></tr><tr><td>train_loss_step</td><td>██▇▇▅▆▅▃▅█▄▆▄▂▄▆▁▁▆▅▄█▃▂▆▃▅▇▄▃▆▆█▄▃▃▇▄▄▅</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▁▂▂▁▂▃▂▃▂▂▄▄▂▄▂▂▅▂▂▅▂▆▆▃▆▆▃▇▇▃▇█▃██▃</td></tr><tr><td>val_acc</td><td>▁▁▂▄▃▃▃▃▅▅▅▃▅▆▅▇▅▇▇▆▇▇█▇▇▅▇▆█▅▅▇▇▇▆▇▆▇▇█</td></tr><tr><td>val_auc</td><td>▁▁▂▃▂▃▃▃▅▅▅▃▅▅▅▆▅▇▆▆▆▆█▆▇▄▇▆█▄▅▇▆█▅▇▅▇▇▇</td></tr><tr><td>val_f1</td><td>▁▁▂▄▃▄▃▃▆▆▆▄▅▅▅▇▅▇▇▇▇▇█▆▇▅▇▇█▅▆▇▇█▆▇▆▇█▇</td></tr><tr><td>val_loss_epoch</td><td>▄▄▄▂▃▃▂▅▂▂▃▅▃▃▃▄▄▁▄▄▂▂▃▂▃▃▂▂▁█▄▂▃▃▃▂▃▃▂▂</td></tr><tr><td>val_loss_step</td><td>▄▄▄▃▄▅▁▄▄█▄▆▅▅▅▄▅▂▅▄▅▄▅▄▅▅▄▄▁▃▂▃▅▄▅▄▄▃▄▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.69378</td></tr><tr><td>train_auc</td><td>0.67003</td></tr><tr><td>train_f1</td><td>0.58896</td></tr><tr><td>train_loss_epoch</td><td>0.59082</td></tr><tr><td>train_loss_step</td><td>0.61224</td></tr><tr><td>trainer/global_step</td><td>899</td></tr><tr><td>val_acc</td><td>0.67518</td></tr><tr><td>val_auc</td><td>0.63161</td></tr><tr><td>val_f1</td><td>0.49718</td></tr><tr><td>val_loss_epoch</td><td>0.62419</td></tr><tr><td>val_loss_step</td><td>0.61229</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_32_onehot_0</strong> at: <a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/dd9j3wgh' target=\"_blank\">https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/dd9j3wgh</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230704_134929-dd9j3wgh\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2087636e58b74434b10a580bc9c7bc69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Thomas\\15_PLA\\notebooks\\wandb\\run-20230704_141331-pilrsn7o</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/pilrsn7o' target=\"_blank\">MLP_2_32_onehot_0</a></strong> to <a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/pilrsn7o' target=\"_blank\">https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/pilrsn7o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 1.4 K \n",
      "2  | head        | Sequential       | 562   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 1.1 K \n",
      "--------------------------------------------------\n",
      "3.0 K     Trainable params\n",
      "0         Non-trainable params\n",
      "3.0 K     Total params\n",
      "0.012     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b92e385607e41088b00ba8a9682cad0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▂▅▇▇▆▇▇▇▇▇▆▇▇▇▇▇▇▇▇▇▇█▇▇▆▇▇▇▇█▇▇▇▇█▇▇▇</td></tr><tr><td>train_auc</td><td>▁▁▁▄▇▇▇▇▇▇▇▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇▆▇▇▇▇█▇▇▇▇█▇▇█</td></tr><tr><td>train_f1</td><td>▁▁▂▅▇▇▇▇▇▇▇█▇▇▇▇█▇▇▇█▇██▇▇▇▇████▇▇▇▇████</td></tr><tr><td>train_loss_epoch</td><td>██▇▄▃▃▃▂▂▂▂▂▂▁▂▁▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▂▁▂▂▁▁▁▂</td></tr><tr><td>train_loss_step</td><td>█▇▇▆▄▆▄▄▃▄▄▅▃▄▂▃▃▂▅▃▆▄▄▂▁▅▄▂▄▂▅▅▁▆▅▃▃▆▄▆</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▁▂▂▁▂▃▂▃▂▂▄▄▂▄▂▂▅▂▂▅▂▆▆▃▆▆▃▇▇▃▇█▃██▃</td></tr><tr><td>val_acc</td><td>▁▁▃▆▆▅▆▆▇▇▆▆█▆█▆▆▆▆▅▇▇▆▆▇▆▇▇▆▆▆▆▇▆▆▆▆▆▇▇</td></tr><tr><td>val_auc</td><td>▁▁▂▆▇▅▅▇▇▇▆▇█▇█▆▆▆▆▆▇█▇▆▇▇▇▇▆▇▆▆▇▇▇▇▆▆▇▆</td></tr><tr><td>val_f1</td><td>▁▁▂▆█▅▆▇▇█▇▇█▇█▇▇▆▇████▇████▇▇▇▇▇▇██▇▇▇▇</td></tr><tr><td>val_loss_epoch</td><td>▇▆▄▂▂▅▅▅▃▃▄▅▁▃▅▂▃▆▅▄█▅█▄▄▅▃▆▂▃▆▆▆▄▄▁▁▅▁▇</td></tr><tr><td>val_loss_step</td><td>▄▄▄▃▃▃▄▄▃▅▃▄▅▃▅▃▅▅▄▄▄▄█▃▄▄▄▄▂▄▄▄▃▄▃▃▁▄▄▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.70293</td></tr><tr><td>train_auc</td><td>0.68553</td></tr><tr><td>train_f1</td><td>0.61988</td></tr><tr><td>train_loss_epoch</td><td>0.59452</td></tr><tr><td>train_loss_step</td><td>0.62206</td></tr><tr><td>trainer/global_step</td><td>899</td></tr><tr><td>val_acc</td><td>0.66058</td></tr><tr><td>val_auc</td><td>0.61238</td></tr><tr><td>val_f1</td><td>0.45614</td></tr><tr><td>val_loss_epoch</td><td>0.6957</td></tr><tr><td>val_loss_step</td><td>0.82599</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_32_onehot_0</strong> at: <a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/pilrsn7o' target=\"_blank\">https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/pilrsn7o</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230704_141331-pilrsn7o\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d5c7a70eac14bd8afe8a011a62dc3f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Thomas\\15_PLA\\notebooks\\wandb\\run-20230704_143803-gq0wc2t9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/gq0wc2t9' target=\"_blank\">MLP_2_32_onehot_1</a></strong> to <a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/gq0wc2t9' target=\"_blank\">https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/gq0wc2t9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 1.4 K \n",
      "2 | head        | Sequential       | 562   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.008     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c610276a03664ad0855e00ab6d52adc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▁▄▅▆▆▇▇▆▆▇▇▆▇▇▇▇▇█▇▇▇▇▇▇▇▇▇▇█▆▇▇▇▇▇▇▇▇</td></tr><tr><td>train_auc</td><td>▁▁▁▄▆▆▆▇▇▆▆▇▇▇▇▆▇▇▇█▇▇▇▆▇▇▇▇▇▇▇▆▇▇▇▇▇▇▇▇</td></tr><tr><td>train_f1</td><td>▁▁▁▅▇▇▇▇▇▇▇▇▇▇▇▇█▇▇██▇█▇▇███▇▇█▇▇▇█▇▇▇▇▇</td></tr><tr><td>train_loss_epoch</td><td>██▇▆▄▃▃▂▃▃▂▂▂▂▂▃▂▁▂▂▃▂▂▂▂▂▂▁▁▂▂▃▂▂▂▂▂▂▃▂</td></tr><tr><td>train_loss_step</td><td>▇▇█▆▇▄▄▄▃▆▃▃▂▆▄▂▄▆▄▁▄▇▄▃▃▃▃▂▄▄▃▅▄▇▁▃▆▆▅▄</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▁▂▂▁▂▃▂▃▂▂▄▄▂▄▂▂▅▂▂▅▂▆▆▃▆▆▃▇▇▃▇█▃██▃</td></tr><tr><td>val_acc</td><td>▁▁▁▃▅▇▇▇▇▇█▇▇▇█▇█▇▇█▇███▇▇███████████▇██</td></tr><tr><td>val_auc</td><td>▁▁▁▃▄▆▆▆▆▆█▇▆▇▇▇▇▇▇▇▆█▇▇▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>val_f1</td><td>▁▁▁▃▅▇▇▇▆▇█▇▇▇▇▇▇▇▇▇▆█▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>val_loss_epoch</td><td>▆▆▅▄▃▃▃▃▃▂▃▄▃▂▃▃█▃▃▁▁▂▁▂▂▂▁▂▃▃▅▃▂▃▅▁▃▃▁▁</td></tr><tr><td>val_loss_step</td><td>▇▇▇▆▇▅▅▅▄█▅▇▅▅▅▄▆▆▅▅▅▆▁▄▅▃▅▄▅▄▅▆▅▆▆▅▅▅▅▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.69378</td></tr><tr><td>train_auc</td><td>0.65933</td></tr><tr><td>train_f1</td><td>0.55034</td></tr><tr><td>train_loss_epoch</td><td>0.5936</td></tr><tr><td>train_loss_step</td><td>0.60323</td></tr><tr><td>trainer/global_step</td><td>899</td></tr><tr><td>val_acc</td><td>0.71168</td></tr><tr><td>val_auc</td><td>0.67901</td></tr><tr><td>val_f1</td><td>0.58639</td></tr><tr><td>val_loss_epoch</td><td>0.55039</td></tr><tr><td>val_loss_step</td><td>0.45211</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_32_onehot_1</strong> at: <a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/gq0wc2t9' target=\"_blank\">https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/gq0wc2t9</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230704_143803-gq0wc2t9\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7a3c58782a5494388120f1eb34ab4be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016916666666899498, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Thomas\\15_PLA\\notebooks\\wandb\\run-20230704_150107-ebxeu7lo</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/ebxeu7lo' target=\"_blank\">MLP_2_32_onehot_1</a></strong> to <a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/ebxeu7lo' target=\"_blank\">https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/ebxeu7lo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 1.4 K \n",
      "2 | head        | Sequential       | 562   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.008     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8dc47ce03db4458b89c5d6cb343cdb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▃▄▄▅▆▆▇▆▆▆▆▆▇▇▆▆█▇▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇█▇▇▇▇▇</td></tr><tr><td>train_auc</td><td>▁▂▂▃▅▅▅▆▆▆▆▆▆▇▇▆▆▇▆▆▆▇▆▇▆▇▇▇▆▇▇▆▇▇█▆▇▇▇▇</td></tr><tr><td>train_f1</td><td>▄▁▁▄▅▆▆▇▆▇▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇▇▇▇█▇▇▇▇▇</td></tr><tr><td>train_loss_epoch</td><td>█▆▅▅▅▄▄▃▃▂▂▂▃▁▂▃▂▁▁▂▃▂▂▁▂▂▂▁▁▂▂▂▂▂▂▂▁▂▂▃</td></tr><tr><td>train_loss_step</td><td>▇█▇▅▅▅▄▅▄▆▂▁▃▆▃▄▄▂▃▂▃█▂▆▃▂▂▂▂▃▂▁▅▄▄▂▃█▃▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▁▂▂▁▂▃▂▃▂▂▄▄▂▄▂▂▅▂▂▅▂▆▆▃▆▆▃▇▇▃▇█▃██▃</td></tr><tr><td>val_acc</td><td>███████▇▇▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>▁▁▁▁▇▇▇██▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_f1</td><td>▁▁▁▁▇▇▇█████████████████████████████████</td></tr><tr><td>val_loss_epoch</td><td>▁▁▁▁▁▂▁▂▂▂▃▃▃▅▄▄▅▆▆▄▄▅▅▆▅▆▆▆▆▃█▄█▆▆▆▅▅▆▄</td></tr><tr><td>val_loss_step</td><td>▁▂▂▁▁▁▁▂▃▅▃▄▃▄▂▄▄█▄▄▄▄▅▅▄▆▄▆▇▅▃▅▆▆▆█▅▅▆▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.64991</td></tr><tr><td>train_auc</td><td>0.61426</td></tr><tr><td>train_f1</td><td>0.48591</td></tr><tr><td>train_loss_epoch</td><td>0.63293</td></tr><tr><td>train_loss_step</td><td>0.61616</td></tr><tr><td>trainer/global_step</td><td>899</td></tr><tr><td>val_acc</td><td>0.40876</td></tr><tr><td>val_auc</td><td>0.5</td></tr><tr><td>val_f1</td><td>0.58031</td></tr><tr><td>val_loss_epoch</td><td>0.80439</td></tr><tr><td>val_loss_step</td><td>0.68516</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_32_onehot_1</strong> at: <a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/ebxeu7lo' target=\"_blank\">https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/ebxeu7lo</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230704_150107-ebxeu7lo\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dba1d48127041ecaec2da225d2722df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Thomas\\15_PLA\\notebooks\\wandb\\run-20230704_152407-qhj7v66s</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/qhj7v66s' target=\"_blank\">MLP_2_32_onehot_1</a></strong> to <a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/qhj7v66s' target=\"_blank\">https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/qhj7v66s</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 1.4 K \n",
      "2 | head        | Sequential       | 562   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.008     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe3fc246aeaf46d4bffaaf18d27f585b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▄▃▄▅▅▅▆▆▆▆▇▇▇▇▆▇▇██▇▇▇▇▇▇█▇▇▇██▇▇▇▇▇█▇</td></tr><tr><td>train_auc</td><td>▁▂▃▃▃▅▅▅▅▅▆▆█▇▇▇▇█▇██▇█▇▇▇███▇███▇█▇▇███</td></tr><tr><td>train_f1</td><td>▁▂▃▃▃▄▄▄▅▄▅▆▇▇▇▆▇▇▇█▇▇█▇▇▇▇█▇▇▇▇█▇▇▇▇███</td></tr><tr><td>train_loss_epoch</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▂▂▂▂▂▂▂▁▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▂▁▁▁▂▂▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▁▂▂▁▂▃▂▃▂▂▄▄▂▄▂▂▅▂▂▅▂▆▆▃▆▆▃▇▇▃▇█▃██▃</td></tr><tr><td>val_acc</td><td>▁▅▅▆▆▇▇▆▇▇▇▇▇▇██▇▇█▇▇█▇█▇▇▇▇▇▇████▇████▇</td></tr><tr><td>val_auc</td><td>▁▃▄▄▄▆▅▅▆▆▇▆▆▇▇█▆▇▇▇▆█▆▇▆▇▆▇▇▆██▇█▇▇██▇▆</td></tr><tr><td>val_f1</td><td>▅▁▂▂▃▅▅▄▅▆▇▆▆▇▇█▆▇▇▇▆█▆▇▇▇▆▇▇▆██▇█▇▇██▇▆</td></tr><tr><td>val_loss_epoch</td><td>▇▅▃▃▂▂▂▂▂▂▂▂▁▁▂▂█▂▂▁▁▂▁▂▂▂▁▃▂▂█▂▂▃▅▁▂▂▁▁</td></tr><tr><td>val_loss_step</td><td>▃▄▃▂▃▂▂▃▂█▂▂▂▂▂▂▃▂▂▂▃▄▁▂▃▁▃▂▂▂▂▃▃▂▃▃▂▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.6755</td></tr><tr><td>train_auc</td><td>0.66805</td></tr><tr><td>train_f1</td><td>0.61622</td></tr><tr><td>train_loss_epoch</td><td>0.61443</td></tr><tr><td>train_loss_step</td><td>0.61676</td></tr><tr><td>trainer/global_step</td><td>899</td></tr><tr><td>val_acc</td><td>0.70438</td></tr><tr><td>val_auc</td><td>0.65631</td></tr><tr><td>val_f1</td><td>0.52071</td></tr><tr><td>val_loss_epoch</td><td>0.58091</td></tr><tr><td>val_loss_step</td><td>0.50695</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_32_onehot_1</strong> at: <a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/qhj7v66s' target=\"_blank\">https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/qhj7v66s</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230704_152407-qhj7v66s\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fe25320e996435198441d2ad23c6f18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Thomas\\15_PLA\\notebooks\\wandb\\run-20230704_154827-1ldpxc5c</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/1ldpxc5c' target=\"_blank\">MLP_2_32_onehot_1</a></strong> to <a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/1ldpxc5c' target=\"_blank\">https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/1ldpxc5c</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 1.4 K \n",
      "2  | head        | Sequential       | 562   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 33    \n",
      "--------------------------------------------------\n",
      "2.0 K     Trainable params\n",
      "0         Non-trainable params\n",
      "2.0 K     Total params\n",
      "0.008     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6973686a24845a6ac88b37fc2550b8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▁▃▆▆▇▇▆▆▇▆█▇█▇▇▇▇▇█▇▇▇▇▇▇█▇▇▇▆▇██▇██▇█</td></tr><tr><td>train_auc</td><td>▁▁▁▃▆▇▇▇▆▇▇▆▇▇█▇▇▇▇▇▇▇▇▇▇▇▇█▆██▆▇██▇██▇█</td></tr><tr><td>train_f1</td><td>▁▁▂▃▇▇▇█▇▇▇▇█▇█▇▇▇█▇▇▇▇▇▇▇██▇██▇▇█████▇█</td></tr><tr><td>train_loss_epoch</td><td>██▇▆▄▄▄▃▄▃▃▄▃▃▃▃▂▃▂▂▃▃▂▃▂▂▂▂▂▂▂▃▂▂▂▂▂▁▂▃</td></tr><tr><td>train_loss_step</td><td>▇▇▆▆▅▅▅█▃▄▆▆▄▅▄▂▅▃▅▄▃▅▄▁▃▂▅▄▅▅▆▆▄▅▅▃▅▆▄▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▁▂▂▁▂▃▂▃▂▂▄▄▂▄▂▂▅▂▂▅▂▆▆▃▆▆▃▇▇▃▇█▃██▃</td></tr><tr><td>val_acc</td><td>▁▁▁▃▄▂▃▃▄▄▄▅▄▅▆▇▆▇▇▇▇▇▇▅▇▇▇▇█▅▅▇▇▇▇█▅▇██</td></tr><tr><td>val_auc</td><td>▁▁▁▄▄▂▃▃▄▄▄▅▄▅▆▇▆▇▇▇▆▇▇▅▇▆█▇█▅▅▇▇▇▇▇▅▇██</td></tr><tr><td>val_f1</td><td>▁▁▁▅▅▃▄▄▅▅▅▆▅▆▆█▇█▇▇▇▇█▆▇▇███▆▆▇▇▇▇▇▆▇██</td></tr><tr><td>val_loss_epoch</td><td>██▇▆▇▆▄▆▃▃▄▄▄▄▅▄▄▃▃▅▃▄▃▃▄▅▃▃▁▅▇▃▃▄▂▂▄▃▂▅</td></tr><tr><td>val_loss_step</td><td>▆▆▆▆▅▆▃▅▅▅▆▄▆▆▇▄▅▄▅▄▅▄▄▆▅▇▅▅▁▄▁▅▅▆▅▅▄▅▄█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.69927</td></tr><tr><td>train_auc</td><td>0.66894</td></tr><tr><td>train_f1</td><td>0.57328</td></tr><tr><td>train_loss_epoch</td><td>0.60294</td></tr><tr><td>train_loss_step</td><td>0.56348</td></tr><tr><td>trainer/global_step</td><td>899</td></tr><tr><td>val_acc</td><td>0.71168</td></tr><tr><td>val_auc</td><td>0.6859</td></tr><tr><td>val_f1</td><td>0.60697</td></tr><tr><td>val_loss_epoch</td><td>0.63615</td></tr><tr><td>val_loss_step</td><td>0.76637</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_32_onehot_1</strong> at: <a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/1ldpxc5c' target=\"_blank\">https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/1ldpxc5c</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230704_154827-1ldpxc5c\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8989eb3c8914d6b984dc3f51731af88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Thomas\\15_PLA\\notebooks\\wandb\\run-20230704_161241-4x4snvec</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/4x4snvec' target=\"_blank\">MLP_2_32_onehot_1</a></strong> to <a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/4x4snvec' target=\"_blank\">https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/4x4snvec</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 1.4 K \n",
      "2  | head        | Sequential       | 562   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 1.1 K \n",
      "--------------------------------------------------\n",
      "3.0 K     Trainable params\n",
      "0         Non-trainable params\n",
      "3.0 K     Total params\n",
      "0.012     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d653cfb3be8846b9a9efb5f8bc384679",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▁▄▅▅▆▆▆▇▆▆█▆▇▆▇▆▇▅▇▇▇▇▇▇█▇▆▇▇██▇▇█▇▇▇▇</td></tr><tr><td>train_auc</td><td>▁▁▁▄▅▆▇▆▆▇▆▇█▆▇▆▇▇▇▅▇▆▇▇▇▇▇▇▇▇▇█▇▇▇█▇▇▆█</td></tr><tr><td>train_f1</td><td>▁▁▁▅▇▇▇▇▇█▇▇█▇▇▇▇▇█▆▇▇▇▇▇▇██▇▇██▇█▇█▇█▇█</td></tr><tr><td>train_loss_epoch</td><td>██▇▅▄▃▃▃▂▂▂▃▂▃▂▃▂▃▂▂▁▂▂▂▁▁▁▂▂▁▁▂▂▁▂▁▂▂▃▂</td></tr><tr><td>train_loss_step</td><td>▇▇▆▅▄█▄▃▅▃▆▄▅▄▆▆▁█▄▆▆▆▇▄▃▇▁▇▃▅▆▁▄▂▆▃▄▄▄█</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▁▂▂▁▂▃▂▃▂▂▄▄▂▄▂▂▅▂▂▅▂▆▆▃▆▆▃▇▇▃▇█▃██▃</td></tr><tr><td>val_acc</td><td>▁▁▁▅▄▆▅▇▇▇▇▇▇▇█▆▆▆▆▇▇▇█▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>val_auc</td><td>▁▁▁▅▄▆▅▆▇▆▆▆▇▇█▆▆▆▆▇▇▇█▇▇▇▇▇▇▆▇▆▇▇▇▇▇▇▇▆</td></tr><tr><td>val_f1</td><td>▁▁▁▆▅▆▅▇▇▇▇▇▇▇█▇▇▆▇████▇▇█▇▇▇▇▇▇▇▇▇▇▇▇█▇</td></tr><tr><td>val_loss_epoch</td><td>█▇▆▅▃▁▃▄▂▁▃▂▂▂▄▄▄▄▃▂▃▂▅▁▂▂▁▃▃▁▅▂▅▄▂▅▁▄▅▄</td></tr><tr><td>val_loss_step</td><td>███▇▅▆▂▆▄▇▆▁▆▆▆▄▆▆▆▅▅▅█▄▅▂▅▃▆▅▆▄▆▇▃▄▁▃▅▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.68921</td></tr><tr><td>train_auc</td><td>0.67099</td></tr><tr><td>train_f1</td><td>0.60094</td></tr><tr><td>train_loss_epoch</td><td>0.59985</td></tr><tr><td>train_loss_step</td><td>0.68898</td></tr><tr><td>trainer/global_step</td><td>899</td></tr><tr><td>val_acc</td><td>0.69708</td></tr><tr><td>val_auc</td><td>0.65289</td></tr><tr><td>val_f1</td><td>0.52571</td></tr><tr><td>val_loss_epoch</td><td>0.60778</td></tr><tr><td>val_loss_step</td><td>0.64427</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_32_onehot_1</strong> at: <a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/4x4snvec' target=\"_blank\">https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/4x4snvec</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230704_161241-4x4snvec\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9eff9c26e31d40f3a0c61c2794b6f2a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01693333333338766, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Thomas\\15_PLA\\notebooks\\wandb\\run-20230704_163854-gc9mhtap</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/gc9mhtap' target=\"_blank\">MLP_2_32_onehot_2</a></strong> to <a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/gc9mhtap' target=\"_blank\">https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/gc9mhtap</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 1.4 K \n",
      "2 | head        | Sequential       | 562   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.008     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f518d7688864823ad76f02240c4e533",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▁▄▆█▆█▇▇▆▇█▇▇█▇▇▇██▆▇▇▇▇█▇▇▇█▇█▇█▇█▇█▇</td></tr><tr><td>train_auc</td><td>▁▁▁▄▆▇▆▇▇▇▆▇█▇▇▇▇▇▇██▆▆▇▇▇█▇▇▇█▆█▇█▇▇▇▇▇</td></tr><tr><td>train_f1</td><td>▁▁▁▆▇▇▇▇▇▇▇██▇▇▇▇▇▇██▆▇▇▇▇██▇▇█▇█▇█▇▇▇▇▇</td></tr><tr><td>train_loss_epoch</td><td>█▇▇▅▄▂▃▂▂▂▂▁▂▂▁▂▂▁▂▂▂▂▂▂▁▁▁▂▂▂▁▁▁▂▂▂▂▁▂▁</td></tr><tr><td>train_loss_step</td><td>▇▇█▅▅▃▃▃▇▆▅▂▃▃▃▃▄▄▂▄▄▇▆▄▃▃▃▅▂▃▄▃▄▃▄▅▄▂▂▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▁▂▂▁▂▃▂▃▂▂▄▄▂▄▂▂▅▂▂▅▂▆▆▃▆▆▃▇▇▃▇█▃██▃</td></tr><tr><td>val_acc</td><td>▁▁▁▂▆█▇▆▇▇▇▆▆▇▇▆▇▇▇▆▇█▇▆▇▇▇▇█▇▇▇▇▆▇▇▇▇▆▇</td></tr><tr><td>val_auc</td><td>▁▁▁▂▅█▇▆▇▇▇▇▆▇▇▇▇▇▇▆▆█▇▆▆▆▇▆█▇▆▆▇▆▆▇▇▇▆▆</td></tr><tr><td>val_f1</td><td>▁▁▁▂▅█▇▆▇▇█▇▇▇▇█▇▇▇▇▆█▇▇▇▇▇▇█▇▇▇▇▇▇█▇▇▇▇</td></tr><tr><td>val_loss_epoch</td><td>██▇▅▂▄▃▃▂▂▄▄▃▄▃▄▃▂▂▂▄▄▄▄▃▃▁▄▄▄▂▂▄▂▄▂▃▂▃▂</td></tr><tr><td>val_loss_step</td><td>▇██▄▆▅▄▄▄▁▃▆▃▄▄▄▃▂▄▄▄▄▅▃▄▅▅▄▆▄▃▄▄▁▃▃▃▄▄▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.68007</td></tr><tr><td>train_auc</td><td>0.65255</td></tr><tr><td>train_f1</td><td>0.55808</td></tr><tr><td>train_loss_epoch</td><td>0.59455</td></tr><tr><td>train_loss_step</td><td>0.54214</td></tr><tr><td>trainer/global_step</td><td>899</td></tr><tr><td>val_acc</td><td>0.69708</td></tr><tr><td>val_auc</td><td>0.66349</td></tr><tr><td>val_f1</td><td>0.56545</td></tr><tr><td>val_loss_epoch</td><td>0.55217</td></tr><tr><td>val_loss_step</td><td>0.53315</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_32_onehot_2</strong> at: <a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/gc9mhtap' target=\"_blank\">https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/gc9mhtap</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230704_163854-gc9mhtap\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65da81d022134982b4d8a482d40c0e30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Thomas\\15_PLA\\notebooks\\wandb\\run-20230704_170259-7id6u5is</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/7id6u5is' target=\"_blank\">MLP_2_32_onehot_2</a></strong> to <a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/7id6u5is' target=\"_blank\">https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/7id6u5is</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 1.4 K \n",
      "2 | head        | Sequential       | 562   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.008     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4337ffd900444ac5b601b3ad4d83d022",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▃▄▄▅▅▅▆▇▆▆▇▇▆▆▇▇▇▇▆▇▇▇▇▇▇▇▇▆▆▇▇▇▇█▇▇▆▇█</td></tr><tr><td>train_auc</td><td>▁▂▂▃▄▄▅▆▆▆▆▇▇▆▆▇▇▆▇▆▇▆▇▇▇█▇█▆▆█▇▇▇█▆▆▆▇█</td></tr><tr><td>train_f1</td><td>▄▁▁▃▄▄▆▆▆▆▆▇█▆▆▇▇▇▇▇▇▆▇▇▇███▇▆█▇▇▇█▆▇▇▇█</td></tr><tr><td>train_loss_epoch</td><td>█▆▆▅▄▄▄▃▃▃▃▃▂▃▂▃▂▁▂▂▃▂▂▂▁▂▂▁▃▂▂▂▂▂▂▂▃▃▂▁</td></tr><tr><td>train_loss_step</td><td>▇▇█▆▆█▂▅▆▄▇▄▃▅▂▂▃▃▃▂▆▇▆▇▅▁▁▂▄▄▃▄▄▃▃▄▄▂▂▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▁▂▂▁▂▃▂▃▂▂▄▄▂▄▂▂▅▂▂▅▂▆▆▃▆▆▃▇▇▃▇█▃██▃</td></tr><tr><td>val_acc</td><td>███████▅█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>▁▁▁▁▁██▆█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁███████████████████████████████████</td></tr><tr><td>val_loss_epoch</td><td>▁▁▁▁▁▁▂▂▂▂▃▃▃▃▃▃▄▅▅▅▄▃▄▅▄▅▅▆▆▄█▆▆▆▅▆▅▄▆▆</td></tr><tr><td>val_loss_step</td><td>▁▁▁▁▁▁▂▂▂▃▃▄▃▃▂▃▄▆▄▄▄▃▅▄▄▅▄▆█▅▄▆▅▆▅▇▅▆▆▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.67733</td></tr><tr><td>train_auc</td><td>0.64959</td></tr><tr><td>train_f1</td><td>0.55373</td></tr><tr><td>train_loss_epoch</td><td>0.61109</td></tr><tr><td>train_loss_step</td><td>0.58657</td></tr><tr><td>trainer/global_step</td><td>899</td></tr><tr><td>val_acc</td><td>0.40511</td></tr><tr><td>val_auc</td><td>0.5</td></tr><tr><td>val_f1</td><td>0.57662</td></tr><tr><td>val_loss_epoch</td><td>0.93752</td></tr><tr><td>val_loss_step</td><td>0.80956</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_32_onehot_2</strong> at: <a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/7id6u5is' target=\"_blank\">https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/7id6u5is</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230704_170259-7id6u5is\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb32d4636633460bb2ea3cd7c3d422cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Thomas\\15_PLA\\notebooks\\wandb\\run-20230704_172914-44f4kevu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/44f4kevu' target=\"_blank\">MLP_2_32_onehot_2</a></strong> to <a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/44f4kevu' target=\"_blank\">https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/44f4kevu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 1.4 K \n",
      "2 | head        | Sequential       | 562   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.008     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21ac7dadf8ba459eb14c8de1955b8bcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▃▃▄▆▅▄▆▆▆▇▇▆▇█▆▇▇▇▇▇▇█▇▇█▇▇▇▇▇██▇▇██▇▇</td></tr><tr><td>train_auc</td><td>▁▂▃▂▄▆▄▄▆▅▆▇▇▆▇▇▇█▇▇▇▇▇▇▇▇██▇▆▇▇█▇▇▇██▇█</td></tr><tr><td>train_f1</td><td>▁▂▂▂▂▅▁▄▅▄▅▆▆▅▆▅▇▇▆▇▇▆▆▆▆▆▇▇▇▅▇▆▇▆▇▆▇▇▇█</td></tr><tr><td>train_loss_epoch</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▂▂▂▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▁▂▂▁▂▃▂▃▂▂▄▄▂▄▂▂▅▂▂▅▂▆▆▃▆▆▃▇▇▃▇█▃██▃</td></tr><tr><td>val_acc</td><td>▁▁▄▂▄▆▅▆▆▅▇█▇█▇▇▇▆▅▆▆█▇▇▆▇▇▇▇▇▇▇▆▆▇▆▇▇▇▆</td></tr><tr><td>val_auc</td><td>▁▁▃▂▃▆▅▅▅▄▆█▆█▇█▆▆▇▆▆█▇▇▆▇▆▆█▇▇▇▆▇▇█▇▇▇▅</td></tr><tr><td>val_f1</td><td>▁▁▄▂▄▆▅▅▆▅▇█▇█▇█▇▆█▆▆█▇▇▇▇▇▇█▇▇▇▇█▇█▇▇▇▆</td></tr><tr><td>val_loss_epoch</td><td>█▅▃▃▃▃▃▂▁▃▃▂▂▃▃▃▂▁▃▂▂▃▂▂▃▂▁▃▃▃▁▂▂▂▂▂▂▂▂▂</td></tr><tr><td>val_loss_step</td><td>▆█▆▄▆▅▅▅▅▅▄▅▄▅▅▅▄▁▄▅▄▅▃▅▅▅▆▄▇▄▃▅▅▃▅▄▂▅▅▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.67367</td></tr><tr><td>train_auc</td><td>0.67089</td></tr><tr><td>train_f1</td><td>0.62696</td></tr><tr><td>train_loss_epoch</td><td>0.61355</td></tr><tr><td>train_loss_step</td><td>0.53744</td></tr><tr><td>trainer/global_step</td><td>899</td></tr><tr><td>val_acc</td><td>0.69343</td></tr><tr><td>val_auc</td><td>0.63887</td></tr><tr><td>val_f1</td><td>0.48148</td></tr><tr><td>val_loss_epoch</td><td>0.56988</td></tr><tr><td>val_loss_step</td><td>0.58359</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_32_onehot_2</strong> at: <a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/44f4kevu' target=\"_blank\">https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/44f4kevu</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230704_172914-44f4kevu\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc8a47a2d7bd4d3f81bdca670e5ba03f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Thomas\\15_PLA\\notebooks\\wandb\\run-20230704_175310-q6sku2ik</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/q6sku2ik' target=\"_blank\">MLP_2_32_onehot_2</a></strong> to <a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/q6sku2ik' target=\"_blank\">https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/q6sku2ik</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 1.4 K \n",
      "2  | head        | Sequential       | 562   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 33    \n",
      "--------------------------------------------------\n",
      "2.0 K     Trainable params\n",
      "0         Non-trainable params\n",
      "2.0 K     Total params\n",
      "0.008     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "905a424c3b6344ebbff12988d1758a81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▁▂▄▆▇▆▆▆▆▅▆▆▇▇▆▇▇▇▇█▇▆▇▇▇▆▇▇▇▇▆▆▇▆▆▇█▇</td></tr><tr><td>train_auc</td><td>▁▁▁▂▅▆▇▇▆▆▆▆▇▆▇▇▇▇▇▇▇█▇▇█▇█▆▇██▇▇▇█▆▇▇██</td></tr><tr><td>train_f1</td><td>▁▁▁▂▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇██▇▇███▇▇██▇▇▇█▇▇▇██</td></tr><tr><td>train_loss_epoch</td><td>██▇▆▅▄▃▄▄▃▂▃▂▂▂▂▂▂▂▂▃▂▂▂▂▂▁▃▁▂▂▂▃▂▁▂▂▂▁▂</td></tr><tr><td>train_loss_step</td><td>▆▇▆▆█▆▄▆▄▃▅▄▄▄▇▄▄▁▅▅▄▅▇▄▅▃▆▅▅▅▅▄▅▅▇▃▅▃▂▄</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▁▂▂▁▂▃▂▃▂▂▄▄▂▄▂▂▅▂▂▅▂▆▆▃▆▆▃▇▇▃▇█▃██▃</td></tr><tr><td>val_acc</td><td>▁▁▁▃▃▄▄▄▅▆▅▆▆▇▇▇▅▇▇▆▇▇▇▇▇▆▇▇█▅▇█▇▇▇█▆█▇▇</td></tr><tr><td>val_auc</td><td>▁▁▁▃▂▃▄▃▅▅▅▅▅▆▇█▅▇▇▇▇▆▇▇▇▅▇▇█▅▇█▆▇▇▇▆▇█▇</td></tr><tr><td>val_f1</td><td>▁▁▁▃▃▃▄▄▅▆▅▆▅▆▇█▅▇▇▇▇▇█▇▇▆▇▇█▅▇█▇▇▇▇▆▇█▇</td></tr><tr><td>val_loss_epoch</td><td>██▇▆▇▄▃▄▂▂▄▄▂▄▃▃▂▂▃▃▂▂▂▂▁▄▂▄▂▄▄▃▃▁▃▂▂▃▂▃</td></tr><tr><td>val_loss_step</td><td>▇▆▇▆▇▆▃▄▅█▅▅▄▄▄▄▄▄▄▅▄▄▁▄▄▅▅▅▂▅▂▄▄▁▃▃▁▃▄▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.69378</td></tr><tr><td>train_auc</td><td>0.66889</td></tr><tr><td>train_f1</td><td>0.58488</td></tr><tr><td>train_loss_epoch</td><td>0.60697</td></tr><tr><td>train_loss_step</td><td>0.58263</td></tr><tr><td>trainer/global_step</td><td>899</td></tr><tr><td>val_acc</td><td>0.70438</td></tr><tr><td>val_auc</td><td>0.65813</td></tr><tr><td>val_f1</td><td>0.53179</td></tr><tr><td>val_loss_epoch</td><td>0.56136</td></tr><tr><td>val_loss_step</td><td>0.56331</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_32_onehot_2</strong> at: <a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/q6sku2ik' target=\"_blank\">https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/q6sku2ik</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230704_175310-q6sku2ik\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "694d94948e0d4c78aaafac890f6e68ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01693333333338766, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Thomas\\15_PLA\\notebooks\\wandb\\run-20230704_182001-g9rmemqc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/g9rmemqc' target=\"_blank\">MLP_2_32_onehot_2</a></strong> to <a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/g9rmemqc' target=\"_blank\">https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/g9rmemqc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 1.4 K \n",
      "2  | head        | Sequential       | 562   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 1.1 K \n",
      "--------------------------------------------------\n",
      "3.0 K     Trainable params\n",
      "0         Non-trainable params\n",
      "3.0 K     Total params\n",
      "0.012     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "558a9b6c3593496f85d6984769ba5095",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▂▂▆▅▆▆▇▆▇▇▇▇▇█▆▇█▇▇▇█▇▇▇▇▇▇▇▇█▇▇▇▇▆▇▇█</td></tr><tr><td>train_auc</td><td>▁▁▂▂▆▆▅▆▇▇▇▇▇▇▇▇▇▇▇▆▇▇▇▇▇▇▇▇▇▇▇█▇▇▇▇▆▇▇█</td></tr><tr><td>train_f1</td><td>▁▁▂▃▇▇▆▇▇▇▇▇▇▇▇█▇▇█▇▇▇▇▇▇▇▇▇▇█▇█▇▇██▇█▇█</td></tr><tr><td>train_loss_epoch</td><td>██▇▆▄▃▃▃▂▃▂▂▂▂▂▂▃▃▂▂▂▂▁▁▂▂▂▂▂▁▂▂▂▂▂▁▂▂▂▁</td></tr><tr><td>train_loss_step</td><td>█▇▇▇▄▄▅▃▄▄▆▄▅▅▅▃▄▃▃▃▃▆▃▅▄▃▆▂▅▆▃▃▄▅▄▃▃▅▃▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▁▂▂▁▂▃▂▃▂▂▄▄▂▄▂▂▅▂▂▅▂▆▆▃▆▆▃▇▇▃▇█▃██▃</td></tr><tr><td>val_acc</td><td>▁▁▁▆▇▇▇▇▇▇██▇▇▇█▇▇▆████▇█▆▇█▇▇▆▇▇▇▇▇▇▆▇▇</td></tr><tr><td>val_auc</td><td>▁▁▁▅▇▆▆▆▆▆▇▇▇▇▇▇▆▆▆▇▇█▇▆▇█▆▇▇▇▆▆▇▆▆▇▇▆▇▆</td></tr><tr><td>val_f1</td><td>▁▁▁▅▇▆▇▇▇▇▇█▇▇▇▇▇▆▇█▇██▇▇█▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>val_loss_epoch</td><td>█▇▆▆▃▃▃▃▂▂▄▄▃▂▂▂▂▅▅▃▄▃▅▃▃▄▂▅▂▃▄▅▂▁▃▂▃▃▄▄</td></tr><tr><td>val_loss_step</td><td>▇▇▇▅▄▄▄▄▅▆▄▅▅▄▃▄▅█▄▄▄▅▇▅▃▅▄▅▂▄▄▄▄▁▅▄▄▄▄▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.70201</td></tr><tr><td>train_auc</td><td>0.68232</td></tr><tr><td>train_f1</td><td>0.6119</td></tr><tr><td>train_loss_epoch</td><td>0.59692</td></tr><tr><td>train_loss_step</td><td>0.53018</td></tr><tr><td>trainer/global_step</td><td>899</td></tr><tr><td>val_acc</td><td>0.69708</td></tr><tr><td>val_auc</td><td>0.65487</td></tr><tr><td>val_f1</td><td>0.53631</td></tr><tr><td>val_loss_epoch</td><td>0.5922</td></tr><tr><td>val_loss_step</td><td>0.66979</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_32_onehot_2</strong> at: <a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/g9rmemqc' target=\"_blank\">https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/g9rmemqc</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230704_182001-g9rmemqc\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce998cb2bc9141c997febb9cd5d55c70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Thomas\\15_PLA\\notebooks\\wandb\\run-20230704_184845-gdy9c5ap</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/gdy9c5ap' target=\"_blank\">MLP_2_32_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/gdy9c5ap' target=\"_blank\">https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/gdy9c5ap</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 1.4 K \n",
      "2 | head        | Sequential       | 562   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.008     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09e3993d4e3c4891b0df0da54bdf42ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▁▂▆▆▆▆▇▇▇▇█▇▆▆██▇▇█▆█▆▆▇▇▇▇▇▇▆▇▆█▇▇▇▇▇</td></tr><tr><td>train_auc</td><td>▁▁▁▂▆▆▅▆▆▇▇▇██▆▆█▇▇▇█▆█▆▆▇▇▇▇▇█▆▇▆▇▇▆▆▇▇</td></tr><tr><td>train_f1</td><td>▁▁▁▂▇▇▆▇▇▇█▇██▇▇██▇██▆█▇▆▇▇▇▇▇█▇▇▇██▇▇▇█</td></tr><tr><td>train_loss_epoch</td><td>██▇▇▄▄▃▃▃▃▃▃▂▂▃▂▂▂▁▃▂▂▂▂▂▂▂▃▂▂▂▂▂▃▂▂▃▂▃▂</td></tr><tr><td>train_loss_step</td><td>█▇▇█▄▃▄▃▃▄▅▅▄▄▃▅▅▂▄▂▂▄▃▃▆▂▄▅▁▄▃▁▃▄▃▃▄▄▁▄</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▁▂▂▁▂▃▂▃▂▂▄▄▂▄▂▂▅▂▂▅▂▆▆▃▆▆▃▇▇▃▇█▃██▃</td></tr><tr><td>val_acc</td><td>▁▁▁▃▅▇▇▇▆▆▆▅▇▇▇▇▇▇▇▇▇▇▇▇▇▆▇▇▇▇▇▇█▇▇▇▇▇▇█</td></tr><tr><td>val_auc</td><td>▁▁▁▃▅▇▇▇▇▆▆▅▇▇▇█▇▇▇▇▇█▇▇█▆█▇██▇████▇██▇█</td></tr><tr><td>val_f1</td><td>▁▁▁▃▆▇▇▇▇▇▆▆▇▇▇█▇▇▇▇▇██▇█▇████▇████▇██▇█</td></tr><tr><td>val_loss_epoch</td><td>█▇▇▅▄▃▄▄▄▃▅▄▅▃▃▃▅▇▂▃▃▂▃▃▁▃▄▅▂▂▃▃▃▂▃▄▃▂▂▄</td></tr><tr><td>val_loss_step</td><td>▄▄▄▂▂▂▂▂▂▃▂▃▁▁▂▁▁█▁▂▂▂▃▂▂▁▂▁▁▂▂▂▂▁▂▂▁▁▁▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.69132</td></tr><tr><td>train_auc</td><td>0.66161</td></tr><tr><td>train_f1</td><td>0.56667</td></tr><tr><td>train_loss_epoch</td><td>0.59917</td></tr><tr><td>train_loss_step</td><td>0.6146</td></tr><tr><td>trainer/global_step</td><td>899</td></tr><tr><td>val_acc</td><td>0.73993</td></tr><tr><td>val_auc</td><td>0.71024</td></tr><tr><td>val_f1</td><td>0.62827</td></tr><tr><td>val_loss_epoch</td><td>0.59226</td></tr><tr><td>val_loss_step</td><td>0.66975</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_32_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/gdy9c5ap' target=\"_blank\">https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/gdy9c5ap</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230704_184845-gdy9c5ap\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "567da1de267946bf8b56144d27bf6326",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01693333333338766, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Thomas\\15_PLA\\notebooks\\wandb\\run-20230704_191710-gl7d5wlp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/gl7d5wlp' target=\"_blank\">MLP_2_32_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/gl7d5wlp' target=\"_blank\">https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/gl7d5wlp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 1.4 K \n",
      "2 | head        | Sequential       | 562   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.008     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfda021ceac742098add112ff1453060",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▃▃▃▄▅▅▄▅▆▆▇▆▆▆▆▇▆▇▅▇▆█▆▇▆▇▇▆▇▇▇▇▇█▇▇▇▇▆</td></tr><tr><td>train_auc</td><td>▁▁▁▁▃▄▄▄▅▆▆▇▆▇▇▆█▆▇▆▇▆█▆▇▆▇▇▆▇▇▆▇▇█▇▆▇▇▇</td></tr><tr><td>train_f1</td><td>▄▁▁▁▃▅▅▆▆▇▇█▇█▇▇█▇▇▇▇▇█▆▇▇█▇▇▇█▇▇▇██▇█▇█</td></tr><tr><td>train_loss_epoch</td><td>█▆▆▆▅▅▄▄▄▂▄▃▃▃▃▂▂▃▂▃▂▃▂▂▁▃▂▂▂▂▂▃▂▂▂▂▃▂▂▂</td></tr><tr><td>train_loss_step</td><td>█▅▅▆▅▃▄▂▃▅▄▂▂▆▂▄▂▅▄▄▁▃▃▂▃▄▃▄▂▅▄▃▂▄▅▅▅▁▃▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▁▂▂▁▂▃▂▃▂▂▄▄▂▄▂▂▅▂▂▅▂▆▆▃▆▆▃▇▇▃▇█▃██▃</td></tr><tr><td>val_acc</td><td>▅▅▅▅█▇▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>▁▁▁▁▇█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_f1</td><td>▁▁▁▁▇█▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>val_loss_epoch</td><td>▁▁▁▁▂▁▂▂▂▅▄▄▃▅▅▅▅▄▆▅▄▄▆▅▆▄▅▄▇▇▅▅█▆▇█▇▇▇▇</td></tr><tr><td>val_loss_step</td><td>▂▂▂▁▂▂▃▃▃▂▅▂▆▆▆▅▇▁▇▄▅▅█▆▆▃▅▆▇▆▂▅▅▇▇██▆▆▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.65023</td></tr><tr><td>train_auc</td><td>0.61819</td></tr><tr><td>train_f1</td><td>0.50453</td></tr><tr><td>train_loss_epoch</td><td>0.62538</td></tr><tr><td>train_loss_step</td><td>0.60122</td></tr><tr><td>trainer/global_step</td><td>899</td></tr><tr><td>val_acc</td><td>0.42125</td></tr><tr><td>val_auc</td><td>0.5</td></tr><tr><td>val_f1</td><td>0.59278</td></tr><tr><td>val_loss_epoch</td><td>0.9101</td></tr><tr><td>val_loss_step</td><td>0.86815</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_32_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/gl7d5wlp' target=\"_blank\">https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/gl7d5wlp</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230704_191710-gl7d5wlp\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96cf52814b0c47e69cb2f560323ce0c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Thomas\\15_PLA\\notebooks\\wandb\\run-20230704_194714-1vlxn0xz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/1vlxn0xz' target=\"_blank\">MLP_2_32_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/1vlxn0xz' target=\"_blank\">https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/1vlxn0xz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 1.4 K \n",
      "2 | head        | Sequential       | 562   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.008     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3992888e797e449391a04e1f95c38cd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▃▄▅▅▅▆▅▆▇▆▇▇▇█▇██▇▇▆▇██▇▇██▇▇██▇▇▇███▇</td></tr><tr><td>train_auc</td><td>▁▃▃▃▄▄▅▆▅▅▇▆▇█▇████▇█▇██▇████▇███▇▇▇███▇</td></tr><tr><td>train_f1</td><td>▃▄▄▁▂▃▄▆▄▅█▇██▆▇██▇██▇█▇▇▇███▇█▇▇▇▇████▇</td></tr><tr><td>train_loss_epoch</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▁▂▂▁▂▃▂▃▂▂▄▄▂▄▂▂▅▂▂▅▂▆▆▃▆▆▃▇▇▃▇█▃██▃</td></tr><tr><td>val_acc</td><td>▁▁▃▅▄▅▄▃▆▃▅▄▄▄▇▇▅▇▇▅▄▇▅▇▇▅▅██▆▄▇▇██▅▇▇▆█</td></tr><tr><td>val_auc</td><td>▁▁▂▅▄▅▄▃▆▃▅▄▄▄▇▇▅▇▇▅▄▇▅▇█▅▅██▆▄████▅▇▇▆█</td></tr><tr><td>val_f1</td><td>▁▂▃▅▅▆▄▄▇▃▅▅▅▅▇▇▆▇▇▆▅▇▆▇█▆▆██▇▅████▆▇▇▇█</td></tr><tr><td>val_loss_epoch</td><td>█▂▂▂▂▂▂▂▂▂▂▂▃▂▁▁▂▂▁▂▁▁▂▁▁▁▂▂▁▁▂▂▁▁▁▁▁▁▁▂</td></tr><tr><td>val_loss_step</td><td>█▅▄▁▃▃▂▃▂▄▃▃▂▂▁▂▂▇▂▁▂▂▄▂▂▂▃▂▁▃▂▂▂▂▂▂▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.66393</td></tr><tr><td>train_auc</td><td>0.63944</td></tr><tr><td>train_f1</td><td>0.55012</td></tr><tr><td>train_loss_epoch</td><td>0.60704</td></tr><tr><td>train_loss_step</td><td>0.60869</td></tr><tr><td>trainer/global_step</td><td>899</td></tr><tr><td>val_acc</td><td>0.73993</td></tr><tr><td>val_auc</td><td>0.7197</td></tr><tr><td>val_f1</td><td>0.657</td></tr><tr><td>val_loss_epoch</td><td>0.57637</td></tr><tr><td>val_loss_step</td><td>0.59483</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_32_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/1vlxn0xz' target=\"_blank\">https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/1vlxn0xz</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230704_194714-1vlxn0xz\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75bfbd8b7042453bace096f490d39462",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01693333333338766, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Thomas\\15_PLA\\notebooks\\wandb\\run-20230704_201716-v8eiemag</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/v8eiemag' target=\"_blank\">MLP_2_32_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/v8eiemag' target=\"_blank\">https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/v8eiemag</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 1.4 K \n",
      "2  | head        | Sequential       | 562   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 33    \n",
      "--------------------------------------------------\n",
      "2.0 K     Trainable params\n",
      "0         Non-trainable params\n",
      "2.0 K     Total params\n",
      "0.008     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "652dfa3ab119422797acebd623403b6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▁▂▆▆▇▆▆▆▆▇▇▇▆▆▆▇▆▅▇▇█▆▇▆▆▇▆▇▇▆▇▇▆▇▇▆▇▆</td></tr><tr><td>train_auc</td><td>▁▁▁▂▆▆▇▇▇▆▆▇▇▇▇▇▆█▇▅▇▇█▆▇▇▇▇▆▇▇▇▇▇▇▇▇▆▇▆</td></tr><tr><td>train_f1</td><td>▁▁▁▃▇▇▇▇▇▇▇█▇█▇▇▇█▇▆▇▇█▇▇██▇▇█████▇██▇█▇</td></tr><tr><td>train_loss_epoch</td><td>███▆▄▃▂▃▃▃▂▂▃▂▂▂▃▂▁▃▂▁▁▂▁▂▂▁▂▁▂▃▁▁▂▂▁▁▂▃</td></tr><tr><td>train_loss_step</td><td>▆▇█▅▇▃▆▃▄▄▆▇▅▂▃▅▁▆▆▃▃▂▁█▄▄▂▄▁▃▅▃▅▃▄▂▆▄▅▆</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▁▂▂▁▂▃▂▃▂▂▄▄▂▄▂▂▅▂▂▅▂▆▆▃▆▆▃▇▇▃▇█▃██▃</td></tr><tr><td>val_acc</td><td>▁▁▁▂▄▅▅▅▅▇▅▆▆▅█▆▇▇▆▆▇▆▇▇▇▄▆█▇▇█▅▅▆█▆▇▆▆█</td></tr><tr><td>val_auc</td><td>▁▁▁▂▄▄▄▄▄▇▄▆▆▅▇▆▇▇▆▆▆▆▆▆▆▄▆█▇▆▇▄▅▆█▅▇▆▆█</td></tr><tr><td>val_f1</td><td>▁▁▁▃▄▅▅▅▅▇▅▆▆▆▇▆▇▇▆▆▇▆▇▇▇▅▆█▇▇▇▅▅▆█▆▇▇▆█</td></tr><tr><td>val_loss_epoch</td><td>██▇▆▇▃▃▃▅▃▅▄▄▅▂▃▂▂▄▄▃▃▁▃▂▄▂▂▂▂▂▄▄▄▁▁▃▃▄▅</td></tr><tr><td>val_loss_step</td><td>██▇▅▆▄▂▅▃▇▇▅▆▃▁▄▅▃▅▄▅▄▁▄▅▅▅▅▄▄▂▆▆▆▄▅▅▅▆█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.6621</td></tr><tr><td>train_auc</td><td>0.61342</td></tr><tr><td>train_f1</td><td>0.44444</td></tr><tr><td>train_loss_epoch</td><td>0.6197</td></tr><tr><td>train_loss_step</td><td>0.6575</td></tr><tr><td>trainer/global_step</td><td>899</td></tr><tr><td>val_acc</td><td>0.72527</td></tr><tr><td>val_auc</td><td>0.70823</td></tr><tr><td>val_f1</td><td>0.64789</td></tr><tr><td>val_loss_epoch</td><td>0.61153</td></tr><tr><td>val_loss_step</td><td>0.70812</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_32_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/v8eiemag' target=\"_blank\">https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/v8eiemag</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230704_201716-v8eiemag\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b85809ec4594405a649eb17d9604437",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Thomas\\15_PLA\\notebooks\\wandb\\run-20230704_204430-9kgezo8d</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/9kgezo8d' target=\"_blank\">MLP_2_32_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/9kgezo8d' target=\"_blank\">https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/9kgezo8d</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 1.4 K \n",
      "2  | head        | Sequential       | 562   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 1.1 K \n",
      "--------------------------------------------------\n",
      "3.0 K     Trainable params\n",
      "0         Non-trainable params\n",
      "3.0 K     Total params\n",
      "0.012     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d320deff3bee4a139bfff3d3de8c36f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▁▂▅▆▆▆▆█▇▆▇█▇█▇▆▇█▇▇▇██▇▇█▇▆▇▇▆▇▆█▇▇▇▇</td></tr><tr><td>train_auc</td><td>▁▁▁▂▆▆▆▆▆█▇▇██▇█▇▆▇█▇▇▆█▇▇▇█▇▇▇▇▆▇▇██▇█▇</td></tr><tr><td>train_f1</td><td>▁▁▁▃▇▇▇▇▇█▇▇██▇█▇▇▇█▇▇▇█▇▇▇█▇▇▇▇▇▇█▇█▇██</td></tr><tr><td>train_loss_epoch</td><td>██▇▆▃▃▂▃▁▂▃▁▂▂▂▁▃▂▂▂▂▃▂▁▂▂▂▁▂▂▁▁▂▁▂▁▂▂▂▂</td></tr><tr><td>train_loss_step</td><td>▇▇█▇▆▄▃▄▆▄▅▁▁▄▆▄▃▂▁▂▂▁▃▄▅▃▅▂▂▃▂▃▆▃▄▆▂▃▆▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▁▂▂▁▂▃▂▃▂▂▄▄▂▄▂▂▅▂▂▅▂▆▆▃▆▆▃▇▇▃▇█▃██▃</td></tr><tr><td>val_acc</td><td>▁▁▁▄▇▅▇▇▇▆▇▇▇▇▇▇▇▇█▇█▇▇▇██▇▇▇██▇▇▇▇█▇▆▇█</td></tr><tr><td>val_auc</td><td>▁▁▁▄▇▅▆▇▇▆▇▇▇▇█▇▇██▆█▇▇███▇▇▇███▇█▇█▇▆▇█</td></tr><tr><td>val_f1</td><td>▁▁▁▅▇▆▆▇▇▆▇▇▇▇█▇▇██▆█▇▇███▇▇▇███▇█▇█▇▆▇█</td></tr><tr><td>val_loss_epoch</td><td>█▇█▅▂▃▂▃▃▄▂▃▃▂▄▃▄▃▄▂▅▄▂▁▁▃▄▃▃▃▄▄▄▁▄▂▂▄▁▂</td></tr><tr><td>val_loss_step</td><td>███▆▆▆▁▇▇▅▆▆▇▆▆▆▅▆▄▅▅▅▃▆▅▄▅▆▆▅▇▅▅▁▄▆▃▆▆▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.68493</td></tr><tr><td>train_auc</td><td>0.65328</td></tr><tr><td>train_f1</td><td>0.55137</td></tr><tr><td>train_loss_epoch</td><td>0.6098</td></tr><tr><td>train_loss_step</td><td>0.59224</td></tr><tr><td>trainer/global_step</td><td>899</td></tr><tr><td>val_acc</td><td>0.72527</td></tr><tr><td>val_auc</td><td>0.70113</td></tr><tr><td>val_f1</td><td>0.62687</td></tr><tr><td>val_loss_epoch</td><td>0.54755</td></tr><tr><td>val_loss_step</td><td>0.51245</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_32_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/9kgezo8d' target=\"_blank\">https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/9kgezo8d</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230704_204430-9kgezo8d\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f257d0b3f00e4e08bd918df9de0b2a70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01693333333338766, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Thomas\\15_PLA\\notebooks\\wandb\\run-20230704_211054-hysuzkc7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/hysuzkc7' target=\"_blank\">MLP_2_32_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/hysuzkc7' target=\"_blank\">https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/hysuzkc7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 1.4 K \n",
      "2 | head        | Sequential       | 562   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.008     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "312f785592ae4794a0b4ccfef48ae352",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▁▂▆▇█▇▇▇▇▇▇█▇█▇██▇█▇▇▇▇██▇▇▇█▇██▇▇▇▇█▇</td></tr><tr><td>train_auc</td><td>▁▁▁▂▆▇▇▇▇▇▇▇██▇▇▇▇███▇▇▆▆▇█▇▇▇█▇▇▇▆▇▇▇█▇</td></tr><tr><td>train_f1</td><td>▁▁▁▃▇▇▇▇▇█▇▇██▇▇█████▇█▇▇▇██▇▇█▇▇▇▇▇▇▇█▇</td></tr><tr><td>train_loss_epoch</td><td>██▇▆▃▃▂▂▃▂▂▃▃▁▁▁▂▁▁▁▂▂▂▂▂▁▁▂▁▁▂▂▁▁▂▁▁▁▁▂</td></tr><tr><td>train_loss_step</td><td>█▇█▇▄▄▇▂▄▆▄▆▂▄▂▅▄▄▅▃▆█▃▆▅▂▅▅▃▆▃▄▄▄▁▃▆▄▂▅</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▁▂▂▁▂▃▂▃▂▂▄▄▂▄▂▂▅▂▂▅▂▆▆▃▆▆▃▇▇▃▇█▃██▃</td></tr><tr><td>val_acc</td><td>▁▁▁▇▇▆▇█▇▇▇▆▇▇▇█▇▇█▇▆█▇▇▇▇▇▇█▇▇▇▇██▇▇█▆▇</td></tr><tr><td>val_auc</td><td>▁▁▁▇▇▆▇██▇▇▆▆▇▇█▇▇█▇▆█▇▇█▆▇▇█▇▇▇▇██▇▇█▆▇</td></tr><tr><td>val_f1</td><td>▁▁▁█▇▆▇██▇▇▆▇▇▇█▇▇█▇▇█▇██▇▇▇█▇▇▇▇██▇▇█▇▇</td></tr><tr><td>val_loss_epoch</td><td>▇▆▆▅▃▃▅▅▅▁▃▆▅▂▃▄▃▆▃▄▅▄▅▂▃▂▃█▅▄▅▂▂▂▄▃▆▄▄▃</td></tr><tr><td>val_loss_step</td><td>▆▆▆▅▅▄▆▄▄▄▄█▃▄▃▄▃█▄▄▃▄▆▄▄▁▅▄▇▄▁▅▄▁▅▄█▄▄▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.68858</td></tr><tr><td>train_auc</td><td>0.6547</td></tr><tr><td>train_f1</td><td>0.55072</td></tr><tr><td>train_loss_epoch</td><td>0.59028</td></tr><tr><td>train_loss_step</td><td>0.61984</td></tr><tr><td>trainer/global_step</td><td>899</td></tr><tr><td>val_acc</td><td>0.68864</td></tr><tr><td>val_auc</td><td>0.6674</td></tr><tr><td>val_f1</td><td>0.58128</td></tr><tr><td>val_loss_epoch</td><td>0.59216</td></tr><tr><td>val_loss_step</td><td>0.54557</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_32_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/hysuzkc7' target=\"_blank\">https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/hysuzkc7</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230704_211054-hysuzkc7\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d99ae19b524472ab7b93e63ace64393",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Thomas\\15_PLA\\notebooks\\wandb\\run-20230704_213833-oi8z6155</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/oi8z6155' target=\"_blank\">MLP_2_32_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/oi8z6155' target=\"_blank\">https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/oi8z6155</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 1.4 K \n",
      "2 | head        | Sequential       | 562   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.008     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8209085323054a8394e0f1c1e025f66d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▃▃▃▅▆▇▆▇▆▇▆▆▇▇▇▇▇█▆▆▆█▇█▇█▇▇██▇▇▇▇▇█▇█▇</td></tr><tr><td>train_auc</td><td>▁▂▁▂▄▆▆▅▆▆▇▆▇▇▇▇▇▇█▇▆▆█▆▇▇▇▇▇▇█▇▇▇▇▇▇▇█▇</td></tr><tr><td>train_f1</td><td>▃▁▁▁▅▇▇▆▆▆█▇▇█▇██▇██▇▇█▇▇█▇▇▇▇█▇▇▇▇█▇▇█▇</td></tr><tr><td>train_loss_epoch</td><td>█▆▆▆▄▅▂▃▃▃▃▃▃▂▂▂▃▃▂▃▃▂▂▂▂▂▁▂▂▁▂▂▁▂▁▂▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▅▆▆▄▄▅▃▃▆▅▅▃▄▂▄▆▃▃▃▆▆▃▇▄▄▅▄▄▅▄▄▆▃▃▄▆▃▁▄</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▁▂▂▁▂▃▂▃▂▂▄▄▂▄▂▂▅▂▂▅▂▆▆▃▆▆▃▇▇▃▇█▃██▃</td></tr><tr><td>val_acc</td><td>▇▇▇▇██▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>▁▁▁▁██▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_f1</td><td>▁▁▁▁████████████████████████████████████</td></tr><tr><td>val_loss_epoch</td><td>▁▁▁▁▁▁▂▂▂▂▂▂▂▄▄▄▂▃▄▄▄▃▄▅▃▃▄▃▅▅▅▅█▅▆▆▆▇▆▅</td></tr><tr><td>val_loss_step</td><td>▃▃▃▂▃▃▂▃▃▃▄▃▅▆▅▄▄▁▅▄▅▅▄▆▅▃▄▄▅▅▃▅▆▅▆▇█▆▆▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.66575</td></tr><tr><td>train_auc</td><td>0.62189</td></tr><tr><td>train_f1</td><td>0.48011</td></tr><tr><td>train_loss_epoch</td><td>0.61387</td></tr><tr><td>train_loss_step</td><td>0.61438</td></tr><tr><td>trainer/global_step</td><td>899</td></tr><tr><td>val_acc</td><td>0.43956</td></tr><tr><td>val_auc</td><td>0.5</td></tr><tr><td>val_f1</td><td>0.61069</td></tr><tr><td>val_loss_epoch</td><td>0.85546</td></tr><tr><td>val_loss_step</td><td>0.78805</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_32_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/oi8z6155' target=\"_blank\">https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/oi8z6155</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230704_213833-oi8z6155\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdb91810ae9a495f8b60507a3c39ad37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Thomas\\15_PLA\\notebooks\\wandb\\run-20230704_220326-vtydw2vg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/vtydw2vg' target=\"_blank\">MLP_2_32_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/vtydw2vg' target=\"_blank\">https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/vtydw2vg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 1.4 K \n",
      "2 | head        | Sequential       | 562   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.008     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "475cc09bbfdc4622b899642b0f418eb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▃▃▄▅▅▅▆▆▆▅▆▇▇▇▇▇▆▇▇▆▇█▇▇▇▇▇▇▇█▇▆▇▇▇█▇▇</td></tr><tr><td>train_auc</td><td>▁▂▃▁▃▄▅▅▅▆▆▆▆▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇▇▆▇▇▇██▇</td></tr><tr><td>train_f1</td><td>▃▄▄▁▄▄▄▅▅▆▅▆▇▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇▆▆▇▇▇▇▇▇</td></tr><tr><td>train_loss_epoch</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▂▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▁▂▂▁▂▃▂▃▂▂▄▄▂▄▂▂▅▂▂▅▂▆▆▃▆▆▃▇▇▃▇█▃██▃</td></tr><tr><td>val_acc</td><td>▁▁▂▂▂▂▄▅▇▆▆▅▅▅▇▅▆▇▇▇▆▇▆█▇▇▇█▇▇▅▇█▇▇▇██▆▇</td></tr><tr><td>val_auc</td><td>▁▁▂▂▂▂▄▅▇▆▆▅▅▅▇▅▆▇▇▇▆▇▆█▇▇▆██▆▅▇██▇▇██▆▇</td></tr><tr><td>val_f1</td><td>▁▁▄▂▃▃▅▅█▇▇▅▆▆█▆▆▇▇█▇▇▇██▇▇██▇▆▇███▇██▇▇</td></tr><tr><td>val_loss_epoch</td><td>█▂▂▁▂▂▁▁▂▁▁▂▂▁▁▂▁▂▁▂▁▁▁▁▁▁▁▂▂▁▂▁▁▁▁▂▂▁▁▁</td></tr><tr><td>val_loss_step</td><td>█▅▄▂▃▃▃▃▃▃▃▅▃▃▃▄▃▅▃▃▃▃▄▃▃▁▄▃▅▃▃▃▃▂▄▃▄▃▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.68402</td></tr><tr><td>train_auc</td><td>0.6599</td></tr><tr><td>train_f1</td><td>0.57702</td></tr><tr><td>train_loss_epoch</td><td>0.6098</td></tr><tr><td>train_loss_step</td><td>0.60429</td></tr><tr><td>trainer/global_step</td><td>899</td></tr><tr><td>val_acc</td><td>0.7033</td></tr><tr><td>val_auc</td><td>0.68227</td></tr><tr><td>val_f1</td><td>0.60099</td></tr><tr><td>val_loss_epoch</td><td>0.60613</td></tr><tr><td>val_loss_step</td><td>0.59189</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_32_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/vtydw2vg' target=\"_blank\">https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/vtydw2vg</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230704_220326-vtydw2vg\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9129d814ce404891847b0597c1f35380",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01691666666592937, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Thomas\\15_PLA\\notebooks\\wandb\\run-20230704_222803-m2eobiek</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/m2eobiek' target=\"_blank\">MLP_2_32_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/m2eobiek' target=\"_blank\">https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/m2eobiek</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 1.4 K \n",
      "2  | head        | Sequential       | 562   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 33    \n",
      "--------------------------------------------------\n",
      "2.0 K     Trainable params\n",
      "0         Non-trainable params\n",
      "2.0 K     Total params\n",
      "0.008     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad04ee8b21fd43ef93e985ab3cc05df8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▂▃▇▆▆▆█▆▆▆█▇▆▇▆▆█▆▇████▆▇▇▇███▇▇▇█▇▇▇▇</td></tr><tr><td>train_auc</td><td>▁▁▂▄▇▆▆▆▇▆▆▆▇▇▇▇▆▆█▆▇▇▇██▆▇▇▇█▇▇▇█▇█▇██▇</td></tr><tr><td>train_f1</td><td>▁▁▂▅▇▇▇▇▇▇▆▇▇▇▇█▇▇█▆█▇▇██▇▇▇▇█▇█▇█▇████▇</td></tr><tr><td>train_loss_epoch</td><td>██▇▆▃▃▂▄▂▄▃▂▂▂▃▂▃▃▂▂▂▃▁▁▃▃▂▂▂▂▂▂▂▂▃▂▂▂▂▂</td></tr><tr><td>train_loss_step</td><td>▇███▅▅▄▇▆▅▆▄▆▃▄▅▄▇▄▄▅▃▃▁▃▂▅▆▄▄▄▃▆▄▄▅▄▄▃▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▁▂▂▁▂▃▂▃▂▂▄▄▂▄▂▂▅▂▂▅▂▆▆▃▆▆▃▇▇▃▇█▃██▃</td></tr><tr><td>val_acc</td><td>▁▁▁▂▄▆▄▆▄▇▄▆▆▅▆▆▇▇▄▇▄▄▆▆▇▆▆▇█▆▆▅▇▅▇▅▆▇▆█</td></tr><tr><td>val_auc</td><td>▁▁▁▂▃▆▄▅▃▇▄▆▆▅▆▅▇▇▄▇▄▄▆▆▆▆▆▇█▆▆▅▆▅▇▅▆▆▅█</td></tr><tr><td>val_f1</td><td>▁▁▁▃▄▆▅▆▄█▅▆▆▆▇▆▇▇▅█▅▄▇▇▇▆▆██▆▇▆▇▆▇▅▇▆▆█</td></tr><tr><td>val_loss_epoch</td><td>▇▇▆▄█▄▅▄▄▃▅▅▆▄▁▃▄▃▆▂▂▄▄▅▅▃▃▄▃▃▃▃▃▄▄▃▃▄▄▄</td></tr><tr><td>val_loss_step</td><td>▇▆▆▄▆▅▆▆▅█▆█▅▅▁▅▆▅▆▆▇▆▆▄▆▃▆▅▄▆▇▆▆▅▄▆▅▆▆▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.6895</td></tr><tr><td>train_auc</td><td>0.64305</td></tr><tr><td>train_f1</td><td>0.50292</td></tr><tr><td>train_loss_epoch</td><td>0.59096</td></tr><tr><td>train_loss_step</td><td>0.54006</td></tr><tr><td>trainer/global_step</td><td>899</td></tr><tr><td>val_acc</td><td>0.7033</td></tr><tr><td>val_auc</td><td>0.68946</td></tr><tr><td>val_f1</td><td>0.63014</td></tr><tr><td>val_loss_epoch</td><td>0.62721</td></tr><tr><td>val_loss_step</td><td>0.68746</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_32_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/m2eobiek' target=\"_blank\">https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/m2eobiek</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230704_222803-m2eobiek\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44b21e1a23734fc6bb799189821d4ffb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>Y:\\coskun-lab\\Thomas\\15_PLA\\notebooks\\wandb\\run-20230704_225230-ye6p5zhb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/ye6p5zhb' target=\"_blank\">MLP_2_32_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/ye6p5zhb' target=\"_blank\">https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/ye6p5zhb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 1.4 K \n",
      "2  | head        | Sequential       | 562   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 1.1 K \n",
      "--------------------------------------------------\n",
      "3.0 K     Trainable params\n",
      "0         Non-trainable params\n",
      "3.0 K     Total params\n",
      "0.012     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "330d2052d03f401882e642d79250c64b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▁▃▇▇▇▇▇█▆▇▇▇▇▇▇▇█▇▇▇█▇█████▇▇▇▇▇█▇█▇▇▇</td></tr><tr><td>train_auc</td><td>▁▁▁▃▆▇▇▆▇▇▆▇▇▇▇▇▇▇▇▇▇▇█▇▇████▇▇▇▇▇█▇█▇█▇</td></tr><tr><td>train_f1</td><td>▁▁▂▄▇▇█▇▇▇▇▇▇█▇▇█▇▇█▇▇██▇████▇▇█▇██▇█▇██</td></tr><tr><td>train_loss_epoch</td><td>█▇▇▆▄▃▄▃▃▂▂▂▂▂▂▂▂▁▁▂▁▂▂▁▂▁▂▂▁▂▂▁▂▁▂▂▂▂▁▂</td></tr><tr><td>train_loss_step</td><td>█▇█▆▅▄▅▄▆▄▄▂▁▄▇▅▂▂▁▃▅▆▂▄▄▆▆▁▃▅▆▄▅▅▃▃▁▃▄▄</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▁▂▂▁▂▃▂▃▂▂▄▄▂▄▂▂▅▂▂▅▂▆▆▃▆▆▃▇▇▃▇█▃██▃</td></tr><tr><td>val_acc</td><td>▁▁▁▄▇▅▆▇█▇▇█▇▇█▆▇▇▇▆▇█▆▇▇▆▆▆▇▇▇▇▇▇▇▇▆▇▇▇</td></tr><tr><td>val_auc</td><td>▁▁▁▄█▄▆██▇▇█▇▇█▆▇▇▇▆▇█▆█▇▆▆▆▇▇█▇▇█▇█▆▇▇▇</td></tr><tr><td>val_f1</td><td>▁▁▁▄█▅▆██▇██▇██▇▇██▇▇█▇█▇▇▇▆▇███████▇███</td></tr><tr><td>val_loss_epoch</td><td>█▇█▅▂▆▄▄▃▃▁▃▅▄▅▄▆▃▇▄▅▂▃▂▁▄▅▆▄▃▃▄▃▃▃▄▃▄▁▂</td></tr><tr><td>val_loss_step</td><td>▆▇▇▅▆▄▅▅▇▅▆▄▆▅█▅▅▅▃▅▅▄▃▆▅▅▃▆▅▄▆▃▅▅▃▇▃▆▄▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.69041</td></tr><tr><td>train_auc</td><td>0.65625</td></tr><tr><td>train_f1</td><td>0.55218</td></tr><tr><td>train_loss_epoch</td><td>0.59298</td></tr><tr><td>train_loss_step</td><td>0.58688</td></tr><tr><td>trainer/global_step</td><td>899</td></tr><tr><td>val_acc</td><td>0.68498</td></tr><tr><td>val_auc</td><td>0.66324</td></tr><tr><td>val_f1</td><td>0.57426</td></tr><tr><td>val_loss_epoch</td><td>0.58254</td></tr><tr><td>val_loss_step</td><td>0.51273</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_32_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/ye6p5zhb' target=\"_blank\">https://wandb.ai/thoomas/PLA_070323_9PPI_v2_Kfold/runs/ye6p5zhb</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230704_225230-ye6p5zhb\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### MLP\n",
    "k_folds = 5\n",
    "kfold = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "for fold, (train_ids, valid_ids) in enumerate(kfold.split(dataset_filtered)):\n",
    "    train_subset = dataset_filtered.index_select(train_ids.tolist())\n",
    "    val_subset = dataset_filtered.index_select(valid_ids.tolist())\n",
    "    \n",
    "    for pool in pools:\n",
    "        # Path to the folder where the pretrained models are saved\n",
    "        CHECKPOINT_PATH = checkpoint_folder / f'{NUM_LAYERS}_{HIDDEN_CHANNELS}_onehot_{fold}' / pool\n",
    "        CHECKPOINT_PATH.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Skip already trained kfold and pool\n",
    "        checkpoint = CHECKPOINT_PATH / \"GraphLevelMLP\" / \"GraphLevelMLP.ckpt\" \n",
    "        if checkpoint.exists():\n",
    "            print(checkpoint)\n",
    "            continue\n",
    "        \n",
    "        # Run training\n",
    "        run = wandb.init(project=project_name, name=f'MLP_{NUM_LAYERS}_{HIDDEN_CHANNELS}_onehot_{fold}', \n",
    "                        group=f'MLP_{pool}')\n",
    "        PPIGraph.train_graph_classifier_kfold('MLP', \n",
    "                                             train_subset, \n",
    "                                             val_subset, \n",
    "                                             dataset, \n",
    "                                             CHECKPOINT_PATH, \n",
    "                                             AVAIL_GPUS, \n",
    "                                             in_channels=9,\n",
    "                                             hidden_channels=HIDDEN_CHANNELS, \n",
    "                                             out_channels = HIDDEN_CHANNELS,\n",
    "                                             num_layers=NUM_LAYERS, \n",
    "                                             epochs=epochs,\n",
    "                                             batch_size=128,\n",
    "                                             embedding=False,\n",
    "                                             graph_pooling=pool)\n",
    "        run.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0eacd3-57dd-42e7-b4af-afb66864d4eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:snowflake]",
   "language": "python",
   "name": "conda-env-snowflake-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
