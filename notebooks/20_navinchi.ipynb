{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import skimage.io\n",
    "\n",
    "from collections import defaultdict\n",
    "from tqdm.notebook import trange, tqdm, tqdm_notebook\n",
    "from joblib import Parallel, delayed\n",
    "import re\n",
    "import h5py\n",
    "import tifffile as tiff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = (Path().cwd().parents[0] / 'data').absolute()\n",
    "data_processed = data_dir / 'processed'\n",
    "data_raw = r'Y:\\coskun-lab\\Shuangyi\\ERK, YAP project_2022\\PLA\\Navincin multiplex'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_dir = (Path().cwd().parents[0]).absolute()\n",
    "\n",
    "module_path = str(p_dir / \"src\")\n",
    " \n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "markers_map = {\n",
    "    'cycle1': {\n",
    "        1: 'DNA', \n",
    "        4: 'sox/oct4',\n",
    "    },\n",
    "    'cycle2': {\n",
    "        1: 'DNA', \n",
    "        4: 'b-catenin/e-cadherin',\n",
    "    },\n",
    "    'cycle3': {\n",
    "        1: 'DNA', \n",
    "        4: 'Cdc25c/p38',\n",
    "    },\n",
    "    'cycle4': {\n",
    "        1: 'DNA', \n",
    "        4: 'pyk2/src',\n",
    "    },\n",
    "    'cycle5': {\n",
    "        1: 'DNA', \n",
    "        4: 'p-Jak2/Stat3',\n",
    "    },\n",
    "    'cycle6': {\n",
    "        1: 'DNA', \n",
    "        2: 'P-EGFR',\n",
    "        3: 'Phalloidin', \n",
    "        4: 'KI67',\n",
    "    },\n",
    "}\n",
    "\n",
    "def get_info(data_raw, marker_dict):\n",
    "    conditions = []\n",
    "    fovs = []\n",
    "    cycles = []\n",
    "    channels = []\n",
    "    markers = []\n",
    "    paths = [] \n",
    "    \n",
    "    # Loop through image folder\n",
    "    for (dirpath, dirnames, filenames) in os.walk(data_raw):\n",
    "        if 'after nuclease' in dirpath or 'Test' in dirpath or 'wrong' in dirpath:\n",
    "            continue\n",
    "        \n",
    "        for name in sorted(filenames):\n",
    "            if \"tif\" in name and \"sti\" in name \\\n",
    "            and 'overlay' not in name \\\n",
    "            and 'Composite' not in name:\n",
    "                # Get information from image name\n",
    "                \n",
    "                d_split = dirpath.split('\\\\')\n",
    "                condition = d_split[-1].split('_')[1]\n",
    "                n_split = name.split('_')\n",
    "                ch = int(n_split[-1][-5])\n",
    "\n",
    "                cycle = 'cycle' + d_split[-1].split('_')[2][-1]\n",
    "                try: marker = marker_dict[cycle][ch] \n",
    "                except: continue\n",
    "        \n",
    "                conditions.append(condition)\n",
    "                fovs.append('FW1')\n",
    "                cycles.append(cycle)\n",
    "                channels.append(ch)\n",
    "                markers.append(marker)\n",
    "                paths.append(os.path.join(dirpath, name))\n",
    "                \n",
    "    info = {\n",
    "            \"Condition\": conditions,\n",
    "            'FOV': fovs, \n",
    "            \"Cycle\": cycles,\n",
    "            \"Channels\": channels,\n",
    "            \"Markers\": markers,\n",
    "            \"Path\": paths\n",
    "        }\n",
    "\n",
    "    df = pd.DataFrame(info)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created df\n"
     ]
    }
   ],
   "source": [
    "df_meta_path = data_dir / 'Navincin' / 'metadata' / 'info.csv'\n",
    "df_meta_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "df_exist = df_meta_path.is_file()\n",
    "\n",
    "if not df_exist:\n",
    "    print('Created df')\n",
    "    df = get_info(data_raw, markers_map)\n",
    "    df.to_csv(df_meta_path, index=False)\n",
    "else:\n",
    "    print('Loaded df')\n",
    "    df = pd.read_csv(df_meta_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Condition\n",
       "O       14\n",
       "ctrl    14\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('Condition').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save hdf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "def save_hdf5(\n",
    "    path: str, name: str, data: np.ndarray, attr_dict=None, mode: str = \"a\"\n",
    ") -> None:\n",
    "    # Read h5 file\n",
    "    hf = h5py.File(path, mode)\n",
    "    # Create z_stack_dataset\n",
    "    if hf.get(name) is None:\n",
    "        data_shape = data.shape\n",
    "        data_type = data.dtype\n",
    "        max_shape = (data_shape[0],) + data_shape[1:]\n",
    "        dset = hf.create_dataset(\n",
    "            name,\n",
    "            shape=data_shape,\n",
    "            maxshape=max_shape,\n",
    "            chunks=True,\n",
    "            dtype=data_type,\n",
    "            compression=\"gzip\",\n",
    "        )\n",
    "        dset[:] = data\n",
    "        if attr_dict is not None:\n",
    "            for attr_key, attr_val in attr_dict.items():\n",
    "                dset.attrs[attr_key] = attr_val\n",
    "    else:\n",
    "        print(f\"Dataset {name} exists\")\n",
    "\n",
    "    hf.close()\n",
    "\n",
    "def joblib_loop(task, pics):\n",
    "    return Parallel(n_jobs=20)(delayed(task)(i) for i in pics)\n",
    "\n",
    "def read_img(path):\n",
    "    return skimage.io.imread(path, as_gray=True)\n",
    "\n",
    "def get_min(imgs):\n",
    "    shapes = np.array([np.array(img.shape) for img in imgs])\n",
    "    return np.min(shapes, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created df\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce4d91bce79d4025b4f95203c66fc4e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\AppData\\Local\\anaconda3\\envs\\scanpy\\Lib\\site-packages\\tqdm\\std.py:1182: FutureWarning: In a future version of pandas, a length 1 tuple will be returned when iterating over a groupby with a grouper equal to a list of length 1. Don't supply a list with a single grouper to avoid this warning.\n",
      "  for obj in iterable:\n"
     ]
    }
   ],
   "source": [
    "df_imgs_path = data_dir /   'Navincin'  / 'metadata' / 'imgs.csv'\n",
    "df_imgs_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "temp_path = data_dir  /  'Navincin'  /  'hdf5' / 'raw'\n",
    "temp_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "df_exist = df_imgs_path.is_file()\n",
    "\n",
    "\n",
    "if not df_exist:\n",
    "    print('Created df')\n",
    "    \n",
    "    group = df.groupby(['Condition'])\n",
    "    rows = []\n",
    "\n",
    "    for name, df_group in tqdm(group, total=len(group)):\n",
    "        file_name = name + '.hdf5'\n",
    "        file_path = temp_path / file_name\n",
    "        rows.append([name]+[file_path])\n",
    "        \n",
    "        channels = df_group.Channels.to_list()\n",
    "        cycles = df_group.Cycle.to_list()\n",
    "        markers = df_group.Markers.to_list()\n",
    "        paths = df_group.Path.to_numpy()\n",
    "            \n",
    "        imgs = joblib_loop(read_img, paths)\n",
    "        min_shape = get_min(imgs)\n",
    "        imgs_cropped = np.array([img[:min_shape[0], :min_shape[1]] for img in imgs])\n",
    "        info = {\"Cycle\": cycles, \"Channel\": channels, \"Marker\": markers}\n",
    "            \n",
    "            # hdf5 as Channel -> Z mapping\n",
    "        save_hdf5(file_path, 'imgs', imgs_cropped, info)\n",
    "    df_imgs = pd.DataFrame(rows, columns=['Condition', 'Path'])        \n",
    "    df_imgs.to_csv(df_imgs_path, index=False)\n",
    "else:\n",
    "    print('Loaded df')\n",
    "    df_imgs = pd.read_csv(df_imgs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Condition</th>\n",
       "      <th>Path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>O</td>\n",
       "      <td>y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\Navi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ctrl</td>\n",
       "      <td>y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\Navi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Condition                                               Path\n",
       "0         O  y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\Navi...\n",
       "1      ctrl  y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\Navi..."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_imgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Registration ImageJ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Registration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tifffile as tf\n",
    "from PIL import Image\n",
    "import PIL.Image\n",
    "PIL.Image.MAX_IMAGE_PIXELS = 933120000\n",
    "import shutil\n",
    "from datetime import date, datetime\n",
    "import skimage.io \n",
    "from skimage import util\n",
    "from skimage.transform import resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import exposure, util\n",
    "\n",
    "def contrast_str(img, n_min=0.1, n_max=100):\n",
    "    p2, p98 = np.percentile(img, (n_min, n_max))\n",
    "    img_rescale = exposure.rescale_intensity(img, in_range=(p2, p98))\n",
    "    img_rescale = util.img_as_ubyte(img_rescale)\n",
    "    return img_rescale\n",
    "\n",
    "def make_imgs_same_dim(imgs):\n",
    "    # Get max dimensions\n",
    "    shapes = np.array([img.shape[1:] for img in imgs])\n",
    "    min_x, min_y = shapes.min(axis=0)\n",
    "    imgs_cropped = [img[:, :min_x, :min_y] for img in imgs]\n",
    "    # imgs_cropped[0] = contrast_str(imgs_cropped[0])\n",
    "    return imgs_cropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thu71\\AppData\\Local\\Temp\\ipykernel_95364\\3871052164.py:9: FutureWarning: In a future version of pandas, a length 1 tuple will be returned when iterating over a groupby with a grouper equal to a list of length 1. Don't supply a list with a single grouper to avoid this warning.\n",
      "  for cycle, df_cycle in df_group.groupby(['Cycle']):\n",
      "C:\\Users\\thu71\\AppData\\Local\\Temp\\ipykernel_95364\\3871052164.py:9: FutureWarning: In a future version of pandas, a length 1 tuple will be returned when iterating over a groupby with a grouper equal to a list of length 1. Don't supply a list with a single grouper to avoid this warning.\n",
      "  for cycle, df_cycle in df_group.groupby(['Cycle']):\n"
     ]
    }
   ],
   "source": [
    "regSavePath = data_dir / 'Navincin' /'imgs' / 'registered_imagej'\n",
    "regSavePath.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "chs = [1, 2, 3, 4]\n",
    "group = df.groupby(['Condition', 'FOV'])\n",
    "for name, df_group in group:\n",
    "    path = df_group.iloc[0].Path\n",
    "    \n",
    "    for cycle, df_cycle in df_group.groupby(['Cycle']):\n",
    "        cycle = cycle[-1]\n",
    "        channel = df_cycle.Channels.tolist()\n",
    "        imgs = [cv2.imread(f, cv2.IMREAD_GRAYSCALE) for f in df_cycle.Path.tolist()]\n",
    "        \n",
    "        for ch in chs: \n",
    "            # Save path per Channel\n",
    "            folderPath = os.path.join(regSavePath, '_'.join(name), 'Original', 'CH' + str(ch)) # 1 index\n",
    "            if not os.path.exists(folderPath):\n",
    "                os.makedirs(folderPath, exist_ok = True)\n",
    "            \n",
    "            fileOut = 'CH' + str(ch) + '_Cycle' + str(cycle).zfill(2) + '.tif'\n",
    "            fileOut = os.path.join(folderPath, fileOut)\n",
    "            if os.path.exists(fileOut):\n",
    "                continue\n",
    "\n",
    "            if ch in channel:\n",
    "                if ch == 1:\n",
    "                    img = contrast_str(imgs[list(channel).index(ch)], n_min=0.1, n_max=99.9)\n",
    "                else:\n",
    "                    img = imgs[list(channel).index(ch)]\n",
    "                tf.imwrite(fileOut, img, photometric = 'minisblack', bigtiff = True)\n",
    "\n",
    "            else:\n",
    "                emptyImage = np.zeros(imgs[0].shape, np.uint8)\n",
    "                # print('Dont exist create empty image', cycle, ch)\n",
    "                tf.imwrite(fileOut, emptyImage, photometric = 'minisblack', bigtiff = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runMacro(\"y:/coskun-lab/Thomas/23_PLA_revision/data/Navincin/imgs/registered_imagej/O_FW1/Original/CH1/22Jan2024_register_transforms.ijm\");\n",
      "runMacro(\"y:/coskun-lab/Thomas/23_PLA_revision/data/Navincin/imgs/registered_imagej/ctrl_FW1/Original/CH1/22Jan2024_register_transforms.ijm\");\n"
     ]
    }
   ],
   "source": [
    "group = df.groupby(['Condition', 'FOV'])\n",
    "chs = [1, 2, 3, 4]\n",
    "\n",
    "for name, channels in group:\n",
    "    name = '_'.join(name)\n",
    "    '''\n",
    "    run(\"Register Virtual Stack Slices\", \"source=[Y:/coskun-lab/Nicky/07 Temp/register large stitch] output=[Y:/coskun-lab/Nicky/07 Temp/register output] feature=Rigid registration=[Rigid                -- translate + rotate                  ] advanced shrinkage save save_dir=[Y:/coskun-lab/Nicky/07 Temp/register output] initial_gaussian_blur=1.60 steps_per_scale_octave=3 minimum_image_size=64 maximum_image_size=1024 feature_descriptor_size=8 feature_descriptor_orientation_bins=8 closest/next_closest_ratio=0.92 maximal_alignment_error=25 inlier_ratio=0.05 feature_extraction_model=Rigid registration_model=[Rigid                -- translate + rotate                  ] interpolate\");\n",
    "    run(\"Transform Virtual Stack Slices\", \"source = [Y:/coskun-lab/Nicky/07 Temp/other channels/original] output = [Y:/coskun-lab/Nicky/07 Temp/other channels/original] transforms = [Y:/coskun-lab/Nicky/07 Temp/register output] interpolate\");\n",
    "    '''\n",
    "    # folder to save registered images separated by channel to apply transforms\n",
    "    # create all folder\n",
    "    for ii, ch in enumerate(chs): # all channels\n",
    "        os.makedirs(os.path.join(regSavePath, name, 'Original', 'CH' + str(ch)), exist_ok = True)\n",
    "        os.makedirs(os.path.join(regSavePath, name, 'Registered', 'CH' + str(ch)), exist_ok = True)\n",
    "    \n",
    "    os.chdir(os.path.join(regSavePath, name, 'Original', 'CH1'))\n",
    "    now = datetime.now() # current date and time\n",
    "    date_time = now.strftime(\"%d%b%Y\")\n",
    "    macro = open(date_time + '_register_transforms.ijm', 'w')\n",
    "    \n",
    "    # register cycles on CH1\n",
    "    macro.write('run(\"Register Virtual Stack Slices\", \"source=[')\n",
    "    # original files\n",
    "    macro.write(os.path.join(regSavePath, name, 'Original', 'CH1').replace('\\\\', '/'))\n",
    "    macro.write('] output=[')\n",
    "    # registered output files\n",
    "    macro.write(os.path.join(regSavePath, name, 'Registered', 'CH1').replace('\\\\', '/'))\n",
    "    \n",
    "    # Rigid registration: translation + rotation\n",
    "    macro.write('] feature=Rigid registration=[Rigid                -- translate + rotate                  ] advanced shrinkage save save_dir=[')\n",
    "    # folder to save recorded transformations \n",
    "    macro.write('] initial_gaussian_blur=1.60 steps_per_scale_octave=3 minimum_image_size=64 maximum_image_size=1024 feature_descriptor_size=8 feature_descriptor_orientation_bins=8 closest/next_closest_ratio=0.92 maximal_alignment_error=25 inlier_ratio=0.05 feature_extraction_model=Rigid registration_model=[Rigid                -- translate + rotate                  ] interpolate\"); \\n')\n",
    "    \n",
    "    # # bigwrap registration\n",
    "    # macro.write('] feature=Similarity registration=[Elastic              -- bUnwarpJ splines                    ] advanced shrinkage save save_dir=[')\n",
    "    # # folder to save recorded transformations \n",
    "    # macro.write('] initial_gaussian_blur=1.60 steps_per_scale_octave=3 minimum_image_size=64 maximum_image_size=1024 feature_descriptor_size=8 feature_descriptor_orientation_bins=8 closest/next_closest_ratio=0.92 maximal_alignment_error=25 inlier_ratio=0.05 feature_extraction_model=Similarity registration_model=[[Elastic              -- bUnwarpJ splines                    ] interpolate registration=Mono image_subsample_factor=0 initial_deformation=[Very Coarse] final_deformation=Fine divergence_weight=0.1 curl_weight=0.1 landmark_weight=1 image_weight=0 consistency_weight=0 stop_threshold=0.01 shear=0.95 scale=0.95 isotropy=1\"); \\n')\n",
    "    \n",
    "    # Or use similarity: translation + rotation + isotropic scale\n",
    "    # macro.write('] feature=Similarity registration=[Similarity           -- translate + rotate + isotropic scale] advanced shrinkage save save_dir=[')\n",
    "    # # folder to save recorded transformations \n",
    "    # macro.write('] initial_gaussian_blur=1.60 steps_per_scale_octave=3 minimum_image_size=64 maximum_image_size=1024 feature_descriptor_size=25 feature_descriptor_orientation_bins=8 closest/next_closest_ratio=0.92 maximal_alignment_error=50 inlier_ratio=0.05 feature_extraction_model=Similarity registration_model=[Similarity           -- translate + rotate + isotropic scale] interpolate\"); \\n')\n",
    "    \n",
    "    macro.write('run(\"Close All\"); \\n\\n')\n",
    "    \n",
    "    # now apply transform to other channels\n",
    "    for ii, ch in enumerate([1,2,3,4]): # each other channel\n",
    "        \n",
    "        macro.write('run(\"Transform Virtual Stack Slices\", \"source=[')\n",
    "        # unregsitered folder\n",
    "        macro.write(os.path.join(regSavePath, name, 'Original', 'CH' + str(ch)).replace('\\\\', '/'))\n",
    "        macro.write('] output=[')\n",
    "        # registered folder\n",
    "        macro.write(os.path.join(regSavePath, name, 'Registered', 'CH' + str(ch)).replace('\\\\', '/'))\n",
    "        macro.write('] transforms=[')\n",
    "        macro.write(os.path.join(regSavePath, name, 'Original', 'CH1').replace('\\\\', '/')) # stored in original registration folder\n",
    "        macro.write('] interpolate\"); \\n')\n",
    "        macro.write('run(\"Close All\"); \\n\\n')\n",
    "    \n",
    "    macro.close()\n",
    "    \n",
    "    # print command to run macro\n",
    "    print('runMacro(\"' + os.path.join(regSavePath, name, 'Original', 'CH1', macro.name).replace('\\\\', '/') + '\");')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine all registered images into single folder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "regSavePath = data_dir /  'Navincin' /'imgs' / 'registered_imagej'\n",
    "\n",
    "regSaveFinalPath = data_dir / 'Navincin' / 'imgs' / 'registered_imagej_final'\n",
    "regSaveFinalPath.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "regSaveCropPath = data_dir /  'Navincin' / 'imgs' /  'registered_crop'\n",
    "regSaveCropPath.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "375c919b2a5849d8b961c442e94c0007",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "816b3145c79349fb89fc418d88411e3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dfgroup = df.groupby(['Condition', 'FOV'])\n",
    "\n",
    "for name, channels in group:\n",
    "    name = '_'.join(name)\n",
    "    for ii, cycle in enumerate(tqdm(channels['Cycle'].unique())): # each cycle\n",
    "    \n",
    "        dfCycle = channels.loc[channels['Cycle'] == cycle]\n",
    "        dfCycle.reset_index(drop = True, inplace = True) # index is channel - 1\n",
    "        cycle = cycle[5:]\n",
    "        for jj, ch in enumerate(dfCycle.Channels): # each channel\n",
    "            \n",
    "            # find registered file\n",
    "            tifPath = os.path.join(regSavePath, name, 'Registered', 'CH' + str(ch), 'CH' + str(ch)+ '_Cycle' + str(cycle).zfill(2) + '.tif')\n",
    "\n",
    "            # File out\n",
    "            fileOut = 'Cycle' + str(cycle).zfill(2) + \\\n",
    "            '_' + 'CH' + str(ch) + '.tif'\n",
    "            folder = regSaveFinalPath / name\n",
    "            folder.mkdir(parents=True, exist_ok=True)\n",
    "            fileOut = os.path.join(regSaveFinalPath, name, fileOut)\n",
    "            # print(tifPath)\n",
    "            # Copy\n",
    "            if os.path.exists(tifPath):\n",
    "                shutil.copyfile(tifPath, fileOut)\n",
    "            else:\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Cropped image to smallest bounding box of non black region\n",
    "\n",
    "# Get channel list\n",
    "group = df.groupby(['Condition', 'FOV'])\n",
    "\n",
    "for name, df_group in group:\n",
    "    channels = df_group.Channels.tolist()\n",
    "    break\n",
    "\n",
    "# Crop\n",
    "for dir in os.listdir(regSaveFinalPath):\n",
    "\n",
    "    # Read imgs\n",
    "    imgs = []\n",
    "    paths = []\n",
    "    for file in os.listdir(regSaveFinalPath / dir):\n",
    "        if 'tif' in file:\n",
    "            path = regSaveFinalPath / dir/ file\n",
    "            imgs.append(tiff.imread(path))\n",
    "            paths.append(file)\n",
    "\n",
    "    # Get bboxs\n",
    "    bboxs = []\n",
    "    for i, img in enumerate(imgs):\n",
    "        if channels[i] != 1:\n",
    "            continue\n",
    "        bbox = skimage.measure.regionprops((img>0).astype(np.uint8))[0]['bbox']\n",
    "        bboxs.append(np.array(bbox))\n",
    "    bboxs = np.stack(bboxs)\n",
    "\n",
    "    bbox_final = [np.max(bboxs[:,0]),\n",
    "                np.max(bboxs[:,1]),\n",
    "                np.min(bboxs[:,2]),\n",
    "                np.min(bboxs[:,3])]\n",
    "\n",
    "    min_row, min_col, max_row, max_col = bbox_final\n",
    "\n",
    "    # Save cropped images\n",
    "    save_dir = regSaveCropPath / dir\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "    for i, img in enumerate(imgs):\n",
    "        save_path = save_dir / paths[i]\n",
    "        tiff.imwrite(save_path, img[min_row:max_row, min_col:max_col], bigtiff = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import util\n",
    "import h5py\n",
    "\n",
    "def get_info(data_raw, marker_dict):\n",
    "    conditions = []\n",
    "    fovs = []\n",
    "    cycles = []\n",
    "    channels = []\n",
    "    markers = []\n",
    "    paths = [] \n",
    "\n",
    "    # Loop through image folder\n",
    "    for (dirpath, dirnames, filenames) in os.walk(data_raw):\n",
    "        for name in sorted(filenames):\n",
    "            if \"tif\" in name:\n",
    "                # Get information from image name\n",
    "                n_split = name.split('_')\n",
    "                                \n",
    "                cond=dirpath.split('\\\\')[-1].split('_')[0]\n",
    "                fov=dirpath.split('\\\\')[-1].split('_')[1]\n",
    "                cycle='cycle'+str(int(n_split[0][-2:]))\n",
    "                ch = int(n_split[1][2])\n",
    "                try:\n",
    "                    marker = marker_dict[cycle][ch]\n",
    "                except:\n",
    "                    continue \n",
    "                    \n",
    "                conditions.append(cond)\n",
    "                fovs.append(fov)\n",
    "                cycles.append(cycle)\n",
    "                channels.append(ch)\n",
    "                markers.append(marker)\n",
    "                paths.append(os.path.join(dirpath, name))\n",
    "                \n",
    "    info = {\n",
    "            \"Condition\": conditions,\n",
    "            \"FOV\": fovs,\n",
    "            \"Cycle\": cycles,\n",
    "            \"Channels\": channels,\n",
    "            \"Markers\": markers,\n",
    "            \"Path\": paths\n",
    "        }\n",
    "\n",
    "    df = pd.DataFrame(info)\n",
    "    return df\n",
    "\n",
    "def save_hdf5(\n",
    "    path: str, name: str, data: np.ndarray, attr_dict=None, mode: str = \"a\"\n",
    ") -> None:\n",
    "    # Read h5 file\n",
    "    hf = h5py.File(path, mode)\n",
    "    # Create z_stack_dataset\n",
    "    if hf.get(name) is None:\n",
    "        data_shape = data.shape\n",
    "        data_type = data.dtype\n",
    "        max_shape = (data_shape[0],) + data_shape[1:]\n",
    "        dset = hf.create_dataset(\n",
    "            name,\n",
    "            shape=data_shape,\n",
    "            maxshape=max_shape,\n",
    "            chunks=True,\n",
    "            dtype=data_type,\n",
    "            compression=\"gzip\",\n",
    "        )\n",
    "        dset[:] = data\n",
    "        if attr_dict is not None:\n",
    "            for attr_key, attr_val in attr_dict.items():\n",
    "                dset.attrs[attr_key] = attr_val\n",
    "    else:\n",
    "        print(f\"Dataset {name} exists\")\n",
    "\n",
    "    hf.close()\n",
    "\n",
    "def read_img(path):\n",
    "    return skimage.io.imread(path, as_gray=True)\n",
    "\n",
    "def joblib_loop(task, pics):\n",
    "    return Parallel(n_jobs=20)(delayed(task)(i) for i in pics)\n",
    "\n",
    "def read_img(path):\n",
    "    return skimage.io.imread(path, as_gray=True)\n",
    "\n",
    "def get_min(imgs):\n",
    "    shapes = np.array([np.array(img.shape) for img in imgs])\n",
    "    return np.min(shapes, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder is already there\n",
      "Created df\n"
     ]
    }
   ],
   "source": [
    "data_raw = data_dir / 'Navincin'/ 'imgs' /  'registered_crop'\n",
    "df_meta_path = data_dir /  'Navincin'/ 'metadata' / 'info_sti.csv'\n",
    "\n",
    "try:\n",
    "    df_meta_path.parent.mkdir(parents=True, exist_ok=False)\n",
    "except FileExistsError:\n",
    "    print(\"Folder is already there\")\n",
    "\n",
    "df_exist = df_meta_path.is_file()\n",
    "\n",
    "if not df_exist:\n",
    "    print('Created df')\n",
    "    df = get_info(data_raw, markers_map)\n",
    "    df.to_csv(df_meta_path, index=False)\n",
    "else:\n",
    "    print('Loaded df')\n",
    "    df = pd.read_csv(df_meta_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Condition\n",
       "O       14\n",
       "ctrl    14\n",
       "dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('Condition').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created df\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2befaf1e7dde4e28874dfdf452fbde78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_imgs_path = data_dir /'Navincin'/ 'metadata' / 'imgs_reg.csv'\n",
    "temp_path =data_dir /'Navincin' / 'hdf5' / 'registered'\n",
    "try:\n",
    "    temp_path.mkdir(parents=True, exist_ok=False)\n",
    "except FileExistsError:\n",
    "    print(\"Folder is already there\")\n",
    "\n",
    "df_exist = df_imgs_path.is_file()\n",
    "\n",
    "if not df_exist:\n",
    "    print('Created df')\n",
    "    group = df.groupby(['Condition','FOV'])\n",
    "    rows = []\n",
    "\n",
    "    for name, df_group in tqdm(group, total=len(group)):\n",
    "        file_name = '_'.join(np.array(name).astype(str)) + '.hdf5'\n",
    "        file_path = temp_path / file_name\n",
    "        rows.append(list(name)+[file_path])\n",
    "        \n",
    "        # if file_path.exists():\n",
    "        #     continue\n",
    "        \n",
    "        channels = df_group.Channels.to_list()\n",
    "        cycles = df_group.Cycle.to_list()\n",
    "        markers = df_group.Markers.to_list()\n",
    "        paths = df_group.Path.to_numpy()\n",
    "            \n",
    "        imgs = joblib_loop(read_img, paths)\n",
    "        min_shape = get_min(imgs)\n",
    "        imgs_cropped = np.array([img[:min_shape[0], :min_shape[1]] for img in imgs])\n",
    "        info = {\"Cycle\": cycles, \"Channel\": channels, \"Marker\": markers}\n",
    "\n",
    "        imgs_cropped = util.img_as_ubyte(imgs_cropped)\n",
    "        \n",
    "        # hdf5 as Channel -> Z mapping\n",
    "        save_hdf5(file_path, 'imgs', imgs_cropped, info)\n",
    "    df_imgs = pd.DataFrame(rows, columns=['Condition', 'FOV', 'Path'])        \n",
    "    df_imgs.to_csv(df_imgs_path, index=False)\n",
    "else:\n",
    "    print('Loaded df')\n",
    "    df_imgs = pd.read_csv(df_imgs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Condition</th>\n",
       "      <th>FOV</th>\n",
       "      <th>Path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>O</td>\n",
       "      <td>FW1</td>\n",
       "      <td>y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\Navi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ctrl</td>\n",
       "      <td>FW1</td>\n",
       "      <td>y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\Navi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Condition  FOV                                               Path\n",
       "0         O  FW1  y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\Navi...\n",
       "1      ctrl  FW1  y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\Navi..."
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_imgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import napari \n",
    "from skimage import exposure, util\n",
    "\n",
    "def contrast_str(img, n_min=0.1, n_max=99.9):\n",
    "    p2, p98 = np.percentile(img, (n_min, n_max))\n",
    "    img_rescale = exposure.rescale_intensity(img, in_range=(p2, p98))\n",
    "    img_rescale = util.img_as_ubyte(img_rescale)\n",
    "    return img_rescale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imgs_path = data_dir / 'Navincin' /'metadata' / 'imgs_reg.csv'\n",
    "df_imgs = pd.read_csv(df_imgs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save combined images\n",
    "for row in df_imgs.itertuples():\n",
    "    # Read image\n",
    "    path = row.Path\n",
    "    with h5py.File(path, \"r\") as f:\n",
    "        imgs = f['imgs'][:]\n",
    "        markers = f['imgs'].attrs['Marker']\n",
    "\n",
    "    napari.view_image(imgs, name=markers, channel_axis=0, visible=False)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['DNA', 'sox/oct4', 'DNA', 'b-catenin/e-cadherin', 'DNA',\n",
       "       'Cdc25c/p38', 'DNA', 'pyk2/src', 'DNA', 'p-Jak2/Stat3', 'DNA',\n",
       "       'P-EGFR', 'Phalloidin', 'KI67'], dtype=object)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "markers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "cyto_markers = ['P-EGFR', 'Phalloidin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_seg_path = data_dir /  'Navincin' / 'imgs' / 'segmentation'\n",
    "whole_seg_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "# Save combined images\n",
    "for row in df_imgs.itertuples():\n",
    "    # Read image\n",
    "    path = row.Path\n",
    "    with h5py.File(path, \"r\") as f:\n",
    "        imgs = f['imgs'][:]\n",
    "        markers = f['imgs'].attrs['Marker']\n",
    "\n",
    "    # Get dapi and cyto imgaes\n",
    "    indices = np.isin(markers, cyto_markers)\n",
    "    img_dapi = imgs[0]\n",
    "    imgs_cyto = imgs[indices,:]\n",
    "    \n",
    "    # Contrast streching and combine to rgb image\n",
    "    img_dapi = contrast_str(img_dapi, n_max=99.9)\n",
    "    imgs_cyto_scaled = [contrast_str(imgs_cyto[0], n_max=99.5), contrast_str(imgs_cyto[1], n_max=99.5)]\n",
    "    img_cyto = np.max(np.array(imgs_cyto_scaled), axis=0)\n",
    "    img_rgb = np.stack([np.zeros(img_dapi.shape),img_cyto, img_dapi], axis=0).astype(np.uint8)\n",
    "    \n",
    "    # Crop and save\n",
    "    file_name = f'{\"_\".join(row[1:3])}.tif'\n",
    "    file_path = whole_seg_path / file_name\n",
    "    tiff.imwrite(file_path, img_rgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QC masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import morphology, measure\n",
    "from skimage.segmentation import clear_border\n",
    "from collections import defaultdict\n",
    "   \n",
    "def count_pixel_label_mask(regionmask, intensity_image):\n",
    "    v,c = np.unique(intensity_image[regionmask], return_counts=True)\n",
    "    return dict(zip(v,c))\n",
    "    \n",
    "# Quality control of mask\n",
    "def qc_nuclei(mask_cyto, mask_nuclei, small_size=10000):\n",
    "    '''\n",
    "    Function to check if cell masks contain nuclei\n",
    "    '''\n",
    "    # Dictionnary storing nuclei and cyto label to cell id \n",
    "    nuclei2cell = {}\n",
    "    cyto2cell = {}\n",
    "    \n",
    "    # Filter out small objects\n",
    "    mask_cyto = morphology.remove_small_objects(mask_cyto,  min_size=small_size)\n",
    "    \n",
    "    # Filter out mask touching border\n",
    "    mask_cyto = clear_border(mask_cyto)\n",
    "    \n",
    "    # Filtered only cell mask region\n",
    "    cell_mask = np.where(mask_cyto > 0, 1, 0)\n",
    "    mask_nuclei_filtered = mask_nuclei * cell_mask\n",
    "    mask_nuclei_filtered =  morphology.remove_small_objects(mask_nuclei_filtered,  min_size=2000)\n",
    "    \n",
    "    nuclei_mask = np.where(mask_nuclei>0, 1, 0)\n",
    "    cyto = (mask_cyto - mask_cyto*nuclei_mask).astype(np.uint16)\n",
    "    \n",
    "    # Count pixel cell label in each nuclei region to assign each nuclei to cell\n",
    "    props = measure.regionprops(mask_nuclei_filtered, intensity_image=mask_cyto, \n",
    "                    extra_properties=(count_pixel_label_mask,))\n",
    "    nuclei_labels = []\n",
    "    cell_labels = []\n",
    "    for prop in props:\n",
    "        df = pd.DataFrame.from_dict(prop['count_pixel_label_mask'], orient='index').reset_index()\n",
    "        df.columns = ['Label', 'Count']\n",
    "        corresponding_label = df.iloc[df['Count'].argmax(axis=0)]['Label']\n",
    "        nuclei_labels.append(prop['Label'])\n",
    "        cell_labels.append(corresponding_label)\n",
    "    \n",
    "    df = pd.DataFrame({'Nuclei': nuclei_labels, 'Cyto': cell_labels})\n",
    "    return mask_cyto, mask_nuclei_filtered, cyto, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read masks paths \n",
    "mask_dir = data_dir /   'Navincin'/ 'imgs' / 'masks'\n",
    "mask_filt_dir = data_dir /   'Navincin' / 'imgs' / 'masks_filtered'\n",
    "mask_filt_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "masks_path = defaultdict(dict) \n",
    "for path in os.listdir(mask_dir):\n",
    "    name = path.split('.')[0]\n",
    "    if 'Nuclei' in name:\n",
    "        masks_path[name[7:]]['nuclei'] = mask_dir / path\n",
    "    else:\n",
    "        masks_path[name]['cyto'] = mask_dir / path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in df_imgs.itertuples():\n",
    "    # Read image\n",
    "    path = row.Path\n",
    "    name = '_'.join([row.Condition, row.FOV])\n",
    "    \n",
    "    # Read masks\n",
    "    mask_cyto_path = masks_path[name]['cyto']\n",
    "    mask_nuclei_path = masks_path[name]['nuclei']\n",
    "    \n",
    "    mask_cyto = skimage.io.imread(mask_cyto_path)\n",
    "    mask_nuclei = skimage.io.imread(mask_nuclei_path)\n",
    "    mask_nuclei = mask_nuclei[:mask_cyto.shape[0], :mask_cyto.shape[1]]\n",
    "    cell, nuclei, cyto, df = qc_nuclei(mask_cyto, mask_nuclei)\n",
    "    \n",
    "    file_path =  mask_filt_dir / f'Nuclei_{name}.tif'\n",
    "    tiff.imwrite(file_path, nuclei)\n",
    "    file_path =  mask_filt_dir / f'Cell_{name}.tif'\n",
    "    tiff.imwrite(file_path, cell)\n",
    "    file_path =  mask_filt_dir / f'Cyto_{name}.tif'\n",
    "    tiff.imwrite(file_path, cyto)\n",
    "    file_path =  mask_filt_dir / f'df_{name}.csv'\n",
    "    df.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quanfitication PPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PLA\n",
    "\n",
    "PPI_save_path =  data_dir /  'Navincin' / 'PPI'\n",
    "PPI_save_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "PPI_imgs_path =  data_dir /  'Navincin'  / 'PPI_imgs'\n",
    "PPI_imgs_path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imgs_path = data_dir / 'Navincin' /'metadata' / 'imgs_reg.csv'\n",
    "df_imgs = pd.read_csv(df_imgs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DNA' 'sox/oct4' 'DNA' 'b-catenin/e-cadherin' 'DNA' 'Cdc25c/p38' 'DNA'\n",
      " 'pyk2/src' 'DNA' 'p-Jak2/Stat3' 'DNA' 'P-EGFR' 'Phalloidin' 'KI67']\n",
      "['DNA' 'sox/oct4' 'DNA' 'b-catenin/e-cadherin' 'DNA' 'Cdc25c/p38' 'DNA'\n",
      " 'pyk2/src' 'DNA' 'p-Jak2/Stat3' 'DNA' 'P-EGFR' 'Phalloidin' 'KI67']\n"
     ]
    }
   ],
   "source": [
    "for row in df_imgs.itertuples():\n",
    "    path = row.Path\n",
    "    with h5py.File(path, 'r') as f:\n",
    "        markers = f['imgs'].attrs['Marker']\n",
    "    print(markers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Condition</th>\n",
       "      <th>FOV</th>\n",
       "      <th>Path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>O</td>\n",
       "      <td>FW1</td>\n",
       "      <td>y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\Navi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ctrl</td>\n",
       "      <td>FW1</td>\n",
       "      <td>y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\Navi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Condition  FOV                                               Path\n",
       "0         O  FW1  y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\Navi...\n",
       "1      ctrl  FW1  y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\Navi..."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "PPI_markers = ['sox/oct4','b-catenin/e-cadherin','Cdc25c/p38', 'pyk2/src', 'p-Jak2/Stat3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading image sox/oct4\n",
      "Processing image sox/oct4\n",
      "(array([0, 1], dtype=uint8), array([31521927,   677781], dtype=int64))\n",
      "Reading image b-catenin/e-cadherin\n",
      "Processing image b-catenin/e-cadherin\n",
      "(array([0, 1], dtype=uint8), array([32161187,    38521], dtype=int64))\n",
      "Reading image Cdc25c/p38\n",
      "Processing image Cdc25c/p38\n",
      "(array([0, 1], dtype=uint8), array([25160641,  7039067], dtype=int64))\n",
      "Reading image pyk2/src\n",
      "Processing image pyk2/src\n",
      "(array([0, 1], dtype=uint8), array([27438771,  4760937], dtype=int64))\n",
      "Reading image p-Jak2/Stat3\n",
      "Processing image p-Jak2/Stat3\n",
      "(array([0, 1], dtype=uint8), array([27451367,  4748341], dtype=int64))\n",
      "File exist. Deleted\n",
      "Reading image sox/oct4\n",
      "Processing image sox/oct4\n",
      "(array([0, 1], dtype=uint8), array([29765331,  2049498], dtype=int64))\n",
      "Reading image b-catenin/e-cadherin\n",
      "Processing image b-catenin/e-cadherin\n",
      "(array([0, 1], dtype=uint8), array([31676908,   137921], dtype=int64))\n",
      "Reading image Cdc25c/p38\n",
      "Processing image Cdc25c/p38\n",
      "(array([0, 1], dtype=uint8), array([27026899,  4787930], dtype=int64))\n",
      "Reading image pyk2/src\n",
      "Processing image pyk2/src\n",
      "(array([0, 1], dtype=uint8), array([28637161,  3177668], dtype=int64))\n",
      "Reading image p-Jak2/Stat3\n",
      "Processing image p-Jak2/Stat3\n",
      "(array([0, 1], dtype=uint8), array([27914961,  3899868], dtype=int64))\n",
      "File exist. Deleted\n"
     ]
    }
   ],
   "source": [
    "thresholds = [0.03, 0.03, 0.01, 0.01, 0.01]\n",
    "max_rad = [4, 10, 10, 10, 10]\n",
    "# m_rad = [4, 10, 10, 10, 10]\n",
    "\n",
    "for row in df_imgs.itertuples():\n",
    "    # Read image\n",
    "    path = row.Path\n",
    "    pla_detect = PLA.PLA_detection(path, name='imgs', m='Marker')\n",
    "    \n",
    "    with h5py.File(path, 'r') as f:\n",
    "        markers = f['imgs'].attrs['Marker']\n",
    "        \n",
    "    imgs_spots = []\n",
    "    imgs_wths = []\n",
    "    imgs_raw = []\n",
    "    for i,RNA in enumerate(PPI_markers): \n",
    "        if RNA in markers:\n",
    "            img_spot, img_wth, _, img = pla_detect.detect_spot(RNA, thres=thresholds[i], min_radius=2, max_radius=max_rad[i])\n",
    "            imgs_spots.append(img_spot)\n",
    "            imgs_wths.append(img_wth)\n",
    "            imgs_raw.append(img)\n",
    "\n",
    "    # Save imgs\n",
    "    file_path = PPI_imgs_path / (row[1] + '_raw.tiff')\n",
    "    tiff.imwrite(file_path, imgs_raw)\n",
    "    file_path = PPI_imgs_path / (row[1] + '_processed.tiff')\n",
    "    tiff.imwrite(file_path, imgs_wths)\n",
    "    file_path = PPI_imgs_path / (row[1] + '_detected.tiff')\n",
    "    tiff.imwrite(file_path, imgs_spots)\n",
    "\n",
    "    # Save PPI dict\n",
    "    name = row[1] +'.pkl'\n",
    "    pla_detect.save_pickle(PPI_save_path / name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract per cell PPI count "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "def read_PPI(path):\n",
    "    with open(path, 'rb') as file:\n",
    "        PPI_dict = pickle.load(file)\n",
    "\n",
    "    return PPI_dict\n",
    "\n",
    "def create_PPI_df(PPI_labels, PPI_loc, name, cyto=True):\n",
    "    if cyto:\n",
    "        columns_name = ['Cyto', 'x', 'y']\n",
    "    else:\n",
    "        columns_name = ['Nuclei', 'x', 'y']\n",
    "    df = pd.DataFrame(np.hstack([PPI_labels[:,np.newaxis], PPI_loc]), \n",
    "                      columns=columns_name)\n",
    "    df['PPI'] = name\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_filt_dir = data_dir /  'Navincin' / 'imgs' / 'masks_filtered'\n",
    "PPI_save_path =  data_dir /  'Navincin' / 'PPI'\n",
    "\n",
    "masks_path = defaultdict(dict) \n",
    "for path in os.listdir(mask_filt_dir):\n",
    "    name = path.split('.')[0]\n",
    "    if 'Nuclei' in name:\n",
    "        masks_path[name[7:]]['nuclei'] = mask_filt_dir / path\n",
    "    elif 'Cyto' in name:\n",
    "        masks_path[name[5:]]['cyto'] =mask_filt_dir / path\n",
    "    elif 'Cell' in name:\n",
    "        masks_path[name[5:]]['cell'] =mask_filt_dir / path    \n",
    "    elif 'df' in name:\n",
    "        masks_path[name[3:]]['df'] =mask_filt_dir / path\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in df_imgs.itertuples():\n",
    "    name = '_'.join(row[1:3])\n",
    "    \n",
    "    # Read masks\n",
    "    mask_cyto_path = masks_path[name]['cell']\n",
    "    mask_nuclei_path = masks_path[name]['nuclei']\n",
    "    df_path =  masks_path[name]['df']\n",
    "    \n",
    "    mask_cyto = skimage.io.imread(mask_cyto_path)\n",
    "    mask_nuclei = skimage.io.imread(mask_nuclei_path)\n",
    "    df_cell_info = pd.read_csv(df_path)\n",
    "    nuclei2cell = dict(zip(df_cell_info.iloc[:,0], df_cell_info.iloc[:,1]))   \n",
    "    \n",
    "    # Read PPi\n",
    "    PPI_dict = read_PPI(PPI_save_path / f'{row[1]}.pkl')\n",
    "    dfs_PPI_cyto = []\n",
    "    dfs_PPI_nuclei = []\n",
    "    for k in PPI_dict.keys():\n",
    "        PPI_loc = PPI_dict[k][:, 1:3].astype(np.uint32)\n",
    "        \n",
    "        # Cyto\n",
    "        PPI_labels = mask_cyto[PPI_loc[:,0], PPI_loc[:,1]]\n",
    "        df_PPI = create_PPI_df(PPI_labels, PPI_loc, k)\n",
    "        dfs_PPI_cyto.append(df_PPI)\n",
    "        \n",
    "        # Nuclei\n",
    "        PPI_labels = mask_nuclei[PPI_loc[:,0], PPI_loc[:,1]]\n",
    "        df_PPI = create_PPI_df(PPI_labels, PPI_loc, k, cyto=False)\n",
    "        dfs_PPI_nuclei.append(df_PPI)\n",
    "    \n",
    "    # Combined DFs\n",
    "    df_PPI_cyto = pd.concat(dfs_PPI_cyto)\n",
    "    df_PPI_nuclei = pd.concat(dfs_PPI_nuclei)\n",
    "    df_PPI_nuclei['Nuclei_Cell'] = df_PPI_nuclei['Nuclei'].apply(lambda x: nuclei2cell.get(x,x))   \n",
    "    df_merged = df_PPI_cyto.merge(df_PPI_nuclei)\n",
    "    df_merged['Condition'] = row[1]\n",
    "    df_merged['FOV'] = row[2]\n",
    "    \n",
    "    # Save dataframe\n",
    "    path = PPI_save_path / f'{name}.csv'\n",
    "    df_merged.to_csv(path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coexpression between IF and PPI markers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def read_mean_pixels(x, y, window_size, path):\n",
    "    x_min = np.clip(x-window_size,a_min=0, a_max=None)\n",
    "    x_max = x+window_size\n",
    "    y_min = np.clip(y-window_size, a_min=0, a_max=None)\n",
    "    y_max = y+window_size\n",
    "    \n",
    "    with h5py.File(path, \"r\") as f:\n",
    "        pixels = f['imgs'][:, y_min:y_max, x_min:x_max]\n",
    "    mean_expression = pixels.mean(axis=(1,2))\n",
    "    return mean_expression\n",
    "    \n",
    "def extract(df, path, window_size=5):\n",
    "    x = df['x'].to_numpy()\n",
    "    y = df['y'].to_numpy()\n",
    "    \n",
    "    # Read markers name \n",
    "    with h5py.File(path, \"r\") as f:\n",
    "        markers = f['imgs'].attrs['Marker']\n",
    "    \n",
    "    # Define partial and joblib\n",
    "    read_partial = partial(read_mean_pixels, window_size=window_size, path=path)\n",
    "    mean_expressions = Parallel(n_jobs=20)(delayed(read_partial)(i,j) for i,j in zip(y,x))\n",
    "    \n",
    "    # create dataframe\n",
    "    _, indices = np.unique(markers, return_index=True)\n",
    "    indices.sort()\n",
    "    marker_unique = markers[indices]\n",
    "    mean_expressions = np.stack(mean_expressions)\n",
    "    df_exp = pd.DataFrame(mean_expressions[:, indices], columns=marker_unique)\n",
    "    return df_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read PPI\n",
    "PPI_save_path =  data_dir / 'Navincin' / 'PPI'\n",
    "\n",
    "dfs = []\n",
    "for path in os.listdir(PPI_save_path):\n",
    "    if 'csv' in path:\n",
    "        df = pd.read_csv(PPI_save_path / path)\n",
    "        dfs.append(df)\n",
    "\n",
    "df = pd.concat(dfs)\n",
    "df.reset_index(inplace=True, drop=True)\n",
    "df_fil = df[df.Cyto > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imgs_path = data_dir  /'Navincin' /'metadata' / 'imgs_reg.csv'\n",
    "df_imgs = pd.read_csv(df_imgs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Co-expression data\n",
    "PPI_exp_path =  data_dir / 'Navincin' / 'PPI' / 'expression'\n",
    "PPI_exp_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "group = df_fil.groupby(['Condition', 'FOV'])\n",
    "\n",
    "for name, df_group in group:\n",
    "    path = df_imgs[(df_imgs.Condition == name[0]) & (df_imgs.FOV == name[1])].Path.item()\n",
    "    df_expression = extract(df_group, path)\n",
    "    \n",
    "    df_merged = pd.concat([df_group.reset_index().rename({'index':'Original Index'}, axis=1), \n",
    "                           df_expression], axis=1)\n",
    "    \n",
    "    # Save coexpression\n",
    "    file_name = '_'.join(name)\n",
    "    save_path = PPI_exp_path / f'{file_name}.csv'\n",
    "    df_merged.to_csv(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Area quantification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.filters import threshold_local, gaussian\n",
    "from skimage.filters import threshold_otsu, rank\n",
    "from skimage.morphology import disk\n",
    "from skimage import exposure, measure\n",
    "\n",
    "def threshold(img):\n",
    "    p2, p98 = np.percentile(img, (2, 99.9))\n",
    "    img_rescale = exposure.rescale_intensity(img, in_range=(p2, p98))\n",
    "\n",
    "    img_blur = gaussian(img_rescale , sigma=15, preserve_range=True).astype(np.uint8)\n",
    "    img_pro = cv2.subtract(img_rescale, img_blur)\n",
    "\n",
    "    thresh = 50\n",
    "    binary = img_pro >= thresh\n",
    "    return binary\n",
    "\n",
    "def custom(regionmask, intensity_image):\n",
    "    return np.sum(intensity_image[regionmask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_filt_dir = data_dir /  'Navincin' / 'imgs' / 'masks_filtered'\n",
    "PPI_save_path =  data_dir /  'Navincin' / 'PPI'\n",
    "\n",
    "masks_path = defaultdict(dict) \n",
    "for path in os.listdir(mask_filt_dir):\n",
    "    name = path.split('.')[0]\n",
    "    if 'Nuclei' in name:\n",
    "        masks_path[name[7:]]['nuclei'] = mask_filt_dir / path\n",
    "    elif 'Cyto' in name:\n",
    "        masks_path[name[5:]]['cyto'] =mask_filt_dir / path\n",
    "    elif 'Cell' in name:\n",
    "        masks_path[name[5:]]['cell'] =mask_filt_dir / path    \n",
    "    elif 'df' in name:\n",
    "        masks_path[name[3:]]['df'] =mask_filt_dir / path\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imgs_path = data_dir / 'Navincin' /'metadata' / 'imgs_reg.csv'\n",
    "df_imgs = pd.read_csv(df_imgs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "marker = 'KI67'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "\n",
    "for row in df_imgs.itertuples():\n",
    "    path = row.Path\n",
    "\n",
    "    # Read markers name \n",
    "    with h5py.File(path, \"r\") as f:\n",
    "        markers = f['imgs'].attrs['Marker']\n",
    "        imgs = f['imgs'][:]\n",
    "\n",
    "    img = imgs[list(markers).index(marker)]\n",
    "    binary = threshold(img)\n",
    "\n",
    "    name = '_'.join(row[1:3])\n",
    "    \n",
    "    # Read masks\n",
    "    mask_cyto_path = masks_path[name]['cell']\n",
    "    mask_nuclei_path = masks_path[name]['nuclei']\n",
    "    df_path =  masks_path[name]['df']\n",
    "    mask_nuclei = skimage.io.imread(mask_nuclei_path)\n",
    "    df_cell_info = pd.read_csv(df_path)\n",
    "    nuclei2cell = dict(zip(df_cell_info.iloc[:,0], df_cell_info.iloc[:,1]))   \n",
    "    \n",
    "    # Get values\n",
    "    props = measure.regionprops_table(mask_nuclei, intensity_image=binary.astype(np.uint8),\n",
    "                                  properties=('label', 'area'),\n",
    "                                  extra_properties=(custom, ))\n",
    "    df = pd.DataFrame(props)\n",
    "    df.columns = ['Nuclei', 'Nuclei Area', 'Ki67 Area']\n",
    "    df['Nuclei_Cell'] = df['Nuclei'].apply(lambda x: nuclei2cell.get(x,x))   \n",
    "    df['Condition'] = row.Condition\n",
    "    dfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Image layer 'binary' at 0x1f7d00bbc50>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import napari\n",
    "\n",
    "viewer = napari.view_image(img)\n",
    "viewer.add_image(binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Co-expression data\n",
    "PPI_exp_path =  data_dir / 'Navincin' / 'PPI' / 'expression'\n",
    "PPI_exp_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "df = pd.concat(dfs)\n",
    "df.to_csv(PPI_exp_path / 'Ki67.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ki67 mean intensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_filt_dir = data_dir / 'Navincin' / 'imgs' / 'masks_filtered'\n",
    "PPI_save_path =  data_dir / 'Navincin' / 'PPI'\n",
    "\n",
    "masks_path = defaultdict(dict) \n",
    "for path in os.listdir(mask_filt_dir):\n",
    "    name = path.split('.')[0]\n",
    "    if 'Nuclei' in name:\n",
    "        masks_path[name[7:]]['nuclei'] = mask_filt_dir / path\n",
    "    elif 'Cyto' in name:\n",
    "        masks_path[name[5:]]['cyto'] =mask_filt_dir / path\n",
    "    elif 'Cell' in name:\n",
    "        masks_path[name[5:]]['cell'] =mask_filt_dir / path    \n",
    "    elif 'df' in name:\n",
    "        masks_path[name[3:]]['df'] =mask_filt_dir / path\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imgs_path = data_dir / 'Navincin' /'metadata' / 'imgs_reg.csv'\n",
    "df_imgs = pd.read_csv(df_imgs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "marker = 'KI67'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "\n",
    "for row in df_imgs.itertuples():\n",
    "    path = row.Path\n",
    "\n",
    "    # Read markers name \n",
    "    with h5py.File(path, \"r\") as f:\n",
    "        markers = f['imgs'].attrs['Marker']\n",
    "        imgs = f['imgs'][:]\n",
    "\n",
    "    img = imgs[list(markers).index(marker)]\n",
    "\n",
    "    name = '_'.join(row[1:3])\n",
    "    \n",
    "    # Read masks\n",
    "    mask_cyto_path = masks_path[name]['cell']\n",
    "    mask_nuclei_path = masks_path[name]['nuclei']\n",
    "    df_path =  masks_path[name]['df']\n",
    "    mask_cyto = skimage.io.imread(mask_cyto_path)\n",
    "    \n",
    "    # Get values\n",
    "    props = measure.regionprops_table(mask_cyto, intensity_image=img,\n",
    "                                  properties=('label', 'area', 'mean_intensity'))\n",
    "    df = pd.DataFrame(props)\n",
    "    df.columns = ['Cell', 'Area', 'Ki67']\n",
    "    df['Condition'] = row.Condition\n",
    "    dfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Co-expression data\n",
    "PPI_exp_path =  data_dir / 'Navincin' / 'PPI' / 'expression'\n",
    "PPI_exp_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "df = pd.concat(dfs)\n",
    "df = df[df.Area > 7000]\n",
    "\n",
    "df.to_csv(PPI_exp_path / 'Ki67_intensity.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cell</th>\n",
       "      <th>Area</th>\n",
       "      <th>Ki67</th>\n",
       "      <th>Condition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>24617.0</td>\n",
       "      <td>3.527968</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>39</td>\n",
       "      <td>22241.0</td>\n",
       "      <td>2.100400</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40</td>\n",
       "      <td>34437.0</td>\n",
       "      <td>4.631995</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>41</td>\n",
       "      <td>11747.0</td>\n",
       "      <td>4.922789</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>42</td>\n",
       "      <td>47756.0</td>\n",
       "      <td>1.849673</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>754</th>\n",
       "      <td>878</td>\n",
       "      <td>18216.0</td>\n",
       "      <td>3.912989</td>\n",
       "      <td>ctrl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>756</th>\n",
       "      <td>881</td>\n",
       "      <td>24188.0</td>\n",
       "      <td>1.322143</td>\n",
       "      <td>ctrl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>757</th>\n",
       "      <td>883</td>\n",
       "      <td>12183.0</td>\n",
       "      <td>4.613396</td>\n",
       "      <td>ctrl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>758</th>\n",
       "      <td>885</td>\n",
       "      <td>14823.0</td>\n",
       "      <td>3.556567</td>\n",
       "      <td>ctrl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>759</th>\n",
       "      <td>886</td>\n",
       "      <td>11959.0</td>\n",
       "      <td>2.663183</td>\n",
       "      <td>ctrl</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1345 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Cell     Area      Ki67 Condition\n",
       "2      38  24617.0  3.527968         O\n",
       "3      39  22241.0  2.100400         O\n",
       "4      40  34437.0  4.631995         O\n",
       "5      41  11747.0  4.922789         O\n",
       "6      42  47756.0  1.849673         O\n",
       "..    ...      ...       ...       ...\n",
       "754   878  18216.0  3.912989      ctrl\n",
       "756   881  24188.0  1.322143      ctrl\n",
       "757   883  12183.0  4.613396      ctrl\n",
       "758   885  14823.0  3.556567      ctrl\n",
       "759   886  11959.0  2.663183      ctrl\n",
       "\n",
       "[1345 rows x 4 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B-catenin mean expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from skimage import measure\n",
    "\n",
    "mask_filt_dir = data_dir / 'Navincin' / 'imgs' / 'masks_filtered'\n",
    "PPI_save_path =  data_dir / 'Navincin' / 'PPI'\n",
    "\n",
    "masks_path = defaultdict(dict) \n",
    "for path in os.listdir(mask_filt_dir):\n",
    "    name = path.split('.')[0]\n",
    "    if 'Nuclei' in name:\n",
    "        masks_path[name[7:]]['nuclei'] = mask_filt_dir / path\n",
    "    elif 'Cyto' in name:\n",
    "        masks_path[name[5:]]['cyto'] =mask_filt_dir / path\n",
    "    elif 'Cell' in name:\n",
    "        masks_path[name[5:]]['cell'] =mask_filt_dir / path    \n",
    "    elif 'df' in name:\n",
    "        masks_path[name[3:]]['df'] =mask_filt_dir / path\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imgs_path = data_dir / 'Navincin' /'metadata' / 'imgs_reg.csv'\n",
    "df_imgs = pd.read_csv(df_imgs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "marker = 'b-catenin/e-cadherin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "\n",
    "for row in df_imgs.itertuples():\n",
    "    path = row.Path\n",
    "\n",
    "    # Read markers name \n",
    "    with h5py.File(path, \"r\") as f:\n",
    "        markers = f['imgs'].attrs['Marker']\n",
    "        imgs = f['imgs'][:]\n",
    "\n",
    "    img = imgs[list(markers).index(marker)]\n",
    "\n",
    "    name = '_'.join(row[1:3])\n",
    "    \n",
    "    # Read masks\n",
    "    mask_cyto_path = masks_path[name]['cell']\n",
    "    mask_nuclei_path = masks_path[name]['nuclei']\n",
    "    df_path =  masks_path[name]['df']\n",
    "    mask_cyto = skimage.io.imread(mask_cyto_path)\n",
    "    \n",
    "    # Get values\n",
    "    props = measure.regionprops_table(mask_cyto, intensity_image=img,\n",
    "                                  properties=('label', 'area', 'mean_intensity'))\n",
    "    df = pd.DataFrame(props)\n",
    "    df.columns = ['Cell', 'Area', marker]\n",
    "    df['Condition'] = row.Condition\n",
    "    dfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Co-expression data\n",
    "PPI_exp_path =  data_dir / 'Navincin' / 'PPI' / 'expression'\n",
    "PPI_exp_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "df = pd.concat(dfs)\n",
    "df = df[df.Area > 7000]\n",
    "\n",
    "df.to_csv(PPI_exp_path / 'bcatenin_intensity.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cell</th>\n",
       "      <th>Area</th>\n",
       "      <th>b-catenin/e-cadherin</th>\n",
       "      <th>Condition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>24617.0</td>\n",
       "      <td>18.369826</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>39</td>\n",
       "      <td>22241.0</td>\n",
       "      <td>9.440987</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40</td>\n",
       "      <td>34437.0</td>\n",
       "      <td>25.626942</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>41</td>\n",
       "      <td>11747.0</td>\n",
       "      <td>8.277688</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>42</td>\n",
       "      <td>47756.0</td>\n",
       "      <td>3.349422</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>754</th>\n",
       "      <td>878</td>\n",
       "      <td>18216.0</td>\n",
       "      <td>32.364515</td>\n",
       "      <td>ctrl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>756</th>\n",
       "      <td>881</td>\n",
       "      <td>24188.0</td>\n",
       "      <td>20.157557</td>\n",
       "      <td>ctrl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>757</th>\n",
       "      <td>883</td>\n",
       "      <td>12183.0</td>\n",
       "      <td>24.922351</td>\n",
       "      <td>ctrl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>758</th>\n",
       "      <td>885</td>\n",
       "      <td>14823.0</td>\n",
       "      <td>28.683937</td>\n",
       "      <td>ctrl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>759</th>\n",
       "      <td>886</td>\n",
       "      <td>11959.0</td>\n",
       "      <td>42.660841</td>\n",
       "      <td>ctrl</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1345 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Cell     Area  b-catenin/e-cadherin Condition\n",
       "2      38  24617.0             18.369826         O\n",
       "3      39  22241.0              9.440987         O\n",
       "4      40  34437.0             25.626942         O\n",
       "5      41  11747.0              8.277688         O\n",
       "6      42  47756.0              3.349422         O\n",
       "..    ...      ...                   ...       ...\n",
       "754   878  18216.0             32.364515      ctrl\n",
       "756   881  24188.0             20.157557      ctrl\n",
       "757   883  12183.0             24.922351      ctrl\n",
       "758   885  14823.0             28.683937      ctrl\n",
       "759   886  11959.0             42.660841      ctrl\n",
       "\n",
       "[1345 rows x 4 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scanpy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
