{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test torch geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import networkx as nx \n",
    "import pickle \n",
    "import torch_geometric \n",
    "\n",
    "data_dir = r'Y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\9PPI\\graphs\\raw'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thu71\\Anaconda3\\envs\\snowflake\\lib\\site-packages\\torch_geometric\\utils\\convert.py:192: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_new.cpp:204.)\n",
      "  data[key] = torch.tensor(value)\n"
     ]
    }
   ],
   "source": [
    "for f in os.listdir(data_dir):\n",
    "    with open(os.path.join(data_dir, f), 'rb') as file:\n",
    "        G = pickle.load(file)\n",
    "    data = torch_geometric.utils.from_networkx(G)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loop code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import skimage.io\n",
    "\n",
    "from collections import defaultdict\n",
    "from tqdm.auto import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "import re\n",
    "import tifffile as tiff\n",
    "import pickle\n",
    "\n",
    "import networkx as nx\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric.utils\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_dir = (Path().cwd().parents[0]).absolute()\n",
    "\n",
    "module_path = str(p_dir / \"src\")\n",
    "\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = (Path().cwd().parents[0] / 'data').absolute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning.pytorch as pl\n",
    "import PPIGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "# Define condition mapping\n",
    "condition_mapping = {'HCC827Ctrl': 0, 'HCC827Osim': 1}\n",
    "\n",
    "# Load graph dataset and process if neede\n",
    "graph_path = data_dir / 'OCT Cell Culture' / 'Whole' / 'graphs' \n",
    "\n",
    "dataset = PPIGraph.GraphDataset(graph_path, 'raw', 'pt', condition_mapping=condition_mapping, n_c=2)\n",
    "train_set, val_set, test_set = PPIGraph.train_test_val_split(dataset)\n",
    "\n",
    "# Create Dataloader\n",
    "train_loader = DataLoader(train_set, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: GraphDataset(2064):\n",
      "======================\n",
      "Number of graphs: 2064\n",
      "Number of features: 5\n",
      "Number of classes: 2\n",
      "Train set: 992, test set: 825, val set: 247\n",
      "Step 1:\n",
      "=======\n",
      "Number of graphs in the current batch: 32\n",
      "DataBatch(edge_index=[2, 22198], pos=[4067, 2], labels=[4067, 5], nuclei=[4067], weight=[22198], condition=[32], fov=[32], id=[32], train_mask=[4067], test_mask=[4067], edge_attr=[22198, 2], x=[4067, 5], y=[32], edge_weight=[22198], name=[32], batch=[4067], ptr=[33])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print some information on the dataset\n",
    "print(f'Dataset: {dataset}:')\n",
    "print('======================')\n",
    "print(f'Number of graphs: {len(dataset)}')\n",
    "print(f'Number of features: {dataset.num_features}')\n",
    "print(f'Number of classes: {dataset.num_classes}')\n",
    "\n",
    "print(f'Train set: {len(train_set)}, test set: {len(test_set)}, val set: {len(val_set)}')\n",
    "for step, data in enumerate(train_loader):\n",
    "\n",
    "    print(f'Step {step + 1}:')\n",
    "    print('=======')\n",
    "    print(f'Number of graphs in the current batch: {data.num_graphs}')\n",
    "    print(data)\n",
    "    print()\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-fold on filter dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from lightning.pytorch.accelerators import find_usable_cuda_devices\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from torch.utils.data import SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out by maximum number of counts per cell\n",
    "min_count = 20\n",
    "max_count = 70\n",
    "\n",
    "graph_path = data_dir / 'OCT Cell Culture' / 'Whole' / 'graphs' \n",
    "dataset = PPIGraph.GraphDataset(graph_path, 'raw', 'pt', condition_mapping=condition_mapping, n_c=2)\n",
    "\n",
    "# Create Dataloader\n",
    "loader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# Get Indices\n",
    "indices = []\n",
    "for step, data in enumerate(loader):\n",
    "    if len(data.x) <= min_count:\n",
    "        continue \n",
    "    \n",
    "    if (data.x.sum(axis=0) >= max_count).any():\n",
    "        continue\n",
    "    indices.append(step)\n",
    "    \n",
    "# Get subset dataset\n",
    "dataset_filtered = dataset.index_select(indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1522"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = '5PPI'\n",
    "condition = 'Kfold'\n",
    "checkpoint_folder = (Path().cwd().parents[0]).absolute() / 'data' /'OCT Cell Culture' / 'Whole' /\"saved_models\" / dataset_name / f\"Graph_GNNs_{condition}\" \n",
    "project_name = f'PLA_01102024_{dataset_name}_{condition}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    }
   ],
   "source": [
    "AVAIL_GPUS = [1]\n",
    "\n",
    "# Setting the seed\n",
    "pl.seed_everything(42)\n",
    "\n",
    "NUM_LAYERS = 2\n",
    "HIDDEN_CHANNELS = 16\n",
    "pools = ['mean', 'max', 'sum', 'attention', 'attention2']\n",
    "num_layers = [2,3,4]\n",
    "hiddens = [16, 32, 64]\n",
    "\n",
    "epochs = 50\n",
    "models = ['MLP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision('medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_2_16_onehot_0\\mean\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_2_16_onehot_0\\max\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_2_16_onehot_0\\sum\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_2_16_onehot_0\\attention\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_2_16_onehot_0\\attention2\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_2_32_onehot_0\\mean\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_2_32_onehot_0\\max\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_2_32_onehot_0\\sum\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_2_32_onehot_0\\attention\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_2_32_onehot_0\\attention2\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_2_64_onehot_0\\mean\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_2_64_onehot_0\\max\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_2_64_onehot_0\\sum\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_2_64_onehot_0\\attention\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_2_64_onehot_0\\attention2\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_3_16_onehot_0\\mean\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_3_16_onehot_0\\max\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_3_16_onehot_0\\sum\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_3_16_onehot_0\\attention\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_3_16_onehot_0\\attention2\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_3_32_onehot_0\\mean\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_3_32_onehot_0\\max\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_3_32_onehot_0\\sum\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_3_32_onehot_0\\attention\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_3_32_onehot_0\\attention2\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_3_64_onehot_0\\mean\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_3_64_onehot_0\\max\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_3_64_onehot_0\\sum\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_3_64_onehot_0\\attention\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_3_64_onehot_0\\attention2\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_4_16_onehot_0\\mean\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_4_16_onehot_0\\max\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_4_16_onehot_0\\sum\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_4_16_onehot_0\\attention\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_4_16_onehot_0\\attention2\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_4_32_onehot_0\\mean\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_4_32_onehot_0\\max\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_4_32_onehot_0\\sum\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_4_32_onehot_0\\attention\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_4_32_onehot_0\\attention2\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_4_64_onehot_0\\mean\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_4_64_onehot_0\\max\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_4_64_onehot_0\\sum\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_4_64_onehot_0\\attention\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_4_64_onehot_0\\attention2\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_2_16_onehot_1\\mean\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_2_16_onehot_1\\max\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_2_16_onehot_1\\sum\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_2_16_onehot_1\\attention\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_2_16_onehot_1\\attention2\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_2_32_onehot_1\\mean\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_2_32_onehot_1\\max\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_2_32_onehot_1\\sum\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_2_32_onehot_1\\attention\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_2_32_onehot_1\\attention2\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_2_64_onehot_1\\mean\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_2_64_onehot_1\\max\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_2_64_onehot_1\\sum\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_2_64_onehot_1\\attention\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_2_64_onehot_1\\attention2\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_3_16_onehot_1\\mean\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_3_16_onehot_1\\max\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_3_16_onehot_1\\sum\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_3_16_onehot_1\\attention\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_3_16_onehot_1\\attention2\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_3_32_onehot_1\\mean\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_3_32_onehot_1\\max\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_3_32_onehot_1\\sum\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_3_32_onehot_1\\attention\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_3_32_onehot_1\\attention2\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_3_64_onehot_1\\mean\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_3_64_onehot_1\\max\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_3_64_onehot_1\\sum\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_3_64_onehot_1\\attention\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_3_64_onehot_1\\attention2\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_4_16_onehot_1\\mean\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_4_16_onehot_1\\max\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_4_16_onehot_1\\sum\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_4_16_onehot_1\\attention\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_4_16_onehot_1\\attention2\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_4_32_onehot_1\\mean\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_4_32_onehot_1\\max\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_4_32_onehot_1\\sum\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_4_32_onehot_1\\attention\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_4_32_onehot_1\\attention2\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_4_64_onehot_1\\mean\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_4_64_onehot_1\\max\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_4_64_onehot_1\\sum\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_4_64_onehot_1\\attention\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_4_64_onehot_1\\attention2\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_2_16_onehot_2\\mean\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_2_16_onehot_2\\max\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_2_16_onehot_2\\sum\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_2_16_onehot_2\\attention\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_2_16_onehot_2\\attention2\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_2_32_onehot_2\\mean\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_2_32_onehot_2\\max\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_2_32_onehot_2\\sum\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_2_32_onehot_2\\attention\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_2_32_onehot_2\\attention2\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_2_64_onehot_2\\mean\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_2_64_onehot_2\\max\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_2_64_onehot_2\\sum\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_2_64_onehot_2\\attention\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_2_64_onehot_2\\attention2\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_3_16_onehot_2\\mean\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_3_16_onehot_2\\max\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_3_16_onehot_2\\sum\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_3_16_onehot_2\\attention\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_3_16_onehot_2\\attention2\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_3_32_onehot_2\\mean\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_3_32_onehot_2\\max\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_3_32_onehot_2\\sum\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_3_32_onehot_2\\attention\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_3_32_onehot_2\\attention2\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_3_64_onehot_2\\mean\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_3_64_onehot_2\\max\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_3_64_onehot_2\\sum\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_3_64_onehot_2\\attention\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_3_64_onehot_2\\attention2\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_4_16_onehot_2\\mean\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_4_16_onehot_2\\max\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_4_16_onehot_2\\sum\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_4_16_onehot_2\\attention\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_4_16_onehot_2\\attention2\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_4_32_onehot_2\\mean\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_4_32_onehot_2\\max\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_4_32_onehot_2\\sum\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_4_32_onehot_2\\attention\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_4_32_onehot_2\\attention2\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_4_64_onehot_2\\mean\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_4_64_onehot_2\\max\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_4_64_onehot_2\\sum\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_4_64_onehot_2\\attention\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_4_64_onehot_2\\attention2\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_2_16_onehot_3\\mean\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_2_16_onehot_3\\max\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_2_16_onehot_3\\sum\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_2_16_onehot_3\\attention\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_2_16_onehot_3\\attention2\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_2_32_onehot_3\\mean\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_2_32_onehot_3\\max\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_2_32_onehot_3\\sum\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_2_32_onehot_3\\attention\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_2_32_onehot_3\\attention2\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_2_64_onehot_3\\mean\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_2_64_onehot_3\\max\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_2_64_onehot_3\\sum\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_2_64_onehot_3\\attention\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_2_64_onehot_3\\attention2\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_3_16_onehot_3\\mean\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_3_16_onehot_3\\max\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_3_16_onehot_3\\sum\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_3_16_onehot_3\\attention\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_3_16_onehot_3\\attention2\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_3_32_onehot_3\\mean\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_3_32_onehot_3\\max\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_3_32_onehot_3\\sum\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_3_32_onehot_3\\attention\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_3_32_onehot_3\\attention2\\GraphLevelMLP\\GraphLevelMLP.ckpt\n",
      "y:\\coskun-lab\\Thomas\\23_PLA_revision\\data\\OCT Cell Culture\\Whole\\saved_models\\5PPI\\Graph_GNNs_Kfold\\MLP_3_64_onehot_3\\mean\\GraphLevelMLP\\GraphLevelMLP.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mthoomas\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d9159b144784614a1436ea216fe6c8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240115_022839-ayagr3uh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/ayagr3uh' target=\"_blank\">MLP_3_64_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/ayagr3uh' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/ayagr3uh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "c:\\Users\\thu71\\Anaconda3\\envs\\snowflake\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:395: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 8.7 K \n",
      "2 | head        | Sequential       | 2.1 K \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "10.8 K    Trainable params\n",
      "0         Non-trainable params\n",
      "10.8 K    Total params\n",
      "0.043     Total estimated model params size (MB)\n",
      "C:\\Users\\thu71\\AppData\\Roaming\\Python\\Python39\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\thu71\\AppData\\Roaming\\Python\\Python39\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\thu71\\AppData\\Roaming\\Python\\Python39\\site-packages\\lightning\\pytorch\\utilities\\data.py:77: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 2. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  warning_cache.warn(\n",
      "C:\\Users\\thu71\\AppData\\Roaming\\Python\\Python39\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\thu71\\AppData\\Roaming\\Python\\Python39\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:280: PossibleUserWarning: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fc8d7ac6f96479c90306fb02a327130",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▂▁▃▃▃▃▄▂▄▃▄▅▃▄▅▅▅▅▇▆▅▄█▄▄▅▆▅▄▇▇▇▇▅▆▅▆▅▇▆</td></tr><tr><td>train_auc</td><td>▁▃▁▃▄▄▄▂▅▃▃▄▄▄▄▄▅▅▇▆▆▆█▆▄▅▇▆▆▇▆▇███▆▇▅▇▇</td></tr><tr><td>train_f1</td><td>▃▃▁▁▁▁▂▃▃▃▂▆▇▂▄▅▅▆▇█▆▆▆█▇▅▇▆▅▆█▇█▇▇▇▆▅▇█</td></tr><tr><td>train_loss_epoch</td><td>█▇▆▆▆▅▅▆▅▅▅▄▅▅▄▄▃▄▃▃▃▃▁▄▄▃▃▃▃▂▃▂▂▂▂▄▂▄▁▃</td></tr><tr><td>train_loss_step</td><td>▅▅█▅▄▅▃▃▄▂▅▄▂▄▃▃▃▁▄▂▁█▃▂▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▁▂▁▂▂▃▃▃▃▄▄▂▄▂▄▂▅▃▅▃▆▆▆▃▃▇▇▇▃▇▄█▄█▄</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>▇▂▂▃▂▂▂▂▂▂▂▁▁▆▇▇▇▇█▇▇▇▇▇▇▇▇▇▇▆▁▇█▇▇▆▁▁▆▆</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_epoch</td><td>█▇▇▇▆▇▆▇▇▅▆▆▆▆▇▅▄▄▄▅▃▃▅▄▄▃▄▆▁▅▂▄▅▃▃▄▅█▇▅</td></tr><tr><td>val_loss_step</td><td>▆▅▆▅▅▅▅▆▅▅▄▆▅▆▆▅▅▄▄▄▅▃▅▅▄▃▄▅▁▅▄▅▅▃▄▄▅█▄▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>49</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.63957</td></tr><tr><td>train_auc</td><td>0.61075</td></tr><tr><td>train_f1</td><td>0.39615</td></tr><tr><td>train_loss_epoch</td><td>0.63865</td></tr><tr><td>train_loss_step</td><td>0.60593</td></tr><tr><td>trainer/global_step</td><td>249</td></tr><tr><td>val_acc</td><td>0.66118</td></tr><tr><td>val_auc</td><td>0.52263</td></tr><tr><td>val_f1</td><td>0.0</td></tr><tr><td>val_loss_epoch</td><td>0.65233</td></tr><tr><td>val_loss_step</td><td>0.66859</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_64_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/ayagr3uh' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/ayagr3uh</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240115_022839-ayagr3uh\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "953ffdae61e24ea793ccd85836ffe815",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240115_024031-erknrxj7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/erknrxj7' target=\"_blank\">MLP_3_64_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/erknrxj7' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/erknrxj7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 8.7 K \n",
      "2 | head        | Sequential       | 2.1 K \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "10.8 K    Trainable params\n",
      "0         Non-trainable params\n",
      "10.8 K    Total params\n",
      "0.043     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57899f1295c64045a2445920e06332d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▃▂▄▄▅▅▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇▇▇▇█▇▇▇█▇▇█▇▇▇▇</td></tr><tr><td>train_auc</td><td>▂▂▂▃▃▃▃▅▇▇▆█▅▆▆▆▅▄▂▂▂▁▂▃▃▄▅▄▅▄▃▁▄▂▂▁▂▂▂▃</td></tr><tr><td>train_f1</td><td>▄▂▄▂▁▃▂▄▆▇▆▇▇▇▇▇▆▇▆▇▆▇▅█▇▇▇▇█▆▇▆█▇▆█▆▇▆▇</td></tr><tr><td>train_loss_epoch</td><td>█▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>███▆▅▄▅▄▃▅▄▄▅▄▃▃▃▄▄▁▄▅▄▃▄</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▁▂▁▂▂▃▃▃▃▄▄▂▄▂▄▂▅▃▅▃▆▆▆▃▃▇▇▇▃▇▄█▄█▄</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▂▃▆▅▆▄▆▆▅▅▅▇▅▅▅▆▆▆▇▇▆▇█▆▇▆█▇▇▆▇█▇▇█</td></tr><tr><td>val_auc</td><td>▃▁▂▅▇▇██▇████████▆▆▅▃▂▆▆███▆██▇█▇▁▄▄▁▃▇▆</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▂▃▆▆██▇▇██▆█▆█▆█▆█▇▇███▇█▆██▇█▇█▇██</td></tr><tr><td>val_loss_epoch</td><td>███▇▆▆▆▅▄▃▄▄▃▃▃▄▃▃▅▂▃▃▄▃▂▃▃▄▁▃▃▂▂▂▃▂▂▃▃▂</td></tr><tr><td>val_loss_step</td><td>█▇█▇▇▇▇▆▄▅▄▅▄▃▃▅▅▃▆▄▅▄▅▄▄▅▄▄▁▄▄▃▃▃▄▄▃▄▄▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>49</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.71511</td></tr><tr><td>train_auc</td><td>0.5327</td></tr><tr><td>train_f1</td><td>0.58641</td></tr><tr><td>train_loss_epoch</td><td>0.55361</td></tr><tr><td>train_loss_step</td><td>0.58241</td></tr><tr><td>trainer/global_step</td><td>249</td></tr><tr><td>val_acc</td><td>0.80263</td></tr><tr><td>val_auc</td><td>0.7369</td></tr><tr><td>val_f1</td><td>0.68085</td></tr><tr><td>val_loss_epoch</td><td>0.48649</td></tr><tr><td>val_loss_step</td><td>0.47378</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_64_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/erknrxj7' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/erknrxj7</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240115_024031-erknrxj7\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24cef742dba642418504b3a375f7a79c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240115_025244-hp0x14uu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/hp0x14uu' target=\"_blank\">MLP_3_64_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/hp0x14uu' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/hp0x14uu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "c:\\Users\\thu71\\Anaconda3\\envs\\snowflake\\lib\\site-packages\\torch_geometric\\deprecation.py:12: UserWarning: 'nn.glob.GlobalAttention' is deprecated, use 'nn.aggr.AttentionalAggregation' instead\n",
      "  warnings.warn(out)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 8.7 K \n",
      "2  | head        | Sequential       | 2.1 K \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 65    \n",
      "--------------------------------------------------\n",
      "10.9 K    Trainable params\n",
      "0         Non-trainable params\n",
      "10.9 K    Total params\n",
      "0.044     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba54abac4a584464bbc6758ec5d6085e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▂▂▅▆▆▇▄▆▆▆▅█▅▇▇▇▇█▇█▇▇▇▇▇▇▇▇▇▇▇▇▆▆▇▇▇▇</td></tr><tr><td>train_auc</td><td>▁▂▃▃▆▇▇▇▆▇▇█▇██████████▇██████████▇█████</td></tr><tr><td>train_f1</td><td>▂▁▁▁▆▇▇▇▆▇▇▆▇█▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇▇▇█▆█▇▇▇█</td></tr><tr><td>train_loss_epoch</td><td>█▇▇▆▄▃▃▂▄▂▃▂▂▂▂▂▁▂▁▁▁▁▁▂▂▂▂▁▂▂▂▁▂▂▃▃▂▂▂▁</td></tr><tr><td>train_loss_step</td><td>██▂▃▇▅▄▃▃▂▅▃▂▄▂▄▁▃▂▁▁▃▆▄▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▁▂▁▂▂▃▃▃▃▄▄▂▄▂▄▂▅▃▅▃▆▆▆▃▃▇▇▇▃▇▄█▄█▄</td></tr><tr><td>val_acc</td><td>▁▁▁▁▇▆▆▇▅▆▄▇▆▆▇▆▅▇▅▇▇▆█▆▅▆▇▇▆▄▇▆█▅▄▃▇▅▇▆</td></tr><tr><td>val_auc</td><td>█▅▂▁▃▄▅▅▅▆▅▅▅▅▆▆▅▆▅▆▆▆▇▆▇▆▆▆▇▆▇▆▇▆▇▅▆▅▆▆</td></tr><tr><td>val_f1</td><td>▁▁▁▁█▇▇▇▅█▅█▇▇▇▆▆█▆██▇█▇█▆▇▇█▅▇▆█▅█▄▇▆▇▇</td></tr><tr><td>val_loss_epoch</td><td>██▆▇▆▅▃▃▃▅▃▄▃▃▅▃▂▄▃▄▃▄▄▂▅▄▃▃▄▅▃▁▃▄▅▄▄▄▅▄</td></tr><tr><td>val_loss_step</td><td>██▆█▅▃▃▃▄▆▁▃▃▂▆▄▃▄▂▃▄▅▄▃▄▄▃▃▄▇▄▃▁▆▃▄▂▅▃▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>49</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.69951</td></tr><tr><td>train_auc</td><td>0.75892</td></tr><tr><td>train_f1</td><td>0.57931</td></tr><tr><td>train_loss_epoch</td><td>0.56134</td></tr><tr><td>train_loss_step</td><td>0.53169</td></tr><tr><td>trainer/global_step</td><td>249</td></tr><tr><td>val_acc</td><td>0.72368</td></tr><tr><td>val_auc</td><td>0.80124</td></tr><tr><td>val_f1</td><td>0.53846</td></tr><tr><td>val_loss_epoch</td><td>0.53733</td></tr><tr><td>val_loss_step</td><td>0.57532</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_64_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/hp0x14uu' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/hp0x14uu</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240115_025244-hp0x14uu\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6b120757266468da83da577fb66112e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.0169333333382383, max=1.0))…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240115_030508-ruw25r8t</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/ruw25r8t' target=\"_blank\">MLP_3_64_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/ruw25r8t' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/ruw25r8t</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 8.7 K \n",
      "2  | head        | Sequential       | 2.1 K \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 4.2 K \n",
      "--------------------------------------------------\n",
      "15.0 K    Trainable params\n",
      "0         Non-trainable params\n",
      "15.0 K    Total params\n",
      "0.060     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abf92d2f03ef433fbb131b3a70564759",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▂▂▆▅▇▅▇▇▇▇▇▇▇▇█▇▇▇▇▇▇▇█▅█▆▇▇█▇▇█▇█▇▇▇▇</td></tr><tr><td>train_auc</td><td>▁▂▂▄▇▆▇▇███▇██▇██████████▇█▇▇███████████</td></tr><tr><td>train_f1</td><td>▁▁▁▁▆▇█▆▇▇█▇▇▇▇▇█▇▇▇▇▇▇██▆█▆▇█▇▇▇█▇██▇▇▇</td></tr><tr><td>train_loss_epoch</td><td>█▇▇▆▃▅▂▂▂▁▂▂▁▂▂▁▂▂▁▁▁▂▂▂▂▃▂▂▂▂▂▂▁▁▂▂▂▁▂▁</td></tr><tr><td>train_loss_step</td><td>█▆▁▂▆▄▃▃▅█▃▁▂▄▅▄▃▅▅▂▂▅▁▂▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▁▂▁▂▂▃▃▃▃▄▄▂▄▂▄▂▅▃▅▃▆▆▆▃▃▇▇▇▃▇▄█▄█▄</td></tr><tr><td>val_acc</td><td>▃▃▃▃▁▇▇▇▇█▇▇▇██▇█▇███▇█▇▆█▇▇█▇▇▇▇▇█▇▇▇██</td></tr><tr><td>val_auc</td><td>▇▆▃▁▃▃▃▄▅▆▆▇▇█▇██▇▇██▇█▇▇█████▇█▇▇█▇█▇██</td></tr><tr><td>val_f1</td><td>▁▁▁▁█▇▇▇▇█▇▇▇█▇▇█▇▇██▇█▆▆█▇▇▇▇▇▇▇▇█▇▇▇██</td></tr><tr><td>val_loss_epoch</td><td>▇▇▆▆█▃▃▄▄▄▂▄▃▃▂▁▃▂▂▃▁▂▂▃▁▄▃▂▂▂▂▃▃▂▃▂▂▂▂▂</td></tr><tr><td>val_loss_step</td><td>▇▇▆▇█▄▃▇▂▃▃▆▂▃▃▂▂▃▃▂▃▂▂▄▃▅▃▃▃▃▂▂▄▄▂▃▃▄▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>49</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.69951</td></tr><tr><td>train_auc</td><td>0.75497</td></tr><tr><td>train_f1</td><td>0.54364</td></tr><tr><td>train_loss_epoch</td><td>0.56225</td></tr><tr><td>train_loss_step</td><td>0.5698</td></tr><tr><td>trainer/global_step</td><td>249</td></tr><tr><td>val_acc</td><td>0.74671</td></tr><tr><td>val_auc</td><td>0.80674</td></tr><tr><td>val_f1</td><td>0.62802</td></tr><tr><td>val_loss_epoch</td><td>0.5075</td></tr><tr><td>val_loss_step</td><td>0.47529</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_64_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/ruw25r8t' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/ruw25r8t</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240115_030508-ruw25r8t\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88e4a6c27c2a49beaf5e3d2ab95efbde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.0169333333382383, max=1.0))…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240115_031646-jtjq3mao</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/jtjq3mao' target=\"_blank\">MLP_4_16_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/jtjq3mao' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/jtjq3mao</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 912   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "1.1 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.1 K     Total params\n",
      "0.004     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5b575cac472477b85621c5ebcdd9820",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▅▅▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▇█▇█▇▇██▇█▇██████████</td></tr><tr><td>train_auc</td><td>▂▂▂▂▂▃▂▁▁▂▂▂▁▂▃▂▂▄▅▇▇▇▇▇▆█▇▇▇▆▇▇▇▇██████</td></tr><tr><td>train_f1</td><td>█▇▅▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁▄█▇▇▇▇▇█▆▇▆█▇▇▇▇▇▇▇▇▇</td></tr><tr><td>train_loss_epoch</td><td>█▆▅▅▅▅▄▄▄▅▄▄▅▄▄▄▄▄▃▂▂▂▂▂▂▁▂▂▁▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▇▆▇▅▇▇▇▅▅▄▆▂▄▂▄▄▃▅▂▁▅▁▄▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▁▂▁▂▂▃▃▃▃▄▄▂▄▂▄▂▅▃▅▃▆▆▆▃▃▇▇▇▃▇▄█▄█▄</td></tr><tr><td>val_acc</td><td>▁▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇███████▇███████████</td></tr><tr><td>val_auc</td><td>▁▄▅▆▆▇████████▇▇▇▇▇▇████████████████████</td></tr><tr><td>val_f1</td><td>▇▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▅█▇██▇█▇▇▆██▇▇▇▇████▇</td></tr><tr><td>val_loss_epoch</td><td>█▆▅▅▅▅▅▅▅▅▅▄▅▅▄▄▄▄▃▃▂▁▂▂▂▂▂▂▂▂▂▁▂▂▂▁▂▂▂▁</td></tr><tr><td>val_loss_step</td><td>█▆▆▅▅▅▅▅▅▅▆▄▅▄▄▅▄▄▄▂▂▁▂▂▂▂▂▂▃▂▂▂▄▂▂▂▂▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>49</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.68145</td></tr><tr><td>train_auc</td><td>0.7125</td></tr><tr><td>train_f1</td><td>0.44092</td></tr><tr><td>train_loss_epoch</td><td>0.59178</td></tr><tr><td>train_loss_step</td><td>0.58926</td></tr><tr><td>trainer/global_step</td><td>249</td></tr><tr><td>val_acc</td><td>0.72368</td></tr><tr><td>val_auc</td><td>0.79877</td></tr><tr><td>val_f1</td><td>0.53846</td></tr><tr><td>val_loss_epoch</td><td>0.53367</td></tr><tr><td>val_loss_step</td><td>0.51982</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_16_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/jtjq3mao' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/jtjq3mao</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240115_031646-jtjq3mao\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "623a83c3223549488d782cb3dc3ff3a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.0169333333382383, max=1.0))…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240115_032816-0688qk61</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/0688qk61' target=\"_blank\">MLP_4_16_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/0688qk61' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/0688qk61</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 912   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "1.1 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.1 K     Total params\n",
      "0.004     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e27c32789436431a9ea2b4ed69fe731c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▆▆▆▆▇▇▇▇▇▇█████████████████████████████</td></tr><tr><td>train_auc</td><td>▅▄▂▄▂▄▄▃▃▄▃▅▁▃▆▅▆▅▆▅▃▅█▆▅▅▅▆█▅▅▆▇▇▆▅▅▆▇▄</td></tr><tr><td>train_f1</td><td>█▆▅▅▄▃▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_epoch</td><td>█▄▅▄▃▃▃▃▂▂▂▂▂▂▁▂▂▂▁▁▂▂▁▂▂▂▁▁▁▂▁▂▁▁▂▁▁▁▁▂</td></tr><tr><td>train_loss_step</td><td>█▅▄▄▂▅▅▄▂▃▂▆▂▃▄▄▂▃▄▂▁▃▂▅▅</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▁▂▁▂▂▃▃▃▃▄▄▂▄▂▄▂▅▃▅▃▆▆▆▃▃▇▇▇▃▇▄█▄█▄</td></tr><tr><td>val_acc</td><td>▁▁▁▁████████████████████████████████████</td></tr><tr><td>val_auc</td><td>█▇▇▇▇▇▇▇▆▆▆▇▇▇▇▇▇▇▇▁▅▅▅▄▄▄▄▄▄▄▄▄▄▇▄▄▄▄▄▄</td></tr><tr><td>val_f1</td><td>████▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_epoch</td><td>█▆▅▅▄▃▃▃▂▂▃▂▂▂▂▁▂▂▂▂▂▂▂▂▂▃▂▁▂▂▃▂▂▂▂▂▁▁▂▂</td></tr><tr><td>val_loss_step</td><td>█▆▅▆▄▄▄▄▄▃▄▂▃▂▂▃▃▂▃▃▃▃▂▃▃▄▃▃▃▃▃▃▃▂▃▃▁▁▃▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>49</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.61741</td></tr><tr><td>train_auc</td><td>0.50363</td></tr><tr><td>train_f1</td><td>0.0</td></tr><tr><td>train_loss_epoch</td><td>0.66995</td></tr><tr><td>train_loss_step</td><td>0.68859</td></tr><tr><td>trainer/global_step</td><td>249</td></tr><tr><td>val_acc</td><td>0.66118</td></tr><tr><td>val_auc</td><td>0.49751</td></tr><tr><td>val_f1</td><td>0.0</td></tr><tr><td>val_loss_epoch</td><td>0.66517</td></tr><tr><td>val_loss_step</td><td>0.6742</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_16_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/0688qk61' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/0688qk61</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240115_032816-0688qk61\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b1a1b1ae4964a789abd5f8fbb6807e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.0169333333382383, max=1.0))…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240115_033930-pvblkn9g</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/pvblkn9g' target=\"_blank\">MLP_4_16_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/pvblkn9g' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/pvblkn9g</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 912   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "1.1 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.1 K     Total params\n",
      "0.004     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ee6ffda899e4e6cad437c8ba778efd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▅▂▁▁▃▃▂▃▅▄▅▆▆▆▆▇▆▆▇▇▇▇▇▇▇▇▇▆▇▇▇▇▇▇▇▇▇▇█</td></tr><tr><td>train_auc</td><td>▂▂▂▂▁▂▂▂▂▂▂▃▁▂▂▃▄▄▃▃▃▃▅▅▄▄▆▅▅▄▆▄▇▆▆▆█▇▆█</td></tr><tr><td>train_f1</td><td>▇▅▇█▆▇▇▇▆▆▅▄▃▂▃▂▁▁▂▂▁▂▃▃▂▂▃▃▂▄▃▃▃▃▄▄▅▄▄▆</td></tr><tr><td>train_loss_epoch</td><td>█▅▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▁▂▁▂▂▃▃▃▃▄▄▂▄▂▄▂▅▃▅▃▆▆▆▃▃▇▇▇▃▇▄█▄█▄</td></tr><tr><td>val_acc</td><td>▇▇▁▁▆▅▄▅▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█</td></tr><tr><td>val_auc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▃▂▃▅▆▅▇▆▇▇▇▇▇▇████████</td></tr><tr><td>val_f1</td><td>▁▁██▃▄▅▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▅</td></tr><tr><td>val_loss_epoch</td><td>█▅▄▄▃▃▃▃▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▁</td></tr><tr><td>val_loss_step</td><td>█▄▄▄▃▃▃▃▃▃▃▂▃▂▂▃▂▂▂▂▂▂▂▂▂▃▂▂▂▂▂▂▃▂▂▂▁▁▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>49</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.65353</td></tr><tr><td>train_auc</td><td>0.6351</td></tr><tr><td>train_f1</td><td>0.35276</td></tr><tr><td>train_loss_epoch</td><td>0.63188</td></tr><tr><td>train_loss_step</td><td>0.62844</td></tr><tr><td>trainer/global_step</td><td>249</td></tr><tr><td>val_acc</td><td>0.71382</td></tr><tr><td>val_auc</td><td>0.80839</td></tr><tr><td>val_f1</td><td>0.31496</td></tr><tr><td>val_loss_epoch</td><td>0.60944</td></tr><tr><td>val_loss_step</td><td>0.62373</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_16_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/pvblkn9g' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/pvblkn9g</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240115_033930-pvblkn9g\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef60cc7d7f0941e486d0abc0d68a8e89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016933333330477276, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240115_035046-uf9a65t1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/uf9a65t1' target=\"_blank\">MLP_4_16_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/uf9a65t1' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/uf9a65t1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 912   \n",
      "2  | head        | Sequential       | 154   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 17    \n",
      "--------------------------------------------------\n",
      "1.1 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.1 K     Total params\n",
      "0.004     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f982705eb7b042018670561d422ddd94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▅▅▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇█▇▇█▇▇█▇██████</td></tr><tr><td>train_auc</td><td>▁▁▁▁▂▂▃▂▁▂▂▂▂▁▁▁▃▄▃▄▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇█████</td></tr><tr><td>train_f1</td><td>█▇▅▃▃▃▃▂▂▂▂▂▁▂▁▁▁▁▁▃▅▆▅▆▆▇▇▇▅▇▇▆▇▇▇▇██▇▇</td></tr><tr><td>train_loss_epoch</td><td>█▆▅▅▅▅▄▅▅▄▄▄▅▄▅▄▄▄▄▄▂▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▂▁▁</td></tr><tr><td>train_loss_step</td><td>▇▆▇▆█▇▆▆▇▇▆▆▄▃▃▄▆▄▄▄▄▁▄▄▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▁▂▁▂▂▃▃▃▃▄▄▂▄▂▄▂▅▃▅▃▆▆▆▃▃▇▇▇▃▇▄█▄█▄</td></tr><tr><td>val_acc</td><td>▁▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇█▇▇█▇██▇▇██▇██</td></tr><tr><td>val_auc</td><td>▁▅▅▆▇▇██████████████████████████████████</td></tr><tr><td>val_f1</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▄▅▆▄█▅█▅▅█▅██▆▆▇▇▆██</td></tr><tr><td>val_loss_epoch</td><td>█▇▆▆▆▆▆▆▅▅▆▆▆▅▅▅▅▅▅▄▄▄▃▂▃▁▃▄▃▃▂▂▃▃▂▂▂▁▂▂</td></tr><tr><td>val_loss_step</td><td>█▇▆▇▆▆▆▆▆▆▆▆▆▅▆▆▅▆▅▅▄▅▄▃▃▁▃▃▄▄▄▃▄▄▃▃▄▂▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>49</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.67077</td></tr><tr><td>train_auc</td><td>0.72046</td></tr><tr><td>train_f1</td><td>0.4559</td></tr><tr><td>train_loss_epoch</td><td>0.58939</td></tr><tr><td>train_loss_step</td><td>0.58556</td></tr><tr><td>trainer/global_step</td><td>249</td></tr><tr><td>val_acc</td><td>0.72039</td></tr><tr><td>val_auc</td><td>0.78844</td></tr><tr><td>val_f1</td><td>0.50867</td></tr><tr><td>val_loss_epoch</td><td>0.53436</td></tr><tr><td>val_loss_step</td><td>0.54487</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_16_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/uf9a65t1' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/uf9a65t1</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240115_035046-uf9a65t1\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fee62320fe94250976e80be14b820c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016933333330477276, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240115_040203-iuqbc05l</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/iuqbc05l' target=\"_blank\">MLP_4_16_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/iuqbc05l' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/iuqbc05l</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 912   \n",
      "2  | head        | Sequential       | 154   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 281   \n",
      "--------------------------------------------------\n",
      "1.3 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.3 K     Total params\n",
      "0.005     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7239683068044de78a90850413b76ca6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▇▇▇▇▇█▇█████████▇███</td></tr><tr><td>train_auc</td><td>▂▂▂▂▂▂▁▁▁▁▂▂▂▂▁▂▂▂▂▃▅▆▇▇▆▇▇█▇▇▇▇█▇██████</td></tr><tr><td>train_f1</td><td>█▇▆▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▂▆▇▇▇▇▆▇▇█▇▇▇▇▇▇▇█▇▇</td></tr><tr><td>train_loss_epoch</td><td>█▆▅▅▅▅▅▅▅▅▄▅▄▄▄▄▄▄▄▄▃▃▂▃▂▂▂▁▂▂▁▂▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▆▇▇▇▇▆▇▇▆▇▅▅▃▄▂▂▅▁▃▁▃▁▂▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▁▂▁▂▂▃▃▃▃▄▄▂▄▂▄▂▅▃▅▃▆▆▆▃▃▇▇▇▃▇▄█▄█▄</td></tr><tr><td>val_acc</td><td>▁▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇██████▇███████████</td></tr><tr><td>val_auc</td><td>▁▄▅▆▇█████████████▇▇▇▇▇█████████████████</td></tr><tr><td>val_f1</td><td>▇▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▃█▇▇█▇███▇██████████</td></tr><tr><td>val_loss_epoch</td><td>█▆▅▅▅▅▅▅▆▆▅▅▅▅▅▅▅▅▄▅▃▃▂▃▃▃▃▃▂▁▂▂▂▂▂▃▂▁▂▁</td></tr><tr><td>val_loss_step</td><td>█▇▇▅▆▆▆▆▅▅▅▇▆▆▆▆▆▆▅▅▄▅▂▃▄▆▃▃▂▁▃▃▄▃▃▂▄▂▃▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>49</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.68473</td></tr><tr><td>train_auc</td><td>0.71219</td></tr><tr><td>train_f1</td><td>0.47397</td></tr><tr><td>train_loss_epoch</td><td>0.58543</td></tr><tr><td>train_loss_step</td><td>0.58505</td></tr><tr><td>trainer/global_step</td><td>249</td></tr><tr><td>val_acc</td><td>0.73684</td></tr><tr><td>val_auc</td><td>0.79742</td></tr><tr><td>val_f1</td><td>0.58333</td></tr><tr><td>val_loss_epoch</td><td>0.51696</td></tr><tr><td>val_loss_step</td><td>0.49294</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_16_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/iuqbc05l' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/iuqbc05l</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240115_040203-iuqbc05l\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fccda6a188fc4f12806f1ff217fed767",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016933333330477276, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240115_041323-0jokh83v</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/0jokh83v' target=\"_blank\">MLP_4_32_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/0jokh83v' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/0jokh83v</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 3.4 K \n",
      "2 | head        | Sequential       | 562   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "3.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "3.9 K     Total params\n",
      "0.016     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6b5db6e775046918dd8965374b68f41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▅▅▅▅▅▅▅▅▅▅▅▇▇▇▇▇▇██▇▇▇█▇█▇██▇▇▇▇██▇█▇▇█</td></tr><tr><td>train_auc</td><td>▁▁▂▁▂▁▂▂▃▃▄▅▆▇▇▇▇▇██▇▇██▇█▇██▇██▇▇▇▇██▇█</td></tr><tr><td>train_f1</td><td>▆▂▁▁▁▁▁▁▁▁▁▂▇▇▇▇▇▇███▇▇▇▇█▇██▆█▆▇▇▇▇█▇██</td></tr><tr><td>train_loss_epoch</td><td>█▇▆▇▆▇▆▆▆▅▅▅▃▃▂▃▂▂▂▁▂▁▂▁▂▁▁▁▁▂▂▂▂▂▂▂▁▁▂▁</td></tr><tr><td>train_loss_step</td><td>▆▆▇▆▆██▄▁▄▃▂▄▂▃▃▂▂▃▄▃▁▃▂▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▁▂▁▂▂▃▃▃▃▄▄▂▄▂▄▂▅▃▅▃▆▆▆▃▃▇▇▇▃▇▄█▄█▄</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▃▆▇▄▇▇▇▆▇▇▇█▆▇█▇▆█▆▆▇▇▇▇▇▇▇▇</td></tr><tr><td>val_auc</td><td>▁▄▇▇████████████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▆▇▇▆▇█▇▇▇▇▇██▇█▇▇█▇▇▇█▇█▇█▇█</td></tr><tr><td>val_loss_epoch</td><td>█▇█▇██▇█▇▆▇▅▄▅▁▁▃▃▃▃▁▃▃▃▄▂▃▁▂▂▃▃▂▂▃▃▃▂▂▃</td></tr><tr><td>val_loss_step</td><td>▇▇▇▆▇▇▇█▆▆▇▆▄▇▁▄▄▄▄▄▄▄▅▃▄▃▄▄▂▂▄▃▃▂▄▄▅▃▃▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>49</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.71018</td></tr><tr><td>train_auc</td><td>0.74101</td></tr><tr><td>train_f1</td><td>0.58519</td></tr><tr><td>train_loss_epoch</td><td>0.57532</td></tr><tr><td>train_loss_step</td><td>0.58442</td></tr><tr><td>trainer/global_step</td><td>249</td></tr><tr><td>val_acc</td><td>0.74013</td></tr><tr><td>val_auc</td><td>0.80312</td></tr><tr><td>val_f1</td><td>0.59898</td></tr><tr><td>val_loss_epoch</td><td>0.54511</td></tr><tr><td>val_loss_step</td><td>0.56327</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_32_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/0jokh83v' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/0jokh83v</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240115_041323-0jokh83v\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ea12ca711b344be88c205e2ff5130bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240115_042438-xopebl3n</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/xopebl3n' target=\"_blank\">MLP_4_32_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/xopebl3n' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/xopebl3n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 3.4 K \n",
      "2 | head        | Sequential       | 562   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "3.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "3.9 K     Total params\n",
      "0.016     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13e12590a40a453e86dbdf69fafd2d75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▅█▇████████████████████████████████████</td></tr><tr><td>train_auc</td><td>▂▁▄▂▄▁▆▇▄▅▅▄▅▂▇▄▅▇█▆▆▇▅█▅▄▆▇█▇▆▇▆▅▇▆█▇▅█</td></tr><tr><td>train_f1</td><td>█▄▅▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_epoch</td><td>█▅▄▄▄▄▃▃▂▃▂▃▃▄▂▂▂▃▃▂▂▂▃▁▂▂▂▂▁▂▂▂▁▂▁▁▂▂▂▂</td></tr><tr><td>train_loss_step</td><td>▄▃▆▃▃█▆▄▃▄▅▄▃▂▃▁▃▃▅▄▃▁▂▃▄</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▁▂▁▂▂▃▃▃▃▄▄▂▄▂▄▂▅▃▅▃▆▆▆▃▃▇▇▇▃▇▄█▄█▄</td></tr><tr><td>val_acc</td><td>▁▁██████████████████████████████████████</td></tr><tr><td>val_auc</td><td>▄▅▄▇▆▆▇▇▇▇▇▇████████▇▇▇▆██▇█▁█▂▇█▇▇▇▃█▇█</td></tr><tr><td>val_f1</td><td>██▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_epoch</td><td>█▇▅▄▃▄▄▄▄▃▅▄▄▄▃▁▃▁▂▄▂▂▃▂▄▂▂▂▂▂▃▂▃▃▄▄▅▅▄▅</td></tr><tr><td>val_loss_step</td><td>█▇▆▄▄▄▄▆▄▄▆▅▄▅▄▃▃▁▃▃▃▂▅▄▃▃▃▃▃▂▄▃▃▃▄▅▆▆▅▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>49</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.61741</td></tr><tr><td>train_auc</td><td>0.55648</td></tr><tr><td>train_f1</td><td>0.0</td></tr><tr><td>train_loss_epoch</td><td>0.65894</td></tr><tr><td>train_loss_step</td><td>0.67322</td></tr><tr><td>trainer/global_step</td><td>249</td></tr><tr><td>val_acc</td><td>0.66118</td></tr><tr><td>val_auc</td><td>0.52867</td></tr><tr><td>val_f1</td><td>0.0</td></tr><tr><td>val_loss_epoch</td><td>0.68208</td></tr><tr><td>val_loss_step</td><td>0.685</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_32_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/xopebl3n' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/xopebl3n</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240115_042438-xopebl3n\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4119338889b44b8b9cb0e9f3b3fd677",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01691666666107873, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240115_043610-6p627f5o</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/6p627f5o' target=\"_blank\">MLP_4_32_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/6p627f5o' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/6p627f5o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 3.4 K \n",
      "2 | head        | Sequential       | 562   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "3.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "3.9 K     Total params\n",
      "0.016     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17b66d74358e44babbc2e6fc72f6fa67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▂▁▁▁▃▄▃▄▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇█▇▇██▇█████</td></tr><tr><td>train_auc</td><td>▇▆▇▇▇▇█▇█▆▆▆▇▇▆▆▅▅▅▅▄▄▃▃▃▃▃▃▃▂▂▁▂▁▂▁▂▁▂▁</td></tr><tr><td>train_f1</td><td>▆▅▆▆▄▄▄▄▁▁▂▄▂▃▄▄▆▅▆▇█▆▇▇▇▆▇▇▇▇▇▇▇█▇██▇▇█</td></tr><tr><td>train_loss_epoch</td><td>█▄▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▅▅▄▄▅▄▄▃▃▃▁▃▂▂▂▂▁▂▃▃▁▂▁▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▁▂▁▂▂▃▃▃▃▄▄▂▄▂▄▂▅▃▅▃▆▆▆▃▃▇▇▇▃▇▄█▄█▄</td></tr><tr><td>val_acc</td><td>▂▁▂▂▂▂▂▂▂▂▂▂▂▂▂▃▅▇▅▅▄▇▆▆█▆▇▇▇█████▇▇▇▇▇▇</td></tr><tr><td>val_auc</td><td>▆▆▆█▆▅▅▅▅▅▅▅▃▄▃▁▁▁▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_f1</td><td>▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▂▅█▅▅▃█▅█▇▆▇▇▆█▇▇▇▇█▇▆█▇█</td></tr><tr><td>val_loss_epoch</td><td>█▇▆▆▆▆▆▆▆▆▆▆▆▅▄▄▄▃▃▃▂▃▃▂▃▂▂▁▂▂▂▂▂▂▂▁▂▁▂▂</td></tr><tr><td>val_loss_step</td><td>█▇▇▆▆▇▆▇▆▆▇▆▆▆▅▄▃▄▄▃▄▄▅▃▃▂▃▃▃▃▃▂▂▂▂▃▃▁▂▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>49</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.70854</td></tr><tr><td>train_auc</td><td>0.27151</td></tr><tr><td>train_f1</td><td>0.54892</td></tr><tr><td>train_loss_epoch</td><td>0.5658</td></tr><tr><td>train_loss_step</td><td>0.59057</td></tr><tr><td>trainer/global_step</td><td>249</td></tr><tr><td>val_acc</td><td>0.78618</td></tr><tr><td>val_auc</td><td>0.15655</td></tr><tr><td>val_f1</td><td>0.68599</td></tr><tr><td>val_loss_epoch</td><td>0.52681</td></tr><tr><td>val_loss_step</td><td>0.54328</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_32_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/6p627f5o' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/6p627f5o</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240115_043610-6p627f5o\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a840aaf984e4d69873332ec8dba54ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016933333330477276, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240115_044735-xiu95m52</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/xiu95m52' target=\"_blank\">MLP_4_32_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/xiu95m52' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/xiu95m52</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 3.4 K \n",
      "2  | head        | Sequential       | 562   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 33    \n",
      "--------------------------------------------------\n",
      "4.0 K     Trainable params\n",
      "0         Non-trainable params\n",
      "4.0 K     Total params\n",
      "0.016     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da589bc9ff6c4e95b253e588250a1dc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▅▅▅▅▅▅▅▅▅▅▅▇▇▇▇▇▇▇▇▇██▇▇██▇█▇▇▇▇▇▇▇▇██▇</td></tr><tr><td>train_auc</td><td>▂▂▁▁▂▂▃▃▄▄▅▅▇▇▇▇▇▇█▇▇███████████████▇███</td></tr><tr><td>train_f1</td><td>▆▁▁▁▁▁▁▁▁▁▂▂▇█▆█▇▇▇▇▇▇▇█▇▇▇██▇▇▇▇▇▇▇▇█▇▇</td></tr><tr><td>train_loss_epoch</td><td>█▇▇▇▆▆▆▆▅▅▅▄▃▄▃▃▃▃▂▃▂▂▂▂▂▁▂▁▁▁▁▂▂▂▁▁▂▁▁▂</td></tr><tr><td>train_loss_step</td><td>▆██▆▆▆▄▃▄▁▅▅▂▅▃▂▂▁▃▂▁▂▂▂▅</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▁▂▁▂▂▃▃▃▃▄▄▂▄▂▄▂▅▃▅▃▆▆▆▃▃▇▇▇▃▇▄█▄█▄</td></tr><tr><td>val_acc</td><td>▂▂▂▂▂▂▂▂▂▂▁▂▇▆▅▃█▅▇▇█▇█▇▇██▇▇██▇█▆█▇▇█▇▇</td></tr><tr><td>val_auc</td><td>▁▆█████▇▇▇▇▇▇███████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▄▇▆█▄█▆██▇▇█▇▇██▇▇█▇▇▇▇█▇██▇▇</td></tr><tr><td>val_loss_epoch</td><td>██▇█▇▇▇▇▆▇▅▅▄▃▅▃▅▅▅▄▃▃▄▂▃▃▃▂▂▄▃▂▃▃▄▂▄▅▃▁</td></tr><tr><td>val_loss_step</td><td>▇▇▇▇▇▇▆▇▇▇▆▆▅▃▆▅▅▇▇▄▅▄▅▄▅▃▅▄▃▅▅▄▄▅▅▅▆█▄▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>49</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.68966</td></tr><tr><td>train_auc</td><td>0.74149</td></tr><tr><td>train_f1</td><td>0.54567</td></tr><tr><td>train_loss_epoch</td><td>0.57705</td></tr><tr><td>train_loss_step</td><td>0.62844</td></tr><tr><td>trainer/global_step</td><td>249</td></tr><tr><td>val_acc</td><td>0.71711</td></tr><tr><td>val_auc</td><td>0.79162</td></tr><tr><td>val_f1</td><td>0.50575</td></tr><tr><td>val_loss_epoch</td><td>0.46186</td></tr><tr><td>val_loss_step</td><td>0.37771</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_32_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/xiu95m52' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/xiu95m52</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240115_044735-xiu95m52\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54ad7659b28d4d2d86fcd1ad7e5b4f1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240115_045906-lswixoc1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/lswixoc1' target=\"_blank\">MLP_4_32_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/lswixoc1' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/lswixoc1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 3.4 K \n",
      "2  | head        | Sequential       | 562   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 1.1 K \n",
      "--------------------------------------------------\n",
      "5.0 K     Trainable params\n",
      "0         Non-trainable params\n",
      "5.0 K     Total params\n",
      "0.020     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ed2b167e37f4810a5c818392c75ad96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▅▅▅▅▅▅▅▅▅▅▅▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▆▇▇▇▇█</td></tr><tr><td>train_auc</td><td>▁▁▁▂▂▁▂▃▃▃▅▄▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇█▇█▇███▇███</td></tr><tr><td>train_f1</td><td>▆▁▁▁▁▁▁▁▁▁▁▁▄▆▇▇▇▇▇▇▇▆▇▆▆▇▇▇▇▇▇▇▆▇▆▇▇▇▇█</td></tr><tr><td>train_loss_epoch</td><td>█▇▇▆▆▆▆▆▆▅▄▅▃▂▃▂▁▂▂▁▁▂▂▂▂▂▂▁▂▁▁▁▂▂▁▂▁▁▂▁</td></tr><tr><td>train_loss_step</td><td>▇▇██▅▆▆▃▃▃▅▃▁▄▃▆▃▃▁▆▃▄▂▃▄</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▁▂▁▂▂▃▃▃▃▄▄▂▄▂▄▂▅▃▅▃▆▆▆▃▃▇▇▇▃▇▄█▄█▄</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▃▄▆▆▅▆▇▇▆▅▅▆█▇▇▇▇▆▆▇▇█▆▇▇▇█▆▇</td></tr><tr><td>val_auc</td><td>▁▅▇▇████▇▇▇█████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▄▅▇▇▇▇██▇▆█▆█▇▇▇█▇▇▇▇█▇▇▇██▇█</td></tr><tr><td>val_loss_epoch</td><td>▇▆█▇██▇▆▆▅▄▆▅▃▂▂▃▃▃▂▃▃▂▃▃▂▃▂▃▂▂▃▂▂▂▂▁▃▂▃</td></tr><tr><td>val_loss_step</td><td>▇▇█▆▆▆▇▄▆▆▄▅▄▄▂▄▃▄▄▄▃▂▃▃▃▃▃▃▄▃▂▃▂▃▃▂▁▄▃▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>49</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.72085</td></tr><tr><td>train_auc</td><td>0.75666</td></tr><tr><td>train_f1</td><td>0.62963</td></tr><tr><td>train_loss_epoch</td><td>0.5793</td></tr><tr><td>train_loss_step</td><td>0.60024</td></tr><tr><td>trainer/global_step</td><td>249</td></tr><tr><td>val_acc</td><td>0.74013</td></tr><tr><td>val_auc</td><td>0.80563</td></tr><tr><td>val_f1</td><td>0.61084</td></tr><tr><td>val_loss_epoch</td><td>0.55564</td></tr><tr><td>val_loss_step</td><td>0.57018</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_32_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/lswixoc1' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/lswixoc1</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240115_045906-lswixoc1\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5123df28b3646e8833afcfb278a7691",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.0169333333382383, max=1.0))…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240115_051038-77ulusbp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/77ulusbp' target=\"_blank\">MLP_4_64_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/77ulusbp' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/77ulusbp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 12.9 K\n",
      "2 | head        | Sequential       | 2.1 K \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "15.0 K    Trainable params\n",
      "0         Non-trainable params\n",
      "15.0 K    Total params\n",
      "0.060     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b6013fa140442f88cf316cb81974307",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▃▃▃▃▃▆▆▆▇▆▇█▇▇█▇███▇▇▇██▇▇▇▇█▇▇██▇▇▇▆▇▇</td></tr><tr><td>train_auc</td><td>▁▁▁▂▅▆▆▇▇▇▇▇█▇███▇███████████████████▇██</td></tr><tr><td>train_f1</td><td>▄▁▁▁▁▁▆▆▇█▆▇██▇██▇▇██▇▇▇▇█▇▇▇█▇▇█▇▇▇█▅█▇</td></tr><tr><td>train_loss_epoch</td><td>█▇▇▇▆▅▄▂▃▃▃▂▁▂▂▂▁▂▂▁▂▁▂▁▁▂▁▂▁▂▂▁▁▂▁▂▂▃▃▁</td></tr><tr><td>train_loss_step</td><td>██▅▆▅▆▅▄▆▄▂▃▄▆▁▆▃▂▄▄▅▆▃▄▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▁▂▁▂▂▃▃▃▃▄▄▂▄▂▄▂▅▃▅▃▆▆▆▃▃▇▇▇▃▇▄█▄█▄</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▃▆▆▅▅▇▇▆▇▆▇█▆▇▇█▆██▆▇▇▇▇▆▆▇█▇▇▆█▇▇</td></tr><tr><td>val_auc</td><td>▆▁▁▂▁▁▂▃▄▄▄▅▆▆▆▆▆█▇▇▇█▇█████████▇█▇▇▇█▇█</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▄▇█▅▆▇▇▆█▆▆█▇▇▇█▇██▇▇▇▇▇▇▇▆█▇▇▆▇▇▇</td></tr><tr><td>val_loss_epoch</td><td>▆██▇▅▅▃▅▆▄▄▃▃▃▄▄▃▄▂▃▄▃▁▃▂▄▅▄▃▃▃▆▂▃▄▁▃▄▃▄</td></tr><tr><td>val_loss_step</td><td>▇▇█▇▆▅▃▆▅▄▅▄▄▄▄▄▄▅▃▄▃▄▁▃▄▅▄▄▅▄▃▄▃▄▄▃▄▅▄▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>49</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.70197</td></tr><tr><td>train_auc</td><td>0.7589</td></tr><tr><td>train_f1</td><td>0.5546</td></tr><tr><td>train_loss_epoch</td><td>0.56542</td></tr><tr><td>train_loss_step</td><td>0.53777</td></tr><tr><td>trainer/global_step</td><td>249</td></tr><tr><td>val_acc</td><td>0.74013</td></tr><tr><td>val_auc</td><td>0.80694</td></tr><tr><td>val_f1</td><td>0.60697</td></tr><tr><td>val_loss_epoch</td><td>0.55543</td></tr><tr><td>val_loss_step</td><td>0.60002</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_64_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/77ulusbp' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/77ulusbp</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240115_051038-77ulusbp\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9efbf92d15354a07a7accef62855d706",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01691666666883975, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240115_052202-7ztmvd2y</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/7ztmvd2y' target=\"_blank\">MLP_4_64_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/7ztmvd2y' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/7ztmvd2y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 12.9 K\n",
      "2 | head        | Sequential       | 2.1 K \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "15.0 K    Trainable params\n",
      "0         Non-trainable params\n",
      "15.0 K    Total params\n",
      "0.060     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0f924d85eee44e3b37f388fbf8e3b17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▅▆▆▆▆▆▆▆▆▆▆▆▇▅▅▅▆▇▆█▆█▆▆▅█▅▅▇▅▇▄█▇█▇▅▆▅</td></tr><tr><td>train_auc</td><td>▂▂▁▃▅▃▃▃▃▅▄▃▃▂▃▄▄▅▅▆▆▆▆▄▆▅█▇▅▆▄▅▄▇▆▆▇▄▆▄</td></tr><tr><td>train_f1</td><td>▇▂▁▁▁▁▁▁▁▁▁▁▁▂▁▁▃▃▂▂▆▄▅▂▃▅▄▄▆▃▂▂▄▃▄▆█▄▃▂</td></tr><tr><td>train_loss_epoch</td><td>█▅▅▄▃▄▃▄▃▃▂▄▃▃▄▂▃▂▂▂▂▃▂▃▂▂▁▂▃▂▃▂▃▂▂▂▁▄▁▃</td></tr><tr><td>train_loss_step</td><td>▆▄▃▅▆▄▆▅█▁▄▃▄▅▁▃▄▁▅▆▆▆▁▃▄</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▁▂▁▂▂▃▃▃▃▄▄▂▄▂▄▂▅▃▅▃▆▆▆▃▃▇▇▇▃▇▄█▄█▄</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>▂▁▇▇▄▇▅▇▅▇▇▅▂▄▅▂▂▁▁██▇▇▇▇▅▄▇▇▇▇▇▄▇▆▆▇▇▁▇</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_epoch</td><td>██▇▄▄▅▂▃▄▅▅▇▅▇▅▆▆▄▆▇▆▇▄▆▄▆▄▄▅▄▄▇▅▅▇▅▅▄▆▁</td></tr><tr><td>val_loss_step</td><td>███▅▆▅▂▃▅▆▆█▅▇▆▆▆▄▆▇▆█▄▆▆▇▆▆▇▅▄▆▆▆▆▅▅▅▅▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>49</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.61248</td></tr><tr><td>train_auc</td><td>0.53698</td></tr><tr><td>train_f1</td><td>0.03279</td></tr><tr><td>train_loss_epoch</td><td>0.66045</td></tr><tr><td>train_loss_step</td><td>0.65106</td></tr><tr><td>trainer/global_step</td><td>249</td></tr><tr><td>val_acc</td><td>0.66118</td></tr><tr><td>val_auc</td><td>0.52543</td></tr><tr><td>val_f1</td><td>0.0</td></tr><tr><td>val_loss_epoch</td><td>0.6418</td></tr><tr><td>val_loss_step</td><td>0.6218</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_64_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/7ztmvd2y' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/7ztmvd2y</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240115_052202-7ztmvd2y\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2141e746abfa401787093e5e7a54cf4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01691666666107873, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240115_053324-nekf2b9k</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/nekf2b9k' target=\"_blank\">MLP_4_64_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/nekf2b9k' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/nekf2b9k</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 12.9 K\n",
      "2 | head        | Sequential       | 2.1 K \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "15.0 K    Trainable params\n",
      "0         Non-trainable params\n",
      "15.0 K    Total params\n",
      "0.060     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00557934c4ff4715bcccd4ef2b6c73bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▃▁▅▁▄▅▄▄▅▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇██▇▇▇▇▇██▇▇████</td></tr><tr><td>train_auc</td><td>▁▂▂▂▁▂▂▂▂▂▂▂▂▃▄▄▅▅▅▅▄▅▅▇██▇▆▇▇▇▇▇███▇██▇</td></tr><tr><td>train_f1</td><td>▄▆▂▆▂▂▃▄▁▂▂▁▁▃▂▄▅▅▃▇▅▆▅▇▇██▆▇▇▇▇██▇▇█▇█▇</td></tr><tr><td>train_loss_epoch</td><td>█▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▄▃▄▄▃▄▃▃▃▃▂▃▃▁▁▂▂▂▂▂▃▁▂▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▁▂▁▂▂▃▃▃▃▄▄▂▄▂▄▂▅▃▅▃▆▆▆▃▃▇▇▇▃▇▄█▄█▄</td></tr><tr><td>val_acc</td><td>▁▆▆▁▆▆▆▆▆▆▆▆▆▆▆▆▆▆█▆▆▆█▇▇▆▇███▇█▇██▇▇███</td></tr><tr><td>val_auc</td><td>▃▃▃▃▁▄▄▃▂▆▅▃▆▇▇█▇▇█▄▅▆█▇▇▇▇██▇▇█▇███████</td></tr><tr><td>val_f1</td><td>▆▁▁▆▁▁▁▁▁▁▁▁▁▁▁▁▁▂█▁▂▂▇▆▅▃▅█▆▆▄▇▅▇▇▆▆█▇█</td></tr><tr><td>val_loss_epoch</td><td>█▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▂▁▁▂▁▁▂▁▁▂▁▁▁▁▂</td></tr><tr><td>val_loss_step</td><td>█▅▃▃▃▃▂▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▁▂▂▃▂▂▂▁▁▁▁▁▂▁▁▁▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>49</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.72085</td></tr><tr><td>train_auc</td><td>0.67356</td></tr><tr><td>train_f1</td><td>0.56633</td></tr><tr><td>train_loss_epoch</td><td>0.55919</td></tr><tr><td>train_loss_step</td><td>0.54903</td></tr><tr><td>trainer/global_step</td><td>249</td></tr><tr><td>val_acc</td><td>0.79276</td></tr><tr><td>val_auc</td><td>0.84732</td></tr><tr><td>val_f1</td><td>0.68657</td></tr><tr><td>val_loss_epoch</td><td>0.5601</td></tr><tr><td>val_loss_step</td><td>0.62134</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_64_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/nekf2b9k' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/nekf2b9k</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240115_053324-nekf2b9k\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d25389e2573541a384c738967dc4b44d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240115_054447-jocqa82e</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/jocqa82e' target=\"_blank\">MLP_4_64_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/jocqa82e' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/jocqa82e</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 12.9 K\n",
      "2  | head        | Sequential       | 2.1 K \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 65    \n",
      "--------------------------------------------------\n",
      "15.1 K    Trainable params\n",
      "0         Non-trainable params\n",
      "15.1 K    Total params\n",
      "0.060     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f67224f09544f4e8c714520ed15a6bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▃▃▃▃▃▄▆▅▆▆▅▆▆▇▇▇▇▇▆▆▇█▇█▇▇▇▇██▇▇▇█▇▇▇▇█</td></tr><tr><td>train_auc</td><td>▁▁▂▁▂▃▄▆▆▆▇▇▇▇▇▇▇▇▇▇▇████▇█████▇█▇██▇▇██</td></tr><tr><td>train_f1</td><td>▄▁▁▁▁▁▂▆▇▆█▄▆█▇▇▇▇█▆▆██▇██▇█▇█▇▇▇▇█▇▇▇▇█</td></tr><tr><td>train_loss_epoch</td><td>██▇▇▇▆▆▅▅▄▄▃▃▃▂▂▂▃▂▃▂▂▁▁▂▂▁▂▂▁▂▂▂▂▁▁▂▂▂▁</td></tr><tr><td>train_loss_step</td><td>▇▇▆█▅▄▄▃▃▃▅▄▂▁▃▃▄▂▃▄▄▄▃▄▅</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▁▂▁▂▂▃▃▃▃▄▄▂▄▂▄▂▅▃▅▃▆▆▆▃▃▇▇▇▃▇▄█▄█▄</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▄▇▇▃▆▇▅▇▇▇▇▆█▇▆▇▇▆▇█▇█▇▇█▇▇▇██▇▇█</td></tr><tr><td>val_auc</td><td>█▁▁▃▁▁▂▁▃▄▃▄▄▄▅▅▇▆▆▇▆▆▆▇▆▆▇▇█▇█▇▆▆▇██▇▇█</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▅▆█▃▆▇▅▇▇▇▆▆█▇▆▇▇▆▇▇▇▇▇▇▇▇▇▇▇█▇▇█</td></tr><tr><td>val_loss_epoch</td><td>▇█▇█▆▆▅▅▃▆▄▅▃▄▃▃▃▂▄▃▂▃▁▃▃▃▂▃▃▃▃▁▃▁▃▃▃▂▂▂</td></tr><tr><td>val_loss_step</td><td>██▇█▇▇▅▅▄▆▆█▅▅▄▄▄▂▆▄▅▄▁▅▄▅▃▄▅▄▄▃▄▁▄▅▅▂▃▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>49</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.71182</td></tr><tr><td>train_auc</td><td>0.76368</td></tr><tr><td>train_f1</td><td>0.57557</td></tr><tr><td>train_loss_epoch</td><td>0.56235</td></tr><tr><td>train_loss_step</td><td>0.61383</td></tr><tr><td>trainer/global_step</td><td>249</td></tr><tr><td>val_acc</td><td>0.73684</td></tr><tr><td>val_auc</td><td>0.80356</td></tr><tr><td>val_f1</td><td>0.60396</td></tr><tr><td>val_loss_epoch</td><td>0.51373</td></tr><tr><td>val_loss_step</td><td>0.51223</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_64_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/jocqa82e' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/jocqa82e</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240115_054447-jocqa82e\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f9a692dee5f419f930be0e4e5037173",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016933333330477276, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240115_055630-e3jd5dwu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/e3jd5dwu' target=\"_blank\">MLP_4_64_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/e3jd5dwu' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/e3jd5dwu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 12.9 K\n",
      "2  | head        | Sequential       | 2.1 K \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 4.2 K \n",
      "--------------------------------------------------\n",
      "19.2 K    Trainable params\n",
      "0         Non-trainable params\n",
      "19.2 K    Total params\n",
      "0.077     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e81fb8177f4e4430bf5fd6640f882fc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▄▄▄▄▄▅▆▆▇▇▆▇▇▇▇▇▇▆▇█▇▇▇▇▇▇█▇▇▇▇▇█▇▇███▇</td></tr><tr><td>train_auc</td><td>▁▁▁▂▃▅▅▆▇▇▇▇▇█▇█▇█▇▇███▇███████▇█▇██████</td></tr><tr><td>train_f1</td><td>▄▁▁▁▁▁▆▆▇▇▇▇▇▇█▇█▇▇▇█▇█▇▆█▇█▇▇▇█▇█▇▇██▇▇</td></tr><tr><td>train_loss_epoch</td><td>███▇▆▅▅▄▄▃▂▂▃▂▂▁▂▂▃▂▂▂▂▂▂▂▁▂▁▁▂▂▂▃▂▂▁▂▁▂</td></tr><tr><td>train_loss_step</td><td>██▆▅▆▃▁▅▇▄▅▁▂▆▇▃▃▂▃▃▄▃▂▄▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▁▂▁▂▂▃▃▃▃▄▄▂▄▂▄▂▅▃▅▃▆▆▆▃▃▇▇▇▃▇▄█▄█▄</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▃▇▆█▇▆█▇▆▇▆▇▆▇██▇██▆█▆▇██▆█▆▆▇█▆▇▆</td></tr><tr><td>val_auc</td><td>▅▁▂▃▂▂▂▃▄▅▅▅▇▇▆▇▆█▆▇▇█▇██▇█▇▇████▇▇████▇</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▄█▆▇▇▆█▇▇▇▆█▆▇██▇██▇█▇▆▇▇▇█▆▇▇█▇▇▇</td></tr><tr><td>val_loss_epoch</td><td>█▇█▇▆▆▅▆▂▃▄▃▅▄▃▃▄▄▄▃▅▄▁▅▄▃▄▃▃▄▄▅▃▃▂▂▃▃▃▃</td></tr><tr><td>val_loss_step</td><td>▇▇█▇▇▆▆▇▅▅▄▄▅▅▄▄▄▅▆▄▄▆▁▄▅▄▄▄▅▅▄▄▄▄▅▅▄▄▄▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>49</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.68391</td></tr><tr><td>train_auc</td><td>0.73905</td></tr><tr><td>train_f1</td><td>0.54971</td></tr><tr><td>train_loss_epoch</td><td>0.56904</td></tr><tr><td>train_loss_step</td><td>0.51664</td></tr><tr><td>trainer/global_step</td><td>249</td></tr><tr><td>val_acc</td><td>0.72039</td></tr><tr><td>val_auc</td><td>0.80356</td></tr><tr><td>val_f1</td><td>0.53039</td></tr><tr><td>val_loss_epoch</td><td>0.52578</td></tr><tr><td>val_loss_step</td><td>0.54156</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_64_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/e3jd5dwu' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/e3jd5dwu</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240115_055630-e3jd5dwu\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f821a1cc71b41fca75cefe2474ba6f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01691666666883975, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240115_060812-u6tqwehw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/u6tqwehw' target=\"_blank\">MLP_2_16_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/u6tqwehw' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/u6tqwehw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 368   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "522       Trainable params\n",
      "0         Non-trainable params\n",
      "522       Total params\n",
      "0.002     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87a9d76f73af4f99ae77d41ad8bfa627",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▄▅▆▆▆▆▆▆▆▆▆▇▇▇▇█▇▇█▇██▇██▇▇▇████▇██▇███</td></tr><tr><td>train_auc</td><td>▂▂▁▂▂▂▂▂▂▃▃▄▆▆▇▇█▇▇▇█▇█▇████████████████</td></tr><tr><td>train_f1</td><td>█▆▄▃▂▂▁▁▁▁▁▂▃▄▅▆▇▇█▇▇▇█▇██▇▇▇█▇▇▇▇▇█▇██▇</td></tr><tr><td>train_loss_epoch</td><td>█▆▇▆▆▅▆▅▅▅▅▄▃▃▃▃▂▂▂▂▂▂▂▂▂▁▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▆▇█▆▅▄▃▄▃▂▁▃▃▂▁▁▄▅▂▁▂▄▂▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▁▂▁▂▂▃▃▃▃▄▄▂▄▂▄▂▅▃▅▃▆▆▆▃▃▇▇▇▃▇▄█▄█▄</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▃▄▆▆▆██▆▆▇▇▆▇▆▇▇▇█▆█▇▇▇▇▇▇▇▇</td></tr><tr><td>val_auc</td><td>▁▁▃▅▆▇▇▇▇▇▇▇▇▇██████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▄▅▇▆▇██▇▇██▇█▇▇▇▇█▇█▇▇▇▇▇█▇▇</td></tr><tr><td>val_loss_epoch</td><td>▇▆█▇▇▇▆▇▆▇▆▆▅▄▄▂▂▂▂▁▁▂▁▂▂▂▁▂▄▂▂▁▂▂▂▂▁▃▂▁</td></tr><tr><td>val_loss_step</td><td>▆▆█▇▆▆▅▆▆▆▅▅▄▄▅▃▃▃▂▃▃▃▁▃▃▂▃▃▆▃▃▂▂▃▃▃▂▅▃▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>49</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.70443</td></tr><tr><td>train_auc</td><td>0.73704</td></tr><tr><td>train_f1</td><td>0.45455</td></tr><tr><td>train_loss_epoch</td><td>0.56589</td></tr><tr><td>train_loss_step</td><td>0.56818</td></tr><tr><td>trainer/global_step</td><td>249</td></tr><tr><td>val_acc</td><td>0.69737</td></tr><tr><td>val_auc</td><td>0.77471</td></tr><tr><td>val_f1</td><td>0.54</td></tr><tr><td>val_loss_epoch</td><td>0.5565</td></tr><tr><td>val_loss_step</td><td>0.53478</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_16_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/u6tqwehw' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/u6tqwehw</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240115_060812-u6tqwehw\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa41ed9448d74da7a2c7ec427516ba90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240115_061956-8mczeqes</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/8mczeqes' target=\"_blank\">MLP_2_16_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/8mczeqes' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/8mczeqes</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 368   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "522       Trainable params\n",
      "0         Non-trainable params\n",
      "522       Total params\n",
      "0.002     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a1911b7d647471384a6fe4057e08d91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▅▆▇▇███████████████████████████████████</td></tr><tr><td>train_auc</td><td>▁▃▂▄▅▅▂▁▂▄▄▄▆▂▁▃▄▂▂▆▄▅▄▃▁▄▃▄▄▆▁▄▄▄▃▆▅▅█▆</td></tr><tr><td>train_f1</td><td>█▅▃▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_epoch</td><td>█▄▄▃▂▂▂▂▂▁▂▁▁▂▂▁▁▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>▇▄▄█▃▃▁▁▆▅▅▄▄▅▅▃▅▆█▄▂▇█▆▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▁▂▁▂▂▃▃▃▃▄▄▂▄▂▄▂▅▃▅▃▆▆▆▃▃▇▇▇▃▇▄█▄█▄</td></tr><tr><td>val_acc</td><td>▁███████████████████████████████████████</td></tr><tr><td>val_auc</td><td>▂▁▂█████▅▅▄▄██▅▅▃▄▄▄▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▄▄</td></tr><tr><td>val_f1</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_epoch</td><td>▇▄▆▆▆▆▄▅▄▅▅▅▅▄▇▂▃▃▇▃▅▄▁▄▇▆▃▆█▆▄▅▂▄▅▆▄▇▆▂</td></tr><tr><td>val_loss_step</td><td>▆▅▆▅▅▅▄▅▅▅▅▅▅▄▇▅▅▃▇▄▅▄▁▅▄▆▅▅█▆▅▅▂▄▅▅▄▇▅▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>49</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.63875</td></tr><tr><td>train_auc</td><td>0.53893</td></tr><tr><td>train_f1</td><td>0.0</td></tr><tr><td>train_loss_epoch</td><td>0.64874</td></tr><tr><td>train_loss_step</td><td>0.64108</td></tr><tr><td>trainer/global_step</td><td>249</td></tr><tr><td>val_acc</td><td>0.57566</td></tr><tr><td>val_auc</td><td>0.49863</td></tr><tr><td>val_f1</td><td>0.0</td></tr><tr><td>val_loss_epoch</td><td>0.66746</td></tr><tr><td>val_loss_step</td><td>0.64342</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_16_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/8mczeqes' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/8mczeqes</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240115_061956-8mczeqes\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95125bc90bab4d31a41cbe63f9e667c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240115_063142-z8hdwmb7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/z8hdwmb7' target=\"_blank\">MLP_2_16_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/z8hdwmb7' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/z8hdwmb7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 368   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "522       Trainable params\n",
      "0         Non-trainable params\n",
      "522       Total params\n",
      "0.002     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a31ccc62006340389ebccd3e7e3bd63e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▂▂▁▃▄▄▅▅▆▆▆▆▆▇▇▆▇▇▇▇▇▇▇▇█▇▇█▇█▇████████▇</td></tr><tr><td>train_auc</td><td>▂▁▁▃▃▄▅▃▄▄▅▃▄▅▅▅▆▅▅▆▅▅▆▆▇▆▇▆▇▇▇█▇██▇█▇██</td></tr><tr><td>train_f1</td><td>▅▅▅▄▅▅▄▂▅▂▁▃▂▆▅▃▆▄▆▅▇▆▆▆▇▇▆▇▆▇▅█▇▇▇▇▇██▆</td></tr><tr><td>train_loss_epoch</td><td>█▄▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▃▃▃▃▂▂▂▂▂▂▂▂▁▂▁▁▂▂▁▁▂▂▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▁▂▁▂▂▃▃▃▃▄▄▂▄▂▄▂▅▃▅▃▆▆▆▃▃▇▇▇▃▇▄█▄█▄</td></tr><tr><td>val_acc</td><td>▄▁▄▄▅▅▅▅▅▄▅▅▅▆▆▅▅▆█▅▇▇▇▇▇▇▇▇█▇█▇█▇▇▇▇▇▇▇</td></tr><tr><td>val_auc</td><td>▁▁▂▂▄▄▄▄▅▅▅▅▅▆▆▆▇▇▇▆▇▇▇▇████████████████</td></tr><tr><td>val_f1</td><td>▁▄▂▁▄▃▂▃▂▁▂▃▄▅▅▄▃▅█▃▇▆▆▆▇▆▇▇█▆█▇█▆▇▇▇▇▇▇</td></tr><tr><td>val_loss_epoch</td><td>█▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▁▂▂▂▁▁▁▂▁▁▁▂▁▁</td></tr><tr><td>val_loss_step</td><td>█▄▄▄▃▃▃▃▃▃▃▃▃▃▃▂▃▂▂▂▂▂▁▂▂▂▂▂▃▃▂▂▂▂▂▂▂▃▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>49</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.69458</td></tr><tr><td>train_auc</td><td>0.66832</td></tr><tr><td>train_f1</td><td>0.40952</td></tr><tr><td>train_loss_epoch</td><td>0.57237</td></tr><tr><td>train_loss_step</td><td>0.56208</td></tr><tr><td>trainer/global_step</td><td>249</td></tr><tr><td>val_acc</td><td>0.69408</td></tr><tr><td>val_auc</td><td>0.79229</td></tr><tr><td>val_f1</td><td>0.53266</td></tr><tr><td>val_loss_epoch</td><td>0.54999</td></tr><tr><td>val_loss_step</td><td>0.52731</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_16_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/z8hdwmb7' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/z8hdwmb7</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240115_063142-z8hdwmb7\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "662614b55eae45dc9ed87d84c4e7ecc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.017183333332650364, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240115_064323-71m5x3vj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/71m5x3vj' target=\"_blank\">MLP_2_16_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/71m5x3vj' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/71m5x3vj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 368   \n",
      "2  | head        | Sequential       | 154   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 17    \n",
      "--------------------------------------------------\n",
      "539       Trainable params\n",
      "0         Non-trainable params\n",
      "539       Total params\n",
      "0.002     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08c86f2c92ea49fa9b198984038e35ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▄▅▅▆▆▆▆▆▆▆▆▆▇▇▇▇▇█▇██▇▇▇▇▇▇▇██▇█▇▇▇█▇▇▇</td></tr><tr><td>train_auc</td><td>▂▂▂▁▂▃▂▃▃▂▄▄▅▅▆▇▆▇▇▇█▇▇▇▇▇██▇▇█████████▇</td></tr><tr><td>train_f1</td><td>█▆▄▃▂▁▁▁▁▁▁▁▂▄▄▇▇▇██▇█▇▇▇▇▆█▇▇█▇█▇▇▇▇█▇▇</td></tr><tr><td>train_loss_epoch</td><td>█▆▆▆▆▆▆▅▅▅▅▅▄▄▃▃▃▂▂▃▁▂▁▂▂▂▁▂▂▂▁▁▁▂▁▁▁▂▁▂</td></tr><tr><td>train_loss_step</td><td>█▆▇▆▇▇█▅▃▁▅▄▄▃▃▃▅▂▅▄▂▃▂▂▅</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▁▂▁▂▂▃▃▃▃▄▄▂▄▂▄▂▅▃▅▃▆▆▆▃▃▇▇▇▃▇▄█▄█▄</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▃▃▆▆█▇▇▇█▆▇▇▇▇▇▇█▇▇▇▇█▆█▇▇▇▇</td></tr><tr><td>val_auc</td><td>▁▂▃▅▇▇▇▇▇▇▇█████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▃▄▆▇█▇█▆▇▆▇▇▇▆▇▇▇▇▇▆▆▇▆█▇▆▇▆</td></tr><tr><td>val_loss_epoch</td><td>▇▇▇██▆▇██▇▆▇▅▄▃▃▄▃▄▃▂▃▃▃▄▄▅▁▄▂▂▄▂▄▂▃▃▁▂▂</td></tr><tr><td>val_loss_step</td><td>▇▇▇█▆▇▆█▆▆▆▆▅▃▃▄▄▄▅▄▄▄▄▃▃▅▄▃▅▃▃▄▂▆▃▃▄▁▃▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>49</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.67488</td></tr><tr><td>train_auc</td><td>0.72352</td></tr><tr><td>train_f1</td><td>0.4142</td></tr><tr><td>train_loss_epoch</td><td>0.58178</td></tr><tr><td>train_loss_step</td><td>0.61374</td></tr><tr><td>trainer/global_step</td><td>249</td></tr><tr><td>val_acc</td><td>0.6875</td></tr><tr><td>val_auc</td><td>0.77112</td></tr><tr><td>val_f1</td><td>0.49735</td></tr><tr><td>val_loss_epoch</td><td>0.55851</td></tr><tr><td>val_loss_step</td><td>0.52743</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_16_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/71m5x3vj' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/71m5x3vj</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240115_064323-71m5x3vj\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d44b5d04451b45618aceed936d4e0d19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.0169333333382383, max=1.0))…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240115_065509-i6im1ad7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/i6im1ad7' target=\"_blank\">MLP_2_16_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/i6im1ad7' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/i6im1ad7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 368   \n",
      "2  | head        | Sequential       | 154   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 281   \n",
      "--------------------------------------------------\n",
      "803       Trainable params\n",
      "0         Non-trainable params\n",
      "803       Total params\n",
      "0.003     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84abe33efad44687a06520d65a4340de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▄▅▆▆▆▆▆▆▆▆▆▆▇▇▇█▇▇▇▇▇▇████████████▇█▇▇█</td></tr><tr><td>train_auc</td><td>▁▁▁▂▁▂▂▂▂▃▃▃▄▆▆▆▇▇▇▇▇▇▇█▇▇█▇██▇▇████████</td></tr><tr><td>train_f1</td><td>█▆▄▃▁▁▁▁▁▁▁▁▂▃▅▆█▇▇▇▇▇▇▇██▇█▇█▇█▇▇█▇█▇▇█</td></tr><tr><td>train_loss_epoch</td><td>█▇▆▆▆▆▅▅▅▅▅▅▄▃▃▃▂▂▂▂▂▂▂▁▂▂▁▂▁▁▂▁▂▁▂▂▁▂▁▁</td></tr><tr><td>train_loss_step</td><td>█▇▅▆▆▇▅▄▃▅▂▃▃▂▂▄▃▃▃▂▅▅▂▂▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▁▂▁▂▂▃▃▃▃▄▄▂▄▂▄▂▅▃▅▃▆▆▆▃▃▇▇▇▃▇▄█▄█▄</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▃▄▅▇▇▇▇▇▇█▇▇█▇███▇▇██▇█▇██▇█</td></tr><tr><td>val_auc</td><td>▁▁▃▅▆▇▇▇▇▇▇▇▇▇▇█████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▃▅▆█▇▇█▇▇█▇▇█▇███▇███▇█▇██▇█</td></tr><tr><td>val_loss_epoch</td><td>██▇▇▇▇▇██▇▇▇▆▅▅▄▄▃▃▄▄▃▃▃▃▅▃▃▃▂▃▁▃▄▂▁▂▄▃▃</td></tr><tr><td>val_loss_step</td><td>▇▇▆▇▇▇▆█▇▇▆▇▆▆▆▄▄▃▄▄▄▄▄▄▃▇▃▄▅▂▄▄▄▆▄▃▁▆▃▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>49</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.70033</td></tr><tr><td>train_auc</td><td>0.74921</td></tr><tr><td>train_f1</td><td>0.50068</td></tr><tr><td>train_loss_epoch</td><td>0.55774</td></tr><tr><td>train_loss_step</td><td>0.54547</td></tr><tr><td>trainer/global_step</td><td>249</td></tr><tr><td>val_acc</td><td>0.71711</td></tr><tr><td>val_auc</td><td>0.77214</td></tr><tr><td>val_f1</td><td>0.59048</td></tr><tr><td>val_loss_epoch</td><td>0.57516</td></tr><tr><td>val_loss_step</td><td>0.58511</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_16_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/i6im1ad7' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/i6im1ad7</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240115_065509-i6im1ad7\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48187b00f68643ecab2770f9c3759e86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016933333330477276, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240115_070650-5siusv7v</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/5siusv7v' target=\"_blank\">MLP_2_32_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/5siusv7v' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/5siusv7v</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 1.2 K \n",
      "2 | head        | Sequential       | 562   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "1.8 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.8 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f05b98f702141e99235ceeb7e6252e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▂▂▂▃▃▄▅▇▇█▅▆▆▆▇▆▆▇▆▇▇▆▆▇▇█▆▇▆▇▇▇▆▆██▇█</td></tr><tr><td>train_auc</td><td>▁▁▂▃▄▅▆▇▇▇▇▇▇▇█▇▇█████▇█████▇▇██████████</td></tr><tr><td>train_f1</td><td>▂▁▁▁▂▂▄▆▇▇██▆█▇▇▇█▇█▇█▇▇████▇█▇▇██▇▇██▇█</td></tr><tr><td>train_loss_epoch</td><td>██▇▇▆▅▅▄▃▃▂▂▃▃▂▂▃▂▂▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▂▂▁</td></tr><tr><td>train_loss_step</td><td>▇█▅▆▄▅▅▅▃▅▅▃▄▄▆▄▄▆▃▃▅▂▄▇▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▁▂▁▂▂▃▃▃▃▄▄▂▄▂▄▂▅▃▅▃▆▆▆▃▃▇▇▇▃▇▄█▄█▄</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▂▄▅████▇█▆█▇▇▇██▇▆▇▇█▇█▇█▆█▇█▇▇▇█▇▆</td></tr><tr><td>val_auc</td><td>▁▂▃▃▄▄▅▆▇▇▇█████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▂▃▅▆████▇█▇█▇█▇███▇█▇█▇█▇█▇█▇█▇▇▇█▇▇</td></tr><tr><td>val_loss_epoch</td><td>█▇▆▆▅▅▄▅▄▃▂▃▃▃▄▂▅▃▃▃▂▃▂▃▂▃▃▃▃▂▂▄▂▂▂▁▂▃▄▃</td></tr><tr><td>val_loss_step</td><td>▇▇▇█▆▆▄▇▄▄▂▃▄▃▆▄▃▄▄▄▄▅▁▄▄▆▄▄▃▂▃▄▃▃▅▄▂▄▄▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>49</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.711</td></tr><tr><td>train_auc</td><td>0.77371</td></tr><tr><td>train_f1</td><td>0.50975</td></tr><tr><td>train_loss_epoch</td><td>0.5443</td></tr><tr><td>train_loss_step</td><td>0.48225</td></tr><tr><td>trainer/global_step</td><td>249</td></tr><tr><td>val_acc</td><td>0.6875</td></tr><tr><td>val_auc</td><td>0.77736</td></tr><tr><td>val_f1</td><td>0.49735</td></tr><tr><td>val_loss_epoch</td><td>0.58203</td></tr><tr><td>val_loss_step</td><td>0.58228</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_32_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/5siusv7v' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/5siusv7v</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240115_070650-5siusv7v\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfc9b3c7600143a998243e455fca3be6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240115_072108-94gwkrzg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/94gwkrzg' target=\"_blank\">MLP_2_32_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/94gwkrzg' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/94gwkrzg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 1.2 K \n",
      "2 | head        | Sequential       | 562   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "1.8 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.8 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddbe8a66d61d4f5f9e84791d56f4e90a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▅▅▆▆▆▆▆▆▆▆▆▆▆▇▇▇▆▇▇▇█▆▆▇▇▅▇▇▇▇▇▇▇▇▇▆▇▅▇</td></tr><tr><td>train_auc</td><td>▃▁▂▃▃▄▂▄▃▄▄▄▄▃▄▆▄▄▆▅▆▆▆▇▆▆▆▆▇▆▇█▇▇▇▇▇█▆█</td></tr><tr><td>train_f1</td><td>▇▂▂▁▁▁▁▁▁▁▁▂▂▂▃▃▅▅▄▄▄▇▆▄▅▅▄▅▆▇▆▅▇▇▆▅▇█▅▆</td></tr><tr><td>train_loss_epoch</td><td>█▆▅▅▄▃▄▄▄▄▃▃▃▃▃▂▃▃▂▃▂▂▃▂▃▂▂▂▂▂▁▂▂▂▂▂▂▁▄▁</td></tr><tr><td>train_loss_step</td><td>▃▆▂▅▄▅▅▄▄▃▄▄▄▄▅▃▄▄▂▄▃▂▄█▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▁▂▁▂▂▃▃▃▃▄▄▂▄▂▄▂▅▃▅▃▆▆▆▃▃▇▇▇▃▇▄█▄█▄</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>▃▃▃▁▁▁▃▃▃▃▃▃▃▃▃▃▃▄▆███▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_epoch</td><td>▄▃▁▂▂▂▁▂▃▂▁▃▃▃▃▃▄▃▃▃▂▆▄▃▆▇▄█▄▆▅▇█▃▅▅▂██▅</td></tr><tr><td>val_loss_step</td><td>▃▄▄▄▄▄▃▄▄▄▃▅▄▅▅▅▄▄▄▄▅▇▄▅▅█▅▅▄▆▆▆█▃▆▅▁█▅▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>49</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.6445</td></tr><tr><td>train_auc</td><td>0.62443</td></tr><tr><td>train_f1</td><td>0.19963</td></tr><tr><td>train_loss_epoch</td><td>0.62466</td></tr><tr><td>train_loss_step</td><td>0.59323</td></tr><tr><td>trainer/global_step</td><td>249</td></tr><tr><td>val_acc</td><td>0.57566</td></tr><tr><td>val_auc</td><td>0.50839</td></tr><tr><td>val_f1</td><td>0.0</td></tr><tr><td>val_loss_epoch</td><td>0.74516</td></tr><tr><td>val_loss_step</td><td>0.75322</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_32_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/94gwkrzg' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/94gwkrzg</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240115_072108-94gwkrzg\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4803285dd4b415aae043004976ba76f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.017183333332650364, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240115_073233-xy7pk7vc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/xy7pk7vc' target=\"_blank\">MLP_2_32_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/xy7pk7vc' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/xy7pk7vc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 1.2 K \n",
      "2 | head        | Sequential       | 562   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "1.8 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.8 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02982533cf9548d081eacdc3aacb1972",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▂▁▂▃▄▅▆▅▅▇▆▇▇▇▇▆▇▇██▇▇▇▇▇▇▇▇▇███▇▇██▇▇█▇</td></tr><tr><td>train_auc</td><td>▂▁▂▂▃▂▃▄▄▅▅▅▆▅▅▆▇▆█▆▆▆▆▇▆▆▆▆▅▆▆▆▅▅▅▅▇▄▅▅</td></tr><tr><td>train_f1</td><td>▃▃▃▁▃▁▄▅▃▅▃▅▅▆▆▄▅▇▇█▇▆▇▆▇▇▇▆▇█▇▇▇▇▇█▇█▆▆</td></tr><tr><td>train_loss_epoch</td><td>█▄▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▃▂▂▂▂▂▂▁▂▁▁▂▂▂▁▁▂▁▁▁▁▂▂▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▁▂▁▂▂▃▃▃▃▄▄▂▄▂▄▂▅▃▅▃▆▆▆▃▃▇▇▇▃▇▄█▄█▄</td></tr><tr><td>val_acc</td><td>▂▁▂▅▂▃▇▆▅▅▆▆▇▆▇▇▇▇▆█▇█▆█▆▇▇▇▆▇▇▇▇▇▆▇█▇▆▆</td></tr><tr><td>val_auc</td><td>▁▅▂▆▃▅▇▇████████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▇▂▆▁▂▇▆▆▆▆▆▇▇▇▇▇▇▇█▇█▇█▇█▇▇▇█▇▇▇▇▇██▇▇▆</td></tr><tr><td>val_loss_epoch</td><td>█▂▂▂▂▂▂▂▂▂▁▁▂▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_step</td><td>█▃▃▃▂▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▂▂▂▁▂▂▁▁▂▂▁▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>49</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.71511</td></tr><tr><td>train_auc</td><td>0.58402</td></tr><tr><td>train_f1</td><td>0.50072</td></tr><tr><td>train_loss_epoch</td><td>0.54077</td></tr><tr><td>train_loss_step</td><td>0.49659</td></tr><tr><td>trainer/global_step</td><td>249</td></tr><tr><td>val_acc</td><td>0.6875</td></tr><tr><td>val_auc</td><td>0.79805</td></tr><tr><td>val_f1</td><td>0.48087</td></tr><tr><td>val_loss_epoch</td><td>0.57606</td></tr><tr><td>val_loss_step</td><td>0.5774</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_32_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/xy7pk7vc' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/xy7pk7vc</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240115_073233-xy7pk7vc\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4ee4f9fdd7341029b636ae8b9d59e70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240115_074406-6j4075jw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/6j4075jw' target=\"_blank\">MLP_2_32_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/6j4075jw' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/6j4075jw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 1.2 K \n",
      "2  | head        | Sequential       | 562   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 33    \n",
      "--------------------------------------------------\n",
      "1.8 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.8 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce89bc93990548ff8c427e9379c169f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▁▁▁▄▃▄▆▇██▅▆█▇▇█▆▆▇▆▇▆▆▇▆▇▇█▇▇▆█▇▇█▇▆▇</td></tr><tr><td>train_auc</td><td>▂▂▁▃▅▆▆▇▇▇▇█▇███▇██▇███████▇████████████</td></tr><tr><td>train_f1</td><td>▂▁▁▁▁▄▅▇▇▇▇▇▆▇█▇▇█▆▇▇▇▇▇█▇▇▇▇█▇▇▆█▆██▇▇█</td></tr><tr><td>train_loss_epoch</td><td>███▇▆▅▄▄▂▃▂▂▃▂▂▁▂▂▃▂▂▂▂▂▂▁▂▃▂▂▂▂▂▂▂▂▁▂▂▂</td></tr><tr><td>train_loss_step</td><td>██▇▇▄▄▃▄▃▃▃▃▄▄▄▃▇▁▄▄▁▄▃▂▄</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▁▂▁▂▂▃▃▃▃▄▄▂▄▂▄▂▅▃▅▃▆▆▆▃▃▇▇▇▃▇▄█▄█▄</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▄▇▆▇██▇▇▇▇██▇▇▇████▆▇▇▇█▇█▇█▇███▇▇▇</td></tr><tr><td>val_auc</td><td>▁▂▃▄▆▆▇██████▇▇▇████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▂▅▇▇█████▇▇██▇█▇████▇█▇▇█▇███▇█▇█▇▇▇</td></tr><tr><td>val_loss_epoch</td><td>▇█▇▇▇▅▅▄▁▄▄▃▂▃▂▃▃▃▁▄▃▃▁▂▃▁▄▅▂▄▂▄▄▃▅▃▃▄▃▃</td></tr><tr><td>val_loss_step</td><td>██▇▇▆▆▆▅▄▄▅▄▄▄▃▄▄▃▂▄▄▄▁▄▄▁▃▄▃▆▃▄▆▄▄▄▄▅▄▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>49</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.69212</td></tr><tr><td>train_auc</td><td>0.74979</td></tr><tr><td>train_f1</td><td>0.50331</td></tr><tr><td>train_loss_epoch</td><td>0.56784</td></tr><tr><td>train_loss_step</td><td>0.57832</td></tr><tr><td>trainer/global_step</td><td>249</td></tr><tr><td>val_acc</td><td>0.70066</td></tr><tr><td>val_auc</td><td>0.77364</td></tr><tr><td>val_f1</td><td>0.54726</td></tr><tr><td>val_loss_epoch</td><td>0.5754</td></tr><tr><td>val_loss_step</td><td>0.57871</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_32_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/6j4075jw' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/6j4075jw</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240115_074406-6j4075jw\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f891be8e20c4d17a9abeabf0a027d4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.0169333333382383, max=1.0))…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240115_075557-xu0di2vj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/xu0di2vj' target=\"_blank\">MLP_2_32_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/xu0di2vj' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/xu0di2vj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 1.2 K \n",
      "2  | head        | Sequential       | 562   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 1.1 K \n",
      "--------------------------------------------------\n",
      "2.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "2.9 K     Total params\n",
      "0.012     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "304b27f7c3794056b2fdba867f9a2501",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▁▁▂▂▃▄▅▆▆▆▇▆▇▆▆▆▇▆▆█▇▆▆▇▇▆▆▅▇▇▇▆▇██▆▇▇</td></tr><tr><td>train_auc</td><td>▁▁▁▂▃▅▆▆▇▇▇▇██▇▇█▇██▇███▇███████████████</td></tr><tr><td>train_f1</td><td>▂▁▁▁▁▃▃▅▇█▇▇█▇█▇▇▇█▇▇█▇▇▇██▇▇▇█▇█▇███▇█▇</td></tr><tr><td>train_loss_epoch</td><td>██▇▇▇▆▅▄▃▃▃▂▂▂▃▃▂▂▂▂▂▂▂▁▂▁▂▁▂▂▂▁▂▂▁▁▁▂▁▂</td></tr><tr><td>train_loss_step</td><td>█▆█▇▃▆▂▄▆▃▂▂▄▁▃▁▃▃▅▄▃▃▁▃▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▁▂▁▂▂▃▃▃▃▄▄▂▄▂▄▂▅▃▅▃▆▆▆▃▃▇▇▇▃▇▄█▄█▄</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▃▅▇▆▆▇▆█▇███▇██▇▇▇█████████▇█▇▇██▇█</td></tr><tr><td>val_auc</td><td>▁▂▂▃▄▅▆▆▇█▇█████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▃▆▇█▆▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>val_loss_epoch</td><td>█▇█▇▆▆▄▄▄▄▁▄▂▃▃▂▃▃▁▂▃▃▃▂▃▂▄▃▄▄▂▂▃▂▃▃▃▃▄▂</td></tr><tr><td>val_loss_step</td><td>▇▇█▆▆▆▅▄▄▄▁▅▄▄▄▄▄▅▂▄▄▄▅▄▄▃▄▃▅▅▄▄▄▃▄▄▄▄▄▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>49</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.70115</td></tr><tr><td>train_auc</td><td>0.75922</td></tr><tr><td>train_f1</td><td>0.49724</td></tr><tr><td>train_loss_epoch</td><td>0.55881</td></tr><tr><td>train_loss_step</td><td>0.5555</td></tr><tr><td>trainer/global_step</td><td>249</td></tr><tr><td>val_acc</td><td>0.71711</td></tr><tr><td>val_auc</td><td>0.77599</td></tr><tr><td>val_f1</td><td>0.59048</td></tr><tr><td>val_loss_epoch</td><td>0.55091</td></tr><tr><td>val_loss_step</td><td>0.53047</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_32_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/xu0di2vj' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/xu0di2vj</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240115_075557-xu0di2vj\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f58e461ab1e427fb7fbd3a5cf8388dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016933333330477276, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240115_081044-lu4uthm4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/lu4uthm4' target=\"_blank\">MLP_2_64_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/lu4uthm4' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/lu4uthm4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 4.5 K \n",
      "2 | head        | Sequential       | 2.1 K \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "6.7 K     Trainable params\n",
      "0         Non-trainable params\n",
      "6.7 K     Total params\n",
      "0.027     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "377bb3903e424bdd90eefa8a94f5620d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▃▃▃▄▆▇▇▇▇▆▇▇▆▆▇▇▆▇▇▆▇▆▆▇▇▇█▇▇██▇▇▆▇▇▇█▇</td></tr><tr><td>train_auc</td><td>▁▁▂▄▆▇▇▇██▇██▇▇▇▇█████▇█▇█▇█████▇███████</td></tr><tr><td>train_f1</td><td>▂▁▁▁▃▆▇██▆▆██▆█▇▇▇▇▇▇▇▇▇▇█▇█▇▇▇█▇▇▇█▇▇█▇</td></tr><tr><td>train_loss_epoch</td><td>█▇▇▆▄▃▂▃▂▂▂▁▂▂▂▂▂▁▁▂▂▂▂▂▂▁▂▁▁▁▂▁▂▁▂▂▁▁▁▂</td></tr><tr><td>train_loss_step</td><td>██▄▃▃▄▂▄▁▅▄▄▃▂▅▃▃▂▄▅▂▁▃▃▆</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▁▂▁▂▂▃▃▃▃▄▄▂▄▂▄▂▅▃▅▃▆▆▆▃▃▇▇▇▃▇▄█▄█▄</td></tr><tr><td>val_acc</td><td>▁▁▁▁▄▇▇██▇▇▆▇██▆▇███▇▇▆██▇██▇██████▇████</td></tr><tr><td>val_auc</td><td>▁▆▇▇▇▇▇▇▇███████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▅█▇██▇█▇▇██▇▇███▇█▇██▇██▇██████▇████</td></tr><tr><td>val_loss_epoch</td><td>█▇█▇▅▄▅▄▃▄▃▃▃▅▃▄▃▄▄▃▄▃▄▃▁▄▄▂▂▂▄▃▂▃▅▂▂▃▄▃</td></tr><tr><td>val_loss_step</td><td>▇▇█▇▄▃▆▄▄▄▂▂▄▆▃▃▄▅▄▃▃▂▄▄▄▅▄▃▁▂▃▄▁▂▄▄▁▃▄▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>49</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.69869</td></tr><tr><td>train_auc</td><td>0.76017</td></tr><tr><td>train_f1</td><td>0.53485</td></tr><tr><td>train_loss_epoch</td><td>0.5574</td></tr><tr><td>train_loss_step</td><td>0.61398</td></tr><tr><td>trainer/global_step</td><td>249</td></tr><tr><td>val_acc</td><td>0.71382</td></tr><tr><td>val_auc</td><td>0.77604</td></tr><tr><td>val_f1</td><td>0.59908</td></tr><tr><td>val_loss_epoch</td><td>0.55774</td></tr><tr><td>val_loss_step</td><td>0.5517</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_64_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/lu4uthm4' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/lu4uthm4</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240115_081044-lu4uthm4\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "277a9cb0de6d4470be7ea207c1f3763a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016933333330477276, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240115_082235-dx3uedwc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/dx3uedwc' target=\"_blank\">MLP_2_64_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/dx3uedwc' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/dx3uedwc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 4.5 K \n",
      "2 | head        | Sequential       | 2.1 K \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "6.7 K     Trainable params\n",
      "0         Non-trainable params\n",
      "6.7 K     Total params\n",
      "0.027     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a851899bca174581a846ab699e76f82e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▂▁▅▅▅▅▅▅▅▅▅▆▆▆▆▅▇▆▇▆▆▇▇▇██▆▇▆▇▇▅▇█▆▅▇▇█▇</td></tr><tr><td>train_auc</td><td>▂▁▃▃▁▄▃▄▅▄▃▄▅▆▅▆▅▆█▇▇▆▆▆▇▆▆▇▆▇█▆▇▇▇▆▇▇▇█</td></tr><tr><td>train_f1</td><td>▄▄▂▁▁▁▁▁▁▁▂▃▄▂▅▄▄▄▆▅▅█▄▇▇▅▄▅▅▆▅▇▆▆▆▅▆▆▇▇</td></tr><tr><td>train_loss_epoch</td><td>█▆▅▅▄▄▄▄▃▄▄▄▃▃▃▃▃▃▂▂▂▃▃▃▂▂▂▂▃▂▁▂▂▂▂▂▁▁▂▁</td></tr><tr><td>train_loss_step</td><td>▆▇▄▄▅▅▅▅▄▄▆▅▄▄█▂▄▄▄▃▃▁▃▆▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▁▂▁▂▂▃▃▃▃▄▄▂▄▂▄▂▅▃▅▃▆▆▆▃▃▇▇▇▃▇▄█▄█▄</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁█▁██▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>▁▇██████████████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁█▁██▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_epoch</td><td>▁▁▂▂▁▁▁▁▂▂▁▁▁▂▁▁▁▂▂▁▃▅▂▅▂▄▃▃▂▃▅▄▄▂▅▃▄▄█▃</td></tr><tr><td>val_loss_step</td><td>▃▃▃▃▃▂▂▂▂▂▂▃▃▄▂▂▃▃▂▄▄█▂▅▄▆▅▄▁▃▅▆▆▁▆▅▆▄▄▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>49</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.65764</td></tr><tr><td>train_auc</td><td>0.66653</td></tr><tr><td>train_f1</td><td>0.36914</td></tr><tr><td>train_loss_epoch</td><td>0.61168</td></tr><tr><td>train_loss_step</td><td>0.62359</td></tr><tr><td>trainer/global_step</td><td>249</td></tr><tr><td>val_acc</td><td>0.57566</td></tr><tr><td>val_auc</td><td>0.50711</td></tr><tr><td>val_f1</td><td>0.0</td></tr><tr><td>val_loss_epoch</td><td>0.74087</td></tr><tr><td>val_loss_step</td><td>0.70797</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_64_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/dx3uedwc' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/dx3uedwc</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240115_082235-dx3uedwc\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67524d9433ce4aaa93ccce7db96f88a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01691666666883975, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240115_083432-53mlkvpx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/53mlkvpx' target=\"_blank\">MLP_2_64_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/53mlkvpx' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/53mlkvpx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 4.5 K \n",
      "2 | head        | Sequential       | 2.1 K \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "6.7 K     Trainable params\n",
      "0         Non-trainable params\n",
      "6.7 K     Total params\n",
      "0.027     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "997b43d1c9bc42b29dcfd5b2b3959815",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▂▁▄▄▅▅▆▇▆▇▇▇▆▇▆▇▇▇▇▇▇▇▆█▇▇█▇▇█▇█▇▇███▇</td></tr><tr><td>train_auc</td><td>▂▁▂▂▃▂▃▄▅▅▅▆▆▆▆▇▇▆▆▆▆▆▇▇▆▆▆▆▇▇▇▇▇▇▇▇▇▇█▆</td></tr><tr><td>train_f1</td><td>▂▃▂▃▄▁▁▄▅▆▆▇▇▅▇▆▆▇▇▇▇▇▆▇▇▇▇▇▇▇▇█▇█▇▇▇▇█▇</td></tr><tr><td>train_loss_epoch</td><td>█▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▃▂▂▂▂▂▂▁▂▂▂▁▁▂▁▁▁▁▂▁▁▁▂▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▁▂▁▂▂▃▃▃▃▄▄▂▄▂▄▂▅▃▅▃▆▆▆▃▃▇▇▇▃▇▄█▄█▄</td></tr><tr><td>val_acc</td><td>▁▄▁▄▆▄▆▇▇▇█▇▇█▇▇████▇██▇███▇▇████▇█▇▇█▇▇</td></tr><tr><td>val_auc</td><td>▄▁▄▃▅▅▅▅▆▆▇▇████▇▇▇▇▇████▇▇▇████████████</td></tr><tr><td>val_f1</td><td>▇▁▇▁▅▁▆▆▇▇█▇▇██▇███████▇███▇▇███▇▇█▇▇█▇▇</td></tr><tr><td>val_loss_epoch</td><td>█▅▂▂▂▂▂▂▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▂▁</td></tr><tr><td>val_loss_step</td><td>█▅▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▁▂▂▂▁▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>49</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.73235</td></tr><tr><td>train_auc</td><td>0.68913</td></tr><tr><td>train_f1</td><td>0.58205</td></tr><tr><td>train_loss_epoch</td><td>0.54479</td></tr><tr><td>train_loss_step</td><td>0.5712</td></tr><tr><td>trainer/global_step</td><td>249</td></tr><tr><td>val_acc</td><td>0.72368</td></tr><tr><td>val_auc</td><td>0.78684</td></tr><tr><td>val_f1</td><td>0.60748</td></tr><tr><td>val_loss_epoch</td><td>0.52613</td></tr><tr><td>val_loss_step</td><td>0.48925</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_64_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/53mlkvpx' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/53mlkvpx</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240115_083432-53mlkvpx\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdffdccc2dee4a6da83a18fbc40e73f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.017183333332650364, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240115_084547-0hjlvk1c</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/0hjlvk1c' target=\"_blank\">MLP_2_64_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/0hjlvk1c' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/0hjlvk1c</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 4.5 K \n",
      "2  | head        | Sequential       | 2.1 K \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 65    \n",
      "--------------------------------------------------\n",
      "6.8 K     Trainable params\n",
      "0         Non-trainable params\n",
      "6.8 K     Total params\n",
      "0.027     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b0b1ec5a5434359b06fdec96481f8f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▂▃▅▆▇▆▆▆▆█▆▇▇▆▆▇▆▆▇▆▆█▆▇▇█▆▇█▆▆▆▇▇▇██▇</td></tr><tr><td>train_auc</td><td>▁▂▃▅▇▇█▇▇▇██▇███▇▇████▇██████▇█▇█▇██████</td></tr><tr><td>train_f1</td><td>▃▁▁▂▇▇▇▇▇▇▇█▇▇▇▇▇█▇▇▇▆▇█▇█▇█▇▇█▆▇▇█▇▇███</td></tr><tr><td>train_loss_epoch</td><td>█▇▇▅▃▃▂▃▂▂▂▂▂▁▂▁▂▂▂▂▂▁▂▂▂▂▁▂▂▂▂▂▁▂▁▁▁▁▁▂</td></tr><tr><td>train_loss_step</td><td>█▆▆▃█▆▃▆▄▃▇▃▄▂▄▃▄▂▄▁▂▆▅▃▄</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▁▂▁▂▂▃▃▃▃▄▄▂▄▂▄▂▅▃▅▃▆▆▆▃▃▇▇▇▃▇▄█▄█▄</td></tr><tr><td>val_acc</td><td>▁▁▁▂▆▇█▆▆▇█▇▇▇▆▆▇▇▇▆▆▇▇▆▇▆▆▇█▇▆▇▇▇▇█▇▇▇▇</td></tr><tr><td>val_auc</td><td>▁▆▇██▆▅▅▅▅▆▆▆▅▆▆▅▆▅▅▅▅▅▆▆▆▆▆▆▆▆▅▆▆▅▅▅▅▆▆</td></tr><tr><td>val_f1</td><td>▁▁▁▂▆██▆▆██▇▇█▇▇█▇█▇▇██▆▇▇█▇██▇████████▇</td></tr><tr><td>val_loss_epoch</td><td>▇█▇▆▅▃▃▂▄▄▃▄▁▅▂▃▄▃▄▃▂▃▁▄▁▂▅▅▆▄▅▃▃▄▃▃▃▂▄▄</td></tr><tr><td>val_loss_step</td><td>▇▆▆▅▃▃▄▁▄▃▄▄▄▆▂▃▃▄▄▃▄▄▁▄▄▂▃▄█▅▄▄▃▅▃▃▃▂▄▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>49</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.71018</td></tr><tr><td>train_auc</td><td>0.75705</td></tr><tr><td>train_f1</td><td>0.53977</td></tr><tr><td>train_loss_epoch</td><td>0.55823</td></tr><tr><td>train_loss_step</td><td>0.56428</td></tr><tr><td>trainer/global_step</td><td>249</td></tr><tr><td>val_acc</td><td>0.71053</td></tr><tr><td>val_auc</td><td>0.77484</td></tr><tr><td>val_f1</td><td>0.57282</td></tr><tr><td>val_loss_epoch</td><td>0.59234</td></tr><tr><td>val_loss_step</td><td>0.6276</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_64_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/0hjlvk1c' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/0hjlvk1c</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240115_084547-0hjlvk1c\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4de1885e11c7414790a7b12e0d1a0919",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01691666666107873, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240115_085719-tzv7b31r</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/tzv7b31r' target=\"_blank\">MLP_2_64_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/tzv7b31r' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/tzv7b31r</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 4.5 K \n",
      "2  | head        | Sequential       | 2.1 K \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 4.2 K \n",
      "--------------------------------------------------\n",
      "10.9 K    Trainable params\n",
      "0         Non-trainable params\n",
      "10.9 K    Total params\n",
      "0.044     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c636b5584a2422db5a7177ee5ab3501",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▂▂▂▃▆▇▇█▆▆▇▆▇▆▆▆▇▆█▇▇▇▇▆▇▇▆█▇▇▆█▆█▇█▇▇</td></tr><tr><td>train_auc</td><td>▁▂▂▃▅▆▇▇▇██▇▇████▇█▇████████████████████</td></tr><tr><td>train_f1</td><td>▃▁▁▁▁▃▆███▇▇█▆█▇▇▇█▇█▇▇██▇█▇▇██▇▇█▇█████</td></tr><tr><td>train_loss_epoch</td><td>█▇▇▆▅▄▃▂▂▂▂▂▂▁▂▂▂▂▁▂▁▁▁▁▂▁▂▂▁▂▁▂▂▁▂▁▁▁▁▂</td></tr><tr><td>train_loss_step</td><td>█▆▆▄▄▂▃▃▅▃▅▃▁▂▂▃▅▄▅▄▁▂▃▃▄</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▁▂▁▂▂▃▃▃▃▄▄▂▄▂▄▂▅▃▅▃▆▆▆▃▃▇▇▇▃▇▄█▄█▄</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▆▇▇██▇▆▆▇▇█▇█▇███▇▆▇███▆▇▇▇▇▇█▇██▇█</td></tr><tr><td>val_auc</td><td>▁▅▇▇████▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▆▇█▇█▇▇▇█▇█▇█▇████▇▇▇██▇█▇██▇█▇▇███</td></tr><tr><td>val_loss_epoch</td><td>█▇▇█▇▃▃▂▄▃▃▆▅▃▃▃▃▃▃▃▃▃▄▄▄▃▂▃▃▄▄▂▃▃▂▁▃▂▂▂</td></tr><tr><td>val_loss_step</td><td>▇▇▇▇▅▄▃▂▃▃▂█▃▃▂▃▃▂▂▃▃▄▅▃▃▂▃▃▂▄▃▄▃▂▄▃▄▁▄▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>49</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.71182</td></tr><tr><td>train_auc</td><td>0.75873</td></tr><tr><td>train_f1</td><td>0.56506</td></tr><tr><td>train_loss_epoch</td><td>0.56041</td></tr><tr><td>train_loss_step</td><td>0.5761</td></tr><tr><td>trainer/global_step</td><td>249</td></tr><tr><td>val_acc</td><td>0.71053</td></tr><tr><td>val_auc</td><td>0.77466</td></tr><tr><td>val_f1</td><td>0.6</td></tr><tr><td>val_loss_epoch</td><td>0.54064</td></tr><tr><td>val_loss_step</td><td>0.50722</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_64_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/tzv7b31r' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/tzv7b31r</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240115_085719-tzv7b31r\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce0dc985862f4212b18625a8634601b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01691666666883975, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240115_090854-8ijp75l4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/8ijp75l4' target=\"_blank\">MLP_3_16_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/8ijp75l4' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/8ijp75l4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 640   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "794       Trainable params\n",
      "0         Non-trainable params\n",
      "794       Total params\n",
      "0.003     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19a6f00fd7dc425fac7764da2e96f979",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▃▄▄▆▅▅▇▅█▆▇▆▆▅▇▅▇▇▇▇██▅▇▅▆▇▆</td></tr><tr><td>train_auc</td><td>▁▂▁▁▁▂▂▂▃▃▄▅▅▆▆▇▇▇█▇█▇█▇▇▇█▇███▇████████</td></tr><tr><td>train_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▂▅▅▇▇▅▇▆█▇▇▆▇▆█▇▇█▇█▇█▇█▆█▇▇</td></tr><tr><td>train_loss_epoch</td><td>███████▇██▇▇▆▄▄▃▃▃▂▂▂▂▂▂▃▃▂▂▂▁▂▂▁▂▂▂▃▂▂▁</td></tr><tr><td>train_loss_step</td><td>▆▇▇▆▇█▆▅▄▄▅▃▆▂▃▃▁▁▃▃▆▂▅▅▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▁▂▁▂▂▃▃▃▃▄▄▂▄▂▄▂▅▃▅▃▆▆▆▃▃▇▇▇▃▇▄█▄█▄</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▂▄▇▆▆▇█▆▇█▇▇▆█▆▇██▇█▇▆▇▇▆█▆▇</td></tr><tr><td>val_auc</td><td>▁▆▆▆▆▆▆▆▆▆▆▆▇▇▇█████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▃▅█▆▆▇█▇▇█▇▇▇█▇▇██▇██▇▇▇▇█▇▇</td></tr><tr><td>val_loss_epoch</td><td>▇█▇▇▇▆▇▇▇▇▇▆▅▄▅▅▄▄▄▃▃▃▁▂▄▂▃▃▃▃▂▃▃▂▄▂▂▃▃▃</td></tr><tr><td>val_loss_step</td><td>▇▇█▇▇█▇▇▇▇█▇▆▄▇▅▅▆▆▄▄▄▁▄▄▃▄▄▆▆▄▄▄▂▄▄▂▄▄▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>49</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.68145</td></tr><tr><td>train_auc</td><td>0.7486</td></tr><tr><td>train_f1</td><td>0.44571</td></tr><tr><td>train_loss_epoch</td><td>0.57064</td></tr><tr><td>train_loss_step</td><td>0.58876</td></tr><tr><td>trainer/global_step</td><td>249</td></tr><tr><td>val_acc</td><td>0.70724</td></tr><tr><td>val_auc</td><td>0.77914</td></tr><tr><td>val_f1</td><td>0.54822</td></tr><tr><td>val_loss_epoch</td><td>0.58779</td></tr><tr><td>val_loss_step</td><td>0.60702</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_16_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/8ijp75l4' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/8ijp75l4</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240115_090854-8ijp75l4\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a811d395f23a47fca9e27ad950c942f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240115_092020-f92atsex</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/f92atsex' target=\"_blank\">MLP_3_16_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/f92atsex' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/f92atsex</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 640   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "794       Trainable params\n",
      "0         Non-trainable params\n",
      "794       Total params\n",
      "0.003     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f79d9bbebd9a475781be7b0370165ab7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▄▄▄▅▄▅▅▅▃▄▆▂▄▄▆▃▆▂▄█▃▆▅▅▅▁▆▂▄▇▄▅▁█▄▁▄▆▆▅</td></tr><tr><td>train_auc</td><td>▁▂▃▁▂▄▃▄▃▅▃▆▅▅▄▅▇▄█▆▆▇▅▄▅▅▇▆▆▆▅▆▆▆▆▆▆▆▇▅</td></tr><tr><td>train_f1</td><td>▁▁▁▁▁▁▁▁▁▁▂▃▁▁▃▂▃▃▃▃▃▅█▂▅▅▂▃▂▄▆▄▄▆▆▆▂▅▆▅</td></tr><tr><td>train_loss_epoch</td><td>█▆▅▆▅▆▅▄▆▅▅▅▄▄▄▅▃▅▂▃▃▃▅▄▄▅▂▃▄▂▄▂▃▂▄▃▃▂▁▃</td></tr><tr><td>train_loss_step</td><td>▆▆▅▄██▇▄▃▇▇▆▅▆▄▃▁▅▆▄▄▁▆▃▄</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▁▂▁▂▂▃▃▃▃▄▄▂▄▂▄▂▅▃▅▃▆▆▆▃▃▇▇▇▃▇▄█▄█▄</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>▅▄▆▆▆▆▆▆▆▄▆▆██▃▃▄▄▄▄▄▄▄▃▄▆▅▅▃▄▂▂▄▄▆▄▆▁▁█</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_epoch</td><td>▁▃▂▂▂▁▂▂▂▂▄▃▁▁▅▃▃▄▅▃▄▃▂▆▅▄▄▃▅▆▄▆▃▄▆▂▄▇▄█</td></tr><tr><td>val_loss_step</td><td>▃▂▃▂▃▃▂▃▃▃▅▄▄▁▆▃▄▄▆▃▄▂▁▄▃▄▄▄▅▆▄▄▁▄▅▅▃█▅█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>49</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.63957</td></tr><tr><td>train_auc</td><td>0.55288</td></tr><tr><td>train_f1</td><td>0.09856</td></tr><tr><td>train_loss_epoch</td><td>0.64445</td></tr><tr><td>train_loss_step</td><td>0.64119</td></tr><tr><td>trainer/global_step</td><td>249</td></tr><tr><td>val_acc</td><td>0.57566</td></tr><tr><td>val_auc</td><td>0.51761</td></tr><tr><td>val_f1</td><td>0.0</td></tr><tr><td>val_loss_epoch</td><td>0.82235</td></tr><tr><td>val_loss_step</td><td>0.89179</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_16_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/f92atsex' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/f92atsex</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240115_092020-f92atsex\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ebe22a72bd24d918384b95c08a8bb43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01691666666107873, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240115_093150-axcrgrkl</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/axcrgrkl' target=\"_blank\">MLP_3_16_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/axcrgrkl' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/axcrgrkl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 640   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "794       Trainable params\n",
      "0         Non-trainable params\n",
      "794       Total params\n",
      "0.003     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▃▂▄▄▅▅▅▅▅▆▆▆▆▆▆▆▆▇▆▇▇▇▆▇▇▇▇▇█▇▇▇▇▇█▇▇█▇</td></tr><tr><td>train_auc</td><td>▁▃▂▂▃▂▂▄▃▂▃▆▃▅▅▅▅▅▆▆▅▆▆▇█▇▇▆▅▆▆▇▇▅▅▇▆▅▆▅</td></tr><tr><td>train_f1</td><td>▅▆▅▄▄▃▂▁▂▁▂▃▂▅▃▅▅▆▄▆▆▇▇▅▆▇▇▇▆█▆██▆▇█▇▇▇▆</td></tr><tr><td>train_loss_epoch</td><td>█▄▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▄▄▃▃▃▃▃▃▃▃▂▃▂▂▂▂▂▂▂▃▁▂▃▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▁▂▁▂▂▃▃▃▃▄▄▂▄▂▄▂▅▃▅▃▆▆▆▃▃▇▇▇▃▇▄█▄█▄</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▃▃▂▅▆▆▆▆▇█▇█▇█▆██▆█▆▇▆█▆█▆▇▆</td></tr><tr><td>val_auc</td><td>▁▄▆▁▇▆▇▇▄▂▂▂▄▅▅▇▆▆▇██████████████▇▇▇██▇▇</td></tr><tr><td>val_f1</td><td>▁▂▁▁▁▁▁▁▁▁▁▂▃▃▂▆▆▆▆▆▇█▇█▇█▆██▇█▆▇▆█▆▇▇▇▆</td></tr><tr><td>val_loss_epoch</td><td>█▄▅▄▄▄▄▄▄▄▄▄▃▃▄▃▃▃▃▂▂▂▁▂▂▁▂▂▂▂▁▂▁▂▂▁▁▂▂▂</td></tr><tr><td>val_loss_step</td><td>█▄▅▄▄▄▃▄▄▄▄▄▃▃▅▃▃▃▄▂▂▂▁▂▂▁▂▂▂▃▂▂▂▂▂▂▁▂▂▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>49</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.67898</td></tr><tr><td>train_auc</td><td>0.56966</td></tr><tr><td>train_f1</td><td>0.3938</td></tr><tr><td>train_loss_epoch</td><td>0.56985</td></tr><tr><td>train_loss_step</td><td>0.61513</td></tr><tr><td>trainer/global_step</td><td>249</td></tr><tr><td>val_acc</td><td>0.69079</td></tr><tr><td>val_auc</td><td>0.75132</td></tr><tr><td>val_f1</td><td>0.5</td></tr><tr><td>val_loss_epoch</td><td>0.5842</td></tr><tr><td>val_loss_step</td><td>0.60673</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_16_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/axcrgrkl' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/axcrgrkl</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240115_093150-axcrgrkl\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53b1873382d84da7a20ba73000079c3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240115_094339-guryprkj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/guryprkj' target=\"_blank\">MLP_3_16_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/guryprkj' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/guryprkj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 640   \n",
      "2  | head        | Sequential       | 154   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 17    \n",
      "--------------------------------------------------\n",
      "811       Trainable params\n",
      "0         Non-trainable params\n",
      "811       Total params\n",
      "0.003     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6c16c16ec27466a81a85bc2bd41693b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▂▁▂▂▅▆▃▂▆▃▅▃▇▅▆█▇▇▇▇▆▅▇▇▅▅▆▅▆</td></tr><tr><td>train_auc</td><td>▁▁▁▂▂▃▁▂▃▃▃▄▄▅▆▆▇▆▆▇▇▇▇▇▇▇██▇▇█▇▇█▇█▇▇▇█</td></tr><tr><td>train_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▃▂▆▆█▃▇▄▆▆▇▇█▇██▇█▆▆▇█▆▆▇▇▇</td></tr><tr><td>train_loss_epoch</td><td>███▇▇▇▇▇▇▇▇▇▆▆▅▄▃▄▄▃▄▂▂▂▃▂▁▁▂▂▂▂▂▁▂▂▂▂▂▁</td></tr><tr><td>train_loss_step</td><td>▆█▆▆▇▇▆▆▆▅▄▅▄▃▄▄▄▃▁▄▂▅▄▄▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▁▂▁▂▂▃▃▃▃▄▄▂▄▂▄▂▅▃▅▃▆▆▆▃▃▇▇▇▃▇▄█▄█▄</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▄▆▇▆▅▇▃▇▆▇▇▇████▆█▇▇▇█▆█▇▇</td></tr><tr><td>val_auc</td><td>▁▅▆▆▆▆▆▆▆▆▆▆▇▇▇█████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▅▆█▆▅█▄█▆██▇█▇▇█▇█▆█▇█▆█▇▆</td></tr><tr><td>val_loss_epoch</td><td>▇▇▆▆▇▆▆▆▇▆▅▅▅█▄▃▃▂▃▃▄▃▃▃▂▃▂▃▃▃▃▁▂▁▂▂▁▂▃▃</td></tr><tr><td>val_loss_step</td><td>▅▅▅▅▅▅▅▄▄▅▃▄▄█▃▂▂▂▃▃▃▂▄▂▂▄▂▂▃▄▂▂▂▁▂▃▁▃▂▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>49</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.68637</td></tr><tr><td>train_auc</td><td>0.74154</td></tr><tr><td>train_f1</td><td>0.4159</td></tr><tr><td>train_loss_epoch</td><td>0.56708</td></tr><tr><td>train_loss_step</td><td>0.55596</td></tr><tr><td>trainer/global_step</td><td>249</td></tr><tr><td>val_acc</td><td>0.69079</td></tr><tr><td>val_auc</td><td>0.77639</td></tr><tr><td>val_f1</td><td>0.5</td></tr><tr><td>val_loss_epoch</td><td>0.60785</td></tr><tr><td>val_loss_step</td><td>0.64292</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_16_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/guryprkj' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/guryprkj</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240115_094339-guryprkj\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a0eb2655b6b439487d66f15ed2a3d7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240115_095518-n5fg7ejo</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/n5fg7ejo' target=\"_blank\">MLP_3_16_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/n5fg7ejo' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/n5fg7ejo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 640   \n",
      "2  | head        | Sequential       | 154   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 281   \n",
      "--------------------------------------------------\n",
      "1.1 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.1 K     Total params\n",
      "0.004     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eee3d2312569430e8491a6e807a1b575",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▂▅▄▄▅▆▅▅▄▆▆▅▇▆▅▇▆▇█▆█▆▆▇▆▆▆▆</td></tr><tr><td>train_auc</td><td>▂▁▂▂▁▂▂▂▃▃▄▅▆▇▇▇▇▇▇█████████▇██████████▇</td></tr><tr><td>train_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▃▆▆▆▅█▆▆▆▇▇▇█▇▆▇▇▇█▇█▇▇▇▇▇▇▇</td></tr><tr><td>train_loss_epoch</td><td>████████▇▇▇▇▅▃▄▄▂▃▂▁▂▂▁▂▂▁▂▁▁▁▂▁▁▂▂▁▁▁▁▂</td></tr><tr><td>train_loss_step</td><td>▆▇▆██▆▆▅▄▃▃▂▄▂▄▁▃▄▃▃▄▂▃▂▄</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▁▂▁▂▂▃▃▃▃▄▄▂▄▂▄▂▅▃▅▃▆▆▆▃▃▇▇▇▃▇▄█▄█▄</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▄▅▇███▆▇▆█▇▇▇▇█▇███▇█▇█▇▇▇▇▇</td></tr><tr><td>val_auc</td><td>▁▅▆▅▅▅▅▆▆▆▆▆▇▇▇█████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▅▅█▇▇█▆█▇█▇▇█▇█████▇█▇█▇▇█▇▇</td></tr><tr><td>val_loss_epoch</td><td>▇▇▇█▆█▇▆▇▆▆▅▄▃▄▃▂▂▃▂▂▂▁▂▂▄▂▃▃▁▃▁▂▃▂▁▁▃▂▂</td></tr><tr><td>val_loss_step</td><td>▆▆▆█▆▆▇▄▆▆▆▄▄▃▅▂▂▂▄▂▂▂▁▂▂▆▁▂▄▁▂▂▂▄▂▂▁▄▂▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>49</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.68227</td></tr><tr><td>train_auc</td><td>0.7258</td></tr><tr><td>train_f1</td><td>0.42667</td></tr><tr><td>train_loss_epoch</td><td>0.58126</td></tr><tr><td>train_loss_step</td><td>0.58229</td></tr><tr><td>trainer/global_step</td><td>249</td></tr><tr><td>val_acc</td><td>0.70066</td></tr><tr><td>val_auc</td><td>0.77741</td></tr><tr><td>val_f1</td><td>0.54271</td></tr><tr><td>val_loss_epoch</td><td>0.58639</td></tr><tr><td>val_loss_step</td><td>0.60903</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_16_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/n5fg7ejo' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/n5fg7ejo</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240115_095518-n5fg7ejo\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "927fa956e13f4327be333fdca48fc68c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240115_100728-4h5ll5m7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/4h5ll5m7' target=\"_blank\">MLP_3_32_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/4h5ll5m7' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/4h5ll5m7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 2.3 K \n",
      "2 | head        | Sequential       | 562   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "2.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "2.9 K     Total params\n",
      "0.011     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df1d0031336f44efb4ec00b1d7999420",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▅▆▆▆▆▆▆▆▆▇▇▇▇▇███████████▇████▇▇▇███▇▇▇</td></tr><tr><td>train_auc</td><td>▁▂▃▂▂▂▂▁▃▄▅▆▇▇█▇▇█▇▇███▇█▇▇█▇████▇█▇▇███</td></tr><tr><td>train_f1</td><td>▇▂▂▂▁▁▁▁▁▂▄▅▇██▇▇▇█▇▇██▇▇▇▇▇▇▇▇▇█▇██▇▇▇▇</td></tr><tr><td>train_loss_epoch</td><td>█▆▆▆▅▅▅▅▄▄▃▃▃▃▂▂▂▂▁▂▁▂▁▂▂▂▂▁▁▁▂▂▂▂▂▂▂▁▁▁</td></tr><tr><td>train_loss_step</td><td>███▆▇▆▃▅▄▄▂▄▁▁▃▄▆▂▄▄▂▄▅▃▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▁▂▁▂▂▃▃▃▃▄▄▂▄▂▄▂▅▃▅▃▆▆▆▃▃▇▇▇▃▇▄█▄█▄</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▃▆█▇▇▆▇▇███▇▇▇▇██▇▇▇██▇█▇██▆█▇</td></tr><tr><td>val_auc</td><td>▁▅▇▇▇███████████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▂▄▇█▇▇▆▇▇▇▇▇▇▇█▇▇▇▇▇▇▇▇▇▇▇▇▇▆▇▇</td></tr><tr><td>val_loss_epoch</td><td>▆█▇▇▆▇▅▅▆▃▃▂▃▁▂▄▃▃▂▁▁▁▂▂▂▂▃▂▃▂▂▃▃▂▃▂▂▃▂▁</td></tr><tr><td>val_loss_step</td><td>▇▇▇█▇▇▄▆▆▆▄▂▃▂▃▃▃▅▄▃▃▂▃▃▃▄▄▄▅▃▃▄▄▃▄▃▂▄▃▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>49</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.69704</td></tr><tr><td>train_auc</td><td>0.74286</td></tr><tr><td>train_f1</td><td>0.48536</td></tr><tr><td>train_loss_epoch</td><td>0.56071</td></tr><tr><td>train_loss_step</td><td>0.55194</td></tr><tr><td>trainer/global_step</td><td>249</td></tr><tr><td>val_acc</td><td>0.71053</td></tr><tr><td>val_auc</td><td>0.77918</td></tr><tr><td>val_f1</td><td>0.57282</td></tr><tr><td>val_loss_epoch</td><td>0.53877</td></tr><tr><td>val_loss_step</td><td>0.49634</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_32_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/4h5ll5m7' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/4h5ll5m7</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240115_100728-4h5ll5m7\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0231b9e47b5b481ab8dabe84ccc95daa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240115_102011-onrk3jwr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/onrk3jwr' target=\"_blank\">MLP_3_32_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/onrk3jwr' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/onrk3jwr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 2.3 K \n",
      "2 | head        | Sequential       | 562   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "2.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "2.9 K     Total params\n",
      "0.011     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aea6adaba4b94a818026090c6f88455b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▃▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇▇▇█▇▇▇█▇▇█▇██</td></tr><tr><td>train_auc</td><td>▃▃▄▃▂▁▄▁▃▃▁▄▃▄▄▄▄▄▂▂▄▅▄▂▃▄▅▆▄▆▄▆▄▄▄▆▄▄▆█</td></tr><tr><td>train_f1</td><td>█▅▅▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▂▂▃▃▂▂▂▂▂▃▂▄▃▃▃▃</td></tr><tr><td>train_loss_epoch</td><td>█▆▄▄▄▃▃▄▃▃▂▃▃▃▂▂▃▂▃▃▂▃▂▂▂▂▁▂▂▂▂▃▂▂▂▂▂▂▁▁</td></tr><tr><td>train_loss_step</td><td>▇▆█▃▆█▃▆▄▅▃▇█▁▃▆▇▆██▄▃▆▅▄</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▁▂▁▂▂▃▃▃▃▄▄▂▄▂▄▂▅▃▅▃▆▆▆▃▃▇▇▇▃▇▄█▄█▄</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁███████████████████████████████████</td></tr><tr><td>val_auc</td><td>▂▂▂▄▄▆▄▄▃▃▁▁▃▆▄▂▄▆▆▆▆▆▆▆▄▇▆▄▄▄█▄████▄▄▄▆</td></tr><tr><td>val_f1</td><td>█████▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_epoch</td><td>█▇▇▆▆▆▄▅▅▅▅▅▅▅▅▅▆▆▅▄▄▆▅▄▅▄▄▁▄▄▃▇▇▃▆▂▅▅▃▄</td></tr><tr><td>val_loss_step</td><td>▅▆▅▄▄▄▂▄▄▄▃▃▄▄▄▃▃▅▃▄▄▅▃▃▃▃▄▄▃▂▃▄█▁▄▃▅▅▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>49</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.64286</td></tr><tr><td>train_auc</td><td>0.58078</td></tr><tr><td>train_f1</td><td>0.11043</td></tr><tr><td>train_loss_epoch</td><td>0.63729</td></tr><tr><td>train_loss_step</td><td>0.63619</td></tr><tr><td>trainer/global_step</td><td>249</td></tr><tr><td>val_acc</td><td>0.57566</td></tr><tr><td>val_auc</td><td>0.50711</td></tr><tr><td>val_f1</td><td>0.0</td></tr><tr><td>val_loss_epoch</td><td>0.68043</td></tr><tr><td>val_loss_step</td><td>0.67859</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_32_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/onrk3jwr' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/onrk3jwr</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240115_102011-onrk3jwr\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a311110fc6354783922d36545c475cb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240115_103150-wo9sp8lm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/wo9sp8lm' target=\"_blank\">MLP_3_32_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/wo9sp8lm' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/wo9sp8lm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 2.3 K \n",
      "2 | head        | Sequential       | 562   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "2.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "2.9 K     Total params\n",
      "0.011     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "788bec2b5bc74835b5a8175da8aa0bc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▂▁▃▂▂▃▃▄▅▅▆▅▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇███▇▇███</td></tr><tr><td>train_auc</td><td>▆▆█▇▇▇▆▆▇▇▅▆▆▆▅▆▆▆▆▆▆▅▅▄▄▄▂▃▄▄▃▃▂▃▃▂▃▁▂▁</td></tr><tr><td>train_f1</td><td>▄▆▃▅▂▄▄▂▂▂▂▁▃▅▆▆▆▇▇▇▇█▇▇▇▇▇▆▅█▆▇█▇█▇▇▇▇▇</td></tr><tr><td>train_loss_epoch</td><td>█▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▅▄▃▃▃▃▃▂▂▂▂▂▁▁▃▂▁▃▂▂▁▂▂▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▁▂▁▂▂▃▃▃▃▄▄▂▄▂▄▂▅▃▅▃▆▆▆▃▃▇▇▇▃▇▄█▄█▄</td></tr><tr><td>val_acc</td><td>▁▄▄▂▄▅▄▄▅▅▅▇▇▇▇█▆█▇█▇██▇▇███▇██████▇██▇█</td></tr><tr><td>val_auc</td><td>▇▇▅▄█▃▃▃▃▃▃▂▂▂▂▂▃▅▄▄▂▂▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_f1</td><td>▇▃▃▇▁▄▁▁▃▃▃▇███████████████████████▇▇█▇█</td></tr><tr><td>val_loss_epoch</td><td>█▄▄▄▄▄▄▄▄▃▃▃▃▂▂▂▂▂▂▁▂▁▂▂▁▂▂▂▂▁▂▁▁▁▂▁▁▁▁▁</td></tr><tr><td>val_loss_step</td><td>█▅▅▅▅▅▄▅▄▄▄▃▃▃▂▃▃▃▃▂▂▁▃▂▂▂▂▂▃▂▂▂▂▂▂▂▁▁▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>49</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.73153</td></tr><tr><td>train_auc</td><td>0.29099</td></tr><tr><td>train_f1</td><td>0.54646</td></tr><tr><td>train_loss_epoch</td><td>0.54316</td></tr><tr><td>train_loss_step</td><td>0.51521</td></tr><tr><td>trainer/global_step</td><td>249</td></tr><tr><td>val_acc</td><td>0.75329</td></tr><tr><td>val_auc</td><td>0.19685</td></tr><tr><td>val_f1</td><td>0.71264</td></tr><tr><td>val_loss_epoch</td><td>0.534</td></tr><tr><td>val_loss_step</td><td>0.51348</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_32_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/wo9sp8lm' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/wo9sp8lm</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240115_103150-wo9sp8lm\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e56b603758941b384b35491245fb9b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240115_104417-xirk4pn0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/xirk4pn0' target=\"_blank\">MLP_3_32_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/xirk4pn0' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/xirk4pn0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 2.3 K \n",
      "2  | head        | Sequential       | 562   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 33    \n",
      "--------------------------------------------------\n",
      "2.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "2.9 K     Total params\n",
      "0.012     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e530a99479094a7795c11648033f6239",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▅▆▅▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇▇█▇▇▇▇▇</td></tr><tr><td>train_auc</td><td>▁▁▂▁▂▁▂▂▂▃▄▄▆▇▆▇▆▇█▇██▇▇▇▇▇▇▇█▇█▇███▇█▇█</td></tr><tr><td>train_f1</td><td>▇▂▂▂▁▁▁▁▁▁▂▃▆▇▇▇▇▆▇▇█▇▇▇▇▇▇▇▆█▆█▇▇█▇▇▇▇▇</td></tr><tr><td>train_loss_epoch</td><td>█▆▆▆▆▆▆▅▅▄▄▄▃▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▂▁▂▁</td></tr><tr><td>train_loss_step</td><td>▇▇█▆▆▆▆▄▃▅▄▃▃▃▁▂▃▅▂▂▅▅▄▁▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▁▂▁▂▂▃▃▃▃▄▄▂▄▂▄▂▅▃▅▃▆▆▆▃▃▇▇▇▃▇▄█▄█▄</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▂▇▇▆▅▆▇▆▇██▇▇█▆▇▇▇▆█▇██▇██▆█▇</td></tr><tr><td>val_auc</td><td>▁▄██████████████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▂▃▇▇▇▆▆█▆█▇█▇██▇▇▇█▆█▇▇█▇██▆█▇</td></tr><tr><td>val_loss_epoch</td><td>▆█▇▇▇▇▆▇▇▅▆▅▃▄▁▄▄▂▃▃▄▂▃▃▂▃▄▃▃▄▄▁▁▂▂▂▁▄▁▂</td></tr><tr><td>val_loss_step</td><td>██▇▇█▇▆█▆▆▇▆▄▆▁▅▅▃▄▃▄▃▄▄▄▄▄▅▅▆▅▄▁▄▄▅▁▆▄▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>49</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.7069</td></tr><tr><td>train_auc</td><td>0.76079</td></tr><tr><td>train_f1</td><td>0.55319</td></tr><tr><td>train_loss_epoch</td><td>0.55816</td></tr><tr><td>train_loss_step</td><td>0.53483</td></tr><tr><td>trainer/global_step</td><td>249</td></tr><tr><td>val_acc</td><td>0.69079</td></tr><tr><td>val_auc</td><td>0.78188</td></tr><tr><td>val_f1</td><td>0.51042</td></tr><tr><td>val_loss_epoch</td><td>0.56663</td></tr><tr><td>val_loss_step</td><td>0.54644</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_32_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/xirk4pn0' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/xirk4pn0</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240115_104417-xirk4pn0\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bda0688c2e1d4c949f4d388316ff5a40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240115_105653-mtta3fyz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/mtta3fyz' target=\"_blank\">MLP_3_32_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/mtta3fyz' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/mtta3fyz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 2.3 K \n",
      "2  | head        | Sequential       | 562   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 1.1 K \n",
      "--------------------------------------------------\n",
      "3.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "3.9 K     Total params\n",
      "0.016     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▆▆▅▆▆▆▆▆▆▆▆▇▇▇▇█▇▇▇████▇▇▇██▇█▇█▇█▇█▇█▇</td></tr><tr><td>train_auc</td><td>▁▁▁▁▂▂▂▂▃▄▄▄▆▆▇▇█▇▇▇██▇█▇▇▇██▇▇████▇████</td></tr><tr><td>train_f1</td><td>▇▂▂▂▁▁▁▁▁▁▃▄▇▇▇▇▇▇▇▇▇▇▇█▇▇▆██▆█▆▇▇█▆▇▇▇▇</td></tr><tr><td>train_loss_epoch</td><td>█▇▆▆▅▆▅▅▄▄▄▃▃▃▂▂▂▂▂▂▁▁▁▁▂▂▁▁▂▁▂▁▁▁▁▂▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▆▆▅▆▇▄▅▁▄▃▅▂▃▂▂▃▃▃▃▃▂▄▃▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▁▂▁▂▂▃▃▃▃▄▄▂▄▂▄▂▅▃▅▃▆▆▆▃▃▇▇▇▃▇▄█▄█▄</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▄▄▆▇▇▇▇▇█████▇██▇█▇▇▇▇▇▇▆█████</td></tr><tr><td>val_auc</td><td>▁▄▇█████████████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▄▅▇▇██▇██████▇██▇█▇█▇▇▇█▇█████</td></tr><tr><td>val_loss_epoch</td><td>▇▇▇▇▇▇█▇▆▆▅▃▂▁▂▂▃▃▂▂▂▂▂▁▃▂▂▄▄▃▂▁▂▂▃▃▄▂▂▂</td></tr><tr><td>val_loss_step</td><td>▇▇▆▆▆▆█▇▅▅▆▂▃▁▂▃▃▄▂▃▃▂▂▃▃▃▃▃▆▄▄▂▂▃▃▃▅▂▂▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>49</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.69294</td></tr><tr><td>train_auc</td><td>0.74767</td></tr><tr><td>train_f1</td><td>0.49866</td></tr><tr><td>train_loss_epoch</td><td>0.5576</td></tr><tr><td>train_loss_step</td><td>0.54081</td></tr><tr><td>trainer/global_step</td><td>249</td></tr><tr><td>val_acc</td><td>0.71053</td></tr><tr><td>val_auc</td><td>0.77506</td></tr><tr><td>val_f1</td><td>0.59633</td></tr><tr><td>val_loss_epoch</td><td>0.57249</td></tr><tr><td>val_loss_step</td><td>0.58508</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_32_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/mtta3fyz' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/mtta3fyz</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240115_105653-mtta3fyz\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47b867dc3a8b44c697d24c9c59bf5760",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240115_110931-4x6ch9qf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/4x6ch9qf' target=\"_blank\">MLP_3_64_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/4x6ch9qf' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/4x6ch9qf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 8.7 K \n",
      "2 | head        | Sequential       | 2.1 K \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "10.8 K    Trainable params\n",
      "0         Non-trainable params\n",
      "10.8 K    Total params\n",
      "0.043     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55ba9c73762f4f03b5d70e89c98d5b09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▂▂▂▃▅▆▇▆▇▇▇▇▇▇▇▇▇▆▆▇▆▇█▇▇▇▇█▇▇▇█▆█▇▇▇▇</td></tr><tr><td>train_auc</td><td>▁▂▂▂▄▆▇▇▇▇▇▇██████████████████▇█████████</td></tr><tr><td>train_f1</td><td>▂▁▁▁▁▂▅█▇▇▇▇█▆▇▇▇▇▇▇▇█▆██▇▇▇▇▇▇▇▇█▇▇▇▇▇▇</td></tr><tr><td>train_loss_epoch</td><td>█▇▇▇▆▄▃▃▂▂▂▂▂▂▂▁▁▂▁▂▂▁▂▂▂▂▁▁▁▂▁▁▂▂▂▁▁▁▂▁</td></tr><tr><td>train_loss_step</td><td>▇██▄▃▄▂▂▅▃▄▂▃▃▄▃▄▂▃▁▃▅▃▃▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▁▂▁▂▂▃▃▃▃▄▄▂▄▂▄▂▅▃▅▃▆▆▆▃▃▇▇▇▃▇▄█▄█▄</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▃▇▅▆█▇█▇██▇█▇███▇█▆▇█████▇█▇▇▇▇▇█▇▇</td></tr><tr><td>val_auc</td><td>▁▇████████████████▇████████████▇▇█▇██▇██</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▃█▆▆▇▇█▇██▇█▇█▇█▇█▆▇▇▇█▇▇▇██▇▇▇▇█▇▇</td></tr><tr><td>val_loss_epoch</td><td>███▆▇▄▁▅▅▃▂▂▄▃▂▁▂▂▂▁▁▁▂▂▁▃▂▁▃▃▁▁▂▂▁▄▃▃▃▂</td></tr><tr><td>val_loss_step</td><td>▆▆█▆▅▄▁▅▃▃▂▃▃▅▂▃▃▃▂▃▃▂▃▃▄▅▃▃▄▄▃▃▃▂▃▃▃▅▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>49</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.69787</td></tr><tr><td>train_auc</td><td>0.76475</td></tr><tr><td>train_f1</td><td>0.51451</td></tr><tr><td>train_loss_epoch</td><td>0.54541</td></tr><tr><td>train_loss_step</td><td>0.50666</td></tr><tr><td>trainer/global_step</td><td>249</td></tr><tr><td>val_acc</td><td>0.70395</td></tr><tr><td>val_auc</td><td>0.7786</td></tr><tr><td>val_f1</td><td>0.54082</td></tr><tr><td>val_loss_epoch</td><td>0.56379</td></tr><tr><td>val_loss_step</td><td>0.54988</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_64_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/4x6ch9qf' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/4x6ch9qf</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240115_110931-4x6ch9qf\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c931b700afa4ff4813b1bf820a1e7e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240115_112448-zou25ayj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/zou25ayj' target=\"_blank\">MLP_3_64_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/zou25ayj' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/zou25ayj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 8.7 K \n",
      "2 | head        | Sequential       | 2.1 K \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "10.8 K    Trainable params\n",
      "0         Non-trainable params\n",
      "10.8 K    Total params\n",
      "0.043     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73ac16ced3b64d16b0c32703f1ad0ce0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▄▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▇▆▆▆▆▆▇▇█</td></tr><tr><td>train_auc</td><td>▁▃▂▂▂▂▃▂▃▂▃▂▂▃▃▄▂▄▄▅▅▄▃▅▅▅▆▆▆▇▄▄▇█▅▆▆▇▆█</td></tr><tr><td>train_f1</td><td>▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▃▂▁▁▁▁▅▃▃▅▄▃▆▅█▂▃█▇</td></tr><tr><td>train_loss_epoch</td><td>█▆▅▄▄▃▄▄▄▃▃▃▃▃▄▄▃▃▃▃▂▃▄▂▃▂▂▂▂▂▃▂▁▁▃▃▁▁▂▁</td></tr><tr><td>train_loss_step</td><td>▅▆█▅▃▃▃▃▄▃▅▄▃▄▅▂▃▂▅▄▄▆▃▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▁▂▁▂▂▃▃▃▃▄▄▂▄▂▄▂▅▃▅▃▆▆▆▃▃▇▇▇▃▇▄█▄█▄</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁█▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>▁▁▁▁▆▆▆███▇▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▁▆▆</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁█▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_epoch</td><td>▆▇█▄▆▃▃▇▆▁▁▇▆▇▆▄▄▃▄▅▅▄▅▅▅▅▆▇▅▇▃▅▅▂▃▄▅▇█▅</td></tr><tr><td>val_loss_step</td><td>▅▄█▄▄▅▃▆▄▅▁▆▅▆▆▅▅▃▃▅▅▃▅▅▅▅▅▅▄▆▅▅▅▂▅▄▅▇▄▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>49</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.6486</td></tr><tr><td>train_auc</td><td>0.61967</td></tr><tr><td>train_f1</td><td>0.15748</td></tr><tr><td>train_loss_epoch</td><td>0.6366</td></tr><tr><td>train_loss_step</td><td>0.61262</td></tr><tr><td>trainer/global_step</td><td>249</td></tr><tr><td>val_acc</td><td>0.57566</td></tr><tr><td>val_auc</td><td>0.5087</td></tr><tr><td>val_f1</td><td>0.0</td></tr><tr><td>val_loss_epoch</td><td>0.68548</td></tr><tr><td>val_loss_step</td><td>0.69005</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_64_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/zou25ayj' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/zou25ayj</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240115_112448-zou25ayj\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcb81150b0e94547aa492028ccc7aa5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016933333330477276, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240115_113802-75xiopsd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/75xiopsd' target=\"_blank\">MLP_3_64_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/75xiopsd' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/75xiopsd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 8.7 K \n",
      "2 | head        | Sequential       | 2.1 K \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "10.8 K    Trainable params\n",
      "0         Non-trainable params\n",
      "10.8 K    Total params\n",
      "0.043     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▃▃▄▅▅▆▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇██▇▇▇██▇▇▇█▇█</td></tr><tr><td>train_auc</td><td>▃▂▃▁▃▄▄▆▇▇▇▇██▅▆▆▆▇▇▄▄▄▄▅▄▃▅▄▅▅▄▄▄▃▃▂▄▄▂</td></tr><tr><td>train_f1</td><td>▄▃▃▁▂▃▄▇▇▅▇▇█▇▇█▆▇▇▇▆█▇▇▇▇▅██▇▇▇▇█▆▇▆█▇█</td></tr><tr><td>train_loss_epoch</td><td>█▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▆▆▄▃▃▁▂▄▄▃▁▂▃▃▃▃▂▂▂▃▃▁▂▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▁▂▁▂▂▃▃▃▃▄▄▂▄▂▄▂▅▃▅▃▆▆▆▃▃▇▇▇▃▇▄█▄█▄</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▇▅▆▆▇▅▆▆█▅▆▇▇▇█▆█▅▇▆▇▆▆▇▆▆▆▅▇▅▆▆▇▆</td></tr><tr><td>val_auc</td><td>▃▄▄▄▅▆▇▇▇▇▇▇█▇▇▆████▆▇█▇█▇██████▇▆▅▁▄▆▆▇</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▇▆▇▇▇▆▇▇█▅▆▇▇▇█▆█▆▇▇▇▇▆▇▆▆▆▆▇▆▇▆▇▇</td></tr><tr><td>val_loss_epoch</td><td>█▇█▆▆▅▃▄▄▃▃▄▃▃▂▃▂▂▂▁▁▂▃▃▁▄▂▁▃▃▂▂▃▂▁▃▂▃▂▂</td></tr><tr><td>val_loss_step</td><td>▆▅█▅▅▅▂▄▃▃▃▄▃▄▂▃▃▂▂▃▃▁▃▂▃▅▃▂▄▄▃▂▃▁▃▂▃▄▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>49</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.74548</td></tr><tr><td>train_auc</td><td>0.48605</td></tr><tr><td>train_f1</td><td>0.60358</td></tr><tr><td>train_loss_epoch</td><td>0.52055</td></tr><tr><td>train_loss_step</td><td>0.48886</td></tr><tr><td>trainer/global_step</td><td>249</td></tr><tr><td>val_acc</td><td>0.71711</td></tr><tr><td>val_auc</td><td>0.75096</td></tr><tr><td>val_f1</td><td>0.59813</td></tr><tr><td>val_loss_epoch</td><td>0.54068</td></tr><tr><td>val_loss_step</td><td>0.52423</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_64_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/75xiopsd' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/75xiopsd</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240115_113802-75xiopsd\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8b5de3beaa2457e89cf2ee442aec90b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240115_115109-gqteq9a2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/gqteq9a2' target=\"_blank\">MLP_3_64_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/gqteq9a2' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/gqteq9a2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 8.7 K \n",
      "2  | head        | Sequential       | 2.1 K \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 65    \n",
      "--------------------------------------------------\n",
      "10.9 K    Trainable params\n",
      "0         Non-trainable params\n",
      "10.9 K    Total params\n",
      "0.044     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99010ce89714481ba7c03edd5b4afe1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▂▂▂▃▆▅▄▆▄▅▇▇▅▇▆▆▇▇▇▇▆▆▆▇▆▆▆▆▇█▇▇▆▆▇▇█▇</td></tr><tr><td>train_auc</td><td>▂▁▃▃▅▆▇▇▆▇▇▇████▇███████▇████▇██████████</td></tr><tr><td>train_f1</td><td>▁▁▁▁▁▂▆▇▆▇▅▆██▆█▇▆█▇▇▇▇█▆▇▆▇▇█▇▇█▇▇▇▇▇█▇</td></tr><tr><td>train_loss_epoch</td><td>██▇▇▆▅▄▂▄▃▃▂▂▂▂▂▂▂▂▂▂▁▂▁▂▂▂▁▂▂▂▁▁▁▁▂▁▂▁▂</td></tr><tr><td>train_loss_step</td><td>██▅▇▄▄▄▆▅▃▃▃▃▄▅▅▁▄▂▃▄▃▃▄▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▁▂▁▂▂▃▃▃▃▄▄▂▄▂▄▂▅▃▅▃▆▆▆▃▃▇▇▇▃▇▄█▄█▄</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▅█▇▇▇▅▇▇▅▇▅▆▇▆▇▇▇▆▆▇▆▇▇▇▆▇▆▇▇▆▆▇▇▇▇</td></tr><tr><td>val_auc</td><td>▁▆██▇▇▇▇▆▇▇▇▇▇▆▇▇▆▇▆▇▇▇▇▇▇▇▇▆█▇█▇▇▇▇▇▇▇▇</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▆█▇▇▇▅▇▇▆█▅▆▇▆▇▇▇▇▆▇▆▇▇█▆▇▆▇▇▇▆▇▇▇▇</td></tr><tr><td>val_loss_epoch</td><td>██▆▇▅▄▃▂▃▃▃▃▂▆▃▅▃▃▃▂▂▃▅▁▁▂▂▃▄▃▂▂▄▄▂▃▃▄▅▃</td></tr><tr><td>val_loss_step</td><td>▇▇▅█▆▄▃▁▄▄▃▃▃█▃▃▄▃▂▃▃▃▇▂▄▁▃▄▇▃▄▃▅▅▄▃▄▇▂▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>49</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.70608</td></tr><tr><td>train_auc</td><td>0.76104</td></tr><tr><td>train_f1</td><td>0.55138</td></tr><tr><td>train_loss_epoch</td><td>0.55354</td></tr><tr><td>train_loss_step</td><td>0.5487</td></tr><tr><td>trainer/global_step</td><td>249</td></tr><tr><td>val_acc</td><td>0.70395</td></tr><tr><td>val_auc</td><td>0.78179</td></tr><tr><td>val_f1</td><td>0.54545</td></tr><tr><td>val_loss_epoch</td><td>0.57979</td></tr><tr><td>val_loss_step</td><td>0.59208</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_64_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/gqteq9a2' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/gqteq9a2</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240115_115109-gqteq9a2\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc14823b94ea4ea986052283588d7665",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01691666666883975, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240115_120358-zmbb0nmj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/zmbb0nmj' target=\"_blank\">MLP_3_64_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/zmbb0nmj' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/zmbb0nmj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 8.7 K \n",
      "2  | head        | Sequential       | 2.1 K \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 4.2 K \n",
      "--------------------------------------------------\n",
      "15.0 K    Trainable params\n",
      "0         Non-trainable params\n",
      "15.0 K    Total params\n",
      "0.060     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4a58e5529364558afca1b4313a67e80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▂▂▂▅▆▆▆▅▆▇▆▇▆█▇▇▇█▇▆▇▇▇▆▆█▆▆▆▆▇▆▅▆▇▇█▇</td></tr><tr><td>train_auc</td><td>▁▁▂▃▄▆▇▇▇▇▇▇██▇█▇▇▇██▇████▇██▇▇███▇▇████</td></tr><tr><td>train_f1</td><td>▂▁▁▁▁▆▇▇▇▆▆▇▇█▇█▇▇███▇██▇▇▇█▇▇▇▇█▇▇▇▇▇█▇</td></tr><tr><td>train_loss_epoch</td><td>█▇▇▇▆▄▂▃▃▂▃▂▂▂▂▂▂▂▂▁▂▂▁▂▁▁▂▁▁▂▂▂▁▂▂▂▁▂▁▁</td></tr><tr><td>train_loss_step</td><td>█▇█▂▇▄▅▃▅▄▆▃▂▂▃▂▆▂▅▁▃▃▁▃▄</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▁▂▁▂▂▃▃▃▃▄▄▂▄▂▄▂▅▃▅▃▆▆▆▃▃▇▇▇▃▇▄█▄█▄</td></tr><tr><td>val_acc</td><td>▁▁▁▁▂▅▇█▇▅▇▇▇▇▆▆▇▇▆▆▇▇▇▆▆▇▇▇▇▇▆▆▇▇▇▇▆▇▇▆</td></tr><tr><td>val_auc</td><td>▁▇█▇████████▇▇▇▇▇▇▇▆▇▇▇▇▇▆▇▆▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>val_f1</td><td>▁▁▁▁▂▆▇█▇▅▇▇▇▇▇▆▇▇▆▇▇▇▇▆▆▇▇▇▇▇▆▆▇▇▇▇▆▇▇▇</td></tr><tr><td>val_loss_epoch</td><td>█▇▇▇▄▄▂▂▃▃▂▃▁▂▃▂▃▃▃▃▃▂▂▃▂▂▃▂▂▂▂▃▁▃▃▂▄▃▁▂</td></tr><tr><td>val_loss_step</td><td>▇▇▇█▆▄▄▄▄▅▄▆▄▃▅▄▄▄▅▅▄▂▃▄▄▄▄▄▄▄▄▄▁▅▄▅▇▄▄▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>49</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.70361</td></tr><tr><td>train_auc</td><td>0.76845</td></tr><tr><td>train_f1</td><td>0.5075</td></tr><tr><td>train_loss_epoch</td><td>0.54761</td></tr><tr><td>train_loss_step</td><td>0.5673</td></tr><tr><td>trainer/global_step</td><td>249</td></tr><tr><td>val_acc</td><td>0.70066</td></tr><tr><td>val_auc</td><td>0.77338</td></tr><tr><td>val_f1</td><td>0.58824</td></tr><tr><td>val_loss_epoch</td><td>0.56238</td></tr><tr><td>val_loss_step</td><td>0.55977</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_64_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/zmbb0nmj' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/zmbb0nmj</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240115_120358-zmbb0nmj\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "339e07f3bdce495b9fec17730f2f79e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.0169333333382383, max=1.0))…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240115_121746-tsbo8ffy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/tsbo8ffy' target=\"_blank\">MLP_4_16_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/tsbo8ffy' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/tsbo8ffy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 912   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "1.1 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.1 K     Total params\n",
      "0.004     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24ff6aedffde4f1bb667f8c81592a949",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇██▇▇██▇█▇▇██████</td></tr><tr><td>train_auc</td><td>▂▂▂▂▁▁▁▂▂▁▂▁▁▁▂▃▂▂▂▃▅▅▆▇▇▇▇▆█▇▇█▇▇██████</td></tr><tr><td>train_f1</td><td>██▅▃▃▃▂▂▂▂▁▁▁▁▁▂▁▁▁▁▁▂▆▆▇▇▆▆▇▆▇▆▆▇▇▇▇█▆▆</td></tr><tr><td>train_loss_epoch</td><td>█▆▅▅▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▃▃▂▂▂▂▁▂▁▂▁▁▂▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>██▆▆▄▇▇▆▅▅▅█▃▄▃▃▃▁▂▃▂▃▂▆▅</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▁▂▁▂▂▃▃▃▃▄▄▂▄▂▄▂▅▃▅▃▆▆▆▃▃▇▇▇▃▇▄█▄█▄</td></tr><tr><td>val_acc</td><td>▁▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▆██▇▇▇▇█▇▇███████▆</td></tr><tr><td>val_auc</td><td>▁▄▅▅▆▆▆▆▇▇▇▇████████████████████████████</td></tr><tr><td>val_f1</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▅▇▇▆▆▆▆█▆▇▇▇▇██▇█▅</td></tr><tr><td>val_loss_epoch</td><td>█▇▇▇▇▆▆▇▇▆▇▆▇▆▆▆▇▇▆▇▅▄▂▃▂▄▃▃▂▁▄▂▂▃▃▂▃▃▃▆</td></tr><tr><td>val_loss_step</td><td>█▇█▆▇▇▆▇▇▇█▆▆▅▆▇▆▇▆▆▆▅▃▄▄▅▄▄▂▁▄▄▂▄▄▄▄▅▄█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>49</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.68555</td></tr><tr><td>train_auc</td><td>0.71556</td></tr><tr><td>train_f1</td><td>0.40986</td></tr><tr><td>train_loss_epoch</td><td>0.57972</td></tr><tr><td>train_loss_step</td><td>0.65006</td></tr><tr><td>trainer/global_step</td><td>249</td></tr><tr><td>val_acc</td><td>0.64803</td></tr><tr><td>val_auc</td><td>0.78241</td></tr><tr><td>val_f1</td><td>0.35152</td></tr><tr><td>val_loss_epoch</td><td>0.65516</td></tr><tr><td>val_loss_step</td><td>0.71791</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_16_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/tsbo8ffy' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/tsbo8ffy</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240115_121746-tsbo8ffy\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bba0d1f951f40ef926b4872f83ff95d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016933333330477276, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240115_123042-lhxipevs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/lhxipevs' target=\"_blank\">MLP_4_16_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/lhxipevs' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/lhxipevs</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 912   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "1.1 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.1 K     Total params\n",
      "0.004     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44d3d1dfad5d419582d829681bb23b98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▄▆▇▇▇▇▇▇▇██████████████████████████████</td></tr><tr><td>train_auc</td><td>▃▄▃▃▄▃▄▃▂▃▄▂▄▃▄▄▄▂▅▄▆▅▄▅▃▅█▆▃▃▅▇▅▇▁▅▇▅▆▆</td></tr><tr><td>train_f1</td><td>█▆▅▄▄▃▃▃▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_epoch</td><td>█▅▄▃▂▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▂▂▁▁▂▂▂▂▁▂▁▂▂▁▁▂▁</td></tr><tr><td>train_loss_step</td><td>█▆▅▅▂▄▇▅▂▂▃▇▃▃▄▃▅▄▄▁▂▃▃▆▅</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▁▂▁▂▂▃▃▃▃▄▄▂▄▂▄▂▅▃▅▃▆▆▆▃▃▇▇▇▃▇▄█▄█▄</td></tr><tr><td>val_acc</td><td>▁▁▁▁████████████████████████████████████</td></tr><tr><td>val_auc</td><td>▄█▅▅▅▆▇▇▇▇▇▇▇▇▇▇▇▆▆▇▂▂▂▂▂▂▂▂▂▇▂▂▁▄▄▄▄▄▄▄</td></tr><tr><td>val_f1</td><td>████▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_epoch</td><td>█▆▅▅▄▄▄▄▃▃▄▃▄▂▃▃▄▄▃▄▃▃▃▅▃▄▄▃▃▃▅▃▃▃▃▃▁▂▃▄</td></tr><tr><td>val_loss_step</td><td>█▆▅▆▅▅▅▅▅▅▆▄▄▂▄▅▄▅▄▄▄▅▄▄▅▆▅▄▄▄▅▄▅▄▅▅▁▃▄▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>49</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.63875</td></tr><tr><td>train_auc</td><td>0.53937</td></tr><tr><td>train_f1</td><td>0.0</td></tr><tr><td>train_loss_epoch</td><td>0.65269</td></tr><tr><td>train_loss_step</td><td>0.67765</td></tr><tr><td>trainer/global_step</td><td>249</td></tr><tr><td>val_acc</td><td>0.57566</td></tr><tr><td>val_auc</td><td>0.50775</td></tr><tr><td>val_f1</td><td>0.0</td></tr><tr><td>val_loss_epoch</td><td>0.68666</td></tr><tr><td>val_loss_step</td><td>0.69371</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_16_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/lhxipevs' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/lhxipevs</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240115_123042-lhxipevs\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "373838a63d3e45a480efa3ee71bc5ade",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240115_124332-2jh87pye</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/2jh87pye' target=\"_blank\">MLP_4_16_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/2jh87pye' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/2jh87pye</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 912   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "1.1 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.1 K     Total params\n",
      "0.004     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa0ba3bab791475dbad0085d811229a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▄▃▁▃▂▂▂▃▄▅▆▇▇▇▇▇▇▇▇▇▇▇█▇▇█▇██▇▇████████</td></tr><tr><td>train_auc</td><td>▃▃▄▃▂▃▃▂▁▂▃▃▂▄▃▃▂▄▄▄▃▅▄▄▄▅▆▆▅█▅▄▄█▇▇█▇██</td></tr><tr><td>train_f1</td><td>▇▆▇█▇▆▇▆▆▅▅▄▂▂▂▃▁▁▁▂▂▁▂▃▂▂▄▃▅▅▃▄▄▄▄▅▃▅▃▄</td></tr><tr><td>train_loss_epoch</td><td>█▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▃▂▂▂▂▂▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▂▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▁▂▁▂▂▃▃▃▃▄▄▂▄▂▄▂▅▃▅▃▆▆▆▃▃▇▇▇▃▇▄█▄█▄</td></tr><tr><td>val_acc</td><td>▇▇▁▁▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇██▇█▇</td></tr><tr><td>val_auc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▃▃▄▃▃▄▄▃▄▅▃▆▆▇▇▇▇█▇████</td></tr><tr><td>val_f1</td><td>▁▁█▇▃▄▄▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▂▁▁▃▃▁▃▁</td></tr><tr><td>val_loss_epoch</td><td>█▄▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▂</td></tr><tr><td>val_loss_step</td><td>█▃▂▂▂▂▂▂▂▂▃▂▂▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>49</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.65846</td></tr><tr><td>train_auc</td><td>0.60646</td></tr><tr><td>train_f1</td><td>0.1875</td></tr><tr><td>train_loss_epoch</td><td>0.63109</td></tr><tr><td>train_loss_step</td><td>0.66844</td></tr><tr><td>trainer/global_step</td><td>249</td></tr><tr><td>val_acc</td><td>0.57895</td></tr><tr><td>val_auc</td><td>0.74986</td></tr><tr><td>val_f1</td><td>0.01538</td></tr><tr><td>val_loss_epoch</td><td>0.6782</td></tr><tr><td>val_loss_step</td><td>0.70517</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_16_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/2jh87pye' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/2jh87pye</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240115_124332-2jh87pye\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bafcf96b3444b2c9ea1b723b5b82ec2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240115_125634-1n9285e1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/1n9285e1' target=\"_blank\">MLP_4_16_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/1n9285e1' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/1n9285e1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 912   \n",
      "2  | head        | Sequential       | 154   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 17    \n",
      "--------------------------------------------------\n",
      "1.1 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.1 K     Total params\n",
      "0.004     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9fdfc353e0d42fdb1eb2e556cbfc873",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▅▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇██▇▇████▇██████</td></tr><tr><td>train_auc</td><td>▁▂▁▁▁▁▂▂▂▂▂▃▂▂▂▃▃▂▃▄▄▄▅▅▆▇▇▇▆█▇▇▇▇▇▇▇███</td></tr><tr><td>train_f1</td><td>██▅▃▃▂▂▂▂▁▂▁▁▁▁▂▁▁▁▁▁▁▂▃▆▇▇▅▅█▆▇▇▇▇▇▇▇▇▇</td></tr><tr><td>train_loss_epoch</td><td>█▆▅▅▅▅▄▄▄▄▄▄▄▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▁▂▂▂▂▁▂▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▇▇▆▇▇▆▇█▇▆▅▅▆▆▆▇▆▂▁▄▄▄▃▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▁▂▁▂▂▃▃▃▃▄▄▂▄▂▄▂▅▃▅▃▆▆▆▃▃▇▇▇▃▇▄█▄█▄</td></tr><tr><td>val_acc</td><td>▁▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▇▆▇▆▇█▆██▇█▇████</td></tr><tr><td>val_auc</td><td>▁▃▅▆▆▆▆▇▇▇██████████████████████████████</td></tr><tr><td>val_f1</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▆▄▆▄▅▇▄▇▇▆▇▆▆▆▇▇</td></tr><tr><td>val_loss_epoch</td><td>█▇▇█▇▆▇▆▆▆▇▇▇▆▆▅▆▆▆▅▅▅▅▄▂▁▁▆▂▃▂▃▂▃▂▄▂▁▂▃</td></tr><tr><td>val_loss_step</td><td>▇▆▇█▆▆▆▆▆▆▆▇▅▅▅▅▅▆▆▅▅▆▆▄▄▁▃▃▃▄▃▄▃▃▃▃▄▂▃▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>49</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.6954</td></tr><tr><td>train_auc</td><td>0.72011</td></tr><tr><td>train_f1</td><td>0.43359</td></tr><tr><td>train_loss_epoch</td><td>0.58021</td></tr><tr><td>train_loss_step</td><td>0.57833</td></tr><tr><td>trainer/global_step</td><td>249</td></tr><tr><td>val_acc</td><td>0.69079</td></tr><tr><td>val_auc</td><td>0.7767</td></tr><tr><td>val_f1</td><td>0.52041</td></tr><tr><td>val_loss_epoch</td><td>0.60793</td></tr><tr><td>val_loss_step</td><td>0.64873</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_16_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/1n9285e1' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/1n9285e1</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240115_125634-1n9285e1\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0aec0d5dc9684779a85fb044b08fb755",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01691666666107873, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240115_130908-8xcesg0r</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/8xcesg0r' target=\"_blank\">MLP_4_16_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/8xcesg0r' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/8xcesg0r</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 912   \n",
      "2  | head        | Sequential       | 154   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 281   \n",
      "--------------------------------------------------\n",
      "1.3 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.3 K     Total params\n",
      "0.005     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a827c15f9a064cd49210850f61e2a52f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇████▇███████▇▇▇███</td></tr><tr><td>train_auc</td><td>▃▂▁▂▂▁▂▁▂▁▂▁▁▂▂▃▄▅▅▆▇▇█▇▇▇▇▇████▇█▇▇▇▇▇█</td></tr><tr><td>train_f1</td><td>██▅▃▃▃▃▃▁▂▁▁▁▁▁▁▁▁▄▆█▆▇▇▇▇▆▇▇█▇█▇▆▇▆▆▇▇▇</td></tr><tr><td>train_loss_epoch</td><td>█▆▅▅▄▅▄▄▄▄▄▄▄▄▄▄▃▃▃▃▂▂▂▁▂▂▂▁▁▁▁▂▁▁▁▁▂▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▄▇▇▅▅▇▆▇▂▄▃▃▂▅▁▂▅▁▂▁▃▃▃▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▁▂▁▂▂▃▃▃▃▄▄▂▄▂▄▂▅▃▅▃▆▆▆▃▃▇▇▇▃▇▄█▄█▄</td></tr><tr><td>val_acc</td><td>▁▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▆█▇▇█▇██▇█████████████</td></tr><tr><td>val_auc</td><td>▁▃▅▆▅▅▆▆▇▇▇█████████████████████████████</td></tr><tr><td>val_f1</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▃█▇▆█▆██▇█████▇██▇▇█▇█</td></tr><tr><td>val_loss_epoch</td><td>█▆▇▆▆▅▆▆▇▇▆▇▆▇▆▆▅▄▃▃▂▃▄▃▁▃▃▃▁▂▁▂▃▂▁▂▁▃▂▁</td></tr><tr><td>val_loss_step</td><td>█▇█▇▇▇██▆▇▆█▇█▇▆▆▅▂▄▄▅▇▃▄▅▃▃▂▄▃▄▄▃▃▃▁▅▄▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>49</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.69294</td></tr><tr><td>train_auc</td><td>0.72368</td></tr><tr><td>train_f1</td><td>0.46875</td></tr><tr><td>train_loss_epoch</td><td>0.57653</td></tr><tr><td>train_loss_step</td><td>0.59907</td></tr><tr><td>trainer/global_step</td><td>249</td></tr><tr><td>val_acc</td><td>0.70066</td></tr><tr><td>val_auc</td><td>0.77612</td></tr><tr><td>val_f1</td><td>0.60262</td></tr><tr><td>val_loss_epoch</td><td>0.55121</td></tr><tr><td>val_loss_step</td><td>0.51469</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_16_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/8xcesg0r' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/8xcesg0r</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240115_130908-8xcesg0r\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc4184558d4740feb59f304ac1798747",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.0169333333382383, max=1.0))…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240115_132138-zqfj15wj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/zqfj15wj' target=\"_blank\">MLP_4_32_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/zqfj15wj' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/zqfj15wj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 3.4 K \n",
      "2 | head        | Sequential       | 562   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "3.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "3.9 K     Total params\n",
      "0.016     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7585b33c0e834ccbae7a5cb4a9142313",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▆▆▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇█▇█▇▇█▇██▇▇██▇█▇██▇█</td></tr><tr><td>train_auc</td><td>▁▁▃▂▂▂▂▃▄▄▅▅▆▇▇▇▇▇▇▇▇█▇▇▇█▇█▇▇▇████▇█▇██</td></tr><tr><td>train_f1</td><td>▆▁▁▁▁▁▁▁▁▁▂▃▇▇▇▇▆▇▇██▆█▇▇█▆██▇▇██▇█▇█▇█▇</td></tr><tr><td>train_loss_epoch</td><td>█▆▆▆▆▆▆▅▅▅▄▄▃▂▂▃▂▂▂▂▁▁▁▂▁▂▂▁▂▂▁▁▁▁▁▂▁▂▂▁</td></tr><tr><td>train_loss_step</td><td>▆▆▆▅▅▇█▄▅▃▅▁▃▃▃▂▃▃▃▂▄▃▂▄▄</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▁▂▁▂▂▃▃▃▃▄▄▂▄▂▄▂▅▃▅▃▆▆▆▃▃▇▇▇▃▇▄█▄█▄</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▅█▅██▆█▆▇██████▇█▆█▆███▆▇███</td></tr><tr><td>val_auc</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▂▂▆█▆██▆█▆▇▇▇██▇▇▇█▇█▆█▇█▆▇▇▇█</td></tr><tr><td>val_loss_epoch</td><td>▇▇▇▇█▇▆█▇▆▆▆▃▄▅▄▃▃▂▅▂▃▃▂▂▁▂▃▄▃▄▃▃▂▄▄▃▃▃▃</td></tr><tr><td>val_loss_step</td><td>▇▇▇▆▇▇▆█▆▆▇▇▅▆▇▅▅▄▃▄▅▄▅▄▅▁▅▅▅▅▅▄▄▂▅▅▄▄▅▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>49</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.69869</td></tr><tr><td>train_auc</td><td>0.74433</td></tr><tr><td>train_f1</td><td>0.51391</td></tr><tr><td>train_loss_epoch</td><td>0.55911</td></tr><tr><td>train_loss_step</td><td>0.58457</td></tr><tr><td>trainer/global_step</td><td>249</td></tr><tr><td>val_acc</td><td>0.71053</td></tr><tr><td>val_auc</td><td>0.77976</td></tr><tr><td>val_f1</td><td>0.6</td></tr><tr><td>val_loss_epoch</td><td>0.57345</td></tr><tr><td>val_loss_step</td><td>0.58933</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_32_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/zqfj15wj' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/zqfj15wj</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240115_132138-zqfj15wj\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e33ccc9cbe2047298ab1ba05887ba727",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240115_133337-swnzsdd8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/swnzsdd8' target=\"_blank\">MLP_4_32_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/swnzsdd8' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/swnzsdd8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 3.4 K \n",
      "2 | head        | Sequential       | 562   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "3.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "3.9 K     Total params\n",
      "0.016     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17b93cc2fa2944dbbc41b899ced53b45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▆▆█████████████████████████████████████</td></tr><tr><td>train_auc</td><td>▁▃▅▅▅▅▆▇▄▅▆▅▇▅▇▇▆█▆▅▇▇▇▅▅▆▅▆▅▆▆▅▆▇█▃▄▇▆▅</td></tr><tr><td>train_f1</td><td>█▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_epoch</td><td>█▅▃▃▃▃▂▂▂▃▂▃▂▂▂▁▂▂▂▂▂▁▁▃▂▁▂▁▂▂▂▁▁▁▂▂▁▂▂▂</td></tr><tr><td>train_loss_step</td><td>▆▄▅▃▅██▆▆▅▇▄▄▃▅▁▅▄▇▄▅▄▃▅▆</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▁▂▁▂▂▃▃▃▃▄▄▂▄▂▄▂▅▃▅▃▆▆▆▃▃▇▇▇▃▇▄█▄█▄</td></tr><tr><td>val_acc</td><td>▁▁██████████████████████████████████████</td></tr><tr><td>val_auc</td><td>▆▆▆▆▄▄▄▄▁▇▁▁▁▁▁▇▇▇▇▇█▇▇▇▇▇▇▇▇▇▇▁▇▇▇▇▇██▇</td></tr><tr><td>val_f1</td><td>██▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_epoch</td><td>██▆▅▆▅▄▆▇▅▆▆▆▆▄▁▆▃▃▆▃▃▆▄▇▄▅▄▆▃▆▄▃▃▅▆▅▇▆▅</td></tr><tr><td>val_loss_step</td><td>▇▆▅▃▄▅▂▆▄▄▆▅▄▇▃▄▃▂▂▃▄▁█▅▃▄▄▄▇▂▄▄▂▂▄▅▄█▄▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>49</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.63875</td></tr><tr><td>train_auc</td><td>0.51411</td></tr><tr><td>train_f1</td><td>0.0</td></tr><tr><td>train_loss_epoch</td><td>0.65152</td></tr><tr><td>train_loss_step</td><td>0.67128</td></tr><tr><td>trainer/global_step</td><td>249</td></tr><tr><td>val_acc</td><td>0.57566</td></tr><tr><td>val_auc</td><td>0.5087</td></tr><tr><td>val_f1</td><td>0.0</td></tr><tr><td>val_loss_epoch</td><td>0.68539</td></tr><tr><td>val_loss_step</td><td>0.69037</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_32_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/swnzsdd8' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/swnzsdd8</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240115_133337-swnzsdd8\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ca185675ed349e5a59fa5248de281d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016933333330477276, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240115_134616-7yi1gvfr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/7yi1gvfr' target=\"_blank\">MLP_4_32_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/7yi1gvfr' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/7yi1gvfr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 3.4 K \n",
      "2 | head        | Sequential       | 562   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "3.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "3.9 K     Total params\n",
      "0.016     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc4033ffc0ce4fae810e2dc5041734be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▂▂▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇█▇▇██▇█▇██▇█</td></tr><tr><td>train_auc</td><td>▁▁▃▂▃▂▁▂▂▂▃▃▅▅▅▄▇▇▆▇▆▇▇▇██▇▇▇█▇▇▇▇██▇█▇▇</td></tr><tr><td>train_f1</td><td>▅▄▆▅▄▃▂▂▁▂▂▄▄▃▆▄▆▅▅▇▆▆▇▇█▇▆██▆█▇█▆█▇█▇██</td></tr><tr><td>train_loss_epoch</td><td>█▄▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▅▅▄▅▄▅▄▄▄▄▃▂▃▂▂▂▂▂▂▃▃▁▂▄</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▁▂▁▂▂▃▃▃▃▄▄▂▄▂▄▂▅▃▅▃▆▆▆▃▃▇▇▇▃▇▄█▄█▄</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▃▃▅█▁▁▇▅▇▅▆▆▅▄█▅▇▅▇▅▆▅█▄▅</td></tr><tr><td>val_auc</td><td>▁▁▁▂▂▁▁▁▂▂▃▃▄▆▄▆▇██▆▆█▇█████▇█▇█████▇█▇█</td></tr><tr><td>val_f1</td><td>▁▂▂▁▁▁▁▁▁▁▁▁▁▂▁▃▃▅█▁▁▇▅▇▅▆▆▅▄█▅▇▅▇▅▆▅█▄▆</td></tr><tr><td>val_loss_epoch</td><td>█▅▅▅▆▅▅▆▅▅▅▅▆▅▆▃▄▃▂▇▄▂▃▁▃▁▂▃▄▂▄▂▂▁▄▂▃▁▄▃</td></tr><tr><td>val_loss_step</td><td>█▆▆▅▆▆▅▇▅▆▆▆▆▆▇▅▅▃▂▃▅▃▄▃▄▁▄▃▅▃▃▂▂▁▃▃▄▂▂▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>49</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.73399</td></tr><tr><td>train_auc</td><td>0.66136</td></tr><tr><td>train_f1</td><td>0.56334</td></tr><tr><td>train_loss_epoch</td><td>0.5484</td></tr><tr><td>train_loss_step</td><td>0.6457</td></tr><tr><td>trainer/global_step</td><td>249</td></tr><tr><td>val_acc</td><td>0.67105</td></tr><tr><td>val_auc</td><td>0.78454</td></tr><tr><td>val_f1</td><td>0.41176</td></tr><tr><td>val_loss_epoch</td><td>0.60458</td></tr><tr><td>val_loss_step</td><td>0.62434</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_32_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/7yi1gvfr' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/7yi1gvfr</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240115_134616-7yi1gvfr\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cf190025d8341d8b74f0a6646764f01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.0169333333382383, max=1.0))…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240115_135823-wv08j85m</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/wv08j85m' target=\"_blank\">MLP_4_32_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/wv08j85m' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/wv08j85m</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 3.4 K \n",
      "2  | head        | Sequential       | 562   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 33    \n",
      "--------------------------------------------------\n",
      "4.0 K     Trainable params\n",
      "0         Non-trainable params\n",
      "4.0 K     Total params\n",
      "0.016     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ab4d44f9d01428b9a7209b4b71c7420",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▅▆▆▆▆▆▆▆▆▆▆▇▆▇▇▇▇█▇▇▇▇▇█▇▇███████▇█████</td></tr><tr><td>train_auc</td><td>▂▁▁▁▁▁▁▂▄▄▄▅▆▆▇▇▇▇▇▇▇▇███▇██████████████</td></tr><tr><td>train_f1</td><td>▇▁▁▁▁▁▁▁▁▁▁▂▆▃▇▅▇▆█▅▆▆▇▇▇▇▇█▇█▇█▇▇▇█▇▇▇▇</td></tr><tr><td>train_loss_epoch</td><td>█▇▆▆▆▆▆▆▅▅▅▅▄▄▄▂▃▃▃▃▂▂▂▂▁▂▁▂▁▂▁▁▁▁▂▂▂▁▂▂</td></tr><tr><td>train_loss_step</td><td>▇██▆▇▇▅▃▄▁▇▇▂▄▄▃▃▂▄▁▂▁▂▁▄</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▁▂▁▂▂▃▃▃▃▄▄▂▄▂▄▂▅▃▅▃▆▆▆▃▃▇▇▇▃▇▄█▄█▄</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▂▄▄▅█▆▆▆▃▇▆▆▇▇█▆▇▆█▅▅█▇▆▆▇▇▇</td></tr><tr><td>val_auc</td><td>▁▅██████████▇▇▇▇████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▂▅▄▆█▇▆▆▃▇▆▆▇▇█▇▇▇█▆▆█▇▆▇▇▇▇</td></tr><tr><td>val_loss_epoch</td><td>▇█▇▇▇█▆▇▇▆▆▇▅▃▆▅▄▅▅▃▆▃▃▃▄▂▅▂▅▄▁▃▅▂▃▄▂▄▅▁</td></tr><tr><td>val_loss_step</td><td>▇▇▇▇▇▇▆▇▇▇▆█▆▃▇▆▄▆▆▅▆▃▃▅▅▃▅▄▆▅▆▄▆▂▅▄▂▅▄▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>49</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.70033</td></tr><tr><td>train_auc</td><td>0.73933</td></tr><tr><td>train_f1</td><td>0.51007</td></tr><tr><td>train_loss_epoch</td><td>0.57457</td></tr><tr><td>train_loss_step</td><td>0.59105</td></tr><tr><td>trainer/global_step</td><td>249</td></tr><tr><td>val_acc</td><td>0.69737</td></tr><tr><td>val_auc</td><td>0.78277</td></tr><tr><td>val_f1</td><td>0.52577</td></tr><tr><td>val_loss_epoch</td><td>0.51677</td></tr><tr><td>val_loss_step</td><td>0.42974</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_32_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/wv08j85m' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/wv08j85m</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240115_135823-wv08j85m\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea143026d67843c78d05c00076f46217",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240115_141111-u9408zg0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/u9408zg0' target=\"_blank\">MLP_4_32_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/u9408zg0' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/u9408zg0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 3.4 K \n",
      "2  | head        | Sequential       | 562   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 1.1 K \n",
      "--------------------------------------------------\n",
      "5.0 K     Trainable params\n",
      "0         Non-trainable params\n",
      "5.0 K     Total params\n",
      "0.020     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "373a8e6cd37e4533911e471fd55c2b4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▆▆▆▆▆▆▆▆▆▆▆▆▇█▇▇▇▇▇▇▇▇▇▇█▇▇██▇█▇▇█▇▇▇█▇</td></tr><tr><td>train_auc</td><td>▁▁▁▂▂▂▁▂▄▄▅▅▆▇▇▇▇▇▇▇█▇▇▇▇▇▇█▇█▇▇▇▇███▇█▇</td></tr><tr><td>train_f1</td><td>▆▁▁▁▁▁▁▁▁▁▁▂▅▆█▇▇▇▆██▅▇▆▇█▇▇▇█▇█▇▇▇▇▇▇██</td></tr><tr><td>train_loss_epoch</td><td>█▇▆▆▆▆▆▆▅▅▄▄▃▂▂▂▂▂▂▃▃▂▂▃▂▂▂▂▂▁▂▂▁▂▂▂▁▁▁▂</td></tr><tr><td>train_loss_step</td><td>█▇██▆▇▆▅▁▅▅▅▄▄▃▃▅▄▅▃▃▃▄▃▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▁▂▁▂▂▃▃▃▃▄▄▂▄▂▄▂▅▃▅▃▆▆▆▃▃▇▇▇▃▇▄█▄█▄</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▇▅▅▆██▅█▆▆▅█▆▆█▇██▇▇▆▇██████▇</td></tr><tr><td>val_auc</td><td>▁▅██████████████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▇▆▆▆██▆▇▆▆▆▇▆▆▇▇▇▇▇▇▆▇▇▇▇▇▇▇▇</td></tr><tr><td>val_loss_epoch</td><td>▇▆▇▆▇█▇▅▇▆▅▅▅▅▅▃▃▄▃▄▄▃▃▃▆▁▃▃▃▃▃▅▃▂▂▃▃▃▄▃</td></tr><tr><td>val_loss_step</td><td>▇██▆▇▇▇▅▆▆▅▆▅▇█▆▄▅▄▅▅▄▃▄▄▁▄▅▄▅▄▄▄▂▄▄▄▅▅▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>49</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.69622</td></tr><tr><td>train_auc</td><td>0.73623</td></tr><tr><td>train_f1</td><td>0.54208</td></tr><tr><td>train_loss_epoch</td><td>0.57066</td></tr><tr><td>train_loss_step</td><td>0.5595</td></tr><tr><td>trainer/global_step</td><td>249</td></tr><tr><td>val_acc</td><td>0.70395</td></tr><tr><td>val_auc</td><td>0.78117</td></tr><tr><td>val_f1</td><td>0.55</td></tr><tr><td>val_loss_epoch</td><td>0.55894</td></tr><tr><td>val_loss_step</td><td>0.54223</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_32_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/u9408zg0' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/u9408zg0</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240115_141111-u9408zg0\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd7ef1db4568486a9b83ecbb3224378e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240115_142503-zlze56g7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/zlze56g7' target=\"_blank\">MLP_4_64_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/zlze56g7' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/zlze56g7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 12.9 K\n",
      "2 | head        | Sequential       | 2.1 K \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "15.0 K    Trainable params\n",
      "0         Non-trainable params\n",
      "15.0 K    Total params\n",
      "0.060     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62dec94f0b434978aa390413e915fa9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▄▄▄▄▄▄▅▆▆▇▇▇▆▆▇▇▇▇▆▇▇▆▇▆▆▆▇█▇▇▇▇▆▇▆█▆▇▇</td></tr><tr><td>train_auc</td><td>▁▁▁▂▄▄▅▆▇▇▇█▇▇▇██▇▇▇█████▇▇███████▇█████</td></tr><tr><td>train_f1</td><td>▄▁▁▁▁▁▂▄▇▇▆▇▇▇▆▇▇▇▇▆▇▇▇▇▇▇▆▇▇▇▇▇▇▆▇▆█▆█▇</td></tr><tr><td>train_loss_epoch</td><td>█▇▇▇▆▅▅▄▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▂▁▂▂▁▁▁▁▂▁▂▂▂▂▃▁</td></tr><tr><td>train_loss_step</td><td>██▅▆▄▅▆▃▃▁▄▄▃▂▃▄▄▃▅▄▅▅▅▄▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▁▂▁▂▂▃▃▃▃▄▄▂▄▂▄▂▅▃▅▃▆▆▆▃▃▇▇▇▃▇▄█▄█▄</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▂█▆▆▆▇█▅▇▆▆█▆▆▇███▇▇▇▇▆█▇▇▇█▆▇▆█▆█</td></tr><tr><td>val_auc</td><td>▇▄▅▄▃▄▅▄▆▇██▆▇▄▇█▄▇▄▅▃▃▁▄▅▂▆▆▄▅▆▆▄▆▆▇▄▇▅</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▂█▇▆▆▇▇▅█▆▆█▆▇▇▇▇█▇▇▇▆▆▇▇▇▆▇▆▇▆█▆▇</td></tr><tr><td>val_loss_epoch</td><td>▆▆█▇▆▆▄▃▄▃▄▃▂▃▄▄▃▃▄▃▄▃▃▄▃▃▃▂▁▃▃▅▄▄▃▁▄▃▄▂</td></tr><tr><td>val_loss_step</td><td>▇▆█▆▆▅▅▄▄▄▆▅▄▄▅▄▅▅▅▄▄▄▅▄▄▅▄▄▁▄▃▄▅▆▄▄▅▄▄▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>49</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.70115</td></tr><tr><td>train_auc</td><td>0.76251</td></tr><tr><td>train_f1</td><td>0.5</td></tr><tr><td>train_loss_epoch</td><td>0.54904</td></tr><tr><td>train_loss_step</td><td>0.545</td></tr><tr><td>trainer/global_step</td><td>249</td></tr><tr><td>val_acc</td><td>0.71711</td></tr><tr><td>val_auc</td><td>0.77803</td></tr><tr><td>val_f1</td><td>0.59434</td></tr><tr><td>val_loss_epoch</td><td>0.54772</td></tr><tr><td>val_loss_step</td><td>0.52145</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_64_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/zlze56g7' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/zlze56g7</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240115_142503-zlze56g7\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db154e3b2ca041a1b61443247e8c890e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240115_143801-sx0jqzpo</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/sx0jqzpo' target=\"_blank\">MLP_4_64_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/sx0jqzpo' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/sx0jqzpo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 12.9 K\n",
      "2 | head        | Sequential       | 2.1 K \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "15.0 K    Trainable params\n",
      "0         Non-trainable params\n",
      "15.0 K    Total params\n",
      "0.060     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9dc3249a6ed4c4cb43f73736c407b73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▆▇▇▆▇▇▇▇▆█▆▇▇▇</td></tr><tr><td>train_auc</td><td>▁▃▂▃▅▃▃▃▄▃▄▅▅▄▄▃▄▅▅▅▅▅▄▆▆▄▅▆▆▅▅█▆▇▅▇▆▆▆▆</td></tr><tr><td>train_f1</td><td>▅▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▁▁▂▂▂▃▃▃▄█▁▁▁</td></tr><tr><td>train_loss_epoch</td><td>█▄▄▃▃▃▄▃▃▃▃▂▂▃▃▃▃▂▂▂▂▃▄▂▂▃▃▂▂▃▃▁▂▁▂▁▂▄▂▂</td></tr><tr><td>train_loss_step</td><td>▆▆▃▆▅▄▅▄▇▁▅▅▄█▄▅▄▄▇▅▇▇▂▅▆</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▁▂▁▂▂▃▃▃▃▄▄▂▄▂▄▂▅▃▅▃▆▆▆▃▃▇▇▇▃▇▄█▄█▄</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>▇▇▇█████▇▇▇█▁▁▁▄█▇████▇▁▇▁▇▇▇██████████▇</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_epoch</td><td>▆▆▇▅▅▆▅▅▆▅▅▆▅▆▅▆▅▅▅▆▆▆▄▆▃▆▅▃▅▄▆█▅▆▇▃▄▅▆▁</td></tr><tr><td>val_loss_step</td><td>▆▆█▆▆▅▅▅▆▆▆▇▆▆▅▆▆▅▆▆▆▆▄▅▆▆▆▆▆▅▅▅▆▇▆▅▅▆▅▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>49</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.63793</td></tr><tr><td>train_auc</td><td>0.57903</td></tr><tr><td>train_f1</td><td>0.0</td></tr><tr><td>train_loss_epoch</td><td>0.64717</td></tr><tr><td>train_loss_step</td><td>0.65359</td></tr><tr><td>trainer/global_step</td><td>249</td></tr><tr><td>val_acc</td><td>0.57566</td></tr><tr><td>val_auc</td><td>0.5068</td></tr><tr><td>val_f1</td><td>0.0</td></tr><tr><td>val_loss_epoch</td><td>0.66255</td></tr><tr><td>val_loss_step</td><td>0.63465</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_64_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/sx0jqzpo' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/sx0jqzpo</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240115_143801-sx0jqzpo\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01919519c2ec4c1a8d538e09451dcf39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016933333330477276, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240115_145022-9t03hgej</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/9t03hgej' target=\"_blank\">MLP_4_64_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/9t03hgej' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/9t03hgej</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 12.9 K\n",
      "2 | head        | Sequential       | 2.1 K \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "15.0 K    Trainable params\n",
      "0         Non-trainable params\n",
      "15.0 K    Total params\n",
      "0.060     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cb2098b67504251a4b84d0b1f0d0fb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▃▂▄▁▄▅▅▄▅▅▅▅▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇█▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>train_auc</td><td>▂▃▃▂▃▃▁▂▂▂▂▃▁▂▂▁▃▂▃▅▇▆▅▆▇▇▇▇███▇▆▇▆▆▆▄▆▇</td></tr><tr><td>train_f1</td><td>▅▆▃▆▃▂▃▂▁▁▁▂▁▁▁▁▃▃▅▆▆▆▅▇▇▇▇▇█▇▇▇▇▇▇▇█▅██</td></tr><tr><td>train_loss_epoch</td><td>█▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▅▄▄▄▄▄▄▄▃▃▃▂▂▂▂▂▂▃▂▂▂▂▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▁▂▁▂▂▃▃▃▃▄▄▂▄▂▄▂▅▃▅▃▆▆▆▃▃▇▇▇▃▇▄█▄█▄</td></tr><tr><td>val_acc</td><td>▁▄▄▁▄▄▄▄▄▄▄▄▄▄▄▄▄▅▄▅▄▅█▆▇▆▇█▇▇▆▇▆█▇▆▆█▆▇</td></tr><tr><td>val_auc</td><td>▅▁▄▄▁▁▁▁▁▁▁▁▁▁▁▂▂▃▃▆▅▆▇▇████████▇▇▇▆████</td></tr><tr><td>val_f1</td><td>▇▁▁▇▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▂▁▃█▄▆▅▇█▆▇▅▇▅█▆▄▅█▄▇</td></tr><tr><td>val_loss_epoch</td><td>█▆▃▃▃▃▃▃▃▃▃▃▃▃▃▃▂▂▃▃▃▂▂▃▂▂▁▁▁▁▂▂▂▂▂▂▂▁▂▁</td></tr><tr><td>val_loss_step</td><td>█▇▅▄▄▄▃▃▄▄▄▄▄▄▃▃▃▃▄▃▄▄▂▃▂▃▂▂▁▂▂▂▃▃▃▂▃▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>49</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.72167</td></tr><tr><td>train_auc</td><td>0.6357</td></tr><tr><td>train_f1</td><td>0.58303</td></tr><tr><td>train_loss_epoch</td><td>0.53989</td></tr><tr><td>train_loss_step</td><td>0.5296</td></tr><tr><td>trainer/global_step</td><td>249</td></tr><tr><td>val_acc</td><td>0.70724</td></tr><tr><td>val_auc</td><td>0.77205</td></tr><tr><td>val_f1</td><td>0.55276</td></tr><tr><td>val_loss_epoch</td><td>0.54563</td></tr><tr><td>val_loss_step</td><td>0.5241</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_64_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/9t03hgej' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/9t03hgej</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240115_145022-9t03hgej\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "202d0df351934cfa96731c883824328e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01691666666883975, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240115_150511-n62hd6hf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/n62hd6hf' target=\"_blank\">MLP_4_64_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/n62hd6hf' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/n62hd6hf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 12.9 K\n",
      "2  | head        | Sequential       | 2.1 K \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 65    \n",
      "--------------------------------------------------\n",
      "15.1 K    Trainable params\n",
      "0         Non-trainable params\n",
      "15.1 K    Total params\n",
      "0.060     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0541c6bf88b446f4a471a147620f5c4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▄▄▄▄▄▄▆▆▆▆▅▇▆▇▇▇▇▇▇█▇█▇▇▇▇▇█▇█▇▆▇█▇▇▇▇▇</td></tr><tr><td>train_auc</td><td>▂▂▂▁▄▄▅▅▆▇▇▆▇▇█▇▇▇███▇▇████████▇████████</td></tr><tr><td>train_f1</td><td>▄▁▁▁▁▁▁▅▆▆▇█▆▆▇▇█▇█▇█▇███▆█▇▇▇█▇▆██▇▇▇▇█</td></tr><tr><td>train_loss_epoch</td><td>█▇▇▇▆▆▅▆▄▃▃▄▃▂▂▂▂▂▂▂▁▂▂▂▂▂▂▂▁▂▁▂▂▂▁▂▁▁▂▁</td></tr><tr><td>train_loss_step</td><td>█▆▅█▄▁▃▄▁▄▃▃▂▄▆▁▄▂▃▂▆▆▂▄▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▁▂▁▂▂▃▃▃▃▄▄▂▄▂▄▂▅▃▅▃▆▆▆▃▃▇▇▇▃▇▄█▄█▄</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▃▄▆▇▃▄▆▆▇▇▇▅▇▆█▆█▆█▅████▇█▆█▇█▆█▇</td></tr><tr><td>val_auc</td><td>█▆▇▇▆▅▅▂▁▂▃▂▂▂▃▄▂▃▅▅▆▅▆▄▆▅▆▅▅▅▅▅▅▆▄▅▅▆▅▆</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▄▅▇▇▄▄▆▆▇█▇▅▇▆█▆▇▆▇▅▇▇▇▇▇▇▆▇▇▇▆▇▆</td></tr><tr><td>val_loss_epoch</td><td>█▇▆▇▆▅▄▆▂▂▃▅▃▄▄▁▃▃▅▂▃▄▃▂▅▁▃▂▃▃▂▁▄▃▃▃▃▃▃▃</td></tr><tr><td>val_loss_step</td><td>█▇▆█▇▇▅▇▅▅▅▆▆▇▆▄▄▅▆▄▅▇▄▄▄▁▄▅▆▄▄▅▆▄▄▅▄▃▄▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>49</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.70608</td></tr><tr><td>train_auc</td><td>0.76647</td></tr><tr><td>train_f1</td><td>0.52895</td></tr><tr><td>train_loss_epoch</td><td>0.54965</td></tr><tr><td>train_loss_step</td><td>0.54634</td></tr><tr><td>trainer/global_step</td><td>249</td></tr><tr><td>val_acc</td><td>0.69079</td></tr><tr><td>val_auc</td><td>0.78166</td></tr><tr><td>val_f1</td><td>0.51042</td></tr><tr><td>val_loss_epoch</td><td>0.59282</td></tr><tr><td>val_loss_step</td><td>0.60895</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_64_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/n62hd6hf' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/n62hd6hf</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240115_150511-n62hd6hf\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e1e9a41e3c443c7a2713a44ce829d65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240115_151754-joqqlk4o</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/joqqlk4o' target=\"_blank\">MLP_4_64_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/joqqlk4o' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/joqqlk4o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 12.9 K\n",
      "2  | head        | Sequential       | 2.1 K \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 4.2 K \n",
      "--------------------------------------------------\n",
      "19.2 K    Trainable params\n",
      "0         Non-trainable params\n",
      "19.2 K    Total params\n",
      "0.077     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11f073a6b3b047fa938cf59195df2de4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▄▄▄▄▄▄▄▇▆▇▆▇▇▇▇▇▇▇▇▆██▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇</td></tr><tr><td>train_auc</td><td>▁▁▁▂▂▄▅▆▇▇▇▇██▇▇▇█▇▇▇███▇████▇██████████</td></tr><tr><td>train_f1</td><td>▄▁▁▁▁▁▁▄█▇▇▇▇▇██▇▇▇█▆██▇▇█▆█▇▇▇█▇██▇▇▇█▇</td></tr><tr><td>train_loss_epoch</td><td>██▇▇▇▆▅▄▃▂▃▂▁▁▂▂▂▁▂▂▂▁▁▂▂▂▂▁▁▂▁▂▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>██▇▆▅▅▇▄▄▄▄▅▃▁▅▄▃▄▄▄▄▄▄▃▄</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▁▂▁▂▂▃▃▃▃▄▄▂▄▂▄▂▅▃▅▃▆▆▆▃▃▇▇▇▃▇▄█▄█▄</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▄██▆▇█▇▇▆▇██▆▆▆▇▇▇▆▇▆▆▇▇▇█▇▇▇▇▇▆▇</td></tr><tr><td>val_auc</td><td>█▅▄▆▅▆▆▆▆▇▇▇▆▄▄▅▄▃▄▆▃▆▅▁▃▆▂▆▅▃▄▄▃▁▅▃▅▃▆▃</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▂▅██▆▇▇█▇▆▇▇▇▆▇▆▇██▆█▆▆▇▇▇▇▇▆▇▇▇▆▇</td></tr><tr><td>val_loss_epoch</td><td>█▆▇▆▆▆▄▃▂▃▄▁▃▁▃▃▃▁▃▅▃▃▂▂▃▄▃▃▃▃▄▂▂▃▃▃▂▂▂▃</td></tr><tr><td>val_loss_step</td><td>█▇█▆▇▆▄▅▄▄▆▁▄▂▅▄▄▂▄▄▄▄▂▅▄▆▅▄▄▅▄▄▄▅▄▄▃▄▄▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>49</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.69869</td></tr><tr><td>train_auc</td><td>0.75781</td></tr><tr><td>train_f1</td><td>0.47194</td></tr><tr><td>train_loss_epoch</td><td>0.55142</td></tr><tr><td>train_loss_step</td><td>0.55426</td></tr><tr><td>trainer/global_step</td><td>249</td></tr><tr><td>val_acc</td><td>0.71053</td></tr><tr><td>val_auc</td><td>0.7748</td></tr><tr><td>val_f1</td><td>0.6</td></tr><tr><td>val_loss_epoch</td><td>0.5828</td></tr><tr><td>val_loss_step</td><td>0.60809</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_64_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/joqqlk4o' target=\"_blank\">https://wandb.ai/thoomas/PLA_01102024_5PPI_Kfold/runs/joqqlk4o</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240115_151754-joqqlk4o\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "k_folds = 5\n",
    "kfold = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "for fold, (train_ids, valid_ids) in enumerate(kfold.split(dataset_filtered)):\n",
    "    train_subset = dataset_filtered.index_select(train_ids.tolist())\n",
    "    val_subset = dataset_filtered.index_select(valid_ids.tolist())\n",
    "    for NUM_LAYERS, HIDDEN_CHANNELS, pool, model in list(itertools.product(*[num_layers, hiddens, pools, models])):\n",
    "            # Path to the folder where the pretrained models are saved\n",
    "        CHECKPOINT_PATH = checkpoint_folder / f'{model}_{NUM_LAYERS}_{HIDDEN_CHANNELS}_onehot_{fold}' / pool\n",
    "        CHECKPOINT_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Skip already trained kfold and pool\n",
    "        checkpoint = CHECKPOINT_PATH / f\"GraphLevel{model}\" / f\"GraphLevel{model}.ckpt\" \n",
    "        if checkpoint.exists():\n",
    "            print(checkpoint)\n",
    "            continue\n",
    "\n",
    "        # Run training\n",
    "        run = wandb.init(project=project_name, name=f'{model}_{NUM_LAYERS}_{HIDDEN_CHANNELS}_onehot_{fold}', \n",
    "                        group=f'{model}_{pool}', \n",
    "                        # mode=\"disabled\"\n",
    "                        )\n",
    "        PPIGraph.train_graph_classifier_kfold(model, \n",
    "                                                train_subset, \n",
    "                                                val_subset, \n",
    "                                                dataset, \n",
    "                                                CHECKPOINT_PATH, \n",
    "                                                AVAIL_GPUS, \n",
    "                                                in_channels=5,\n",
    "                                                hidden_channels=HIDDEN_CHANNELS, \n",
    "                                                out_channels = HIDDEN_CHANNELS,\n",
    "                                                num_layers=NUM_LAYERS, \n",
    "                                                epochs=epochs,\n",
    "                                                embedding=False,\n",
    "                                                batch_size=256, \n",
    "                                                graph_pooling=pool)\n",
    "        run.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
