{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections import defaultdict\n",
    "from tqdm.auto import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import networkx as nx\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric.utils\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_dir = (Path().cwd().parents[0]).absolute()\n",
    "\n",
    "module_path = str(p_dir / \"src\")\n",
    "\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = (Path().cwd().parents[0] / 'data').absolute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning.pytorch as pl\n",
    "import PPIGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "# Define condition mapping\n",
    "condition_mapping = {'control': 0, '100nM': 1}\n",
    "\n",
    "# Load graph dataset and process if neede\n",
    "graph_path = data_dir / '13cyc' / 'graphs' \n",
    "\n",
    "dataset = PPIGraph.GraphDataset(graph_path, 'raw', 'pt', condition_mapping=condition_mapping, n_c=2)\n",
    "train_set, val_set, test_set = PPIGraph.train_test_val_split(dataset)\n",
    "\n",
    "# Create Dataloader\n",
    "train_loader = DataLoader(train_set, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: GraphDataset(2230):\n",
      "======================\n",
      "Number of graphs: 2230\n",
      "Number of features: 13\n",
      "Number of classes: 2\n",
      "Train set: 1071, test set: 892, val set: 267\n",
      "Step 1:\n",
      "=======\n",
      "Number of graphs in the current batch: 32\n",
      "DataBatch(edge_index=[2, 16240], pos=[2847, 2], labels=[2847, 13], nuclei=[2847], weight=[16240], condition=[32], fov=[32], id=[32], train_mask=[2847], test_mask=[2847], edge_attr=[16240, 2], x=[2847, 13], y=[32], edge_weight=[16240], name=[32], batch=[2847], ptr=[33])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print some information on the dataset\n",
    "print(f'Dataset: {dataset}:')\n",
    "print('======================')\n",
    "print(f'Number of graphs: {len(dataset)}')\n",
    "print(f'Number of features: {dataset.num_features}')\n",
    "print(f'Number of classes: {dataset.num_classes}')\n",
    "\n",
    "print(f'Train set: {len(train_set)}, test set: {len(test_set)}, val set: {len(val_set)}')\n",
    "for step, data in enumerate(train_loader):\n",
    "\n",
    "    print(f'Step {step + 1}:')\n",
    "    print('=======')\n",
    "    print(f'Number of graphs in the current batch: {data.num_graphs}')\n",
    "    print(data)\n",
    "    print()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: GraphDataset(2230):\n",
      "======================\n",
      "Number of graphs: 2230\n",
      "Number of features: 13\n",
      "Number of classes: 2\n",
      "Train set: 1071, test set: 892, val set: 267\n",
      "Step 1:\n",
      "=======\n",
      "Number of graphs in the current batch: 32\n",
      "DataBatch(edge_index=[2, 15970], pos=[2799, 2], labels=[2799, 13], nuclei=[2799], weight=[15970], condition=[32], fov=[32], id=[32], train_mask=[2799], test_mask=[2799], edge_attr=[15970, 2], x=[2799, 13], y=[32], edge_weight=[15970], name=[32], batch=[2799], ptr=[33])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print some information on the dataset\n",
    "print(f'Dataset: {dataset}:')\n",
    "print('======================')\n",
    "print(f'Number of graphs: {len(dataset)}')\n",
    "print(f'Number of features: {dataset.num_features}')\n",
    "print(f'Number of classes: {dataset.num_classes}')\n",
    "\n",
    "print(f'Train set: {len(train_set)}, test set: {len(test_set)}, val set: {len(val_set)}')\n",
    "for step, data in enumerate(train_loader):\n",
    "\n",
    "    print(f'Step {step + 1}:')\n",
    "    print('=======')\n",
    "    print(f'Number of graphs in the current batch: {data.num_graphs}')\n",
    "    print(data)\n",
    "    print()\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-fold on filter dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from lightning.pytorch.accelerators import find_usable_cuda_devices\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from torch.utils.data import SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out by maximum number of counts per cell\n",
    "min_count = 20\n",
    "max_count = 70\n",
    "\n",
    "graph_path = data_dir / '13cyc' / 'graphs' \n",
    "dataset = PPIGraph.GraphDataset(graph_path, 'raw', 'pt', condition_mapping=condition_mapping, n_c=2)\n",
    "\n",
    "# Create Dataloader\n",
    "loader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# Get Indices\n",
    "indices = []\n",
    "for step, data in enumerate(loader):\n",
    "    if len(data.x) <= min_count:\n",
    "        continue \n",
    "    \n",
    "    if (data.x.sum(axis=0) >= max_count).any():\n",
    "        continue\n",
    "    indices.append(step)\n",
    "    \n",
    "# Get subset dataset\n",
    "dataset_filtered = dataset.index_select(indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2117"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = '13PPI'\n",
    "condition = 'Kfold'\n",
    "checkpoint_folder = (Path().cwd().parents[0]).absolute() / 'data' / '13PPI' /\"saved_models\" / dataset_name / f\"Graph_GNNs_{condition}\" \n",
    "project_name = f'PLA_10152023_{dataset_name}_{condition}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    }
   ],
   "source": [
    "AVAIL_GPUS = [1]\n",
    "\n",
    "# Setting the seed\n",
    "pl.seed_everything(42)\n",
    "\n",
    "NUM_LAYERS = 2\n",
    "HIDDEN_CHANNELS = 16\n",
    "pools = ['mean', 'max', 'sum', 'attention', 'attention2']\n",
    "num_layers = [2,3,4]\n",
    "hiddens = [16, 32, 64]\n",
    "\n",
    "\n",
    "epochs = 80\n",
    "# model = 'GAT'\n",
    "model = 'MLP'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mthoomas\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76104bb923a944548dd4beccad57e30d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240101_144630-aioxabmr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/aioxabmr' target=\"_blank\">MLP_2_16_onehot_0</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/aioxabmr' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/aioxabmr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "c:\\Users\\thu71\\Anaconda3\\envs\\snowflake\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:395: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 496   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "650       Trainable params\n",
      "0         Non-trainable params\n",
      "650       Total params\n",
      "0.003     Total estimated model params size (MB)\n",
      "C:\\Users\\thu71\\AppData\\Roaming\\Python\\Python39\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\thu71\\AppData\\Roaming\\Python\\Python39\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\thu71\\AppData\\Roaming\\Python\\Python39\\site-packages\\lightning\\pytorch\\utilities\\data.py:77: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 2. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  warning_cache.warn(\n",
      "C:\\Users\\thu71\\AppData\\Roaming\\Python\\Python39\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\thu71\\AppData\\Roaming\\Python\\Python39\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:280: PossibleUserWarning: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7286de89b08842dea973bc2e628febd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▂▅▇▇▇█▇█▇▇████▇██▇██▇▇█▇█▇█████▇██▇█▇▇</td></tr><tr><td>train_auc</td><td>▁▁▃▅▇▇████▇▇████████████████████████████</td></tr><tr><td>train_f1</td><td>▁▅▅▇▇▇██████████████████████████████████</td></tr><tr><td>train_loss_epoch</td><td>██▇▆▃▂▂▂▂▂▂▂▁▂▁▂▂▂▂▁▁▁▂▁▂▂▂▂▂▁▁▂▁▂▂▁▂▂▁▁</td></tr><tr><td>train_loss_step</td><td>███▆▄▆▃▃▃▄▃▂▃▂▄▄▂▄▂▃▄▃▃▄▃▃▃▃▂▁▅▄▃▄▂▃▂▃▄▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▁▅▇▇████▇████▇▇▇▇██▇▇▇█▇▇▇█▇█████████▇█</td></tr><tr><td>val_auc</td><td>▁▆▇█████████████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▂▆▇▇███████████████████████████████████</td></tr><tr><td>val_loss_epoch</td><td>██▇▅▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_step</td><td>███▆▃▂▂▃▂▂▂▂▂▃▁▂▂▂▃▂▂▁▃▂▃▂▂▁▂▁▂▂▂▂▂▃▁▁▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.75133</td></tr><tr><td>train_auc</td><td>0.82571</td></tr><tr><td>train_f1</td><td>0.74593</td></tr><tr><td>train_loss_epoch</td><td>0.51592</td></tr><tr><td>train_loss_step</td><td>0.5132</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.78774</td></tr><tr><td>val_auc</td><td>0.87024</td></tr><tr><td>val_f1</td><td>0.78469</td></tr><tr><td>val_loss_epoch</td><td>0.46202</td></tr><tr><td>val_loss_step</td><td>0.46068</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_16_onehot_0</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/aioxabmr' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/aioxabmr</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240101_144630-aioxabmr\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "445ce783d18d45b3a14175c4f10a68c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016933333332417533, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240101_151651-1owtlxpq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/1owtlxpq' target=\"_blank\">MLP_2_16_onehot_0</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/1owtlxpq' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/1owtlxpq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 496   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "650       Trainable params\n",
      "0         Non-trainable params\n",
      "650       Total params\n",
      "0.003     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc101b1487b3496f955f2767ebe97d89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▃▅▇▆▆▆▇▆▇▆▇▇▆▇▇▇▇▆▇▆▇▇█▆▇▇▇▇█▇▇▇▇▇▇▇▇█</td></tr><tr><td>train_auc</td><td>▁▁▃▃▄▅▅▅▆▆▆▆▇▇▇▇▇▇▆▆▇▇▇▆▇▆▇▇▇▇▇▇▇▇▇█▇▇▇█</td></tr><tr><td>train_f1</td><td>▁▃▅▄▆▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▆█▆█▇▇▇██▇█▇█▇▇▇█</td></tr><tr><td>train_loss_epoch</td><td>█▆▆▅▄▄▄▃▃▄▃▃▂▂▃▂▂▃▂▃▂▃▃▃▂▃▃▂▃▂▂▂▂▂▂▁▂▂▂▁</td></tr><tr><td>train_loss_step</td><td>█▇▆▄▅▄▅▄▃▄▆▃▃▄▃▂▃▆▂▅▁▄▁▅▂▄▂▃▅▁▃▄▃▃▃▃▃▃▃▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▂█▂█▃████████▆▇▆▇▇▆▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>▁█████▇█████████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▃█▃█▄██████████████▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>val_loss_epoch</td><td>██▆▄▃▂▃▂▂▂▂▃▁▂▁▂▁▂▂▁▂▂▂▂▂▂▃▂▃▃▃▄▄▅▄▃▅▅▄▅</td></tr><tr><td>val_loss_step</td><td>██▇▄▁▃▄▃▅▃▂▄▂▃▃▃▂▄▃▂▃▃▂▂▃▄▃▂▅▄▂▆▄▅▄▃▄▇▆▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.6834</td></tr><tr><td>train_auc</td><td>0.72378</td></tr><tr><td>train_f1</td><td>0.65729</td></tr><tr><td>train_loss_epoch</td><td>0.61997</td></tr><tr><td>train_loss_step</td><td>0.60505</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.50236</td></tr><tr><td>val_auc</td><td>0.79435</td></tr><tr><td>val_f1</td><td>0.66876</td></tr><tr><td>val_loss_epoch</td><td>0.67749</td></tr><tr><td>val_loss_step</td><td>0.67428</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_16_onehot_0</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/1owtlxpq' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/1owtlxpq</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240101_151651-1owtlxpq\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c0e24ed091e4376a96e69b5af4e8d78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01693333333435779, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240101_154714-lee59tp2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/lee59tp2' target=\"_blank\">MLP_2_16_onehot_0</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/lee59tp2' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/lee59tp2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 496   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "650       Trainable params\n",
      "0         Non-trainable params\n",
      "650       Total params\n",
      "0.003     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d0ee3eb85ae492bbe71496fb311df93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▂▄▄▅▅▆▆▇▇▇▇██▇▇██▇▇█▇█▇▇▇█████████████</td></tr><tr><td>train_auc</td><td>▁▂▃▄▅▆▆▆▇▇▆▆▇▆▇▇▆▇▇▇▆▇▆▇▆▇▇▇▇▇██▇▇▇█▇██▇</td></tr><tr><td>train_f1</td><td>▁▂▃▃▄▅▅▆▆▇▇▇▇███▇██▇██▇▇██▇▇█▇██████████</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▃▂▂▂▂▂▂▂▂▂▁▁▁▁▂▂▁▁▁▁▁▁▁▂▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▆▄▅▅▆▇▇▇▇▇▇▇▇█▇████▇▇█████████▇██▇▇█▇▇█</td></tr><tr><td>val_auc</td><td>▁▅▆▆▇▇▇▇████████████████████████████████</td></tr><tr><td>val_f1</td><td>▄▅▁▂▂▄▅▆▆▆▇▇▇▆▇▆▇▇▇█▇▆▇█████▇██▇██▇▇▇▇▇█</td></tr><tr><td>val_loss_epoch</td><td>█▄▄▄▄▃▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_step</td><td>█▅▅▄▄▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▂▂▂▁▁▁▂▂▂▁▂▂▁▁▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.75783</td></tr><tr><td>train_auc</td><td>0.73274</td></tr><tr><td>train_f1</td><td>0.75995</td></tr><tr><td>train_loss_epoch</td><td>0.51772</td></tr><tr><td>train_loss_step</td><td>0.5086</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.8066</td></tr><tr><td>val_auc</td><td>0.88018</td></tr><tr><td>val_f1</td><td>0.81193</td></tr><tr><td>val_loss_epoch</td><td>0.46529</td></tr><tr><td>val_loss_step</td><td>0.46624</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_16_onehot_0</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/lee59tp2' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/lee59tp2</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240101_154714-lee59tp2\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3219f3a7e8024e5cbed5fd3e75871232",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01693333333435779, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240101_161732-71zl0l57</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/71zl0l57' target=\"_blank\">MLP_2_16_onehot_0</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/71zl0l57' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/71zl0l57</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "c:\\Users\\thu71\\Anaconda3\\envs\\snowflake\\lib\\site-packages\\torch_geometric\\deprecation.py:12: UserWarning: 'nn.glob.GlobalAttention' is deprecated, use 'nn.aggr.AttentionalAggregation' instead\n",
      "  warnings.warn(out)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 496   \n",
      "2  | head        | Sequential       | 154   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 17    \n",
      "--------------------------------------------------\n",
      "667       Trainable params\n",
      "0         Non-trainable params\n",
      "667       Total params\n",
      "0.003     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6b51e6f77344b9ebb1ebb3ff187bbff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▃▆▆▆▇▇▇█▇████████▇▇██▇▇█████████▇█▇███</td></tr><tr><td>train_auc</td><td>▁▂▃▅▆▇▇▇████████████████████████████████</td></tr><tr><td>train_f1</td><td>▂▁▅▆▆▆▇▇███████████▇████████████████████</td></tr><tr><td>train_loss_epoch</td><td>██▇▆▄▄▃▂▂▂▂▁▂▁▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂</td></tr><tr><td>train_loss_step</td><td>███▅▄▅▃▄▂▂▃▂▃▃▃▂▂▂▂▄▄▂▂▁▃▃▃▃▇▃▄▁▂▃▄▃▂▂▂▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▁▅▅▅▆▅█▇▇▇▇▇▇▇██▇▇█████▇▇█▇█▇▇██▇▇███▇█</td></tr><tr><td>val_auc</td><td>▁▆▇▇▇███████████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▆▆▆▇▆██▇▇▇█▇███████████████▇███▇████▇█</td></tr><tr><td>val_loss_epoch</td><td>███▆▄▄▄▂▂▂▂▂▁▂▁▁▁▂▁▁▁▁▁▁▁▂▁▁▁▁▁▁▂▂▂▁▁▁▁▁</td></tr><tr><td>val_loss_step</td><td>███▆▅▅▅▃▂▃▃▁▂▂▂▃▂▃▁▂▁▂▃▃▂▃▂▂▂▂▄▃▃▄▃▂▂▃▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.75369</td></tr><tr><td>train_auc</td><td>0.8225</td></tr><tr><td>train_f1</td><td>0.74864</td></tr><tr><td>train_loss_epoch</td><td>0.52071</td></tr><tr><td>train_loss_step</td><td>0.49348</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.79953</td></tr><tr><td>val_auc</td><td>0.87081</td></tr><tr><td>val_f1</td><td>0.81481</td></tr><tr><td>val_loss_epoch</td><td>0.45839</td></tr><tr><td>val_loss_step</td><td>0.41593</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_16_onehot_0</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/71zl0l57' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/71zl0l57</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240101_161732-71zl0l57\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83895c271ba54c5692c2b1ad810c8e50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016916666666899498, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240101_164753-f5m8ts0u</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/f5m8ts0u' target=\"_blank\">MLP_2_16_onehot_0</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/f5m8ts0u' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/f5m8ts0u</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 496   \n",
      "2  | head        | Sequential       | 154   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 281   \n",
      "--------------------------------------------------\n",
      "931       Trainable params\n",
      "0         Non-trainable params\n",
      "931       Total params\n",
      "0.004     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▃▅▆▇▇█▇███▇███████████████████████████</td></tr><tr><td>train_auc</td><td>▁▁▂▄▇▇▇█▇███████████████████████████████</td></tr><tr><td>train_f1</td><td>▁▂▄▆▆▇▇██▇██▇█████▇████████▇█████▇██▇███</td></tr><tr><td>train_loss_epoch</td><td>███▆▃▃▂▂▂▁▂▂▂▁▂▂▁▁▂▁▂▁▂▁▂▁▁▂▁▂▂▁▁▂▁▂▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>███▆▄▄▄▂▃▄▃▃▄▅▁▃▃▂▃▃▃▂▄▃▄▂▂▅▄▃▂▃▃▆▃▄▂▂▂▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▂▆▆▆█▇▇▇█▇▇█▇▇▇█▇▇██▇█▇▇▇██▇▇▇▇▇██████▇</td></tr><tr><td>val_auc</td><td>▁▆▇█████████████████████████████████████</td></tr><tr><td>val_f1</td><td>▆▁▆▇▇██▇▇█▇███▇██▇▇██████████▇██████████</td></tr><tr><td>val_loss_epoch</td><td>███▆▃▂▂▂▂▁▁▂▁▁▂▂▂▂▁▁▁▁▁▂▁▁▁▁▁▂▁▂▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_step</td><td>███▆▄▃▃▂▂▂▁▃▃▂▂▃▄▃▂▂▂▂▁▃▂▁▁▂▂▂▂▄▂▂▂▂▁▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.7596</td></tr><tr><td>train_auc</td><td>0.83238</td></tr><tr><td>train_f1</td><td>0.7646</td></tr><tr><td>train_loss_epoch</td><td>0.50749</td></tr><tr><td>train_loss_step</td><td>0.45558</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.78302</td></tr><tr><td>val_auc</td><td>0.87115</td></tr><tr><td>val_f1</td><td>0.7665</td></tr><tr><td>val_loss_epoch</td><td>0.46708</td></tr><tr><td>val_loss_step</td><td>0.47589</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_16_onehot_0</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/f5m8ts0u' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/f5m8ts0u</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240101_164753-f5m8ts0u\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60d966aab2ed4720931244b886d6cfcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240101_172012-al2v6gjs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/al2v6gjs' target=\"_blank\">MLP_2_32_onehot_0</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/al2v6gjs' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/al2v6gjs</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 1.5 K \n",
      "2 | head        | Sequential       | 562   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "2.1 K     Trainable params\n",
      "0         Non-trainable params\n",
      "2.1 K     Total params\n",
      "0.008     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2327d48aff6d4c41a07e5cc9f740db35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▃▅▆▇▇▇▇▇▇▇▇█▇██████████████████▇███▇███</td></tr><tr><td>train_auc</td><td>▁▃▆▇▇███████████████████████████████████</td></tr><tr><td>train_f1</td><td>▃▁▆▆▇▇█▇▇██▇▇▇██████████████████████████</td></tr><tr><td>train_loss_epoch</td><td>██▅▄▃▂▂▂▂▂▂▂▂▁▂▂▁▂▂▂▂▂▂▁▁▂▁▂▁▂▁▁▂▂▂▁▂▁▁▁</td></tr><tr><td>train_loss_step</td><td>██▇▅▃▂▃▄▄▃▃▄▃▃▃▃▂▂▄▂▃▆▂▂▅▄▃▄▄▃▃▃▃▂▁▄▃▃▃▄</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▁▄███▅▇█▆▆▇▇▇▇▇▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>val_auc</td><td>▁▁▁▄▇▇██▇▇█████▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>val_f1</td><td>▆▁▄███▅▇▇▆▆▇█▇█▇▇▆▇▆▇▇▇▇▇▇▇▇▇▇▇▇█▇▇▇▇▇▇▇</td></tr><tr><td>val_loss_epoch</td><td>█▇▄▂▂▁▂▁▁▁▁▁▁▂▁▁▁▂▂▁▁▁▁▁▂▁▁▁▂▁▂▁▁▁▁▁▂▁▁▁</td></tr><tr><td>val_loss_step</td><td>██▅▃▃▃▃▃▃▁▂▂▃▄▃▂▃▃▃▃▃▃▂▂▃▂▃▃▃▃▄▃▂▁▃▃▃▂▃▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.76432</td></tr><tr><td>train_auc</td><td>0.84049</td></tr><tr><td>train_f1</td><td>0.76516</td></tr><tr><td>train_loss_epoch</td><td>0.49797</td></tr><tr><td>train_loss_step</td><td>0.53888</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.78774</td></tr><tr><td>val_auc</td><td>0.86935</td></tr><tr><td>val_f1</td><td>0.78365</td></tr><tr><td>val_loss_epoch</td><td>0.45223</td></tr><tr><td>val_loss_step</td><td>0.42364</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_32_onehot_0</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/al2v6gjs' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/al2v6gjs</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240101_172012-al2v6gjs\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dca5e557d2546028f9648803cb5768d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01693333333435779, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240101_175141-wswklu23</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/wswklu23' target=\"_blank\">MLP_2_32_onehot_0</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/wswklu23' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/wswklu23</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 1.5 K \n",
      "2 | head        | Sequential       | 562   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "2.1 K     Trainable params\n",
      "0         Non-trainable params\n",
      "2.1 K     Total params\n",
      "0.008     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▄▆▅▆▆▇▇▇▇▇▇▇▇▇▇█▇▇█▇▇██▇█▇██▇▇▇▇█▇▇████</td></tr><tr><td>train_auc</td><td>▁▃▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇████▇▇▇▇▇█▇▇▇▇█▇█▇███</td></tr><tr><td>train_f1</td><td>▂▁▅▄▆▅▆█▇▇▇▇▆▇▇▇█▇▇█▇▇████▇▇█▇▇▇▆▇▇▇████</td></tr><tr><td>train_loss_epoch</td><td>█▇▅▅▄▄▃▃▂▂▃▂▃▃▂▂▂▂▂▂▂▁▂▂▂▂▂▂▁▂▂▂▃▂▂▂▂▂▂▁</td></tr><tr><td>train_loss_step</td><td>█▇▆▅▄▄▅▅▄▃▃▄▄▅▂▄▂▄▄▃▄▃▄▁▅▄▄▄▄▄▄▄▃▂▃▃▃▃▃▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁███████▇▅▅▅▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>▁█▇▇▇▇▇▇▇▇▇▇▇▇▇█████▇▇▇▇▇██▇██▇▇▇▇▆▇▇▇▇▇</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁████████▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>val_loss_epoch</td><td>▇▆▃▃▃▁▂▁▁▁▂▁▂▂▃▃▃▄▄▄▄▅▆▅▄▅▇▇▆▅▅▅▆▅▆█▆▇▆▆</td></tr><tr><td>val_loss_step</td><td>▅▄▃▁▂▁▁▁▁▁▂▁▂▂▂▃▁▃▃▃▃▄▄▂▃▄▅▅▃▃▃▃▅▄▃█▄▅▄▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.7277</td></tr><tr><td>train_auc</td><td>0.77961</td></tr><tr><td>train_f1</td><td>0.72608</td></tr><tr><td>train_loss_epoch</td><td>0.55216</td></tr><tr><td>train_loss_step</td><td>0.53685</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.50236</td></tr><tr><td>val_auc</td><td>0.7998</td></tr><tr><td>val_f1</td><td>0.66876</td></tr><tr><td>val_loss_epoch</td><td>0.68875</td></tr><tr><td>val_loss_step</td><td>0.66764</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_32_onehot_0</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/wswklu23' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/wswklu23</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240101_175141-wswklu23\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95c4400e64cd4aecad7f6722870868a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016933333332417533, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240101_182124-uchmupj6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/uchmupj6' target=\"_blank\">MLP_2_32_onehot_0</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/uchmupj6' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/uchmupj6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 1.5 K \n",
      "2 | head        | Sequential       | 562   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "2.1 K     Trainable params\n",
      "0         Non-trainable params\n",
      "2.1 K     Total params\n",
      "0.008     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▃▄▅▆▇▇▇▇▇▇▇▇██▇████▇█▇▇█████▇▇▇███████▇</td></tr><tr><td>train_auc</td><td>▁▂▃▅▆▇▇▇▇▇▇▇▇█▇▇████▇█▇██▇█████████████▇</td></tr><tr><td>train_f1</td><td>▁▂▃▄▅▆▇▇▇▇▇▇▇██▇████▇███████████████████</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▆▆▅▄▄▄▄▄▃▄▄▃▄▃▃▃▂▃▂▂▄▂▂▄▃▃▃▃▃▃▃▃▂▁▃▃▃▃▄</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▇▆▇██▆▇█▆▇▇█▇█▇█▇█▇█▇█▇▇████▇██████▇███</td></tr><tr><td>val_auc</td><td>▆▁▆▇▇▇█████▇████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▇▆▇██▆██▇█████▇█▇█▇█▇█▇████████████▇███</td></tr><tr><td>val_loss_epoch</td><td>█▆▆▄▄▃▃▃▂▃▂▃▂▂▂▂▂▂▂▂▂▂▁▂▂▂▁▂▂▂▂▁▂▁▂▁▂▂▁▂</td></tr><tr><td>val_loss_step</td><td>█▇▆▅▄▄▄▅▃▄▂▃▃▄▄▃▃▃▃▄▄▃▂▃▃▃▂▃▄▃▄▃▂▁▄▃▃▃▂▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.75842</td></tr><tr><td>train_auc</td><td>0.81019</td></tr><tr><td>train_f1</td><td>0.75899</td></tr><tr><td>train_loss_epoch</td><td>0.5064</td></tr><tr><td>train_loss_step</td><td>0.56832</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.80896</td></tr><tr><td>val_auc</td><td>0.87807</td></tr><tr><td>val_f1</td><td>0.81379</td></tr><tr><td>val_loss_epoch</td><td>0.44974</td></tr><tr><td>val_loss_step</td><td>0.4356</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_32_onehot_0</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/uchmupj6' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/uchmupj6</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240101_182124-uchmupj6\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0e3ec88d8f34869becac4381b961df5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016916666666899498, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240101_185125-u6jg72vc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/u6jg72vc' target=\"_blank\">MLP_2_32_onehot_0</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/u6jg72vc' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/u6jg72vc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 1.5 K \n",
      "2  | head        | Sequential       | 562   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 33    \n",
      "--------------------------------------------------\n",
      "2.1 K     Trainable params\n",
      "0         Non-trainable params\n",
      "2.1 K     Total params\n",
      "0.008     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08222e6d1fdd4558b4bceebde84e6daa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▄▅▆▆▇▇▇▇█▇█▇▇█████▇███▇██████████████▇█</td></tr><tr><td>train_auc</td><td>▁▃▆▆▇▇█▇██▇████████████▇████████████████</td></tr><tr><td>train_f1</td><td>▁▄▅▆▆▇▇▇████▇▇████████████████████████▇█</td></tr><tr><td>train_loss_epoch</td><td>█▇▅▄▄▃▂▂▂▂▂▂▂▂▂▂▂▁▁▂▁▁▁▂▁▁▂▁▁▂▁▂▁▁▁▁▁▁▂▁</td></tr><tr><td>train_loss_step</td><td>█▇▄▅▄▂▃▁▂▂▁▂▃▁▁▃▃▃▂▁▂▂▃▃▃▂▂▃▃▃▂▂▂▄▂▂▃▃▃▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▆▆▆▆▇█▇▇█▇▇▇▇▇█▇▇█▇▇█▇▇█▇██▇███▇██▇▇▇██</td></tr><tr><td>val_auc</td><td>▄▂▁▃▅▇█▇██▇█▇▇▇█▇██▇███▇████████████████</td></tr><tr><td>val_f1</td><td>▁▂▂▁▂▅▇▅▄█▅▆▆▆▆▆▆▆█▄▆▇▆▅█▆▇▇▄▆▇▇▆█▇▆▆▅▇▇</td></tr><tr><td>val_loss_epoch</td><td>█▆▃▄▃▂▁▂▂▁▂▁▁▁▁▁▂▁▁▂▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_step</td><td>█▇▄▅▃▃▁▃▂▂▂▂▃▂▂▃▃▁▂▃▃▄▂▂▂▂▂▂▂▃▃▂▂▃▂▁▃▁▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.76196</td></tr><tr><td>train_auc</td><td>0.83897</td></tr><tr><td>train_f1</td><td>0.76419</td></tr><tr><td>train_loss_epoch</td><td>0.50257</td></tr><tr><td>train_loss_step</td><td>0.52544</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.78774</td></tr><tr><td>val_auc</td><td>0.87493</td></tr><tr><td>val_f1</td><td>0.78571</td></tr><tr><td>val_loss_epoch</td><td>0.45126</td></tr><tr><td>val_loss_step</td><td>0.48431</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_32_onehot_0</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/u6jg72vc' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/u6jg72vc</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240101_185125-u6jg72vc\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10905d73bc76458a8278b0301cc4916d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240101_192100-edtj7tw6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/edtj7tw6' target=\"_blank\">MLP_2_32_onehot_0</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/edtj7tw6' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/edtj7tw6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 1.5 K \n",
      "2  | head        | Sequential       | 562   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 1.1 K \n",
      "--------------------------------------------------\n",
      "3.1 K     Trainable params\n",
      "0         Non-trainable params\n",
      "3.1 K     Total params\n",
      "0.013     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▄▅▆▇█▇▇▇█▇█████████████████████████████</td></tr><tr><td>train_auc</td><td>▁▄▆▇▇█▇▇████████████████████████████████</td></tr><tr><td>train_f1</td><td>▁▄▆▆▇██▇▇███████████████████████████████</td></tr><tr><td>train_loss_epoch</td><td>█▇▅▃▃▂▂▂▂▂▂▂▁▁▂▂▁▂▂▁▁▂▂▂▁▁▁▁▁▁▁▁▁▂▁▂▁▁▁▂</td></tr><tr><td>train_loss_step</td><td>██▅▅▄▃▄▃▄▂▃▃▂▂▃▃▃▄▂▃▁▁▃▃▄▂▃▁▃▃▃▂▄▁▁▃▂▃▂▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▄▄▆█▇▇██▆▆▇▇▇▇▇▇▇▇▇▆▇▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>val_auc</td><td>▃▂▁▅▇████████▇▇▇▇█▇███▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>val_f1</td><td>▄▁▁▅▇▆▅██▄▄▆▆▆▆▇▆▇▅▆▃▆▆▅▇▆▆▇▇▇▇▆▇▇▇▆▆▆▆▆</td></tr><tr><td>val_loss_epoch</td><td>█▇▃▂▁▂▂▁▁▂▂▂▂▂▁▁▁▁▁▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂</td></tr><tr><td>val_loss_step</td><td>█▇▃▃▂▃▃▃▂▁▃▃▃▄▂▂▃▁▂▂▃▂▂▂▃▁▂▂▃▂▃▁▂▁▂▃▃▂▁▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.75428</td></tr><tr><td>train_auc</td><td>0.83568</td></tr><tr><td>train_f1</td><td>0.75472</td></tr><tr><td>train_loss_epoch</td><td>0.50336</td></tr><tr><td>train_loss_step</td><td>0.47392</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.7783</td></tr><tr><td>val_auc</td><td>0.86981</td></tr><tr><td>val_f1</td><td>0.76263</td></tr><tr><td>val_loss_epoch</td><td>0.46936</td></tr><tr><td>val_loss_step</td><td>0.48537</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_32_onehot_0</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/edtj7tw6' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/edtj7tw6</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240101_192100-edtj7tw6\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a94d687add3546fea4ebd2526a95c8e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240101_195239-0qap24u7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/0qap24u7' target=\"_blank\">MLP_2_64_onehot_0</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/0qap24u7' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/0qap24u7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 5.1 K \n",
      "2 | head        | Sequential       | 2.1 K \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "7.2 K     Trainable params\n",
      "0         Non-trainable params\n",
      "7.2 K     Total params\n",
      "0.029     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▄▆▇▇▇██▇▇████████████████▇████████████▇</td></tr><tr><td>train_auc</td><td>▁▄▇▇▇███████████████████████████████████</td></tr><tr><td>train_f1</td><td>▁▃▇▆▇▇██▇▇████████████████▇██████▇▇█████</td></tr><tr><td>train_loss_epoch</td><td>█▆▄▃▂▂▁▁▁▁▂▁▂▁▂▁▂▁▁▁▂▁▂▂▁▁▂▁▁▁▁▁▂▁▂▁▁▁▁▂</td></tr><tr><td>train_loss_step</td><td>█▆▅▄▃▄▄▃▂▄▃▄▃▂▃▃▄▄▃▃▂▃▂▂▂▃▂▃▃▂▂▂▃▃▃▃▂▁▂▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▂▇▁▇█▇▇█▇▇▆▇▇▇▇▇▇▆▇▇▇▇▇▇▇▅█▇▇▇▇▆▇█▇▇▇▇▇▇</td></tr><tr><td>val_auc</td><td>▁▅▅▇████▇▇▇████▇▇▇▇▇▇████▇▇▇▇▇▇▇███████▇</td></tr><tr><td>val_f1</td><td>▇█▁▇▇█▇▇▇▇▆█▇▇▇▇▆▆█▇▆▇▇▇▇▅█▇▇▇▆▅▇██▇▆▆▇▆</td></tr><tr><td>val_loss_epoch</td><td>█▅▄▂▁▁▁▁▂▁▂▂▁▁▂▁▁▂▂▂▁▁▂▁▁▂▁▁▁▁▂▂▁▁▁▁▂▂▁▁</td></tr><tr><td>val_loss_step</td><td>█▆▅▂▃▂▂▁▃▃▂▃▃▂▃▁▂▄▃▃▃▃▄▂▃▄▂▂▂▂▃▃▂▁▃▂▃▄▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.75665</td></tr><tr><td>train_auc</td><td>0.83521</td></tr><tr><td>train_f1</td><td>0.76185</td></tr><tr><td>train_loss_epoch</td><td>0.50104</td></tr><tr><td>train_loss_step</td><td>0.50675</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.78066</td></tr><tr><td>val_auc</td><td>0.86817</td></tr><tr><td>val_f1</td><td>0.77262</td></tr><tr><td>val_loss_epoch</td><td>0.45915</td></tr><tr><td>val_loss_step</td><td>0.44938</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_64_onehot_0</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/0qap24u7' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/0qap24u7</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240101_195239-0qap24u7\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5e29e2a4ba948b1a6919ba0658d9c57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240101_202408-glxqbvbe</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/glxqbvbe' target=\"_blank\">MLP_2_64_onehot_0</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/glxqbvbe' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/glxqbvbe</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 5.1 K \n",
      "2 | head        | Sequential       | 2.1 K \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "7.2 K     Trainable params\n",
      "0         Non-trainable params\n",
      "7.2 K     Total params\n",
      "0.029     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf2909bb1e304a0394adba6658303d8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▃▆▆▆▇▇▇▇▇█▇▇▇▇▇▇▇██▇▇█▇▇▇▇███▇█▇█▇▇▇▇█▇</td></tr><tr><td>train_auc</td><td>▁▁▄▆▆▆▇▇█▆▇▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▆▇▇█▇█▇█▇█▇██▇</td></tr><tr><td>train_f1</td><td>▁▂▆▆▆▆▇▇▇▇█▆▇▇▇▇▇▇██▇▇▇▇▇█▇██▇▇█▇█▇█▇▇██</td></tr><tr><td>train_loss_epoch</td><td>█▆▅▄▃▃▃▂▂▂▂▃▂▂▂▂▂▁▂▁▂▂▂▂▂▂▃▁▁▁▂▁▂▁▂▁▂▁▁▂</td></tr><tr><td>train_loss_step</td><td>█▇▇▄▆▄▅▄▃▄▄▆▃▂▃▃▄▃▃▃▂▄▃▁▃▄▃▃▃▄▂▂▃▄▃▁▂▃▃▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▁█████▇▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>▁▇█▇▇████████▇█▇████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁█████▇▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_epoch</td><td>▃▂▁▁▁▁▂▂▃▃▃▃▃▄▃▃▄▄▄▅▅▅▅▅▅▄▅▅▅▅▅▆▆▆▅▆▆█▆▅</td></tr><tr><td>val_loss_step</td><td>▂▂▂▁▁▁▂▂▃▃▃▂▃▃▂▂▃▄▃▅▄▅▄▄▅▃▄▅▅▅▅▅▅▄▄▄▆█▆▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.73774</td></tr><tr><td>train_auc</td><td>0.74922</td></tr><tr><td>train_f1</td><td>0.74801</td></tr><tr><td>train_loss_epoch</td><td>0.53341</td></tr><tr><td>train_loss_step</td><td>0.54419</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.50236</td></tr><tr><td>val_auc</td><td>0.81458</td></tr><tr><td>val_f1</td><td>0.66876</td></tr><tr><td>val_loss_epoch</td><td>0.90814</td></tr><tr><td>val_loss_step</td><td>0.92732</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_64_onehot_0</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/glxqbvbe' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/glxqbvbe</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240101_202408-glxqbvbe\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "944d2fe5264b49c9a030397026ca6196",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016916666666899498, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240101_205446-qb64csq9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/qb64csq9' target=\"_blank\">MLP_2_64_onehot_0</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/qb64csq9' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/qb64csq9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 5.1 K \n",
      "2 | head        | Sequential       | 2.1 K \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "7.2 K     Trainable params\n",
      "0         Non-trainable params\n",
      "7.2 K     Total params\n",
      "0.029     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce605693aa674584b6c4a765e5c30d0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▄▅▆▆▆▇▇▇▇▇▇▇▇▇██▇█████▇█▇▇██▇▇█▇▇▇██▇██</td></tr><tr><td>train_auc</td><td>██▇▆▄▄▃▂▃▃▂▂▂▂▂▂▂▁▁▁▁▁▂▂▂▂▂▂▂▂▃▂▃▂▂▂▂▂▂▂</td></tr><tr><td>train_f1</td><td>▁▄▅▆▆▆▇▇▇▇▇▇▇▇▇▇█▇█████▇█▇███▇██▇█▇██▇██</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▄▄▃▃▄▃▂▂▃▃▃▂▂▂▂▃▂▂▂▁▂▂▁▁▂▂▂▂▁▁▁▂▃▂▂▁▂▂▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>█▁▃▆▅▇█▇▇▅▅▇▆▆▆█▄▇▇▇▇▆▇▇▆▆▇▇▇█▅▇▇▇▇▇▇▆▅▅</td></tr><tr><td>val_auc</td><td>▇█▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_f1</td><td>█▁▃▆▅██▇█▆▅█▆█▆▇▅▇▇▇▇▆▇█▆▆█▇▇█▅▇▇▇█▇▇█▅▅</td></tr><tr><td>val_loss_epoch</td><td>█▇▆▄▃▄▃▂▃▃▂▃▂▃▂▁▂▂▂▂▂▂▂▂▂▂▂▁▁▂▃▁▂▁▃▁▂▄▃▂</td></tr><tr><td>val_loss_step</td><td>█▇▆▄▄▄▃▃▅▄▃▄▃▄▄▁▃▄▄▄▃▅▄▂▄▄▂▂▂▄▄▂▃▃▄▃▅▇▃▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.77259</td></tr><tr><td>train_auc</td><td>0.22895</td></tr><tr><td>train_f1</td><td>0.77861</td></tr><tr><td>train_loss_epoch</td><td>0.49524</td></tr><tr><td>train_loss_step</td><td>0.53915</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.7783</td></tr><tr><td>val_auc</td><td>0.1194</td></tr><tr><td>val_f1</td><td>0.75521</td></tr><tr><td>val_loss_epoch</td><td>0.45059</td></tr><tr><td>val_loss_step</td><td>0.42501</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_64_onehot_0</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/qb64csq9' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/qb64csq9</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240101_205446-qb64csq9\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "683a582d9f0e4a9cb109687e186e5e8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016933333332417533, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240101_212741-rwol2ahs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/rwol2ahs' target=\"_blank\">MLP_2_64_onehot_0</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/rwol2ahs' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/rwol2ahs</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 5.1 K \n",
      "2  | head        | Sequential       | 2.1 K \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 65    \n",
      "--------------------------------------------------\n",
      "7.3 K     Trainable params\n",
      "0         Non-trainable params\n",
      "7.3 K     Total params\n",
      "0.029     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f425f6009cb4430eaea1abbc60126d77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▆▇▇▇██████▇██▇███▇███▇▇████████████▇███</td></tr><tr><td>train_auc</td><td>▁▅▇▇▇███████████████████████████████████</td></tr><tr><td>train_f1</td><td>▁▆▇▇▇████▇█▇██▇█▇▇▇███▇▇▇█████████▇█▇███</td></tr><tr><td>train_loss_epoch</td><td>█▆▃▂▂▂▂▂▁▂▂▂▁▁▂▁▁▂▂▂▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▂▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▅▆▃▄▁▃▂▂▃▃▂▃▁▂▄▁▄▂▃▂▁▁▃▄▁▄▂▁▂▃▂▂▂▂▁▃▂▁▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▆█▇▇▆▇█▇▇▇█▇██▇██████▇█████▇▇█▇█▇██▇███</td></tr><tr><td>val_auc</td><td>▂▁▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇████████▇███</td></tr><tr><td>val_f1</td><td>▁▆█▇▇▆▇█▇▇▇████▇██████▇█████▇████▇██▇███</td></tr><tr><td>val_loss_epoch</td><td>█▄▂▂▂▃▂▂▂▂▂▁▁▁▁▂▁▁▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▂▁▁▂▁▁▁</td></tr><tr><td>val_loss_step</td><td>█▅▃▃▂▄▂▃▃▂▁▂▂▃▂▃▂▃▁▃▃▂▄▂▂▂▁▂▂▃▂▂▂▃▂▂▃▂▁▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.7596</td></tr><tr><td>train_auc</td><td>0.84841</td></tr><tr><td>train_f1</td><td>0.76268</td></tr><tr><td>train_loss_epoch</td><td>0.4804</td></tr><tr><td>train_loss_step</td><td>0.50103</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.79009</td></tr><tr><td>val_auc</td><td>0.88027</td></tr><tr><td>val_f1</td><td>0.79446</td></tr><tr><td>val_loss_epoch</td><td>0.44194</td></tr><tr><td>val_loss_step</td><td>0.46705</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_64_onehot_0</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/rwol2ahs' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/rwol2ahs</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240101_212741-rwol2ahs\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8c3a2a003dc40b7a91681b1f11909ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016933333332417533, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240101_220039-pwvotqcs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/pwvotqcs' target=\"_blank\">MLP_2_64_onehot_0</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/pwvotqcs' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/pwvotqcs</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 5.1 K \n",
      "2  | head        | Sequential       | 2.1 K \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 4.2 K \n",
      "--------------------------------------------------\n",
      "11.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "11.4 K    Total params\n",
      "0.046     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▅▇▇▇██▇▇▇██████████▇██████████████▇████</td></tr><tr><td>train_auc</td><td>▁▅▇▇████▇███████████████████████████████</td></tr><tr><td>train_f1</td><td>▁▄▆▇▇██▇▇▇██████████▇█▇████████████▇███▇</td></tr><tr><td>train_loss_epoch</td><td>█▆▄▃▂▂▂▂▂▁▁▁▂▁▁▁▁▁▁▂▁▁▁▁▂▁▁▁▁▂▁▂▁▁▁▂▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▆▅▃▃▂▄▂▂▄▃▄▃▃▃▃▄▃▂▁▃▂▂▁▄▂▅▄▂▃▃▂▂▂▂▁▃▂▂▄</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▇▇▁▇▅█▇▇▇█▇▇▇▇▇▇▇▇▇▇▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>val_auc</td><td>▁▂▅▆▇▇▇▇▇█▇█▇▇▇▇▇████▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>val_f1</td><td>█▇▁▇▅█▇▇▇█▇▇▇▇▇▇▇▇▇▇▇▆█▇█▇▇▇▇▇▇▇▇▇▇█▇█▇▇</td></tr><tr><td>val_loss_epoch</td><td>█▄▄▂▂▁▁▁▂▁▂▁▂▂▁▁▂▂▁▁▁▂▁▁▂▁▁▁▁▂▁▁▁▁▁▁▂▁▁▁</td></tr><tr><td>val_loss_step</td><td>█▅▄▃▃▃▃▃▃▂▄▂▄▄▂▁▄▃▂▂▃▃▃▃▃▂▃▄▂▃▃▃▁▂▂▃▃▁▁▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.76019</td></tr><tr><td>train_auc</td><td>0.84743</td></tr><tr><td>train_f1</td><td>0.75483</td></tr><tr><td>train_loss_epoch</td><td>0.49116</td></tr><tr><td>train_loss_step</td><td>0.55029</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.78538</td></tr><tr><td>val_auc</td><td>0.86899</td></tr><tr><td>val_f1</td><td>0.77966</td></tr><tr><td>val_loss_epoch</td><td>0.46394</td></tr><tr><td>val_loss_step</td><td>0.48397</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_64_onehot_0</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/pwvotqcs' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/pwvotqcs</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240101_220039-pwvotqcs\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9cd5400ab314e9496f5ab4a68f18f17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240101_223341-sxj9tvsm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/sxj9tvsm' target=\"_blank\">MLP_3_16_onehot_0</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/sxj9tvsm' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/sxj9tvsm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 768   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "922       Trainable params\n",
      "0         Non-trainable params\n",
      "922       Total params\n",
      "0.004     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80472004a24342529561386a0d67bb63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▂▂▅▇▇▇█▇▇█▇▇▇██████▇█▇███▇▇██▇█████▇█▇</td></tr><tr><td>train_auc</td><td>▁▁▂▂▄▇▇▇█▇███▇██████████████████████████</td></tr><tr><td>train_f1</td><td>▅▁▃▃▅▇▇▇▇▇▇█▇▇▇█▇█▇██▇█▇██▇▇▇▇█▇██▇████▇</td></tr><tr><td>train_loss_epoch</td><td>█▇▇▇▆▂▃▂▁▂▂▁▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>████▇▄▅▄▂▄▃▄▂▂▄▄▂▃▃▄▄▄▂▃▃▂▄▄▃▃▂▁▁▃▃▃▄▃▃▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▁▁▇███▇▇█▇▇█▇█▇▇▇▇▇▇▇███████████▇█▇▇▇▇█</td></tr><tr><td>val_auc</td><td>▁▇▇█████████████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁███▇▆▆▇▆▆▇▆▆▆▆▅▆▆▆▆▆▆▆▇▇▇██▆▇▇▆▇▆▆▅▆▇</td></tr><tr><td>val_loss_epoch</td><td>████▆▂▂▂▂▂▂▁▁▂▂▂▁▂▂▁▁▁▁▁▂▁▁▂▂▂▁▁▁▂▁▁▂▁▂▂</td></tr><tr><td>val_loss_step</td><td>████▆▄▂▄▃▂▂▂▂▃▂▃▂▃▃▂▂▂▁▂▃▂▃▃▄▃▃▂▃▃▂▁▃▂▃▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.74129</td></tr><tr><td>train_auc</td><td>0.81667</td></tr><tr><td>train_f1</td><td>0.73227</td></tr><tr><td>train_loss_epoch</td><td>0.52536</td></tr><tr><td>train_loss_step</td><td>0.51782</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.79481</td></tr><tr><td>val_auc</td><td>0.87182</td></tr><tr><td>val_f1</td><td>0.8</td></tr><tr><td>val_loss_epoch</td><td>0.46846</td></tr><tr><td>val_loss_step</td><td>0.51456</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_16_onehot_0</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/sxj9tvsm' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/sxj9tvsm</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240101_223341-sxj9tvsm\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f4a072607aa4c6ca3eb179e509e9312",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016933333332417533, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240101_230433-vq5ltt6s</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/vq5ltt6s' target=\"_blank\">MLP_3_16_onehot_0</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/vq5ltt6s' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/vq5ltt6s</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 768   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "922       Trainable params\n",
      "0         Non-trainable params\n",
      "922       Total params\n",
      "0.004     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▂▂▁▃▄▄▅▄▅▅▅▄▅▆▆▇▇▆▇▇▆▇▇▇█▆▇▆▇▇▇▇▇▇▇▆▇▇</td></tr><tr><td>train_auc</td><td>▁▂▂▂▂▂▃▄▅▄▆▅▅▅▅▆▆▇▇▆▇▇▇▇▇▇█▆▇▇▇▇▇██▇▇▇▇▇</td></tr><tr><td>train_f1</td><td>▅▄▂▂▁▁▂▅▆▅▅▆▇▆▇▆▇▇█▆▆▆▇█▆▅█▅▇▇█▆▆▇▆▇▇▇▇▆</td></tr><tr><td>train_loss_epoch</td><td>█▆▆▆▅▅▅▄▄▅▄▄▄▄▄▃▃▁▂▃▂▂▂▂▂▂▁▃▂▂▂▂▂▁▁▂▂▂▂▂</td></tr><tr><td>train_loss_step</td><td>█▇▇▇▇▆█▆▆▇▆▆▆▆▆▅▆▄▆▄▅▇▃▃▆▄█▃▄▄▆▃▁▃▃▃▄▆▅▄</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>▂▁▄▄▄▄▄▄▄▂▇▇▇█▇███▇███████████████████▇▇</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_epoch</td><td>▁▂▁▁▁▁▁▂▄▄▄▅▆▅▅▂▂▂▄▃▄▅▅▇▆▅▆▇█▆▆▇▇▇██▇█▇█</td></tr><tr><td>val_loss_step</td><td>▂▂▁▂▂▁▁▂▄▂▃▅▄▃▄▂▂▂▃▃▅▆▄▅▆▄▅▇█▅▄▇▇▅▇▇▄▆▆█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.63615</td></tr><tr><td>train_auc</td><td>0.67939</td></tr><tr><td>train_f1</td><td>0.60714</td></tr><tr><td>train_loss_epoch</td><td>0.64399</td></tr><tr><td>train_loss_step</td><td>0.64266</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.50236</td></tr><tr><td>val_auc</td><td>0.76317</td></tr><tr><td>val_f1</td><td>0.66876</td></tr><tr><td>val_loss_epoch</td><td>0.74329</td></tr><tr><td>val_loss_step</td><td>0.77065</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_16_onehot_0</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/vq5ltt6s' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/vq5ltt6s</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240101_230433-vq5ltt6s\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "955f318500c145ba9fda17e394258b9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240101_233603-d4fl4ivu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/d4fl4ivu' target=\"_blank\">MLP_3_16_onehot_0</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/d4fl4ivu' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/d4fl4ivu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 768   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "922       Trainable params\n",
      "0         Non-trainable params\n",
      "922       Total params\n",
      "0.004     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "551ec0a779234032a632e480ef20a312",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▃▃▄▅▅▆▆▆▇▇▇▇▇▇██▇▇▇▇▇▇▇▇▇▇▇▇█▇█▇██████</td></tr><tr><td>train_auc</td><td>▁▁▂▂▂▃▅▅▆▆▇▇▇▇▇▇█▇▇▇▇▇▇▇▇▇▇▆▇▇▇▆▇▆▇▇▆▆▆▆</td></tr><tr><td>train_f1</td><td>▂▁▄▄▅▄▅▆▆▆▇▇▆▇▇▇██▇▇█▇▇█▇▇▇▇▇▇█▇▇▇██████</td></tr><tr><td>train_loss_epoch</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▃▂▂▂▂▂▂▂▂▂▂▂▁▂▂▁▁▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▄▄▅▇▇▆▆▅█▆▆█▆▇▇█▇██▇▇▇▇▇▇▇██▇▆█▇▇█▆▇▇▇█</td></tr><tr><td>val_auc</td><td>▁▂▄▅▇▇▇▇████████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▅▅▆█▇▆▆▆█▆▇█▆▇▇█▇██▇▇▇▇▇▇▇███▆█▇▇█▆▇▇▇█</td></tr><tr><td>val_loss_epoch</td><td>█▆▆▆▅▄▃▄▃▂▃▂▂▂▂▂▁▂▂▂▂▂▁▂▂▁▂▂▁▁▃▁▁▂▁▂▂▂▁▁</td></tr><tr><td>val_loss_step</td><td>█▆▅▅▄▅▃▄▃▂▃▂▁▃▂▂▁▂▂▁▁▁▁▂▂▂▂▂▂▁▃▂▁▂▂▁▂▂▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.73597</td></tr><tr><td>train_auc</td><td>0.66242</td></tr><tr><td>train_f1</td><td>0.73752</td></tr><tr><td>train_loss_epoch</td><td>0.54584</td></tr><tr><td>train_loss_step</td><td>0.55748</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.80896</td></tr><tr><td>val_auc</td><td>0.88434</td></tr><tr><td>val_f1</td><td>0.8</td></tr><tr><td>val_loss_epoch</td><td>0.47538</td></tr><tr><td>val_loss_step</td><td>0.50376</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_16_onehot_0</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/d4fl4ivu' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/d4fl4ivu</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240101_233603-d4fl4ivu\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5eae7a1863b4386bec4958d95206eb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240102_000622-6rvrtdq0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/6rvrtdq0' target=\"_blank\">MLP_3_16_onehot_0</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/6rvrtdq0' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/6rvrtdq0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 768   \n",
      "2  | head        | Sequential       | 154   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 17    \n",
      "--------------------------------------------------\n",
      "939       Trainable params\n",
      "0         Non-trainable params\n",
      "939       Total params\n",
      "0.004     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1787bd68979429e8abf21042ddf3f08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▂▃▆▆▇▇▇▇▇██▇█▇███▇████▇▇█▇██▇█████████</td></tr><tr><td>train_auc</td><td>▁▂▂▃▅▇▇▇▇▇▇██▇███████████▇██████████████</td></tr><tr><td>train_f1</td><td>▅▁▃▄▅▆▇▇▇▇▇▇█▇█████▇████▇██▇██▇██▇████▇█</td></tr><tr><td>train_loss_epoch</td><td>█▇▇▇▅▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▁▁▁▂▁▁▂▂▂</td></tr><tr><td>train_loss_step</td><td>████▆▅▅▅▄▂▃▃▂▁▂▃▃▂▄▁▃▄▃▃▂▃▃▄▃▂▃▂▁▂▁▃▄▁▁▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▁▇▆▅██▆█▇██▇██▆▇███▇▇███▇▇█▇▇█▇▇██▇█▇█▇</td></tr><tr><td>val_auc</td><td>▁▇▇█████████████████████████████████████</td></tr><tr><td>val_f1</td><td>▇▁▇▇▆██▇████▇██▇█████▇███▇▇█████████████</td></tr><tr><td>val_loss_epoch</td><td>███▇▅▄▂▂▂▁▂▁▂▂▂▂▂▁▁▁▁▁▁▁▁▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_step</td><td>███▇▆▅▄▂▃▂▃▂▂▃▃▄▃▂▂▃▂▂▃▁▂▃▃▂▂▄▂▂▃▂▁▂▂▃▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.75546</td></tr><tr><td>train_auc</td><td>0.82196</td></tr><tr><td>train_f1</td><td>0.76069</td></tr><tr><td>train_loss_epoch</td><td>0.52738</td></tr><tr><td>train_loss_step</td><td>0.5007</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.79009</td></tr><tr><td>val_auc</td><td>0.87217</td></tr><tr><td>val_f1</td><td>0.78346</td></tr><tr><td>val_loss_epoch</td><td>0.45786</td></tr><tr><td>val_loss_step</td><td>0.47126</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_16_onehot_0</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/6rvrtdq0' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/6rvrtdq0</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240102_000622-6rvrtdq0\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e084348e3f44cf0bcbd99bca0129cf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016916666666899498, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240102_003646-6tvsbi3m</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/6tvsbi3m' target=\"_blank\">MLP_3_16_onehot_0</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/6tvsbi3m' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/6tvsbi3m</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 768   \n",
      "2  | head        | Sequential       | 154   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 281   \n",
      "--------------------------------------------------\n",
      "1.2 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.2 K     Total params\n",
      "0.005     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a2819ca7c854c0a8c8c4a51b31e6cfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▁▄▆▆▇▇█▇█▇▇▇█▇████▇████▇████████▇█████</td></tr><tr><td>train_auc</td><td>▁▁▁▃▆▆▇▇▇▇██▇███████████████████████████</td></tr><tr><td>train_f1</td><td>▅▁▃▄▆▆▇▇█▇▇▇███▇▇██▇▇▇▇▇███████▇██▇▇███▇</td></tr><tr><td>train_loss_epoch</td><td>██▇▇▅▅▃▂▂▂▂▂▂▂▂▂▂▁▁▂▂▁▂▁▁▂▂▁▁▁▁▁▂▁▂▁▁▁▂▁</td></tr><tr><td>train_loss_step</td><td>███▇▅▄▄▅▃▄▁▂▄▂▂▃▃▂▄▅▂▂▂▂▄▃▂▂▃▂▂▂▄▃▄▁▂▃▃▄</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▇▁▇▅▆▆█▇██▇▆▇▇██▇██████▇▇▇▇▇▇▇█▇▇██▇▇▇█</td></tr><tr><td>val_auc</td><td>▁▇▇█▇███████████████████████████████████</td></tr><tr><td>val_f1</td><td>▄▇▄▆▁▃▄█▅▇█▆▅▆▆██▇▇██▇█▇▇▇▆▇▇▆▆█▆▇██▇▆▆▇</td></tr><tr><td>val_loss_epoch</td><td>███▇▄▄▃▁▂▁▁▂▂▂▂▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_step</td><td>███▇▄▄▅▂▂▂▂▂▃▂▄▂▂▂▂▁▂▂▁▁▂▂▂▁▁▂▂▁▂▁▁▁▂▃▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.74897</td></tr><tr><td>train_auc</td><td>0.83046</td></tr><tr><td>train_f1</td><td>0.7391</td></tr><tr><td>train_loss_epoch</td><td>0.51725</td></tr><tr><td>train_loss_step</td><td>0.57483</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.79953</td></tr><tr><td>val_auc</td><td>0.87173</td></tr><tr><td>val_f1</td><td>0.8</td></tr><tr><td>val_loss_epoch</td><td>0.45442</td></tr><tr><td>val_loss_step</td><td>0.44893</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_16_onehot_0</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/6tvsbi3m' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/6tvsbi3m</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240102_003646-6tvsbi3m\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d2a345f78804dc88dd093bac84a81e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01693333333435779, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240102_010658-oxeo80p5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/oxeo80p5' target=\"_blank\">MLP_3_32_onehot_0</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/oxeo80p5' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/oxeo80p5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 2.6 K \n",
      "2 | head        | Sequential       | 562   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "3.1 K     Trainable params\n",
      "0         Non-trainable params\n",
      "3.1 K     Total params\n",
      "0.012     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50b6d662aa624c8ba00999badd245b41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▃▆▇▇▇█▇▇▇█▇██████████████▇█████████████</td></tr><tr><td>train_auc</td><td>▁▃▆▇▇▇██████████████████████████████████</td></tr><tr><td>train_f1</td><td>▂▁▆▇▇▇█▇█▇█▇██████████████▇███████▇█████</td></tr><tr><td>train_loss_epoch</td><td>██▅▃▃▂▂▂▂▂▂▂▂▁▂▂▁▁▁▁▁▁▂▂▂▂▂▁▁▁▁▁▂▂▂▁▂▁▁▁</td></tr><tr><td>train_loss_step</td><td>██▆▄▄▂▄▃▄▄▃▄▄▅▃▄▃▃▄▄▃▄▄▂▃▂▃▃▂▃▄▄▅▃▃▂▃▃▂▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▇▆▆██▅█▇█▇█▇▇██▇▇▇▇▇█▇▇▇▇█▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>val_auc</td><td>▁▅▄▇██████████████▇█▇████▇▇██▇▇█▇███▇▇█▇</td></tr><tr><td>val_f1</td><td>▃▅▅▃██▁█▅█▆▇▆▇▇▇▇▆▆▇▇▇▅▇▇▅█▇▆▆▇▇▆▇▇▇▆▆▆▇</td></tr><tr><td>val_loss_epoch</td><td>█▇▃▂▂▁▃▁▂▁▂▁▂▁▁▁▁▁▂▁▁▁▂▁▁▂▁▂▂▁▁▁▁▁▂▁▁▁▁▁</td></tr><tr><td>val_loss_step</td><td>█▇▄▃▂▂▄▂▂▂▃▃▃▁▃▃▃▃▃▂▂▁▃▂▂▃▂▄▄▂▁▃▁▂▃▂▂▂▃▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.76728</td></tr><tr><td>train_auc</td><td>0.83794</td></tr><tr><td>train_f1</td><td>0.76005</td></tr><tr><td>train_loss_epoch</td><td>0.49218</td></tr><tr><td>train_loss_step</td><td>0.42286</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.78774</td></tr><tr><td>val_auc</td><td>0.87097</td></tr><tr><td>val_f1</td><td>0.78571</td></tr><tr><td>val_loss_epoch</td><td>0.45636</td></tr><tr><td>val_loss_step</td><td>0.4628</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_32_onehot_0</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/oxeo80p5' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/oxeo80p5</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240102_010658-oxeo80p5\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "337b866252034246b2602c96e0e8515e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01691666666495924, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240102_013727-v4d3o0hj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/v4d3o0hj' target=\"_blank\">MLP_3_32_onehot_0</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/v4d3o0hj' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/v4d3o0hj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 2.6 K \n",
      "2 | head        | Sequential       | 562   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "3.1 K     Trainable params\n",
      "0         Non-trainable params\n",
      "3.1 K     Total params\n",
      "0.012     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36946a25ac4b45ffaedf4760eec547fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▃▆▇▆▇▇▇▇▇▇▇█▇▇████████▇█████▇█▇█▇▇▇█▇██</td></tr><tr><td>train_auc</td><td>▁▂▅▇▆▇▇▇▇▇▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇██▇▇▇█▇▇▇▇█▇█</td></tr><tr><td>train_f1</td><td>▃▁▆▇▆▆▆▇▇▇▇▆▇▇▇▇▇▇█████▇▇█▇██▇█▇██▇▇▇▇▇█</td></tr><tr><td>train_loss_epoch</td><td>█▇▅▄▄▃▃▃▂▃▃▃▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▂▂▂▃▂▃▁▂▂▁</td></tr><tr><td>train_loss_step</td><td>██▆▆▅▄▄▄▄▄▂▅▅▇▃▅▄▃▃▇▂▅▂▄▅▃▃▅▂▃▂▄▄▃▃▁▅▃▄▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▂███████████████▅▅██████▅██████████</td></tr><tr><td>val_auc</td><td>▁▇▇▇▇▇▇▇▇▇▇▇▇█████████████▇█▇█▇██▇▇█▇▇▇▇</td></tr><tr><td>val_f1</td><td>▅▁▁▁▁▂███████████████▇▇██████▇██████████</td></tr><tr><td>val_loss_epoch</td><td>██▅▅▆▄▅▄▄▄▄▄▄▃▄▃▄▄▄▄▄▄▄▄▃▄▃▄▃▃▃▃▃▂▂▂▁▂▂▁</td></tr><tr><td>val_loss_step</td><td>██▆▇▇▅▅▅▅▅▅▅▅▄▅▄▅▅▅▄▅▄▅▅▄▄▅▅▃▃▃▅▄▃▄▃▂▃▄▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.71589</td></tr><tr><td>train_auc</td><td>0.76547</td></tr><tr><td>train_f1</td><td>0.71111</td></tr><tr><td>train_loss_epoch</td><td>0.56656</td></tr><tr><td>train_loss_step</td><td>0.51933</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.77123</td></tr><tr><td>val_auc</td><td>0.79453</td></tr><tr><td>val_f1</td><td>0.80561</td></tr><tr><td>val_loss_epoch</td><td>0.5947</td></tr><tr><td>val_loss_step</td><td>0.57169</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_32_onehot_0</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/v4d3o0hj' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/v4d3o0hj</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240102_013727-v4d3o0hj\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f82c2d990ead4243a7f76ea89c80d655",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.017183333336530875, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240102_020812-hj4riqjp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/hj4riqjp' target=\"_blank\">MLP_3_32_onehot_0</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/hj4riqjp' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/hj4riqjp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 2.6 K \n",
      "2 | head        | Sequential       | 562   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "3.1 K     Trainable params\n",
      "0         Non-trainable params\n",
      "3.1 K     Total params\n",
      "0.012     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▂▄▅▆▆▇▆▇▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇███▇█▇▇█▇▇▇█</td></tr><tr><td>train_auc</td><td>▃▃▂▄▆▇▇█▇█▇▆▇█▆▆▅▅▅▄▄▃▃▄▁▃▂▂▃▃▄▃▄▃▃▄▃▄▃▄</td></tr><tr><td>train_f1</td><td>▁▃▁▃▄▅▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇█▇█▇▇███▇█▇▇▇▇▇▇▇█</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▄▄▃▃▃▂▃▂▂▃▂▂▃▂▃▂▂▂▂▂▂▂▁▂▂▂▂▁▂▂▂▃▂▂▂▂▂▂▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▁▄▃▄▄▂▆▇▆█▆▆██▇█▇▇▇██▅█▆██▇▇▇█▇▇█▇███▇▇</td></tr><tr><td>val_auc</td><td>▅▃▃▇█▇████████████▅▆▃▄▂▁▁▁▁▁▁▁▂▂▆▅▃▄▂▁▄▅</td></tr><tr><td>val_f1</td><td>▁▁▅▄▅▅▃▇▇▆█▆▇██▇█▇▇▇██▆█▇██▇██████████▇▇</td></tr><tr><td>val_loss_epoch</td><td>█▄▄▄▃▃▄▂▂▂▂▂▂▁▂▂▁▂▂▁▁▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_step</td><td>█▅▅▅▄▄▆▃▃▃▃▃▃▂▃▃▂▃▃▂▃▁▃▂▃▃▂▃▃▂▂▃▁▂▂▂▂▂▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.78145</td></tr><tr><td>train_auc</td><td>0.50028</td></tr><tr><td>train_f1</td><td>0.78029</td></tr><tr><td>train_loss_epoch</td><td>0.48833</td></tr><tr><td>train_loss_step</td><td>0.4254</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.78066</td></tr><tr><td>val_auc</td><td>0.53201</td></tr><tr><td>val_f1</td><td>0.75067</td></tr><tr><td>val_loss_epoch</td><td>0.47094</td></tr><tr><td>val_loss_step</td><td>0.49728</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_32_onehot_0</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/hj4riqjp' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/hj4riqjp</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240102_020812-hj4riqjp\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "952661da48774d3fbb777308ea4818a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01693333333435779, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240102_023819-y807hdcf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/y807hdcf' target=\"_blank\">MLP_3_32_onehot_0</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/y807hdcf' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/y807hdcf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 2.6 K \n",
      "2  | head        | Sequential       | 562   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 33    \n",
      "--------------------------------------------------\n",
      "3.2 K     Trainable params\n",
      "0         Non-trainable params\n",
      "3.2 K     Total params\n",
      "0.013     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▅▆▇▇█▇▇▇▇▇▇███▇████████████████████▇██</td></tr><tr><td>train_auc</td><td>▁▂▅▆▇██▇█▇██████████████████████████████</td></tr><tr><td>train_f1</td><td>▄▁▅▇▇▇███▇█▇████████████████████████████</td></tr><tr><td>train_loss_epoch</td><td>██▆▅▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▁▂▂▂▁▂▁▂▂▁▂▂▂▁▁▁▂</td></tr><tr><td>train_loss_step</td><td>██▇▅▅▃▄▄▅▂▃▄▃▄▃▄▃▄▃▂▃▁▃▂▃▃▃▄▃▃▅▄▂▃▃▃▂▃▄▄</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▆▅▅███▇▇███▇██▇▇██▇█▇▇███▇▇▇█▇▇██▇▇▇███</td></tr><tr><td>val_auc</td><td>▅▅▁▆▇▇█████████████▇▇█▇██▇▇████████████▇</td></tr><tr><td>val_f1</td><td>▄▃▃▁███▆▇█▇█▇███▇█▇▇▇▆██▇█▇█▆█▇█▇█▇█▇██▇</td></tr><tr><td>val_loss_epoch</td><td>██▄▄▂▂▁▂▂▁▁▁▂▁▁▂▁▁▁▁▁▂▂▁▁▁▁▂▂▁▁▂▁▁▁▂▁▁▁▁</td></tr><tr><td>val_loss_step</td><td>██▄▅▃▃▁▃▂▂▁▂▃▁▂▃▂▂▂▂▂▃▃▁▃▁▂▂▃▁▃▂▄▂▁▂▃▃▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.76137</td></tr><tr><td>train_auc</td><td>0.83277</td></tr><tr><td>train_f1</td><td>0.76263</td></tr><tr><td>train_loss_epoch</td><td>0.51091</td></tr><tr><td>train_loss_step</td><td>0.54216</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.78774</td></tr><tr><td>val_auc</td><td>0.87084</td></tr><tr><td>val_f1</td><td>0.78155</td></tr><tr><td>val_loss_epoch</td><td>0.458</td></tr><tr><td>val_loss_step</td><td>0.45859</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_32_onehot_0</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/y807hdcf' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/y807hdcf</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240102_023819-y807hdcf\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7d03b7426eb4d7294e37a7d3ced764c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016933333330477276, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240102_030858-zar660f7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/zar660f7' target=\"_blank\">MLP_3_32_onehot_0</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/zar660f7' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/zar660f7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 2.6 K \n",
      "2  | head        | Sequential       | 562   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 1.1 K \n",
      "--------------------------------------------------\n",
      "4.2 K     Trainable params\n",
      "0         Non-trainable params\n",
      "4.2 K     Total params\n",
      "0.017     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▅▆▇▇█▇▇▇▇▇▇▇█▇▇█▇▇▇███████▇▇██▇▇██▇▇██</td></tr><tr><td>train_auc</td><td>▁▂▆▇▇█████▇▇████████████████████████████</td></tr><tr><td>train_f1</td><td>▁▂▅▆▇▇█▇▇▇▇▇▇▇█▇▇▇█▇▇███████▇██▇█▇██▇▇██</td></tr><tr><td>train_loss_epoch</td><td>██▅▃▃▂▁▂▂▂▂▂▁▂▁▁▂▂▂▁▂▁▂▁▂▁▁▁▂▁▁▁▁▁▁▁▂▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▇▅▃▃▂▃▃▂▃▅▃▁▂▁▂▂▃▃▂▂▂▃▃▄▄▃▂▃▁▃▂▂▃▃▃▃▃▂▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▇▆▇██████▇▇█▇█▇█▇▇████▇▇▇█▇█▇▇█████▇█▇█</td></tr><tr><td>val_auc</td><td>▂▄▁▆▇█████▇███▇▇▇▇▇█▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇▇▇▇▇</td></tr><tr><td>val_f1</td><td>▁▇▃▄▆█▆█▇▇▄▃█▆▇▆▇▇▅▇█▇▇▆▆▆▇▆▇▆▆█▇█▇▇▆▇▆▇</td></tr><tr><td>val_loss_epoch</td><td>█▇▃▂▁▁▁▁▁▁▂▂▁▁▁▂▁▂▂▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_step</td><td>█▇▄▄▂▃▂▃▃▃▅▄▃▂▃▃▂▃▃▄▂▂▂▃▂▃▂▂▃▂▃▁▄▂▃▃▄▂▂▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.76905</td></tr><tr><td>train_auc</td><td>0.84304</td></tr><tr><td>train_f1</td><td>0.76959</td></tr><tr><td>train_loss_epoch</td><td>0.49164</td></tr><tr><td>train_loss_step</td><td>0.47595</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.79245</td></tr><tr><td>val_auc</td><td>0.87048</td></tr><tr><td>val_f1</td><td>0.79048</td></tr><tr><td>val_loss_epoch</td><td>0.45858</td></tr><tr><td>val_loss_step</td><td>0.48144</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_32_onehot_0</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/zar660f7' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/zar660f7</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240102_030858-zar660f7\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47f0082044e04e17b92775124c0b9b79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240102_033924-qajgk5mw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/qajgk5mw' target=\"_blank\">MLP_3_64_onehot_0</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/qajgk5mw' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/qajgk5mw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 9.2 K \n",
      "2 | head        | Sequential       | 2.1 K \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "11.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "11.4 K    Total params\n",
      "0.045     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▅█▇█▇▇▇█▇███▇█████▇▇███████████████▇███</td></tr><tr><td>train_auc</td><td>▁▆▇▇▇▇███▇██████████████████████████████</td></tr><tr><td>train_f1</td><td>▁▅█▆█▇▇▇▇▇▇▇██▇█████▇████▇███▇██████▇█▇█</td></tr><tr><td>train_loss_epoch</td><td>█▆▃▂▂▃▂▂▂▂▂▂▂▂▁▂▁▁▁▂▂▁▁▁▂▁▂▁▁▁▂▁▁▁▂▁▂▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▅▄▃▃▄▃▂▃▄▁▁▁▃▂▄▄▁▂▃▃▃▃▂▁▃▃▁▃▂▃▂▂▁▂▁▃▃▁▅</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁█▅█▇▇█▇██▇▇▇▇█▇█▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>val_auc</td><td>▁▄▆▇████▇▇███▇█▇█▇▇▇▇▇▇▇▆▇▇▇▆▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>val_f1</td><td>▁█▆█▇▇█▇███▇█▇███▇█▇█▇██▇█▇▇▇███▇██▇█▇██</td></tr><tr><td>val_loss_epoch</td><td>█▃▃▂▁▂▁▁▁▁▁▂▁▂▂▁▁▂▁▂▁▂▁▂▂▁▁▂▂▁▁▂▁▁▁▁▁▂▁▁</td></tr><tr><td>val_loss_step</td><td>█▄▃▃▁▃▂▃▂▃▃▃▂▄▃▃▂▄▂▃▃▄▃▄▃▃▂▃▄▃▂▄▁▃▃▂▂▃▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.77259</td></tr><tr><td>train_auc</td><td>0.84621</td></tr><tr><td>train_f1</td><td>0.76737</td></tr><tr><td>train_loss_epoch</td><td>0.49341</td></tr><tr><td>train_loss_step</td><td>0.5885</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.79481</td></tr><tr><td>val_auc</td><td>0.86977</td></tr><tr><td>val_f1</td><td>0.80092</td></tr><tr><td>val_loss_epoch</td><td>0.45243</td></tr><tr><td>val_loss_step</td><td>0.43882</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_64_onehot_0</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/qajgk5mw' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/qajgk5mw</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240102_033924-qajgk5mw\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f52c95c240d474f8bbaf0f229995d78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01693333333435779, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240102_040956-8gwrdyd4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/8gwrdyd4' target=\"_blank\">MLP_3_64_onehot_0</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/8gwrdyd4' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/8gwrdyd4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 9.2 K \n",
      "2 | head        | Sequential       | 2.1 K \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "11.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "11.4 K    Total params\n",
      "0.045     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▆▆▆▆▇▇▇▆▇▇▇▇▇▇▇▇▇█▇▇█▇███▇█▇████▇█████</td></tr><tr><td>train_auc</td><td>▁▁▅▆▆▇▇▇▇▇▇▇▇▇▇▇██▇█▇██▇███▇█▇██▇███▇███</td></tr><tr><td>train_f1</td><td>▁▄▆▅▅▆▆▇▇▆▇▇▇▇▆▇▇█▇█▇██▇█████▇████▇█▇███</td></tr><tr><td>train_loss_epoch</td><td>█▇▅▄▄▃▃▃▃▃▂▃▂▃▂▂▂▂▂▁▂▂▂▂▁▁▁▂▂▂▁▁▂▂▂▁▂▁▁▁</td></tr><tr><td>train_loss_step</td><td>██▇▆▅▃▄▄▅▅▃▃▂▅▃▄▅▂▂▃▃▃▄▄▃▃▄▃▂▂▄▂▃▂▂▅▂▄▁▄</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▁██████▅▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>▁▃█▆█▇▇▇▆▆▅▆▆▆▆▆▆▆▆▆▆▆▆▆▇▇▆▆▆▆▆▆▆▆▆▆▆▆▆▆</td></tr><tr><td>val_f1</td><td>▁▁██████▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>val_loss_epoch</td><td>▃▃▁▁▁▁▁▁▁▂▂▂▃▂▃▃▄▄▅▄▄▆▆▆▇▆█▆█▇█▇▅▆▆▇▅▆▇▇</td></tr><tr><td>val_loss_step</td><td>▃▃▁▁▁▁▂▂▁▂▁▂▃▂▃▄▄▄▅▄▄▆▆▆▅▆█▆▇▇▇▇▆▇▅▆▄▇▆▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.74719</td></tr><tr><td>train_auc</td><td>0.81083</td></tr><tr><td>train_f1</td><td>0.74123</td></tr><tr><td>train_loss_epoch</td><td>0.5221</td></tr><tr><td>train_loss_step</td><td>0.57494</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.50236</td></tr><tr><td>val_auc</td><td>0.80346</td></tr><tr><td>val_f1</td><td>0.66876</td></tr><tr><td>val_loss_epoch</td><td>0.84049</td></tr><tr><td>val_loss_step</td><td>0.81115</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_64_onehot_0</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/8gwrdyd4' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/8gwrdyd4</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240102_040956-8gwrdyd4\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fce8f40c7aa942f581256469455195d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240102_044009-tdn042jx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/tdn042jx' target=\"_blank\">MLP_3_64_onehot_0</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/tdn042jx' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/tdn042jx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 9.2 K \n",
      "2 | head        | Sequential       | 2.1 K \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "11.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "11.4 K    Total params\n",
      "0.045     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fbe93b41f4a4cfc923a0b264e86c6ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▂▄▆▆▇▇▇▇▇▇▇▇▇▇▇█▇▇▇█▇▇█▇▇▇▇▇██████▇███</td></tr><tr><td>train_auc</td><td>▁▁▃▅▇▇▇█▇▇▇█▇▇▇▇██▇▇▇▇▇▇▇▇█▇█▇████▇█▇▇▇▇</td></tr><tr><td>train_f1</td><td>▃▁▃▅▆▆▇▇▇▇▇▇▇▇▇█▇█▇▇▇█████▇██▇██████▇███</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▃▂▂▂▂▁▁▁▂▁▁▁▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▃▇▆▅█▆█▇▇▇█▇▇▇▇█▇█▇███▇███▇▇███████████</td></tr><tr><td>val_auc</td><td>▃▁▆▇█▇███▇██▇█▇█▇███████████████████████</td></tr><tr><td>val_f1</td><td>▁▇▇▇▆█▆███▇██▇█████████████▇████████████</td></tr><tr><td>val_loss_epoch</td><td>█▇▆▄▃▃▃▂▂▃▂▂▃▃▂▂▂▂▂▂▁▂▂▂▁▂▂▃▁▁▂▂▁▁▁▁▂▁▁▁</td></tr><tr><td>val_loss_step</td><td>█▇▆▅▃▄▃▃▃▄▃▃▃▄▄▃▃▄▃▃▂▃▃▃▃▂▃▄▃▃▂▄▁▃▂▂▂▃▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.78972</td></tr><tr><td>train_auc</td><td>0.78984</td></tr><tr><td>train_f1</td><td>0.78528</td></tr><tr><td>train_loss_epoch</td><td>0.4861</td></tr><tr><td>train_loss_step</td><td>0.58484</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.79953</td></tr><tr><td>val_auc</td><td>0.88103</td></tr><tr><td>val_f1</td><td>0.80638</td></tr><tr><td>val_loss_epoch</td><td>0.44194</td></tr><tr><td>val_loss_step</td><td>0.40671</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_64_onehot_0</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/tdn042jx' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/tdn042jx</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240102_044009-tdn042jx\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27ad15ec54344c0593fa17f5d4369967",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.017183333332650364, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240102_050951-w37lofo1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/w37lofo1' target=\"_blank\">MLP_3_64_onehot_0</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/w37lofo1' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/w37lofo1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 9.2 K \n",
      "2  | head        | Sequential       | 2.1 K \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 65    \n",
      "--------------------------------------------------\n",
      "11.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "11.4 K    Total params\n",
      "0.046     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▅▇▇▇█▇▇███▇▇▇████████▇▇▇▇▇██▇█▇▇████▇██</td></tr><tr><td>train_auc</td><td>▁▆▇▇███████▇██████████▇▇█████▇██████████</td></tr><tr><td>train_f1</td><td>▁▅▇▇▇██████▇▇▇█████████▇███████▇████████</td></tr><tr><td>train_loss_epoch</td><td>█▅▃▃▂▂▂▂▂▂▂▃▂▂▁▂▁▂▂▁▂▁▂▂▂▂▂▁▂▂▁▂▂▂▂▁▂▁▂▁</td></tr><tr><td>train_loss_step</td><td>█▅▅▃▄▄▂▁▃▁▃▃▃▂▃▃▂▄▃▂▂▄▄▂▃▄▃▂▃▄▂▂▄▂▂▁▅▃▂▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▅▇▇▇██▇███▇█████▇▇▇█▇▆████▇▇▆▇█▇█▇▇▇▇▇▇</td></tr><tr><td>val_auc</td><td>▂▁▅▆▇▇▇▇▇▇▇▇██▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇██▇▇▇█</td></tr><tr><td>val_f1</td><td>▁▆▇▇██████████████▇▇█▇▇█████▇▇██▇████▇█▇</td></tr><tr><td>val_loss_epoch</td><td>█▄▂▁▂▁▁▁▁▁▁▂▁▁▁▁▂▂▁▂▁▂▂▁▁▁▁▁▂▂▁▁▂▂▁▁▁▂▂▂</td></tr><tr><td>val_loss_step</td><td>█▄▄▂▃▃▃▃▃▂▃▂▂▂▂▃▄▂▂▂▂▂▂▂▁▁▂▃▂▄▂▂▃▄▃▃▃▂▃▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.76787</td></tr><tr><td>train_auc</td><td>0.85154</td></tr><tr><td>train_f1</td><td>0.76977</td></tr><tr><td>train_loss_epoch</td><td>0.47754</td></tr><tr><td>train_loss_step</td><td>0.46087</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.76651</td></tr><tr><td>val_auc</td><td>0.87273</td></tr><tr><td>val_f1</td><td>0.74016</td></tr><tr><td>val_loss_epoch</td><td>0.47029</td></tr><tr><td>val_loss_step</td><td>0.45689</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_64_onehot_0</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/w37lofo1' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/w37lofo1</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240102_050951-w37lofo1\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b93e2387572420ea37683708ea8e8da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240102_053949-rmivsm7j</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/rmivsm7j' target=\"_blank\">MLP_3_64_onehot_0</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/rmivsm7j' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/rmivsm7j</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 9.2 K \n",
      "2  | head        | Sequential       | 2.1 K \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 4.2 K \n",
      "--------------------------------------------------\n",
      "15.6 K    Trainable params\n",
      "0         Non-trainable params\n",
      "15.6 K    Total params\n",
      "0.062     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba3ef387c7394fa29c077bb3a901a757",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▃▇▇▇████▇██████████████████▇██████▇████</td></tr><tr><td>train_auc</td><td>▁▃▇▇▇███████████████████████████████████</td></tr><tr><td>train_f1</td><td>▁▃▆▆▇▇█▇█▇▇███▇▇▇█▇▇██▇█▇▇██▇▇▇███▇▇▇▇██</td></tr><tr><td>train_loss_epoch</td><td>█▇▃▃▂▂▂▂▂▂▂▁▂▂▂▂▂▁▂▂▁▁▂▁▁▁▁▁▁▁▁▂▁▁▂▂▁▂▁▂</td></tr><tr><td>train_loss_step</td><td>█▇▄▂▃▂▂▃▃▃▄▁▃▂▁▂▁▂▃▂▂▄▂▁▃▁▃▃▃▃▁▂▁▃▂▃▁▃▂▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁█▇█████▇█▇▇███▇▇████▇███▇██▇███████████</td></tr><tr><td>val_auc</td><td>▁▄▄▇██▇▇▇███▇▇▇▇█▇▇▇▇▇▇▇▇▇█████████▇████</td></tr><tr><td>val_f1</td><td>▁███████▇█▇████▇▇████▇██████████████████</td></tr><tr><td>val_loss_epoch</td><td>█▇▂▂▁▁▁▂▂▁▂▂▁▁▁▂▂▁▁▁▁▂▁▁▁▂▂▁▁▂▁▁▁▁▁▂▁▁▁▁</td></tr><tr><td>val_loss_step</td><td>█▇▅▃▃▂▂▄▂▂▂▃▃▂▂▃▂▁▂▃▂▃▃▃▂▄▄▃▂▄▂▂▃▂▃▃▁▂▃▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.77791</td></tr><tr><td>train_auc</td><td>0.84228</td></tr><tr><td>train_f1</td><td>0.77512</td></tr><tr><td>train_loss_epoch</td><td>0.49714</td></tr><tr><td>train_loss_step</td><td>0.5108</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.79481</td></tr><tr><td>val_auc</td><td>0.87073</td></tr><tr><td>val_f1</td><td>0.79433</td></tr><tr><td>val_loss_epoch</td><td>0.45371</td></tr><tr><td>val_loss_step</td><td>0.43523</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_64_onehot_0</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/rmivsm7j' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/rmivsm7j</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240102_053949-rmivsm7j\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bf0fbc6a0d64a8fa32c1bc1ea10eb42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.0171999999981684, max=1.0))…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240102_060958-hhqfy8vv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/hhqfy8vv' target=\"_blank\">MLP_4_16_onehot_0</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/hhqfy8vv' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/hhqfy8vv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 1.0 K \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "1.2 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.2 K     Total params\n",
      "0.005     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "782236a908874ccd81026bfad8e4091f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▁▂▄▇▇▇█▇▇██▇▇██▇██▇▇██████████████▇███</td></tr><tr><td>train_auc</td><td>▁▂▂▂▄▇████▇██▇▇█████▇███████████████████</td></tr><tr><td>train_f1</td><td>▁▅▄▄▆▇▇██▇███████▇███▇██████████████████</td></tr><tr><td>train_loss_epoch</td><td>███▇▆▃▂▂▂▂▂▁▁▂▂▂▂▁▁▁▂▂▁▂▂▁▂▁▁▂▁▂▂▂▁▁▂▁▁▁</td></tr><tr><td>train_loss_step</td><td>████▇▃▃▅▃▃▂▃▂▂▂▁▃▁▁▂▁▂▁▃▂▅▂▃▄▃▂▃▃▂▂▂▁▁▂▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▁▁▃▆██▇▇████▇█████▇▇██████▇▇▇█▇▇████▇██</td></tr><tr><td>val_auc</td><td>▁▅▆▇▇███████████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▃▇███▇████▇██████▇██████████▇████████</td></tr><tr><td>val_loss_epoch</td><td>████▆▁▂▂▁▂▂▁▁▂▁▁▁▁▁▂▁▂▁▁▁▁▂▁▁▁▁▂▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_step</td><td>████▆▁▃▃▃▃▃▃▃▃▂▂▃▂▁▃▂▃▂▂▃▁▄▂▂▂▂▃▃▁▃▂▃▂▃▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.75192</td></tr><tr><td>train_auc</td><td>0.81323</td></tr><tr><td>train_f1</td><td>0.75207</td></tr><tr><td>train_loss_epoch</td><td>0.53121</td></tr><tr><td>train_loss_step</td><td>0.51809</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.79245</td></tr><tr><td>val_auc</td><td>0.87346</td></tr><tr><td>val_f1</td><td>0.77778</td></tr><tr><td>val_loss_epoch</td><td>0.46417</td></tr><tr><td>val_loss_step</td><td>0.45587</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_16_onehot_0</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/hhqfy8vv' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/hhqfy8vv</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240102_060958-hhqfy8vv\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bfd32f22cdf4729a31f6c07e8f920a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01693333333435779, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240102_063936-xu4rqbby</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/xu4rqbby' target=\"_blank\">MLP_4_16_onehot_0</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/xu4rqbby' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/xu4rqbby</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 1.0 K \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "1.2 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.2 K     Total params\n",
      "0.005     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed1493106133469fa0b3a533e1d016f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▁▃▄▆▅▅▅▆▆▆▇▇▆▇▇▇▇▇▇▇███▇█▇▇▇██▇▇▇█▆▇▇▇</td></tr><tr><td>train_auc</td><td>▁▂▂▄▅▆▆▆▆▆▅▆▇▇▆▇▆▆▆▇▆▇▇▇▆▆▇▇▇▇██▇▆▇█▇▇█▇</td></tr><tr><td>train_f1</td><td>▁▄▄▅▅▆▆▆▅▇▆▇██▇▇▇▇▇██▇█▇█▇█▇▇▇███▇▇█▆▇▇█</td></tr><tr><td>train_loss_epoch</td><td>█▇▇▆▆▄▅▄▄▄▄▄▃▃▃▂▂▂▃▃▃▂▂▂▂▃▂▂▂▂▂▂▃▃▂▁▂▂▃▃</td></tr><tr><td>train_loss_step</td><td>███▇▇▆▄▅▅▄▄▄▅▅▂▃▄▃▃▅▄▅▃▂▂▅▂▅▄▃▁▅▅▃▃▆▃▅▃▄</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>▃▃▄▄▁█▅▄▄▄▄▄▄▄▄▄▄█▄███▅▅▄▆▄▅▆▅▅▂▅▅▅▅▅▅▅▄</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_epoch</td><td>██▇▆▇█▇▆▆▅▆▆▄▅▃▃▂▁▃▄▄▃▂▂▂▃▂▂▂▂▂▁▂▂▂▃▂▄▃▃</td></tr><tr><td>val_loss_step</td><td>██▇▆▆█▆▅▇▄▅▄▄▆▂▂▃▁▃▆▄▃▂▄▄▃▃▃▃▃▄▃▄▃▄▄▂▅▄▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.67336</td></tr><tr><td>train_auc</td><td>0.66324</td></tr><tr><td>train_f1</td><td>0.66667</td></tr><tr><td>train_loss_epoch</td><td>0.63053</td></tr><tr><td>train_loss_step</td><td>0.63115</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.49764</td></tr><tr><td>val_auc</td><td>0.77559</td></tr><tr><td>val_f1</td><td>0.0</td></tr><tr><td>val_loss_epoch</td><td>0.67324</td></tr><tr><td>val_loss_step</td><td>0.67169</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_16_onehot_0</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/xu4rqbby' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/xu4rqbby</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240102_063936-xu4rqbby\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "157de020491f40e68ed3b8123b64bb86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01691666666495924, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240102_070913-ahr5ft4g</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/ahr5ft4g' target=\"_blank\">MLP_4_16_onehot_0</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/ahr5ft4g' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/ahr5ft4g</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 1.0 K \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "1.2 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.2 K     Total params\n",
      "0.005     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "298b1f68ce6e49b99c0eeaa62675a000",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▁▂▁▃▃▄▅▅▆▇▇▇▇▇▇▇▇▇▇▇██▇█▇▇█▇██████████</td></tr><tr><td>train_auc</td><td>▁▁▁▃▂▃▃▄▅▅▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇▇▇█▇▇██</td></tr><tr><td>train_f1</td><td>▄▄▂▁▂▃▄▅▆▆▆▇▇▇▇█▇▇█▇████████████████████</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▅▄▄▃▃▃▃▃▂▂▂▂▁▂▁▂▁▁▁▁▁▂▂▁▂▁▂▂▂▁▁▁▂▂▁▂▂▂▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▁▁▂▂▃▇▆▇▇▆▇▇█▆██▇█████████████▇██▇███▇█</td></tr><tr><td>val_auc</td><td>▁▁▃▃▅▇▆▇▇▇▇█▇█▇█████████████████████████</td></tr><tr><td>val_f1</td><td>▅▅▁▅▁▄██████████████████████████████████</td></tr><tr><td>val_loss_epoch</td><td>█████▇▇▆▅▅▅▄▄▂▄▂▂▃▁▂▁▂▂▂▂▂▂▂▁▂▂▁▁▂▃▂▁▁▂▁</td></tr><tr><td>val_loss_step</td><td>████▇▇▇▆▆▆▆▅▅▃▅▃▄▃▁▃▂▃▂▂▂▁▄▂▁▂▂▂▂▂▅▂▃▂▄▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.7407</td></tr><tr><td>train_auc</td><td>0.78001</td></tr><tr><td>train_f1</td><td>0.74668</td></tr><tr><td>train_loss_epoch</td><td>0.53505</td></tr><tr><td>train_loss_step</td><td>0.5222</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.81368</td></tr><tr><td>val_auc</td><td>0.88016</td></tr><tr><td>val_f1</td><td>0.81839</td></tr><tr><td>val_loss_epoch</td><td>0.4644</td></tr><tr><td>val_loss_step</td><td>0.44832</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_16_onehot_0</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/ahr5ft4g' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/ahr5ft4g</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240102_070913-ahr5ft4g\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f46a6268288f45ec863865478fa6ae0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01691666666883975, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240102_073935-8ytr5yao</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/8ytr5yao' target=\"_blank\">MLP_4_16_onehot_0</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/8ytr5yao' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/8ytr5yao</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 1.0 K \n",
      "2  | head        | Sequential       | 154   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 17    \n",
      "--------------------------------------------------\n",
      "1.2 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.2 K     Total params\n",
      "0.005     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36ba72427c3a49df8f8ebad6cf7e9c3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▁▁▃▆▆▇▇▇▇▇▇█▇▇▇██▇██▇▇█▇███▇██▇████▇██</td></tr><tr><td>train_auc</td><td>▂▁▁▂▃▆▇▇█▇▇████████████▇████████████████</td></tr><tr><td>train_f1</td><td>▁▅▃▃▅▇▇█▇█▇█████████████████████████████</td></tr><tr><td>train_loss_epoch</td><td>████▇▅▃▃▂▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▂▂▂▁▂▂▁▂</td></tr><tr><td>train_loss_step</td><td>█▇██▇▃▃▃▃▃▄▃▁▂▂▆▁▂▅▃▃▂▂▄▃▂▄▂▂▁▄▁▃▅▃▃▄▃▃▄</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▁▁▁▅▇▇▇▇▆█▇▇█▆▆▇▇▆█▇▇█▇▇▇▇▇▇▆█▇▇█▇▇▇▇▇▇</td></tr><tr><td>val_auc</td><td>▁▅▆▇▇▇▇█████████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▆▇███▇█▇▇█▇▇▇▇▆█▇▇█▇▇▇▇▇▇▇█▇▇█▇▇▇▇▇▇</td></tr><tr><td>val_loss_epoch</td><td>████▇▃▂▂▂▃▂▂▃▁▃▃▂▂▄▁▂▃▁▂▂▃▂▂▁▃▁▂▂▁▃▂▂▂▂▂</td></tr><tr><td>val_loss_step</td><td>████▇▄▂▃▂▄▃▃▅▂▃▄▂▁▅▂▃▄▂▃▂▄▃▃▂▆▂▃▃▁▄▃▁▃▃▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.74897</td></tr><tr><td>train_auc</td><td>0.80891</td></tr><tr><td>train_f1</td><td>0.75533</td></tr><tr><td>train_loss_epoch</td><td>0.54081</td></tr><tr><td>train_loss_step</td><td>0.58642</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.73585</td></tr><tr><td>val_auc</td><td>0.87237</td></tr><tr><td>val_f1</td><td>0.68182</td></tr><tr><td>val_loss_epoch</td><td>0.49791</td></tr><tr><td>val_loss_step</td><td>0.43334</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_16_onehot_0</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/8ytr5yao' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/8ytr5yao</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240102_073935-8ytr5yao\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8776ba2b88340988a956953f221de79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01691666666495924, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240102_080934-d1nsa80u</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/d1nsa80u' target=\"_blank\">MLP_4_16_onehot_0</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/d1nsa80u' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/d1nsa80u</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 1.0 K \n",
      "2  | head        | Sequential       | 154   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 281   \n",
      "--------------------------------------------------\n",
      "1.5 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.5 K     Total params\n",
      "0.006     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▁▁▂▅▇▇▇▇▇▇▇▇▇▇▇▇██▇██▇▇█▇█████████████</td></tr><tr><td>train_auc</td><td>▁▁▁▁▃▆▇▇▇▇▇▇▇▇▇▇▇▇██████████████████████</td></tr><tr><td>train_f1</td><td>▁▄▃▄▅▆▇▇████▇█▇██▇██████████████████████</td></tr><tr><td>train_loss_epoch</td><td>████▇▅▃▃▃▂▂▂▂▂▃▃▂▂▂▁▂▁▂▂▁▁▂▁▂▂▂▁▁▂▁▂▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>███▇█▄▄▃▄▃▃▁▃▄▂▄▃▃▂▃▂▁▃▃▂▂▂▄▄▄▃▂▁▃▂▂▂▃▄▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▁▁▁▄█▇███▇▇▇██▇▇████▇▇█▇████████▇███▇▇█</td></tr><tr><td>val_auc</td><td>▁▅▆▇▇▇▇▇████████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▅█▇█████████████████████████████████</td></tr><tr><td>val_loss_epoch</td><td>████▇▄▂▂▂▂▂▂▂▁▂▂▂▁▂▁▁▁▁▂▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_step</td><td>████▇▄▃▃▃▃▂▂▂▂▂▃▂▂▂▃▃▂▂▃▃▂▃▃▂▂▂▁▃▂▂▂▃▁▃▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.76196</td></tr><tr><td>train_auc</td><td>0.83122</td></tr><tr><td>train_f1</td><td>0.75767</td></tr><tr><td>train_loss_epoch</td><td>0.51322</td></tr><tr><td>train_loss_step</td><td>0.5175</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.81132</td></tr><tr><td>val_auc</td><td>0.87255</td></tr><tr><td>val_f1</td><td>0.81567</td></tr><tr><td>val_loss_epoch</td><td>0.44449</td></tr><tr><td>val_loss_step</td><td>0.4031</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_16_onehot_0</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/d1nsa80u' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/d1nsa80u</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240102_080934-d1nsa80u\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8fcee4427764b45adeae2c6aa1edd1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01691666666883975, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240102_083956-z28n7d33</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/z28n7d33' target=\"_blank\">MLP_4_32_onehot_0</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/z28n7d33' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/z28n7d33</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 3.6 K \n",
      "2 | head        | Sequential       | 562   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "4.2 K     Trainable params\n",
      "0         Non-trainable params\n",
      "4.2 K     Total params\n",
      "0.017     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e398fe001bc94685b7c8359e83002001",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▃▆▇▇▇▇▇▇▇█▇▇██▇█▇▇███▇▇▇█▇█▇▇█████████</td></tr><tr><td>train_auc</td><td>▁▁▃▆▇▇█▇█████▇██████████████████████████</td></tr><tr><td>train_f1</td><td>▃▁▃▆▇▇▇▇▇▇█▇▇▇██▇█▇▇▇▇█▇▇▇▇█▇▇▇▇██▇▇███▇</td></tr><tr><td>train_loss_epoch</td><td>███▄▃▂▂▂▂▂▂▂▂▂▂▁▂▁▂▂▁▁▁▁▂▂▂▂▂▂▂▁▁▁▂▂▁▁▁▂</td></tr><tr><td>train_loss_step</td><td>███▆▄▄▂▅▂▃▂▁▁▃▄▄▃▄▄▄▁▁▁▃▃▂▂▄▃▃▃▃▂▄▃▃▂▂▃▄</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▂▇▅██▇███▇▇██▇█▇█▇█▇█▇█▇██▇██▇█▇▇█▇█▇██</td></tr><tr><td>val_auc</td><td>▁▄▄▄▇▇██████████████▇▇▇▇▇▇▇▇▇█▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>val_f1</td><td>▁▇█▆██▇███▇███▇█▇█▇███▇████▇█████▇█▇████</td></tr><tr><td>val_loss_epoch</td><td>██▇▄▂▁▂▁▁▂▂▁▁▁▂▂▂▁▂▁▁▁▂▁▂▁▁▂▁▁▂▁▁▂▁▂▁▂▂▁</td></tr><tr><td>val_loss_step</td><td>██▇▅▃▁▃▂▁▂▃▃▃▁▃▃▄▂▂▂▂▂▃▂▃▂▂▂▁▃▃▁▂▃▃▃▁▂▄▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.7596</td></tr><tr><td>train_auc</td><td>0.83226</td></tr><tr><td>train_f1</td><td>0.75348</td></tr><tr><td>train_loss_epoch</td><td>0.50867</td></tr><tr><td>train_loss_step</td><td>0.56765</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.79717</td></tr><tr><td>val_auc</td><td>0.87128</td></tr><tr><td>val_f1</td><td>0.79717</td></tr><tr><td>val_loss_epoch</td><td>0.46015</td></tr><tr><td>val_loss_step</td><td>0.48143</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_32_onehot_0</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/z28n7d33' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/z28n7d33</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240102_083956-z28n7d33\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccd5d35baea2447a90134b0f1c1b1bd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01693333333435779, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240102_091101-qrva0eqh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/qrva0eqh' target=\"_blank\">MLP_4_32_onehot_0</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/qrva0eqh' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/qrva0eqh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 3.6 K \n",
      "2 | head        | Sequential       | 562   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "4.2 K     Trainable params\n",
      "0         Non-trainable params\n",
      "4.2 K     Total params\n",
      "0.017     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▁▄▅▆▅▆▅▆▇▇▆▇▇▇▇▇▇█▇▇▇▇▇▇▇█▇▇▇▇██▇▇█▇▇█</td></tr><tr><td>train_auc</td><td>▂▁▂▄▅▆▆▆▆▇▇▇▇▇▇█▇▇▇█▇▇▇█▇█▇█▇█▇██▇▇██▇▇█</td></tr><tr><td>train_f1</td><td>▃▂▁▄▅▆▅▆▅▅▇▇▆▇▇▇█▇▆▇▇▇▇▇▇▇▇██▇▇▇██▇▇█▇▇█</td></tr><tr><td>train_loss_epoch</td><td>███▇▆▄▅▅▅▄▄▃▃▃▃▂▃▃▃▂▂▂▃▂▂▂▃▂▂▂▂▂▁▃▂▂▁▂▂▁</td></tr><tr><td>train_loss_step</td><td>██▇▇▇▆▃▇▄▄▄▃▄▄▄▃▃▄▄▅▂▁▃▄▃▃▂▂▃▁▃▃▄▃▄▅▂▁▃▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▁▁██████▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁██████████▅██</td></tr><tr><td>val_auc</td><td>▂▆▆▄▅▄▁▁▄▆▅▆▆█▇▄▆▅▇▅▄▆▃▅▅▅▅▅▆▇▅▅▆▅█▆▅▇▆▅</td></tr><tr><td>val_f1</td><td>▁▁▁██████▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁██████████▅██</td></tr><tr><td>val_loss_epoch</td><td>▆▆▆▄▄▄▄▄▄▄▄▄▅▅▅▆▅▅▆▆▇█▅▄▄▄▄▃▁▂▂▃▄▂▂▂▂▃▁▁</td></tr><tr><td>val_loss_step</td><td>▆▆▆▅▅▄▅▄▄▄▄▅▅▄▄▄▄▆▆▆▇█▇▄▄▄▅▃▂▃▃▄▆▅▂▄▁▃▄▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.72888</td></tr><tr><td>train_auc</td><td>0.77734</td></tr><tr><td>train_f1</td><td>0.72888</td></tr><tr><td>train_loss_epoch</td><td>0.56677</td></tr><tr><td>train_loss_step</td><td>0.56894</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.77123</td></tr><tr><td>val_auc</td><td>0.78892</td></tr><tr><td>val_f1</td><td>0.80561</td></tr><tr><td>val_loss_epoch</td><td>0.61185</td></tr><tr><td>val_loss_step</td><td>0.65645</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_32_onehot_0</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/qrva0eqh' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/qrva0eqh</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240102_091101-qrva0eqh\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7cb78951aa84d5f9baadf4653e79846",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01693333333435779, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240102_094133-9umyq23t</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/9umyq23t' target=\"_blank\">MLP_4_32_onehot_0</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/9umyq23t' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/9umyq23t</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 3.6 K \n",
      "2 | head        | Sequential       | 562   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "4.2 K     Trainable params\n",
      "0         Non-trainable params\n",
      "4.2 K     Total params\n",
      "0.017     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "651ab8e774bf41b0b1d41fb85caddea7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▁▁▂▃▅▆▆▆▇▇▆▇▇▇▇▇▇▇▇█▇▇▇▇██▇▇█▇▇▇▇▇████</td></tr><tr><td>train_auc</td><td>▇▆▆█▇▇▇▅████▆▆▅▅▃▄▃▃▃▃▃▃▃▄▄▄▄▃▂▁▁▂▃▂▂▁▁▂</td></tr><tr><td>train_f1</td><td>▁▁▂▁▃▄▅▆▅▆▇▇▆▇▇▇▇▇▇▇▇█▇█▇███▇██▇▇█▇▇████</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▃▂▂▂▂▂▂▂▂▁▁▁▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▂▁▁▁▂▃▅▃▆▅▅▇█▇▇███▇▇██████▇▇█████▇███▇██</td></tr><tr><td>val_auc</td><td>▅▆▅▆▆▇▇▆█▇█▆▄▂▁▂▂▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_f1</td><td>▇▁▂▂▂▄▆▄▇▆▆▇█▇▇███▇▇█████████████▇██████</td></tr><tr><td>val_loss_epoch</td><td>████▇▆▅▆▄▅▄▂▂▂▂▂▂▂▂▂▂▁▂▁▂▁▂▁▁▂▁▁▂▂▁▁▁▁▁▁</td></tr><tr><td>val_loss_step</td><td>████▇▆▅▆▄▅▅▃▃▂▂▃▃▂▂▁▂▂▃▁▃▂▃▁▁▃▂▁▂▂▂▂▁▂▃▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.7596</td></tr><tr><td>train_auc</td><td>0.39082</td></tr><tr><td>train_f1</td><td>0.75556</td></tr><tr><td>train_loss_epoch</td><td>0.51377</td></tr><tr><td>train_loss_step</td><td>0.53908</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.8184</td></tr><tr><td>val_auc</td><td>0.11724</td></tr><tr><td>val_f1</td><td>0.81623</td></tr><tr><td>val_loss_epoch</td><td>0.45343</td></tr><tr><td>val_loss_step</td><td>0.47836</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_32_onehot_0</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/9umyq23t' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/9umyq23t</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240102_094133-9umyq23t\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bafcde13f1ea49e18667e32ea8575ad8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01693333333435779, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240102_101213-ghygfbsz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/ghygfbsz' target=\"_blank\">MLP_4_32_onehot_0</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/ghygfbsz' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/ghygfbsz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 3.6 K \n",
      "2  | head        | Sequential       | 562   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 33    \n",
      "--------------------------------------------------\n",
      "4.2 K     Trainable params\n",
      "0         Non-trainable params\n",
      "4.2 K     Total params\n",
      "0.017     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbc7270bd11a4d88afb0b91b97b18cd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▃▅▆▇▇▇▇▇▇▇▇▇▇▇█▇▇▇██▇▇▇██▇███▇██▇█████</td></tr><tr><td>train_auc</td><td>▁▁▄▆▇▇▇███████▇█████████████████████████</td></tr><tr><td>train_f1</td><td>▃▁▅▅▆▆▇▇▇█▇▇▇▇▇▇█▇▇▇██▇▇███████▇██▇█████</td></tr><tr><td>train_loss_epoch</td><td>██▇▆▃▃▂▂▂▂▂▂▂▁▂▁▂▂▂▂▁▁▂▂▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▂</td></tr><tr><td>train_loss_step</td><td>███▅▄▅▂▃▃▁▁▃▂▄▃▃▅▃▃▂▁▃▃▂▃▃▄▂▂▂▂▂▃▃▃▂▂▁▂▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▃▅▆▆███▇▇█▇▇▇████▇██▇▇▇████▇█▇██▇▇▇▇▇▇█</td></tr><tr><td>val_auc</td><td>▁█▇█████████████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▃▆▇▇███▇███▇███████████████████████████</td></tr><tr><td>val_loss_epoch</td><td>██▇▅▂▂▁▂▂▁▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▂▁▂▁▁</td></tr><tr><td>val_loss_step</td><td>██▇▅▃▃▁▃▂▁▂▃▄▂▂▂▃▁▂▁▁▂▂▂▃▁▂▃▃▂▂▂▃▁▁▂▂▃▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.76078</td></tr><tr><td>train_auc</td><td>0.831</td></tr><tr><td>train_f1</td><td>0.75734</td></tr><tr><td>train_loss_epoch</td><td>0.51842</td></tr><tr><td>train_loss_step</td><td>0.54299</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.79481</td></tr><tr><td>val_auc</td><td>0.87075</td></tr><tr><td>val_f1</td><td>0.8</td></tr><tr><td>val_loss_epoch</td><td>0.45848</td></tr><tr><td>val_loss_step</td><td>0.47107</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_32_onehot_0</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/ghygfbsz' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/ghygfbsz</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240102_101213-ghygfbsz\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cad34a1809944dfb692d4ca8c0a864d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01691666666495924, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240102_104432-m114qzj0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/m114qzj0' target=\"_blank\">MLP_4_32_onehot_0</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/m114qzj0' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/m114qzj0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 3.6 K \n",
      "2  | head        | Sequential       | 562   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 1.1 K \n",
      "--------------------------------------------------\n",
      "5.3 K     Trainable params\n",
      "0         Non-trainable params\n",
      "5.3 K     Total params\n",
      "0.021     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▂▄▇▇▇▇▇▇█▇▇▇▇██▇██▇██████████▇██████▇█</td></tr><tr><td>train_auc</td><td>▁▁▂▄▇▇▇▇▇█████▇█████████████████████████</td></tr><tr><td>train_f1</td><td>▁▂▂▁▇▇▇▇▇▇█▇██▇██▇██▇███████████▇█████▇█</td></tr><tr><td>train_loss_epoch</td><td>███▇▃▃▂▂▂▂▂▂▂▂▂▁▁▂▁▁▂▂▁▁▁▂▁▂▂▁▂▂▂▁▁▂▁▂▁▂</td></tr><tr><td>train_loss_step</td><td>███▇▅▃▂▃▃▂▄▃▅▃▄▁▃▄▂▄▂▃▃▄▂▃▃▁▄▃▃▃▃▂▃▃▁▃▅▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▁█▇▇█▇██▇███▇▇██▇█▇███▇▇▇▇▇▇██▇█▇▇▇▇█▇▇</td></tr><tr><td>val_auc</td><td>▁▆▆▇▇▇████████████▇█████████████████████</td></tr><tr><td>val_f1</td><td>▁▇█▇▇██████████████████████████▇██▇███▇█</td></tr><tr><td>val_loss_epoch</td><td>███▆▂▂▁▂▁▁▁▁▁▂▁▁▁▁▁▂▁▁▁▂▁▁▂▁▁▁▁▁▁▁▂▁▁▁▂▁</td></tr><tr><td>val_loss_step</td><td>███▇▄▃▂▂▃▂▁▂▃▂▂▃▂▂▂▄▃▃▃▃▂▂▄▂▂▂▃▂▂▂▃▃▂▃▃▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.76905</td></tr><tr><td>train_auc</td><td>0.83432</td></tr><tr><td>train_f1</td><td>0.76231</td></tr><tr><td>train_loss_epoch</td><td>0.51278</td></tr><tr><td>train_loss_step</td><td>0.50807</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.78066</td></tr><tr><td>val_auc</td><td>0.87224</td></tr><tr><td>val_f1</td><td>0.76808</td></tr><tr><td>val_loss_epoch</td><td>0.45422</td></tr><tr><td>val_loss_step</td><td>0.4473</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_32_onehot_0</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/m114qzj0' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/m114qzj0</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240102_104432-m114qzj0\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "652cf993d5854793801669f33c3b446b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01693333333435779, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240102_111602-dk2un6if</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/dk2un6if' target=\"_blank\">MLP_4_64_onehot_0</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/dk2un6if' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/dk2un6if</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 13.4 K\n",
      "2 | head        | Sequential       | 2.1 K \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "15.5 K    Trainable params\n",
      "0         Non-trainable params\n",
      "15.5 K    Total params\n",
      "0.062     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49e591900691428b94bd6572b6f710cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▃▇█▇██▇█▇█████▇█▇█▇▇▇█▇████▇▇████▇█████</td></tr><tr><td>train_auc</td><td>▁▂▇▇█████▇███████▇█▇████████████████████</td></tr><tr><td>train_f1</td><td>▁▄▇█▇████▇█████▇▇██▇▇▇██▇▇██▇▇████▇█████</td></tr><tr><td>train_loss_epoch</td><td>██▃▂▂▂▂▂▂▃▁▁▁▁▁▂▁▃▁▂▂▁▁▂▁▁▁▁▂▁▁▁▁▁▂▂▁▁▁▂</td></tr><tr><td>train_loss_step</td><td>██▄▄▃▃▂▃▃▃▂▂▃▃▃▄▃▄▄▃▂▂▁▃▂▃▄▃▅▃▄▄▂▄▂▃▂▃▄▄</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▆▇█████▇▇██▇███▇▇████▇▇█████████▇█████▇</td></tr><tr><td>val_auc</td><td>▁▄▄▆▇█▇▇▇███▇▇▇▇▇▇█▇▇█▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>val_f1</td><td>▂▁▆▇████▇▆█▇▆▆▆▆▇▄█▆▇█▆▄▇█▇▆▇██▇▇▆▇█▆█▇▆</td></tr><tr><td>val_loss_epoch</td><td>█▇▂▁▁▁▁▁▂▂▁▁▂▂▂▁▃▂▁▂▁▁▁▂▁▂▁▁▁▂▁▁▁▂▁▁▂▁▁▁</td></tr><tr><td>val_loss_step</td><td>█▇▃▃▂▂▄▂▂▂▂▃▃▃▂▂▄▃▁▄▂▃▁▃▃▃▂▁▂▄▂▂▂▄▂▂▂▃▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.76491</td></tr><tr><td>train_auc</td><td>0.83792</td></tr><tr><td>train_f1</td><td>0.76478</td></tr><tr><td>train_loss_epoch</td><td>0.50411</td></tr><tr><td>train_loss_step</td><td>0.52733</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.7783</td></tr><tr><td>val_auc</td><td>0.87095</td></tr><tr><td>val_f1</td><td>0.7602</td></tr><tr><td>val_loss_epoch</td><td>0.46376</td></tr><tr><td>val_loss_step</td><td>0.46376</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_64_onehot_0</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/dk2un6if' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/dk2un6if</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240102_111602-dk2un6if\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da2fa91c322b4cb5be93ac3230bd93e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240102_115005-gfrjo7ta</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/gfrjo7ta' target=\"_blank\">MLP_4_64_onehot_0</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/gfrjo7ta' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/gfrjo7ta</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 13.4 K\n",
      "2 | head        | Sequential       | 2.1 K \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "15.5 K    Trainable params\n",
      "0         Non-trainable params\n",
      "15.5 K    Total params\n",
      "0.062     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "483998d689ff4130915aff6872b72efa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▃▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇█▇▇█▇▇█▇▇█▇▇█▇▇█</td></tr><tr><td>train_auc</td><td>▁▁▃▆▆▇▇▇▇▇▇▇▇▇█▇▇▇▇█▇▇█▇█▇██▇▇█▇▇▇█████▇</td></tr><tr><td>train_f1</td><td>▁▃▂▅▆▇▇▆▆▇▆█▇▇▇▇▇█▇▇▇▇█▇█▇▇█▆▇▇▇▆▇▇▇█▇▇█</td></tr><tr><td>train_loss_epoch</td><td>██▇▅▄▃▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▃▂▁▂▂▂▂▂▁▂▂▂</td></tr><tr><td>train_loss_step</td><td>██▇▆▅▅▃▃▄▃▄▅▃▄▃▄▄▂▃▃▂▄▁▃▂▄▃▄▄▂▅▅▃▃▃▃▃▂▄▅</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>▁▆▇██▇██▇█▇▇███▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_epoch</td><td>▃▃▃▁▂▃▃▄▄▄▅▄▅▅▅▆█▅▆▇▅▅▆▇▇▅██▇▆▅▃▆▆▅▆▇▅▆▃</td></tr><tr><td>val_loss_step</td><td>▃▂▃▁▂▂▂▃▄▅▅▃▄▅▅▄█▆▃▇▄▃▇▇▅▅█▇▆▇▅▃▆▃▁▄▇▅▄▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.73597</td></tr><tr><td>train_auc</td><td>0.78413</td></tr><tr><td>train_f1</td><td>0.74266</td></tr><tr><td>train_loss_epoch</td><td>0.56154</td></tr><tr><td>train_loss_step</td><td>0.59854</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.50236</td></tr><tr><td>val_auc</td><td>0.79519</td></tr><tr><td>val_f1</td><td>0.66876</td></tr><tr><td>val_loss_epoch</td><td>0.72447</td></tr><tr><td>val_loss_step</td><td>0.73218</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_64_onehot_0</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/gfrjo7ta' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/gfrjo7ta</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240102_115005-gfrjo7ta\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1a16686c8fd4a26b4821fb5cc7c5cf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240102_122557-xb7k7htx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/xb7k7htx' target=\"_blank\">MLP_4_64_onehot_0</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/xb7k7htx' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/xb7k7htx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 13.4 K\n",
      "2 | head        | Sequential       | 2.1 K \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "15.5 K    Trainable params\n",
      "0         Non-trainable params\n",
      "15.5 K    Total params\n",
      "0.062     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b17cd89922414ddeb726256d115137de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▁▃▅▆▆▇▇▇▇▇▇▇███▇█▇█▇█▇█▇▇███▇████▇████</td></tr><tr><td>train_auc</td><td>▁▁▁▂▄▆▇▇▇▇▇▇▇▇███▇▇▇█▇▇▇█▇████████▇█████</td></tr><tr><td>train_f1</td><td>▂▁▂▂▅▆▆▇▇▇▇█▇▇██▇▇█▇█▇███▇████▇█████████</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▂▁▂▂▁▂▂▂▂▂▂▁▁▁▂▂▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▃▁▂▂▆▆▆▇██▇▆▇▇▇▇▇▇██▇██▇▇███▇█▇███▇▇▇███</td></tr><tr><td>val_auc</td><td>▁▂▃▅▇████▇██████▇███████████████████████</td></tr><tr><td>val_f1</td><td>▆▆▃▁▆▆▆▇███▆▇▇▇▇█▇██▇██▇▇███▇█▇███▇▇▇███</td></tr><tr><td>val_loss_epoch</td><td>███▇▅▄▃▂▂▂▃▃▃▃▂▂▃▂▂▂▂▂▁▃▂▂▁▁▂▁▂▂▁▂▂▂▁▂▁▁</td></tr><tr><td>val_loss_step</td><td>████▅▅▅▃▃▂▄▅▄▅▃▃▄▃▃▄▂▃▁▃▄▄▂▁▂▃▃▃▃▃▃▃▂▃▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.77437</td></tr><tr><td>train_auc</td><td>0.79601</td></tr><tr><td>train_f1</td><td>0.77071</td></tr><tr><td>train_loss_epoch</td><td>0.49204</td></tr><tr><td>train_loss_step</td><td>0.47726</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.80425</td></tr><tr><td>val_auc</td><td>0.88359</td></tr><tr><td>val_f1</td><td>0.79707</td></tr><tr><td>val_loss_epoch</td><td>0.44173</td></tr><tr><td>val_loss_step</td><td>0.43516</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_64_onehot_0</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/xb7k7htx' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/xb7k7htx</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240102_122557-xb7k7htx\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60a06f000fbe458980215e104bd78970",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240102_125802-q7jnlevl</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/q7jnlevl' target=\"_blank\">MLP_4_64_onehot_0</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/q7jnlevl' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/q7jnlevl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 13.4 K\n",
      "2  | head        | Sequential       | 2.1 K \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 65    \n",
      "--------------------------------------------------\n",
      "15.6 K    Trainable params\n",
      "0         Non-trainable params\n",
      "15.6 K    Total params\n",
      "0.062     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c011e31318a14838bee74b3653a2c16f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▃▆▇▇▇█▇██▇▇█▇█▇████████▇███▇██████▇████</td></tr><tr><td>train_auc</td><td>▁▃▇▇▇██▇███▇███▇████████▇███████████████</td></tr><tr><td>train_f1</td><td>▁▁▇▇▇▇████▇▇███▇████████▇███▇██████▇████</td></tr><tr><td>train_loss_epoch</td><td>██▅▃▂▂▂▂▂▂▂▂▂▁▁▂▁▂▂▁▂▁▁▁▂▁▂▁▂▁▂▂▁▁▁▂▁▁▁▂</td></tr><tr><td>train_loss_step</td><td>█▇▄▄▃▃▄▄▂▂▄▄▂▂▃▄▃▅▃▃▃▃▄▂▄▅▃▄▃▃▃▁▃▃▅▃▃▄▂▆</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▇▅▇▇██▆▇▇█▇▇▇▇██▇███████▇▇█▇███▇▇██▇▇█▆</td></tr><tr><td>val_auc</td><td>▁▅▅▆▆▇▇██▇▇████▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇██▇▇▇▇▇▇▇▇</td></tr><tr><td>val_f1</td><td>▃▇▁▆▅█▆▄▆▅▇▇▅▆▆█▆▇▇▇▆▇▇▇█▇▆▆▇▇▇█▆▆▇█▆▆▆▄</td></tr><tr><td>val_loss_epoch</td><td>█▇▄▂▂▂▁▃▁▂▁▂▂▁▁▁▂▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▂▂▁▁▂▁▂▃</td></tr><tr><td>val_loss_step</td><td>█▇▄▄▁▃▂▅▂▄▃▂▄▃▂▃▃▂▁▂▁▃▁▂▃▃▂▂▃▂▂▂▃▃▃▂▃▁▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.75842</td></tr><tr><td>train_auc</td><td>0.83158</td></tr><tr><td>train_f1</td><td>0.75842</td></tr><tr><td>train_loss_epoch</td><td>0.51566</td></tr><tr><td>train_loss_step</td><td>0.60561</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.73585</td></tr><tr><td>val_auc</td><td>0.87081</td></tr><tr><td>val_f1</td><td>0.68715</td></tr><tr><td>val_loss_epoch</td><td>0.49974</td></tr><tr><td>val_loss_step</td><td>0.47826</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_64_onehot_0</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/q7jnlevl' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/q7jnlevl</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240102_125802-q7jnlevl\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b277bb6a5b72484cab9354a94beba89d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01691666666495924, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240102_133121-p47f8fml</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/p47f8fml' target=\"_blank\">MLP_4_64_onehot_0</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/p47f8fml' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/p47f8fml</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 13.4 K\n",
      "2  | head        | Sequential       | 2.1 K \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 4.2 K \n",
      "--------------------------------------------------\n",
      "19.7 K    Trainable params\n",
      "0         Non-trainable params\n",
      "19.7 K    Total params\n",
      "0.079     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "464fdd92db6b414dbd8047e5ca19f761",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▇▇▇▇███▇▇███▇▇███▇██▇███████▇████████▇</td></tr><tr><td>train_auc</td><td>▁▂▇▇▇▇▇███▇███▇████████████████████████▇</td></tr><tr><td>train_f1</td><td>▁▂▇▇▇█████▇███████████▇██████▇█████████▇</td></tr><tr><td>train_loss_epoch</td><td>██▃▃▃▂▂▂▁▂▂▂▂▁▂▁▁▂▁▂▂▂▂▁▁▂▁▁▁▂▁▁▁▁▁▂▁▁▁▂</td></tr><tr><td>train_loss_step</td><td>██▅▅▄▄▃▃▁▅▂▃▂▃▂▄▃▄▄▃▅▃▄▄▂▃▂▁▃▃▂▃▄▂▃▂▂▃▂▅</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁██▇█▆███▇█▇██▇▇█▇██▇██▇█▇▇▇██▇▇██▇▇▇▇██</td></tr><tr><td>val_auc</td><td>▁▆▆▇▇▇█████▇██████▇▇█████▇▇▇▇▇▇▇█▇▇▇▇▇▇█</td></tr><tr><td>val_f1</td><td>▂▇▇▇█▁█▇▇▇▇▄▇█▄▆▇▆▆▇▇▇█▆▇▆▅▆▆▇▆▆▇▇▆▅▆▆▇▇</td></tr><tr><td>val_loss_epoch</td><td>█▇▁▂▁▃▁▁▁▂▁▂▁▁▂▁▁▁▁▁▂▁▂▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▂</td></tr><tr><td>val_loss_step</td><td>█▇▃▃▁▄▂▂▂▃▂▃▂▂▃▂▂▂▂▂▃▃▃▃▂▄▃▁▃▃▂▂▁▂▂▃▂▂▂▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.74424</td></tr><tr><td>train_auc</td><td>0.81889</td></tr><tr><td>train_f1</td><td>0.74333</td></tr><tr><td>train_loss_epoch</td><td>0.52345</td></tr><tr><td>train_loss_step</td><td>0.56547</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.79009</td></tr><tr><td>val_auc</td><td>0.87115</td></tr><tr><td>val_f1</td><td>0.7896</td></tr><tr><td>val_loss_epoch</td><td>0.46648</td></tr><tr><td>val_loss_step</td><td>0.47679</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_64_onehot_0</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/p47f8fml' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/p47f8fml</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240102_133121-p47f8fml\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2b7af9c1a7c450089294146cf090a4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240102_140409-6ptcg4yd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/6ptcg4yd' target=\"_blank\">MLP_2_16_onehot_1</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/6ptcg4yd' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/6ptcg4yd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 496   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "650       Trainable params\n",
      "0         Non-trainable params\n",
      "650       Total params\n",
      "0.003     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cde115bc944b4622b49fab3bdf6cd5c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▃▆▇▇▇▇███████████████████████████▇████</td></tr><tr><td>train_auc</td><td>▁▁▃▅▇██▇████████████████████████████████</td></tr><tr><td>train_f1</td><td>▁▅▅▇█▇█▇████████████████████████████████</td></tr><tr><td>train_loss_epoch</td><td>██▇▆▃▂▂▂▂▁▂▁▂▁▁▂▁▁▁▁▁▁▁▂▂▁▂▁▁▁▁▁▂▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>███▆▅▄▄▄▂▃▄▁▅▃▃▄▃▄▁▃▄▃▂▃▃▃▃▂▃▃▃▃▃▃▄▃▃▂▃▄</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▂▆▇███████████████████████████▇████████</td></tr><tr><td>val_auc</td><td>▁▆▇▇▇███████████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▂▇▇████████████████████████████████████</td></tr><tr><td>val_loss_epoch</td><td>██▇▅▂▂▂▂▁▁▁▁▁▁▁▁▁▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_step</td><td>███▆▄▃▃▃▂▂▃▂▁▃▂▂▂▄▃▁▃▃▂▃▁▁▂▃▃▂▃▂▃▂▃▂▂▃▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.76846</td></tr><tr><td>train_auc</td><td>0.83141</td></tr><tr><td>train_f1</td><td>0.76329</td></tr><tr><td>train_loss_epoch</td><td>0.51355</td></tr><tr><td>train_loss_step</td><td>0.54642</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.76415</td></tr><tr><td>val_auc</td><td>0.85474</td></tr><tr><td>val_f1</td><td>0.76526</td></tr><tr><td>val_loss_epoch</td><td>0.4799</td></tr><tr><td>val_loss_step</td><td>0.48739</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_16_onehot_1</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/6ptcg4yd' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/6ptcg4yd</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240102_140409-6ptcg4yd\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79cc3c886f064f139edf9359cff78278",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01691666666883975, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240102_143755-iyp9krdy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/iyp9krdy' target=\"_blank\">MLP_2_16_onehot_1</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/iyp9krdy' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/iyp9krdy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 496   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "650       Trainable params\n",
      "0         Non-trainable params\n",
      "650       Total params\n",
      "0.003     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a919cdc8adfd4718bccb437d389994cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▄▆▆▅▇▆▇▆▇▇▇▇▇▇▇▇▇▇██▇▇█▇▇▇████▇▇███▇▇█</td></tr><tr><td>train_auc</td><td>▁▁▃▄▅▅▆▆▇▆▇▇▇▇█▇▇▇█▇██▇▇█▇▇▇▇▇▇█▇▇██▇▇▇█</td></tr><tr><td>train_f1</td><td>▁▃▅▅▆▅▆▅▇▆▆▆▇▇▇▇▇▇▇▇███▇█▆▇▇█▇█▇▇████▇▇▇</td></tr><tr><td>train_loss_epoch</td><td>█▆▆▅▄▅▃▄▃▄▃▃▂▂▂▂▃▂▁▂▁▂▂▂▁▂▂▂▂▂▂▂▃▂▁▁▂▂▃▂</td></tr><tr><td>train_loss_step</td><td>█▇▆▅▇▄▅▃▃▃▅▂▃▄▂▂▃▄▁▃▄▁▁▃▁▂▂▂▄▄▃▃▄▁▁▃▂▂▂▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▂▂▂▂▂▂▂▂▄███████▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>▁▅▅▇▇▇▇▇▇▇█▇▇▇▇▇▇█▇▇▇▇▄▇▇██▂██▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▄███████▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>val_loss_epoch</td><td>█▇▆▄▃▃▂▂▁▂▂▁▁▁▁▂▄▃▂▂▄▅▄▄▅▇▆▅▇▇▇█▆▆▆▆▆▇▅▆</td></tr><tr><td>val_loss_step</td><td>▇▆▆▄▂▄▃▃▃▃▃▃▂▃▄▄▃▃▃▁▆▅▂▃▅▇▃▄█▅▄▇▆▅▆▅▅█▆▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.68931</td></tr><tr><td>train_auc</td><td>0.73638</td></tr><tr><td>train_f1</td><td>0.66751</td></tr><tr><td>train_loss_epoch</td><td>0.61446</td></tr><tr><td>train_loss_step</td><td>0.60196</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.48821</td></tr><tr><td>val_auc</td><td>0.77437</td></tr><tr><td>val_f1</td><td>0.6561</td></tr><tr><td>val_loss_epoch</td><td>0.68305</td></tr><tr><td>val_loss_step</td><td>0.68665</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_16_onehot_1</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/iyp9krdy' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/iyp9krdy</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240102_143755-iyp9krdy\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d28644c05a1e47d9a4f7b565f44276aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240102_150932-oqrr8pmf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/oqrr8pmf' target=\"_blank\">MLP_2_16_onehot_1</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/oqrr8pmf' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/oqrr8pmf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 496   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "650       Trainable params\n",
      "0         Non-trainable params\n",
      "650       Total params\n",
      "0.003     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ed01db6c22040e1acd7f375d6f52319",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▃▃▄▅▅▅▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇█▇▇▇▇▇█▇▇</td></tr><tr><td>train_auc</td><td>▁▃▃▄▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▆▇▇▆▇▇▇▇▇▇▇▇█▇▇█▇███</td></tr><tr><td>train_f1</td><td>▁▁▃▂▄▅▅▅▆▇▇▇▇▇▇▇▇▇▇▇██▇▇▇▇▇▇▇███▇▇█▇████</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▃▂▂▂▂▂▂▁▂▁▁▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▅▅▅▆▆▇▇▇▇▇█▇▇▇██▇▇█▇▇▇▇▇███▇▇▇▇▇▇▇█▇▇▇█</td></tr><tr><td>val_auc</td><td>▁▅▅▆▆▇▇▇████████████████████████████████</td></tr><tr><td>val_f1</td><td>▃▂▁▁▃▄▄▆▆▆▆▇▆▅▇█▇▇▆█▆▅▅▆▆▇██▇▇▇▅▇▆▆▇▆▅▇▇</td></tr><tr><td>val_loss_epoch</td><td>█▃▃▃▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_step</td><td>█▄▄▄▄▃▃▃▂▂▂▂▂▂▂▂▂▃▂▁▂▂▂▂▁▁▂▂▂▁▂▂▂▁▂▁▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.75724</td></tr><tr><td>train_auc</td><td>0.74588</td></tr><tr><td>train_f1</td><td>0.7642</td></tr><tr><td>train_loss_epoch</td><td>0.52233</td></tr><tr><td>train_loss_step</td><td>0.56642</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.78302</td></tr><tr><td>val_auc</td><td>0.87638</td></tr><tr><td>val_f1</td><td>0.77778</td></tr><tr><td>val_loss_epoch</td><td>0.46515</td></tr><tr><td>val_loss_step</td><td>0.47113</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_16_onehot_1</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/oqrr8pmf' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/oqrr8pmf</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240102_150932-oqrr8pmf\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c1628da4aa94608badcbdfc9205f9f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240102_154052-kwd5ox39</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/kwd5ox39' target=\"_blank\">MLP_2_16_onehot_1</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/kwd5ox39' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/kwd5ox39</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 496   \n",
      "2  | head        | Sequential       | 154   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 17    \n",
      "--------------------------------------------------\n",
      "667       Trainable params\n",
      "0         Non-trainable params\n",
      "667       Total params\n",
      "0.003     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87d4f2b6ed504be7a2bba697d56c4264",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▄▅▆▆▇▇▇▇████▇██▇██▇███████████████████</td></tr><tr><td>train_auc</td><td>▁▂▃▅▆▇▇▇▇███████████████████████████████</td></tr><tr><td>train_f1</td><td>▂▁▅▆▆▆▇▇▇▇███▇▇████▇████████████████████</td></tr><tr><td>train_loss_epoch</td><td>██▇▆▅▄▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▂▂▁▂▂▁▁▂▂▂▂▂▂▂▁▁</td></tr><tr><td>train_loss_step</td><td>███▆▄▄▃▄▂▁▁▃▄▄▂▂▂▂▁▃▂▂▁▂▂▁▂▂▄▁▃▂▂▃▃▂▁▂▂▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▄▁▆▅▅▆▆█▇█▇▇▇████▇▇██▇██▇███████████▇███</td></tr><tr><td>val_auc</td><td>▁▆▇▆▇▇██████████████████████████████████</td></tr><tr><td>val_f1</td><td>▆▁▇▆▆▇▇█▇███████████████████████████████</td></tr><tr><td>val_loss_epoch</td><td>███▅▅▃▃▂▂▁▂▁▂▁▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▂▁▂▁▁▁▁▂▁▁</td></tr><tr><td>val_loss_step</td><td>███▆▆▄▃▃▃▂▂▂▃▂▃▂▄▃▂▂▂▂▃▃▁▂▂▂▃▂▄▄▄▂▃▁▂▃▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.76432</td></tr><tr><td>train_auc</td><td>0.84042</td></tr><tr><td>train_f1</td><td>0.76065</td></tr><tr><td>train_loss_epoch</td><td>0.49634</td></tr><tr><td>train_loss_step</td><td>0.50922</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.77123</td></tr><tr><td>val_auc</td><td>0.8553</td></tr><tr><td>val_f1</td><td>0.77069</td></tr><tr><td>val_loss_epoch</td><td>0.47504</td></tr><tr><td>val_loss_step</td><td>0.46879</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_16_onehot_1</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/kwd5ox39' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/kwd5ox39</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240102_154052-kwd5ox39\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1f17fbbea614018b32421763647c43b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016933333330477276, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240102_161418-ici6c3uw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/ici6c3uw' target=\"_blank\">MLP_2_16_onehot_1</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/ici6c3uw' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/ici6c3uw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 496   \n",
      "2  | head        | Sequential       | 154   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 281   \n",
      "--------------------------------------------------\n",
      "931       Trainable params\n",
      "0         Non-trainable params\n",
      "931       Total params\n",
      "0.004     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf45c61114f24daebf4fd9acff48168b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▃▅▆▇▇█▇▇██▇▇█▇████▇█████▇███▇█████████</td></tr><tr><td>train_auc</td><td>▁▁▂▄▇▇▇█▇███████████████████████████████</td></tr><tr><td>train_f1</td><td>▁▂▄▆▆▇▇█▇▇██▇▇█████▇▇██████▇████████████</td></tr><tr><td>train_loss_epoch</td><td>███▆▄▃▃▂▂▂▂▂▂▂▂▂▁▂▁▂▂▁▁▂▂▂▂▂▂▁▂▂▁▂▂▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>███▆▅▄▅▂▄▂▄▃▄▃▃▃▃▃▂▂▃▃▃▂▄▁▃▄▃▃▃▃▃▃▂▄▃▄▃▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▃▇▇▇█▇▇██▇███▇██▇██████████▇███████████</td></tr><tr><td>val_auc</td><td>▁▆▇▇▇▇██████████████████████████████████</td></tr><tr><td>val_f1</td><td>▆▁▇▇▇█▇▇▇█▇███▇██▇███▇▇█▇███▇▇█▇████████</td></tr><tr><td>val_loss_epoch</td><td>███▅▃▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▂▁▁▂▁▁▁▁▁</td></tr><tr><td>val_loss_step</td><td>███▅▅▂▄▂▃▃▂▄▃▃▂▃▂▃▃▂▂▂▂▄▂▃▂▃▂▃▂▃▁▁▄▂▄▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.77318</td></tr><tr><td>train_auc</td><td>0.83768</td></tr><tr><td>train_f1</td><td>0.76867</td></tr><tr><td>train_loss_epoch</td><td>0.50104</td></tr><tr><td>train_loss_step</td><td>0.44508</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.77123</td></tr><tr><td>val_auc</td><td>0.85387</td></tr><tr><td>val_f1</td><td>0.76049</td></tr><tr><td>val_loss_epoch</td><td>0.48007</td></tr><tr><td>val_loss_step</td><td>0.48456</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_16_onehot_1</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/ici6c3uw' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/ici6c3uw</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240102_161418-ici6c3uw\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3a29a5f861546d3864898dad581e5dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240102_164604-u1gumseb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/u1gumseb' target=\"_blank\">MLP_2_32_onehot_1</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/u1gumseb' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/u1gumseb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 1.5 K \n",
      "2 | head        | Sequential       | 562   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "2.1 K     Trainable params\n",
      "0         Non-trainable params\n",
      "2.1 K     Total params\n",
      "0.008     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d942be6dbdba4ca49f37ad9e8459f6b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▃▆▇▇▇████████████████████▇█████████████</td></tr><tr><td>train_auc</td><td>▁▃▆▇▇▇██████████████████████████████████</td></tr><tr><td>train_f1</td><td>▃▁▆▇▇▇██████▇█████████████▇█████████████</td></tr><tr><td>train_loss_epoch</td><td>██▆▃▃▂▂▂▁▂▂▂▁▁▂▂▁▂▁▁▁▂▁▁▂▂▁▁▁▁▁▁▁▂▁▁▂▁▂▁</td></tr><tr><td>train_loss_step</td><td>██▇▅▃▄▄▃▅▃▃▃▂▄▂▃▃▃▃▄▅▄▄▁▄▃▄▃▂▄▃▃▃▂▃▃▃▄▃▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▅▆███▇█████████████████████████████████</td></tr><tr><td>val_auc</td><td>▁▂▂▄▆▇██████████████████████████████████</td></tr><tr><td>val_f1</td><td>▄▁▅█▇█▅█▇▇▆▇▇▇█▇▇▇▇▇▇▇▇▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>val_loss_epoch</td><td>█▇▄▄▂▂▂▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▂▁▂▁▁▁▁▁▁▂▁▁▁▁</td></tr><tr><td>val_loss_step</td><td>█▇▅▅▂▂▃▃▂▂▁▂▂▃▃▃▂▃▃▃▂▂▁▂▂▁▄▃▃▃▁▃▃▂▃▃▁▁▂▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.76551</td></tr><tr><td>train_auc</td><td>0.84338</td></tr><tr><td>train_f1</td><td>0.76688</td></tr><tr><td>train_loss_epoch</td><td>0.49321</td></tr><tr><td>train_loss_step</td><td>0.49109</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.76179</td></tr><tr><td>val_auc</td><td>0.85447</td></tr><tr><td>val_f1</td><td>0.7443</td></tr><tr><td>val_loss_epoch</td><td>0.48344</td></tr><tr><td>val_loss_step</td><td>0.49862</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_32_onehot_1</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/u1gumseb' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/u1gumseb</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240102_164604-u1gumseb\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d44bd547d5f0429a844fbc683fda3630",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01693333333435779, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240102_171557-l3r8shnd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/l3r8shnd' target=\"_blank\">MLP_2_32_onehot_1</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/l3r8shnd' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/l3r8shnd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 1.5 K \n",
      "2 | head        | Sequential       | 562   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "2.1 K     Trainable params\n",
      "0         Non-trainable params\n",
      "2.1 K     Total params\n",
      "0.008     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "897f151245284cebb1f3f68838f9af5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▄▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇█▇▇▇█▇▇▇███▇▇▇▇▇█▇████</td></tr><tr><td>train_auc</td><td>▁▃▆▆▇▇▇▇▇▇▇▇▇▇██▇▇████████▇██▇██████████</td></tr><tr><td>train_f1</td><td>▂▁▅▅▅▅▆▇▆▇▆▇▆▆▆▆▇▇█▇▇▇▇▇▇▇▇▇█▆▇▇▇▇▇▇████</td></tr><tr><td>train_loss_epoch</td><td>█▇▅▄▄▄▃▃▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▁▁▂▁▂▂▂▂▁▁▁▂▁</td></tr><tr><td>train_loss_step</td><td>█▆▅▆▄▅▅▅▆▃▃▂▃▄▃▄▄▄▃▄▄▄▄▁▆▃▃▅▃▄▃▄▃▂▄▃▃▃▂▄</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▂▂▂▂▂████▅▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>▁▅▇▇▇▇▇▇▇▇▇█▇▅▇███▇▆▆▇▇▆▇▇▇▇▇▇██▇█▇▇▇▇▇▇</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁█████▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>val_loss_epoch</td><td>▃▃▂▁▂▁▁▁▁▂▃▃▄▄▅▄▅▅▅▅▅▆▆▆▆▆▇▇▆▆▆▆▇▆▆█▇▇▆▇</td></tr><tr><td>val_loss_step</td><td>▃▃▂▁▁▂▁▂▁▂▃▃▄▄▄▄▃▄▄▃▄▅▅▄▅▄▆▆▄▅▄▅▆▆▅█▅▆▅▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.74188</td></tr><tr><td>train_auc</td><td>0.79343</td></tr><tr><td>train_f1</td><td>0.73942</td></tr><tr><td>train_loss_epoch</td><td>0.55398</td></tr><tr><td>train_loss_step</td><td>0.58223</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.48821</td></tr><tr><td>val_auc</td><td>0.79012</td></tr><tr><td>val_f1</td><td>0.6561</td></tr><tr><td>val_loss_epoch</td><td>0.78287</td></tr><tr><td>val_loss_step</td><td>0.77937</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_32_onehot_1</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/l3r8shnd' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/l3r8shnd</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240102_171557-l3r8shnd\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c42690ba09dc4a718824f8e22ea381e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240102_174620-fw4zl1pk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/fw4zl1pk' target=\"_blank\">MLP_2_32_onehot_1</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/fw4zl1pk' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/fw4zl1pk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 1.5 K \n",
      "2 | head        | Sequential       | 562   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "2.1 K     Trainable params\n",
      "0         Non-trainable params\n",
      "2.1 K     Total params\n",
      "0.008     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▃▃▅▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇█▇█▇▇▇██▇█▇███▇▇████▇</td></tr><tr><td>train_auc</td><td>▁▁▂▄▆▆▇▆▇▇█▇▇▇▇▇██▇██▇█████▇████████████</td></tr><tr><td>train_f1</td><td>▁▁▂▄▅▅▇▆▆▇▇▇▇▇▇█▇█▇█▇█▇████▇█████▇▇████▇</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▅▆▄▃▃▃▃▃▃▂▂▂▃▂▃▃▂▂▃▃▃▃▁▃▂▃▂▁▃▂▂▂▂▂▂▃▂▂▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▅▄▅▆▇▆▇▇▇▇████▇█▇█▇████▇████▇██████▇███</td></tr><tr><td>val_auc</td><td>▅▁▃▇▇▇██████████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▆▅▆▇█▇██▇█████▇█▇█▇████████████████████</td></tr><tr><td>val_loss_epoch</td><td>█▅▅▄▃▃▃▂▂▂▁▂▁▂▂▂▂▂▁▂▁▂▁▂▁▁▁▂▂▁▁▂▁▁▁▁▁▁▁▂</td></tr><tr><td>val_loss_step</td><td>█▅▅▅▃▃▃▄▂▂▁▃▂▃▃▃▂▂▃▃▂▂▁▃▂▂▃▃▃▃▂▃▃▂▂▃▁▂▂▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.76905</td></tr><tr><td>train_auc</td><td>0.78977</td></tr><tr><td>train_f1</td><td>0.76375</td></tr><tr><td>train_loss_epoch</td><td>0.50149</td></tr><tr><td>train_loss_step</td><td>0.49779</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.79717</td></tr><tr><td>val_auc</td><td>0.87141</td></tr><tr><td>val_f1</td><td>0.80973</td></tr><tr><td>val_loss_epoch</td><td>0.48014</td></tr><tr><td>val_loss_step</td><td>0.48446</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_32_onehot_1</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/fw4zl1pk' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/fw4zl1pk</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240102_174620-fw4zl1pk\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b15e42cdbea941efbade775db8595dae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240102_181705-6nm97hkc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/6nm97hkc' target=\"_blank\">MLP_2_32_onehot_1</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/6nm97hkc' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/6nm97hkc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 1.5 K \n",
      "2  | head        | Sequential       | 562   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 33    \n",
      "--------------------------------------------------\n",
      "2.1 K     Trainable params\n",
      "0         Non-trainable params\n",
      "2.1 K     Total params\n",
      "0.008     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4d66b5813cc40bfb3cc6cc342bb75ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▄▅▆▆▇▇▇▇▇▇▇██▇████▇███▇████████████████</td></tr><tr><td>train_auc</td><td>▁▄▆▆▇▇▇█▇█▇████████████▇██████████████▇█</td></tr><tr><td>train_f1</td><td>▁▄▅▆▆▇▇▇▇▇▇███▇████████▇▇███████████████</td></tr><tr><td>train_loss_epoch</td><td>█▇▅▄▃▃▂▂▂▂▂▂▂▂▂▁▁▂▁▂▁▁▂▂▁▂▁▁▂▂▁▂▁▂▂▁▁▁▂▁</td></tr><tr><td>train_loss_step</td><td>█▇▅▅▅▄▃▃▃▂▂▃▄▃▃▃▄▄▁▂▃▂▃▃▃▃▃▂▃▃▃▂▂▃▃▂▁▂▃▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▆▆▆▆▇█▇▇█▇██▇███▇▇▇▇██████████▇████████</td></tr><tr><td>val_auc</td><td>▂▁▁▃▄▆▇▇▇██▇████████████████████████████</td></tr><tr><td>val_f1</td><td>▂▃▂▁▂▃▇▆▅█▅█▇▅█▇▇▅█▅▅▇▇▅█▅██▅█▇▇▇██▆▇▇▇▇</td></tr><tr><td>val_loss_epoch</td><td>█▆▄▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▁▁▂▂▂▂▁▂▁▁▂▁▂▁▂▂▁▁▁</td></tr><tr><td>val_loss_step</td><td>█▆▄▅▅▃▂▅▂▄▃▃▃▃▃▅▄▂▂▂▃▃▃▄▃▃▅▃▃▃▂▃▂▅▂▃▃▁▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.77082</td></tr><tr><td>train_auc</td><td>0.8449</td></tr><tr><td>train_f1</td><td>0.77675</td></tr><tr><td>train_loss_epoch</td><td>0.48775</td></tr><tr><td>train_loss_step</td><td>0.48491</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.76651</td></tr><tr><td>val_auc</td><td>0.8577</td></tr><tr><td>val_f1</td><td>0.75676</td></tr><tr><td>val_loss_epoch</td><td>0.46578</td></tr><tr><td>val_loss_step</td><td>0.43727</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_32_onehot_1</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/6nm97hkc' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/6nm97hkc</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240102_181705-6nm97hkc\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1467a4582a2e421497a8f5c14658800a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01693333333435779, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240102_184639-jdlur6ut</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/jdlur6ut' target=\"_blank\">MLP_2_32_onehot_1</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/jdlur6ut' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/jdlur6ut</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 1.5 K \n",
      "2  | head        | Sequential       | 562   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 1.1 K \n",
      "--------------------------------------------------\n",
      "3.1 K     Trainable params\n",
      "0         Non-trainable params\n",
      "3.1 K     Total params\n",
      "0.013     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a617ce043a24d33813a6ff5ef885d62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▄▆▇▇█▇▇▇███████████▇███████████████████</td></tr><tr><td>train_auc</td><td>▁▄▆▇▇█▇▇████████████████████████████████</td></tr><tr><td>train_f1</td><td>▁▄▆▇▇█▇▇▇███████████████████████▇███████</td></tr><tr><td>train_loss_epoch</td><td>█▇▅▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▂▂▂▁▂▂▂▁▂▂▁▂▂▁▁▁▂▂▁</td></tr><tr><td>train_loss_step</td><td>██▆▄▂▁▄▃▃▄▃▄▄▃▃▂▃▃▃▃▂▁▃▃▂▃▄▃▂▃▂▂▃▄▁▂▂▃▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▆▆▇█████▇██████████▇███████████████████</td></tr><tr><td>val_auc</td><td>▁▂▂▅▇▇██████████████████████████████████</td></tr><tr><td>val_f1</td><td>▂▁▂▄▆▇▇▇█▅▇▇▆▇▇█▇▇▆▆▅▇▇▅▇▇▇▇▇▇▇▇▇█▇▆▇▆▆▆</td></tr><tr><td>val_loss_epoch</td><td>█▇▄▃▂▁▂▂▂▁▁▁▁▂▁▂▂▁▁▁▂▂▁▂▁▁▁▂▁▁▂▁▁▂▁▁▂▁▁▁</td></tr><tr><td>val_loss_step</td><td>█▇▄▃▄▂▃▄▂▂▂▁▂▃▃▃▄▃▃▂▃▃▃▃▂▂▃▄▂▂▄▃▂▄▂▂▄▂▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.77614</td></tr><tr><td>train_auc</td><td>0.85144</td></tr><tr><td>train_f1</td><td>0.78029</td></tr><tr><td>train_loss_epoch</td><td>0.47795</td></tr><tr><td>train_loss_step</td><td>0.43998</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.76179</td></tr><tr><td>val_auc</td><td>0.85501</td></tr><tr><td>val_f1</td><td>0.74813</td></tr><tr><td>val_loss_epoch</td><td>0.47791</td></tr><tr><td>val_loss_step</td><td>0.47535</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_32_onehot_1</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/jdlur6ut' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/jdlur6ut</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240102_184639-jdlur6ut\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d75a7db2e1a410aa5a1d6a5eeedd6ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01691666666495924, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240102_191608-euk9vqro</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/euk9vqro' target=\"_blank\">MLP_2_64_onehot_1</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/euk9vqro' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/euk9vqro</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 5.1 K \n",
      "2 | head        | Sequential       | 2.1 K \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "7.2 K     Trainable params\n",
      "0         Non-trainable params\n",
      "7.2 K     Total params\n",
      "0.029     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6b3dee137a94a32b23096499bc45ec0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▅▇▇█▇▇█▇▇████████████████████████████▇█</td></tr><tr><td>train_auc</td><td>▁▅▇█████████████████████████████████████</td></tr><tr><td>train_f1</td><td>▁▅▇▇█▇▇█▇████▇▇███▇███████▇███▇██▇▇███▇█</td></tr><tr><td>train_loss_epoch</td><td>█▆▃▂▂▂▂▁▂▂▁▂▁▂▁▁▁▁▂▂▁▁▁▁▁▁▂▁▁▁▁▁▁▂▂▂▁▁▂▁</td></tr><tr><td>train_loss_step</td><td>█▆▃▃▃▃▄▁▂▃▃▂▂▅▂▂▂▃▄▃▁▃▃▃▃▂▄▂▂▂▂▂▄▃▃▄▃▃▂▄</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▇▇█▇█████▇█▇▇▇▇█▇▇██████▇▇█▇██▇█▇▇█▇▇▇█</td></tr><tr><td>val_auc</td><td>▁▁▃▅▇▇██████████████▇███████████████████</td></tr><tr><td>val_f1</td><td>▁█▄▇▃█▇▇▆▆▇▇▆▇▆▅▆▆▇▅▅▇▅▆▆▄▇▆▆▆▇▅▆▆▇▅▅▇▇▅</td></tr><tr><td>val_loss_epoch</td><td>█▅▂▂▂▂▂▁▁▂▁▁▁▂▁▁▁▁▂▂▁▁▂▁▁▂▁▁▁▁▂▁▁▁▂▁▂▁▂▁</td></tr><tr><td>val_loss_step</td><td>█▆▄▂▃▃▃▃▃▃▂▂▂▄▃▂▃▂▃▃▂▂▄▃▃▂▂▃▂▂▄▁▃▁▂▃▃▃▃▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.77673</td></tr><tr><td>train_auc</td><td>0.85074</td></tr><tr><td>train_f1</td><td>0.78023</td></tr><tr><td>train_loss_epoch</td><td>0.48113</td></tr><tr><td>train_loss_step</td><td>0.51586</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.76651</td></tr><tr><td>val_auc</td><td>0.85485</td></tr><tr><td>val_f1</td><td>0.7455</td></tr><tr><td>val_loss_epoch</td><td>0.47275</td></tr><tr><td>val_loss_step</td><td>0.43328</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_64_onehot_1</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/euk9vqro' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/euk9vqro</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240102_191608-euk9vqro\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "661384d5fc0846948805d8360ff72fe8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016933333330477276, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240102_194608-shizrfpr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/shizrfpr' target=\"_blank\">MLP_2_64_onehot_1</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/shizrfpr' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/shizrfpr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 5.1 K \n",
      "2 | head        | Sequential       | 2.1 K \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "7.2 K     Trainable params\n",
      "0         Non-trainable params\n",
      "7.2 K     Total params\n",
      "0.029     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e51a80cf1d3405ebd9a2e1d76b1594e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▄▆▇▇▇▇▇█▇▇▇█▇▇████████████▇███████████▇</td></tr><tr><td>train_auc</td><td>▁▂▅▆▇▆▇▇▇▇▇▇▇▇▇▇▇▇█▇█▇▇▇█▇█▇█▇▇██████▇██</td></tr><tr><td>train_f1</td><td>▁▃▆▇▇▆▇▇█▇▇▇▇▇▇███▇████████▇██▇██▇█████▇</td></tr><tr><td>train_loss_epoch</td><td>█▆▄▃▃▃▃▂▂▂▂▂▁▂▂▂▁▂▂▂▂▂▁▂▁▂▂▂▁▂▂▁▂▁▁▁▂▁▁▂</td></tr><tr><td>train_loss_step</td><td>█▇▅▄▄▄▄▃▁▃▄▅▂▃▃▂▃▂▃▄▃▃▂▂▂▃▂▃▃▃▁▂▂▂▁▂▂▁▂▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▁███▇▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>▁▇██████████████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁███▇▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_epoch</td><td>▃▂▁▁▁▂▃▃▃▄▃▃▅▅▅▅▅▆▇▆▆▆▇▆▇▆▇▇█▆▆▇▇▇▇▆▆▆▇▆</td></tr><tr><td>val_loss_step</td><td>▃▂▂▁▁▁▃▃▃▄▃▃▅▅▄▄▅▆▆▆▆▆▆▅▇▆▆▇█▆▆▇▆▆▆▅▅▇▇▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.73302</td></tr><tr><td>train_auc</td><td>0.79112</td></tr><tr><td>train_f1</td><td>0.73598</td></tr><tr><td>train_loss_epoch</td><td>0.52998</td></tr><tr><td>train_loss_step</td><td>0.55005</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.48821</td></tr><tr><td>val_auc</td><td>0.7921</td></tr><tr><td>val_f1</td><td>0.6561</td></tr><tr><td>val_loss_epoch</td><td>0.93718</td></tr><tr><td>val_loss_step</td><td>0.92962</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_64_onehot_1</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/shizrfpr' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/shizrfpr</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240102_194608-shizrfpr\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4b301e43fef41db9774f554d4906a25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240102_201706-7ggckn8h</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/7ggckn8h' target=\"_blank\">MLP_2_64_onehot_1</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/7ggckn8h' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/7ggckn8h</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 5.1 K \n",
      "2 | head        | Sequential       | 2.1 K \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "7.2 K     Trainable params\n",
      "0         Non-trainable params\n",
      "7.2 K     Total params\n",
      "0.029     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ad4c482791e46369cab9fece0c7b3bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▃▅▆▇▆▇▇▇▇▇▇▇█▇███▇███▇███▇█▇█▇█████████</td></tr><tr><td>train_auc</td><td>██▇▆▄▃▃▂▂▂▂▂▁▂▂▂▁▂▁▁▂▂▂▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁</td></tr><tr><td>train_f1</td><td>▁▄▆▆▆▆▇▇▇▇▇▇▇▇▇▇██▇███████████▇█████████</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▄▃▃▂▃▂▁▁▂▃▂▂▂▁▂▁▂▂▂▁▂▂▂▂▂▂▂▂▁▂▂▂▂▁▂▂▁▁▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▄▇▁▄▅▇▇▇▆▇█▇▇▇▇█▆▇█▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇▇██▇▆</td></tr><tr><td>val_auc</td><td>█▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_f1</td><td>▄█▁▄▆██▇▇▇██▇█▇█▆██▇▇▇▇▇▇▇▇███▇▇▇█▇▇██▇▇</td></tr><tr><td>val_loss_epoch</td><td>▇▇█▅▄▄▃▂▂▂▂▂▁▃▁▂▂▂▂▂▁▁▁▁▂▁▁▂▁▁▂▁▁▁▁▁▂▁▁▂</td></tr><tr><td>val_loss_step</td><td>▇▆█▆▄▅▅▄▄▄▂▃▂▅▂▃▃▂▃▄▂▂▃▂▄▁▁▃▁▂▃▁▂▂▂▃▄▂▃▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.77791</td></tr><tr><td>train_auc</td><td>0.20621</td></tr><tr><td>train_f1</td><td>0.78366</td></tr><tr><td>train_loss_epoch</td><td>0.48604</td></tr><tr><td>train_loss_step</td><td>0.56677</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.76415</td></tr><tr><td>val_auc</td><td>0.12574</td></tr><tr><td>val_f1</td><td>0.72973</td></tr><tr><td>val_loss_epoch</td><td>0.46111</td></tr><tr><td>val_loss_step</td><td>0.44965</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_64_onehot_1</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/7ggckn8h' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/7ggckn8h</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240102_201706-7ggckn8h\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8a43a4a348944bc8de0dee44d4f1d23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01691666666883975, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240102_204731-5t5ajybw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/5t5ajybw' target=\"_blank\">MLP_2_64_onehot_1</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/5t5ajybw' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/5t5ajybw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 5.1 K \n",
      "2  | head        | Sequential       | 2.1 K \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 65    \n",
      "--------------------------------------------------\n",
      "7.3 K     Trainable params\n",
      "0         Non-trainable params\n",
      "7.3 K     Total params\n",
      "0.029     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8f6e3f309d643c1b3f5a143bd4f203a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▆▇▇██████████████▇█████▇███▇███▇███████</td></tr><tr><td>train_auc</td><td>▁▆▇▇████████████████████████████████████</td></tr><tr><td>train_f1</td><td>▁▆▇▇▇█▇██▇████████▇█████▇███▇███████████</td></tr><tr><td>train_loss_epoch</td><td>█▅▃▂▂▂▂▂▂▂▂▂▁▂▁▂▁▂▂▂▁▁▁▁▁▁▁▁▂▁▁▁▂▂▁▂▁▂▁▂</td></tr><tr><td>train_loss_step</td><td>█▆▆▄▂▄▃▅▂▃▃▃▃▂▄▅▃▃▃▃▃▄▁▆▃▃▅▄▄▃▃▃▄▃▂▃▄▂▅▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▅▇▇▇▆█▇▇█▇█▇▇█▇▇▇▇███▇▇█▇█▇▇█▇██▇██▇██▇</td></tr><tr><td>val_auc</td><td>▄▁▄▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇██████▇███████▇▇██████</td></tr><tr><td>val_f1</td><td>▁▆█▇▇▆███████████▇████▇▇███▇▇███████▇███</td></tr><tr><td>val_loss_epoch</td><td>█▄▃▃▂▄▂▂▂▁▂▂▁▂▁▁▁▂▁▂▂▁▂▃▂▂▁▂▂▁▂▁▁▁▁▁▂▁▁▂</td></tr><tr><td>val_loss_step</td><td>█▅▄▅▃▄▃▃▃▂▃▄▃▄▃▂▁▄▂▄▄▃▅▃▄▄▄▃▃▃▃▁▁▂▃▂▃▂▃▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.76787</td></tr><tr><td>train_auc</td><td>0.84311</td></tr><tr><td>train_f1</td><td>0.76977</td></tr><tr><td>train_loss_epoch</td><td>0.48707</td></tr><tr><td>train_loss_step</td><td>0.49555</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.76651</td></tr><tr><td>val_auc</td><td>0.86231</td></tr><tr><td>val_f1</td><td>0.78891</td></tr><tr><td>val_loss_epoch</td><td>0.48346</td></tr><tr><td>val_loss_step</td><td>0.52167</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_64_onehot_1</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/5t5ajybw' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/5t5ajybw</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240102_204731-5t5ajybw\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a2668ec888e4f97bd4a039e7c5705f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240102_212126-wzpzdrp6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/wzpzdrp6' target=\"_blank\">MLP_2_64_onehot_1</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/wzpzdrp6' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/wzpzdrp6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 5.1 K \n",
      "2  | head        | Sequential       | 2.1 K \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 4.2 K \n",
      "--------------------------------------------------\n",
      "11.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "11.4 K    Total params\n",
      "0.046     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3e4bd63ebad442a93ba2e369483a070",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▆▇▇▇██▇▇█▇██▇█████▇█████████▇██████████</td></tr><tr><td>train_auc</td><td>▁▆▇▇▇███████████████████████████████████</td></tr><tr><td>train_f1</td><td>▁▅▇▇▇██▇███▇███▇███▇████████▇██▇████████</td></tr><tr><td>train_loss_epoch</td><td>█▅▃▂▃▂▂▂▂▁▂▁▁▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▂▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▅▄▃▃▄▂▂▄▃▄▂▃▂▃▃▄▃▂▅▂▃▃▂▂▂▂▂▂▂▂▄▂▄▁▂▁▁▂▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▄▃▇▇▄▁█▅▃▄▄▄▄▃▄▂▂▅▃▃▆▃▃▄▆▅▃▃▂▆▅▅▄▄▆▂▄▂▅▅</td></tr><tr><td>val_auc</td><td>▄▁▆▇▇▇██████████████████████████████████</td></tr><tr><td>val_f1</td><td>▆▅█▇▆▁▇▇▁█▂▆▇▂▅█▁▅▃█▇▄▄▇▇▅▄▂▇▇▆▆▇▅▇▆▄▆▆▆</td></tr><tr><td>val_loss_epoch</td><td>█▄▂▂▂▂▁▁▂▂▂▂▂▂▁▂▁▁▂▂▁▂▁▂▁▁▂▂▂▁▁▁▁▁▁▁▁▁▂▁</td></tr><tr><td>val_loss_step</td><td>█▄▃▃▃▄▂▁▃▅▃▃▄▃▃▃▂▁▃▂▂▃▃▄▃▁▃▄▂▂▃▁▃▂▂▃▂▃▃▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.78086</td></tr><tr><td>train_auc</td><td>0.84439</td></tr><tr><td>train_f1</td><td>0.78086</td></tr><tr><td>train_loss_epoch</td><td>0.48619</td></tr><tr><td>train_loss_step</td><td>0.4909</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.76651</td></tr><tr><td>val_auc</td><td>0.8546</td></tr><tr><td>val_f1</td><td>0.76372</td></tr><tr><td>val_loss_epoch</td><td>0.46821</td></tr><tr><td>val_loss_step</td><td>0.42857</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_64_onehot_1</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/wzpzdrp6' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/wzpzdrp6</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240102_212126-wzpzdrp6\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42bea8a023574a14beea6edd66ad6bc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240102_215454-21a8tb7i</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/21a8tb7i' target=\"_blank\">MLP_3_16_onehot_1</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/21a8tb7i' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/21a8tb7i</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 768   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "922       Trainable params\n",
      "0         Non-trainable params\n",
      "922       Total params\n",
      "0.004     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee05bf530aff4006ac04f3b513cdf8bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▂▂▅▇▇█▇█▇█▇▇█▇████▇████████▇█▇▇███▇███</td></tr><tr><td>train_auc</td><td>▁▁▂▂▅▇██▇███▇███████████████████████████</td></tr><tr><td>train_f1</td><td>▄▁▄▃▅▇▇█▇▇▇█▇██▇█████▇█████▇█▇▇▇▇█▇█▇██▇</td></tr><tr><td>train_loss_epoch</td><td>█▇▇▇▆▃▂▂▂▂▂▂▂▂▁▂▁▂▂▁▂▁▁▁▁▂▂▂▁▁▁▁▁▂▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>███▇▆▃▂▂▃▃▃▂▃▃▂▁▃▄▂▃▂▃▁▂▂▄▂▃▃▂▃▂▂▂▄▂▃▃▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▁▁▇█▇█▇▇████▇▇▇▇▇▇████████▇██▇██████▇██</td></tr><tr><td>val_auc</td><td>▁▇▇█████████████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁█▇▆▇▄▅▆▆▆▇▅▄▅▄▃▅▇▇▇▇▆▆██▇▇▇▅▇▇▆▇█▇▆▆▇</td></tr><tr><td>val_loss_epoch</td><td>████▅▂▂▂▂▁▂▂▁▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_step</td><td>████▅▂▂▁▂▂▃▂▂▃▂▂▁▃▂▂▂▂▁▂▂▁▃▂▂▃▂▁▁▁▂▃▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.75428</td></tr><tr><td>train_auc</td><td>0.82874</td></tr><tr><td>train_f1</td><td>0.7494</td></tr><tr><td>train_loss_epoch</td><td>0.51465</td></tr><tr><td>train_loss_step</td><td>0.47963</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.78066</td></tr><tr><td>val_auc</td><td>0.85429</td></tr><tr><td>val_f1</td><td>0.78422</td></tr><tr><td>val_loss_epoch</td><td>0.47941</td></tr><tr><td>val_loss_step</td><td>0.47536</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_16_onehot_1</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/21a8tb7i' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/21a8tb7i</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240102_215454-21a8tb7i\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53bd1d98a8c746aabd222a40c412f061",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01693333333435779, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240102_222615-nw8muu9u</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/nw8muu9u' target=\"_blank\">MLP_3_16_onehot_1</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/nw8muu9u' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/nw8muu9u</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 768   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "922       Trainable params\n",
      "0         Non-trainable params\n",
      "922       Total params\n",
      "0.004     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f550ea88572488088458cce2a36259b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▂▂▃▃▃▄▄▄▄▅▅▅▆▆▇█▇███▇▇███▇▇▇▇▇███▇█▇▇▇</td></tr><tr><td>train_auc</td><td>▁▂▂▂▂▂▃▃▄▄▄▅▄▅▅▆▆▇▇▇▇▇▇▇███▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>train_f1</td><td>▅▄▄▁▄▂▁▅▄▂▅▆▆▅▅▅▇██▇█▇▇▇▆▆█▆▆▇█▆▇▇▆▇▇▇▅▅</td></tr><tr><td>train_loss_epoch</td><td>█▆▆▆▅▅▅▅▅▅▄▄▅▄▃▃▃▂▃▂▂▂▂▂▂▁▁▂▂▂▂▂▂▁▁▂▁▂▂▂</td></tr><tr><td>train_loss_step</td><td>█▇▇▇▇▇▇▇▇█▇▆▇▆▄▃▆▆▆▄▄▄▆▄▅▃▅▆▆▄▄▂▁▄▄▅▄▇▅▅</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>▁▆▃▃▄▅▅▄▄▁▁█▇███████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_epoch</td><td>▁▂▁▁▁▁▁▂▃▃▄▄▅▄▄▄▄▄▅▅▇▇▆▆▇▅▆▆▇▆▇▇▇▇▇███▆█</td></tr><tr><td>val_loss_step</td><td>▁▂▁▂▂▁▁▂▃▂▃▅▃▂▄▃▃▃▃▄▆▆▅▅▆▄▅▅▆▆▅▆▆▅▇▆▄▆▅█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.62374</td></tr><tr><td>train_auc</td><td>0.6774</td></tr><tr><td>train_f1</td><td>0.60063</td></tr><tr><td>train_loss_epoch</td><td>0.64448</td></tr><tr><td>train_loss_step</td><td>0.6525</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.48821</td></tr><tr><td>val_auc</td><td>0.77103</td></tr><tr><td>val_f1</td><td>0.6561</td></tr><tr><td>val_loss_epoch</td><td>0.76496</td></tr><tr><td>val_loss_step</td><td>0.79806</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_16_onehot_1</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/nw8muu9u' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/nw8muu9u</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240102_222615-nw8muu9u\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffab3600502c495d9c276309a01ddb85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01691666666495924, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240102_225909-k3sidrw2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/k3sidrw2' target=\"_blank\">MLP_3_16_onehot_1</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/k3sidrw2' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/k3sidrw2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 768   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "922       Trainable params\n",
      "0         Non-trainable params\n",
      "922       Total params\n",
      "0.004     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b16fa1f93e444e69196d72ab209a472",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▂▃▄▅▅▇▆▆▇▇▇▇▇▇█▇▇█▇██▇██▇██▇█▇█▇██████</td></tr><tr><td>train_auc</td><td>▁▁▂▃▂▄▅▆▆▇▇▇▇▇█▇█▇▆▇▆▇▇▇█▇▇▇▇▇▇▇█▆▇▇▇▇▆▆</td></tr><tr><td>train_f1</td><td>▁▁▃▃▅▅▆▆▆▆▆▇▆▇▇▇█▇▇█▇▇▇▇▇▇▇▇▇▇█▇▇▇▇███▇▇</td></tr><tr><td>train_loss_epoch</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▃▂▂▂▂▂▁▁▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▄▅▅▇▅▅▄▅▆▇▇▇▇▆█▇▆▇▇▇▇▇▇▇▇▇▇██▆██▇█▇▆▇▇▇</td></tr><tr><td>val_auc</td><td>▁▃▄▅▆▇▇▇▇▇██████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▆▆▅▇▆▆▄▄▆▇▇▇▆▆█▇▆▇▇▇▇▇▇▇▇▇▇██▆██▇█▆▆▇▇▇</td></tr><tr><td>val_loss_epoch</td><td>█▇▆▆▅▅▄▅▄▄▃▃▂▃▂▂▂▃▂▂▂▂▂▂▂▂▂▂▁▂▂▂▁▂▁▂▂▂▁▁</td></tr><tr><td>val_loss_step</td><td>█▇▆▆▅▅▄▅▄▄▄▃▃▄▂▃▂▃▃▃▂▂▂▃▃▂▃▂▂▃▃▂▁▂▁▃▃▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.73243</td></tr><tr><td>train_auc</td><td>0.66768</td></tr><tr><td>train_f1</td><td>0.74129</td></tr><tr><td>train_loss_epoch</td><td>0.54858</td></tr><tr><td>train_loss_step</td><td>0.5309</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.76651</td></tr><tr><td>val_auc</td><td>0.87215</td></tr><tr><td>val_f1</td><td>0.74151</td></tr><tr><td>val_loss_epoch</td><td>0.47478</td></tr><tr><td>val_loss_step</td><td>0.45047</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_16_onehot_1</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/k3sidrw2' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/k3sidrw2</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240102_225909-k3sidrw2\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "680e638aba8e41409a2d38a77c256d35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01691666666883975, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240102_233021-mm581ytc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/mm581ytc' target=\"_blank\">MLP_3_16_onehot_1</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/mm581ytc' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/mm581ytc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 768   \n",
      "2  | head        | Sequential       | 154   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 17    \n",
      "--------------------------------------------------\n",
      "939       Trainable params\n",
      "0         Non-trainable params\n",
      "939       Total params\n",
      "0.004     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "709f191cf5464aeea8154e626664579e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▂▃▅▆▇▇▇▇▇▇▇▇██▇▇█▇█████████▇██████▇██▇</td></tr><tr><td>train_auc</td><td>▁▂▂▃▅▆▇▇▇█▇█▇███▇████████████▇██████████</td></tr><tr><td>train_f1</td><td>▅▁▃▄▄▅▇▇█▇▇▇█▇██▇▇█▇▇██████▇▇███████▇███</td></tr><tr><td>train_loss_epoch</td><td>█▇▇▇▆▄▃▃▃▂▂▂▃▂▂▁▂▂▂▂▂▂▂▂▂▂▂▂▂▃▂▂▂▁▂▂▂▂▂▂</td></tr><tr><td>train_loss_step</td><td>███▇▇▇▃▄▃▃▄▃▃▂▂▃▂▁▄▂▃▂▂▁▂▃▄▃▁▄▃▃▂▃▃▂▃▂▃▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▂▂▇▄▇▇█▇█▇█▇██▇▇█▇▇█▇▇█▇▇▇▇█▇▇▇█▇▇▇█▇▇▇</td></tr><tr><td>val_auc</td><td>▁▆▇▇▇▇██████████████████████████████████</td></tr><tr><td>val_f1</td><td>▇▁▇█▅▇██▇█▇█▇██▇███▇█▇▇██▇▇▇█▇▇▇█▇█▇██▇▇</td></tr><tr><td>val_loss_epoch</td><td>███▇▆▄▂▂▂▁▁▂▂▂▁▃▁▁▁▂▁▁▁▁▁▂▂▁▁▂▂▁▁▁▁▁▁▁▁▂</td></tr><tr><td>val_loss_step</td><td>███▇▇▅▄▂▃▂▂▃▂▃▃▄▂▃▁▂▃▂▂▃▂▃▂▁▂▃▃▃▂▂▁▁▂▃▂▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.7466</td></tr><tr><td>train_auc</td><td>0.82396</td></tr><tr><td>train_f1</td><td>0.76127</td></tr><tr><td>train_loss_epoch</td><td>0.5266</td></tr><tr><td>train_loss_step</td><td>0.51886</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.74764</td></tr><tr><td>val_auc</td><td>0.85434</td></tr><tr><td>val_f1</td><td>0.70028</td></tr><tr><td>val_loss_epoch</td><td>0.50639</td></tr><tr><td>val_loss_step</td><td>0.52525</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_16_onehot_1</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/mm581ytc' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/mm581ytc</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240102_233021-mm581ytc\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2ae587e4fe54c5f813c1b79719ef80f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01693333333435779, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240103_000230-3cm7rvux</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/3cm7rvux' target=\"_blank\">MLP_3_16_onehot_1</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/3cm7rvux' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/3cm7rvux</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 768   \n",
      "2  | head        | Sequential       | 154   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 281   \n",
      "--------------------------------------------------\n",
      "1.2 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.2 K     Total params\n",
      "0.005     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10974a130346409da3a48dac6afcb3b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▁▄▆▇▇▇▇██▇█▇███████▇██████████████████</td></tr><tr><td>train_auc</td><td>▁▁▁▃▆▇▇▇▇▇▇▇█▇███▇████████▇█████████████</td></tr><tr><td>train_f1</td><td>▅▁▄▄▆▇▆▆▇█▇▇█▇█▇███▇█▇▇████████████████▇</td></tr><tr><td>train_loss_epoch</td><td>██▇▇▅▄▄▃▃▂▂▂▂▂▁▂▂▂▂▂▁▂▂▂▂▁▂▂▂▂▂▂▁▁▂▁▁▁▂▁</td></tr><tr><td>train_loss_step</td><td>███▇▆▅▄▆▄▂▁▅▄▄▃▂▃▃▂▄▂▃▄▂▅▂▂▂▃▂▂▄▃▃▄▃▃▄▃▄</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▁▁▇▅▅▆█▇▇█▇▇█▇██████▇█▇▇▇▇██▇▇█▇█▇█████</td></tr><tr><td>val_auc</td><td>▁▇▇▇▇▇▇█████████████████████████████████</td></tr><tr><td>val_f1</td><td>▄▄▄▇▂▁▂█▆▆█▆▆▇▆███▇██▆█▇▅▆▇█▇▆▆█▆█████▇█</td></tr><tr><td>val_loss_epoch</td><td>███▇▄▄▄▂▂▂▂▂▂▂▂▁▁▂▁▂▁▁▁▁▂▂▁▂▂▁▁▂▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_step</td><td>███▇▅▅▆▃▄▃▄▃▃▃▄▁▂▄▂▄▃▂▃▃▃▂▂▃▃▂▂▄▁▂▂▃▁▂▃▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.75487</td></tr><tr><td>train_auc</td><td>0.83194</td></tr><tr><td>train_f1</td><td>0.74555</td></tr><tr><td>train_loss_epoch</td><td>0.51361</td></tr><tr><td>train_loss_step</td><td>0.54344</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.77594</td></tr><tr><td>val_auc</td><td>0.85367</td></tr><tr><td>val_f1</td><td>0.77327</td></tr><tr><td>val_loss_epoch</td><td>0.47727</td></tr><tr><td>val_loss_step</td><td>0.47098</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_16_onehot_1</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/3cm7rvux' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/3cm7rvux</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240103_000230-3cm7rvux\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8252a2dde9141eab14242baf6fc588a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240103_003336-z2mg574d</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/z2mg574d' target=\"_blank\">MLP_3_32_onehot_1</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/z2mg574d' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/z2mg574d</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 2.6 K \n",
      "2 | head        | Sequential       | 562   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "3.1 K     Trainable params\n",
      "0         Non-trainable params\n",
      "3.1 K     Total params\n",
      "0.012     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44eb60e837d44821961a7ecc6c09283a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▃▆▇▇▇▇▇▇▇██████▇████████▇██████████████</td></tr><tr><td>train_auc</td><td>▁▃▆▇████████████████████████████████████</td></tr><tr><td>train_f1</td><td>▂▁▇▇▇▇▇▇█▇███▇██▇█████████▇███████████▇█</td></tr><tr><td>train_loss_epoch</td><td>██▅▂▂▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▂▂▁▁▁▁▁▂▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>██▆▃▄▁▃▂▂▅▂▂▃▃▃▂▂▁▂▁▂▃▃▁▁▂▃▂▂▃▁▂▃▂▃▂▃▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▇▆█▇█▇█▇▇██▇███▇▇▇██████▇█▇██████▇▇███▇</td></tr><tr><td>val_auc</td><td>▁▂▁▅▇▇██████████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▅▁█▃█▂█▃▇██▅██▇▇▅▆▇▇▇▆▇▆▅█▇▇▇▇▇▆▇█▆▇▆▇▇</td></tr><tr><td>val_loss_epoch</td><td>█▇▄▃▂▁▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▂▁▂▁▁▁▁▁</td></tr><tr><td>val_loss_step</td><td>█▇▅▄▃▃▃▄▄▃▄▂▂▃▃▁▂▃▂▂▂▂▃▂▁▂▂▄▂▂▁▃▄▃▃▁▁▃▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.78145</td></tr><tr><td>train_auc</td><td>0.84033</td></tr><tr><td>train_f1</td><td>0.77764</td></tr><tr><td>train_loss_epoch</td><td>0.49503</td></tr><tr><td>train_loss_step</td><td>0.47204</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.76179</td></tr><tr><td>val_auc</td><td>0.85483</td></tr><tr><td>val_f1</td><td>0.77506</td></tr><tr><td>val_loss_epoch</td><td>0.47884</td></tr><tr><td>val_loss_step</td><td>0.47125</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_32_onehot_1</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/z2mg574d' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/z2mg574d</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240103_003336-z2mg574d\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f197bab05714e5e877311b739d986db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240103_010631-m3g48znu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/m3g48znu' target=\"_blank\">MLP_3_32_onehot_1</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/m3g48znu' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/m3g48znu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 2.6 K \n",
      "2 | head        | Sequential       | 562   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "3.1 K     Trainable params\n",
      "0         Non-trainable params\n",
      "3.1 K     Total params\n",
      "0.012     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▃▆▇▆▇▇█▇▇▇▇▇▇█▇▇▇██▇███▇██▇███▇███▇████</td></tr><tr><td>train_auc</td><td>▁▂▅▇▆▇▇▇▇▇▇▇▇▇▇▇█▇██▇▇▇▇▇█▇██████▇█▇████</td></tr><tr><td>train_f1</td><td>▃▁▇▆▅▆▇▇▇▇▇▆▇▇▇▇▇▇██▇▇▇▇▇█▇▇██▇▇██▇▇████</td></tr><tr><td>train_loss_epoch</td><td>█▇▅▃▄▃▃▂▃▃▃▂▂▂▂▂▁▂▂▂▂▂▂▂▂▂▂▁▁▂▁▂▂▂▂▂▁▁▁▂</td></tr><tr><td>train_loss_step</td><td>██▇▅▆▃▃▄▄▃▃▄▄▅▃▄▃▄▃▄▄▅▂▂▄▃▂▄▂▅▁▄▃▂▃▃▃▂▃▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▂▂▂▂▂████████████▅▅█▁▁▁▁▁▁▁██▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>▁▆▅▇▆▆▅▅▇▆▆▇▇▆▆▆▆▇██▇▇▆▇▇▆▆▆▆▆▆▆▇▇▇▆▆▆▆▇</td></tr><tr><td>val_f1</td><td>▇▁▁▁▁███████████████▇▇▇▇▇▇▇██▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>val_loss_epoch</td><td>▇▇▃▃▃▂▂▁▂▁▂▁▂▁▂▂▃▃▃▃▄▅▅▆▆▄▄▃▃▄▄▇▇▆█▇█▇▇▆</td></tr><tr><td>val_loss_step</td><td>▅▅▃▄▄▃▂▂▂▂▂▂▃▁▃▂▂▃▃▁▃▃▄▆▅▃▅▃▃▃▁█▇▄▇▅▄▅▅▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.72416</td></tr><tr><td>train_auc</td><td>0.76792</td></tr><tr><td>train_f1</td><td>0.72674</td></tr><tr><td>train_loss_epoch</td><td>0.56873</td></tr><tr><td>train_loss_step</td><td>0.54416</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.48821</td></tr><tr><td>val_auc</td><td>0.7893</td></tr><tr><td>val_f1</td><td>0.6561</td></tr><tr><td>val_loss_epoch</td><td>0.6837</td></tr><tr><td>val_loss_step</td><td>0.65804</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_32_onehot_1</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/m3g48znu' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/m3g48znu</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240103_010631-m3g48znu\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07437ea2236845d2a3e131c82d20aa94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01691666666883975, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240103_013827-4x9dqb2c</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/4x9dqb2c' target=\"_blank\">MLP_3_32_onehot_1</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/4x9dqb2c' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/4x9dqb2c</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 2.6 K \n",
      "2 | head        | Sequential       | 562   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "3.1 K     Trainable params\n",
      "0         Non-trainable params\n",
      "3.1 K     Total params\n",
      "0.012     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52b2d36e928e44ad964cb15d9abcdbc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▂▅▆▆▇▇▆▆▇▇▇▇█▇▇▇█▇▇██▇▇▇█████▇████████</td></tr><tr><td>train_auc</td><td>▁▂▁▃▅▆▅▆▅▅▅▆▆▆▇▆▇▇▇▇▇▇▇▇▇▇▇███▇▇▇█▇▇████</td></tr><tr><td>train_f1</td><td>▁▃▃▄▅▆▆▇▆▆▇▇▇▇▇▇▇█▇▇▇██▇█▇██████▇███████</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▄▄▃▃▂▂▂▂▂▂▂▂▂▃▂▁▁▂▁▂▂▂▁▁▂▂▂▂▂▁▂▂▁▂▂▂▁▁▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▁▄▅▄▄▄▇▇▇▆▇▇▇▇▇█▇█▇▇█▆█▆▇█▇█▇█▇▇███▇█▇█</td></tr><tr><td>val_auc</td><td>▁▁▄▇▇▇██████████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▅▆▅▅▅▇▇▇▇▇▇▇▇▇█▇█▇██▇█▇▇█▇█▇█▇▇███▇█▇█</td></tr><tr><td>val_loss_epoch</td><td>█▅▄▄▃▃▃▂▂▂▂▂▂▂▁▂▁▂▁▂▁▁▂▁▂▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_step</td><td>█▆▅▅▅▅▄▄▃▃▃▃▂▃▂▂▂▃▂▂▁▂▄▂▃▂▂▃▂▃▂▃▂▃▂▁▂▃▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.76905</td></tr><tr><td>train_auc</td><td>0.79659</td></tr><tr><td>train_f1</td><td>0.77067</td></tr><tr><td>train_loss_epoch</td><td>0.51509</td></tr><tr><td>train_loss_step</td><td>0.50951</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.77594</td></tr><tr><td>val_auc</td><td>0.87607</td></tr><tr><td>val_f1</td><td>0.7619</td></tr><tr><td>val_loss_epoch</td><td>0.45411</td></tr><tr><td>val_loss_step</td><td>0.44851</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_32_onehot_1</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/4x9dqb2c' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/4x9dqb2c</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240103_013827-4x9dqb2c\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98ab805f9e1a4da382c6f187b1c0974d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240103_020904-4hvcoqtd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/4hvcoqtd' target=\"_blank\">MLP_3_32_onehot_1</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/4hvcoqtd' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/4hvcoqtd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 2.6 K \n",
      "2  | head        | Sequential       | 562   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 33    \n",
      "--------------------------------------------------\n",
      "3.2 K     Trainable params\n",
      "0         Non-trainable params\n",
      "3.2 K     Total params\n",
      "0.013     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1b13bf8b3ff45eba50934aaafacde89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▃▆▇▇███▇██▇███▇██████▇████████████▇████</td></tr><tr><td>train_auc</td><td>▁▂▆▇▇███████████████████████████████████</td></tr><tr><td>train_f1</td><td>▃▁▅▇▇███▇██▇███▇██████▇████████████▇████</td></tr><tr><td>train_loss_epoch</td><td>██▆▄▃▂▂▂▂▁▁▂▂▂▂▂▂▂▂▂▁▂▂▁▁▁▁▁▂▂▁▁▁▂▁▂▁▁▁▂</td></tr><tr><td>train_loss_step</td><td>██▇▄▅▄▂▄▄▁▂▄▃▂▂▃▂▃▄▄▄▂▁▁▃▂▃▃▃▁▃▃▁▄▃▃▄▃▄▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁█▅▆▇▇█▇█▇███▇█▇██▇██▇██████▇██████▇████</td></tr><tr><td>val_auc</td><td>▃▄▁▅▆▇▇▇████████████████████████████████</td></tr><tr><td>val_f1</td><td>▃▇▁▂█▅█▆██▆▇█▆██▇█▅▇█▄███▇▇█▅▆▇█▇█▅██▇█▅</td></tr><tr><td>val_loss_epoch</td><td>█▇▄▃▃▂▂▁▂▂▁▁▂▂▁▂▁▁▁▁▁▂▂▁▁▁▁▁▁▁▁▁▁▂▂▂▁▁▁▂</td></tr><tr><td>val_loss_step</td><td>██▅▅▄▄▄▂▃▃▂▃▅▄▂▃▂▃▂▄▃▄▃▃▃▂▃▁▃▃▃▂▃▄▅▃▂▄▂▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.76196</td></tr><tr><td>train_auc</td><td>0.83175</td></tr><tr><td>train_f1</td><td>0.76826</td></tr><tr><td>train_loss_epoch</td><td>0.50964</td></tr><tr><td>train_loss_step</td><td>0.47662</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.75943</td></tr><tr><td>val_auc</td><td>0.85634</td></tr><tr><td>val_f1</td><td>0.71978</td></tr><tr><td>val_loss_epoch</td><td>0.495</td></tr><tr><td>val_loss_step</td><td>0.51538</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_32_onehot_1</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/4hvcoqtd' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/4hvcoqtd</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240103_020904-4hvcoqtd\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "188e064e254d480c9d602edd4df7e57d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240103_023919-0jspkpoy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/0jspkpoy' target=\"_blank\">MLP_3_32_onehot_1</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/0jspkpoy' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/0jspkpoy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 2.6 K \n",
      "2  | head        | Sequential       | 562   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 1.1 K \n",
      "--------------------------------------------------\n",
      "4.2 K     Trainable params\n",
      "0         Non-trainable params\n",
      "4.2 K     Total params\n",
      "0.017     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bf9bb1c3e174314a41a3bd350cbb04a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▅▆▇▇█▇█▇█▇███▇██▇██████▇████████▇█████</td></tr><tr><td>train_auc</td><td>▁▂▆▇█▇█████▇████████████████████████████</td></tr><tr><td>train_f1</td><td>▁▁▅▆▇▇█▇█▇█▇▇██▇▇███▇██▇█▇██████████████</td></tr><tr><td>train_loss_epoch</td><td>██▅▄▂▂▂▂▁▂▂▂▂▁▁▂▂▂▂▁▂▁▂▁▂▂▂▂▂▁▁▁▁▁▁▁▂▁▁▁</td></tr><tr><td>train_loss_step</td><td>██▆▃▃▃▄▃▅▄▄▄▅▂▂▂▂▃▃▂▃▂▅▂▃▃▄▂▂▂▁▃▂▅▂▂▃▂▃▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁█▆▆██▇███▇██▇████▇██▇███▇████▇██▇██▇▇██</td></tr><tr><td>val_auc</td><td>▄▄▁▅▆▇▇▇████████████████████████████████</td></tr><tr><td>val_f1</td><td>▁█▂▁▇█▅▇▇▇▄▆█▅█▆▇█▆▇▇▇▇█▇▅▇▇▇▇▇▇█▇▇▆▄▇▇▇</td></tr><tr><td>val_loss_epoch</td><td>█▇▄▃▂▂▂▂▁▂▂▂▁▁▁▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▂▂▁▁</td></tr><tr><td>val_loss_step</td><td>█▇▄▃▃▂▃▄▃▄▃▃▃▂▃▂▁▄▅▂▃▃▁▂▃▂▂▂▃▃▄▃▃▁▃▂▂▃▂▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.77141</td></tr><tr><td>train_auc</td><td>0.84203</td></tr><tr><td>train_f1</td><td>0.77114</td></tr><tr><td>train_loss_epoch</td><td>0.49434</td></tr><tr><td>train_loss_step</td><td>0.48291</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.77123</td></tr><tr><td>val_auc</td><td>0.85465</td></tr><tr><td>val_f1</td><td>0.7696</td></tr><tr><td>val_loss_epoch</td><td>0.48268</td></tr><tr><td>val_loss_step</td><td>0.51377</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_32_onehot_1</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/0jspkpoy' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/0jspkpoy</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240103_023919-0jspkpoy\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5244565ca53d4c25a89c58a844b21443",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01691666666883975, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240103_030920-uu184vgx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/uu184vgx' target=\"_blank\">MLP_3_64_onehot_1</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/uu184vgx' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/uu184vgx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 9.2 K \n",
      "2 | head        | Sequential       | 2.1 K \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "11.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "11.4 K    Total params\n",
      "0.045     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78527328749d4968841c6a7b14c0169d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▆▇█▇▇█▇▇▇▇██▇█▇█▇██▇█████▇█████▇█▇███▇█</td></tr><tr><td>train_auc</td><td>▁▆▇██▇███▇██████████████████████████████</td></tr><tr><td>train_f1</td><td>▁▆▇█▇▇█▇▇▇▇██▇█▇█▇██▇████████▇██████▇███</td></tr><tr><td>train_loss_epoch</td><td>█▅▂▂▂▂▂▁▂▂▂▁▁▂▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▂▁▁▂▂▁▁▂▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▅▄▄▂▃▄▂▃▂▃▃▂▂▄▄▂▁▁▄▅▃▃▃▂▃▂▃▃▃▄▂▂▁▃▁▃▂▂▄</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▇▆█▇▆▇███▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▆▇▇▇</td></tr><tr><td>val_auc</td><td>▁▂▅▆▇▇███████████████████████████▇██████</td></tr><tr><td>val_f1</td><td>▁▇▇█▇▆▇████▇█▆██▇████▇▇██▇▇█▆███▆█▇▇█▇██</td></tr><tr><td>val_loss_epoch</td><td>█▂▂▂▂▂▁▁▁▁▁▁▁▂▂▁▁▁▂▁▂▁▁▁▁▁▂▁▂▂▁▁▂▂▁▁▂▁▁▂</td></tr><tr><td>val_loss_step</td><td>█▂▃▄▃▃▁▃▂▂▂▂▂▃▃▂▁▂▄▁▃▁▁▂▃▃▂▃▃▃▂▁▃▄▃▁▂▃▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.77732</td></tr><tr><td>train_auc</td><td>0.84776</td></tr><tr><td>train_f1</td><td>0.77248</td></tr><tr><td>train_loss_epoch</td><td>0.48691</td></tr><tr><td>train_loss_step</td><td>0.5272</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.75943</td></tr><tr><td>val_auc</td><td>0.85556</td></tr><tr><td>val_f1</td><td>0.77232</td></tr><tr><td>val_loss_epoch</td><td>0.48469</td></tr><tr><td>val_loss_step</td><td>0.50229</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_64_onehot_1</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/uu184vgx' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/uu184vgx</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240103_030920-uu184vgx\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d34665a7e39f4372bf709013ec9f349a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01693333333435779, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240103_034021-hfxfx72d</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/hfxfx72d' target=\"_blank\">MLP_3_64_onehot_1</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/hfxfx72d' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/hfxfx72d</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 9.2 K \n",
      "2 | head        | Sequential       | 2.1 K \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "11.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "11.4 K    Total params\n",
      "0.045     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91e555b608e744c4b294efcb8c48ef97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▆▆▆▇▇▆▇▇▇▇▇▇█▇█▇▇█▇▇█▇███▇▇▇█▇███████▇</td></tr><tr><td>train_auc</td><td>▁▂▅▆▆▆▇▇▇▇▇█▇██▇█▇█▇▇▇▇█████▇███████████</td></tr><tr><td>train_f1</td><td>▁▂▆▅▆▇▇▆▇▆▆▇▇▇▇▇█▇▇█▇▇█▇███▇█▇▇█████▇██▇</td></tr><tr><td>train_loss_epoch</td><td>█▇▅▅▄▄▃▃▃▃▃▂▂▂▂▂▂▂▁▂▂▂▂▂▂▂▂▁▂▂▁▂▁▁▁▁▁▁▁▂</td></tr><tr><td>train_loss_step</td><td>██▆▅▄▅▄▃▄▄▂▁▂▃▃▆▄▁▂▅▃▂▄▃▃▄▃▁▂▄▄▃▂▁▃▂▂▂▃▄</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▂▂██████▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>▁▁▅█▆▆▆▇▇▆▇▆▇▅▇▆▇▆▆▆▆▆▆▆▆▆▅▇▆▆▆▇▇▅▅▇▅▅▃▅</td></tr><tr><td>val_f1</td><td>▁▁██████▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>val_loss_epoch</td><td>▃▃▁▁▁▂▁▂▂▂▂▃▄▄▄▃▄▄▅▄▅▅▄▅▅▆▅▅▄▅▆▅▆█▆█▇███</td></tr><tr><td>val_loss_step</td><td>▃▃▁▁▁▂▁▂▁▂▂▃▃▃▄▃▃▄▅▃▃▅▄▅▃▆▅▅▃▅▄▄▇█▅▆▅█▇▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.73479</td></tr><tr><td>train_auc</td><td>0.79786</td></tr><tr><td>train_f1</td><td>0.73065</td></tr><tr><td>train_loss_epoch</td><td>0.5299</td></tr><tr><td>train_loss_step</td><td>0.54971</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.48821</td></tr><tr><td>val_auc</td><td>0.77491</td></tr><tr><td>val_f1</td><td>0.6561</td></tr><tr><td>val_loss_epoch</td><td>0.84186</td></tr><tr><td>val_loss_step</td><td>0.82945</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_64_onehot_1</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/hfxfx72d' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/hfxfx72d</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240103_034021-hfxfx72d\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29138a4cecb14be9b27b074836b5dec6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240103_041110-mfpjruh8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/mfpjruh8' target=\"_blank\">MLP_3_64_onehot_1</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/mfpjruh8' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/mfpjruh8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 9.2 K \n",
      "2 | head        | Sequential       | 2.1 K \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "11.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "11.4 K    Total params\n",
      "0.045     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bdcbf6a60664126a11601ed48e73ef9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▂▄▆▆▆▇▇▇▇▇▇▇▇██▇▇▇▇████████▇██▇███████</td></tr><tr><td>train_auc</td><td>▁▁▂▅▆▆▇▇▇▇▇██▇████▇▇▇▇▇▇█▇▇▇▇▇██▇███████</td></tr><tr><td>train_f1</td><td>▃▁▂▄▆▆▆▇▇▇▇▇▇▇▇██▇▇█████████████████████</td></tr><tr><td>train_loss_epoch</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▃▃▂▂▂▂▁▂▂▂▂▂▂▂▂▁▁▁▂▂▁▁▁▁▁▁▁▂▂▂▁▁▁▂▁▁▁▂▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▆▃▄▆▄▇▆▆▇▇▇▇▆█▇▇▇█▇█▇▇█▇▇▇▇▆██▇▆█▇█▇▇██</td></tr><tr><td>val_auc</td><td>▁▂▇▇▇▇▇▇█▇██████████▇███████████████████</td></tr><tr><td>val_f1</td><td>▁█▄▅▆▅▇▇▇█▇▇▇▆█▇▇▇█▇█▇▇██▇▇▇▇██▇▇█▇█▇███</td></tr><tr><td>val_loss_epoch</td><td>█▅▅▄▃▄▃▂▂▂▂▂▂▃▂▁▁▁▂▂▂▁▁▁▁▁▂▂▂▁▂▁▂▂▂▁▁▁▂▁</td></tr><tr><td>val_loss_step</td><td>█▆▅▅▄▅▃▃▃▃▂▂▂▄▃▂▂▂▄▂▂▂▂▂▂▂▂▃▃▃▂▁▂▄▃▂▃▂▃▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.78145</td></tr><tr><td>train_auc</td><td>0.8089</td></tr><tr><td>train_f1</td><td>0.77844</td></tr><tr><td>train_loss_epoch</td><td>0.49958</td></tr><tr><td>train_loss_step</td><td>0.52546</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.79717</td></tr><tr><td>val_auc</td><td>0.87705</td></tr><tr><td>val_f1</td><td>0.79717</td></tr><tr><td>val_loss_epoch</td><td>0.45994</td></tr><tr><td>val_loss_step</td><td>0.47239</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_64_onehot_1</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/mfpjruh8' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/mfpjruh8</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240103_041110-mfpjruh8\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cc8620be8864bed804ad35b59fcad29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240103_044113-6g3mtc7f</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/6g3mtc7f' target=\"_blank\">MLP_3_64_onehot_1</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/6g3mtc7f' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/6g3mtc7f</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 9.2 K \n",
      "2  | head        | Sequential       | 2.1 K \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 65    \n",
      "--------------------------------------------------\n",
      "11.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "11.4 K    Total params\n",
      "0.046     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fa24d0e13da4a43ae1f202acff6744f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▅▆▇▇█▇█▇████▇█▇██▇█████████▇▇██▇███████</td></tr><tr><td>train_auc</td><td>▁▆▇▇██▇███████████▇██████████▇██████████</td></tr><tr><td>train_f1</td><td>▁▅▆▇██▇█▇███▇▇█▇██▇██████████▇██████████</td></tr><tr><td>train_loss_epoch</td><td>█▅▃▃▂▂▂▂▂▁▁▂▁▂▁▁▁▂▂▁▁▁▁▁▁▁▂▁▂▂▁▁▂▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▅▄▂▄▃▅▁▄▁▄▂▃▃▃▄▃▃▃▁▁▂▂▃▃▂▂▃▃▃▂▃▃▃▄▂▄▂▂▄</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▆██▇▇██▇██▇███▇█▇████████▇█▆███▇█████▇▇</td></tr><tr><td>val_auc</td><td>▅▁▆▇▇▇▇█████████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▇██▇▇██▇██▇████████████████▇███▇█████▇▇</td></tr><tr><td>val_loss_epoch</td><td>█▄▃▂▂▃▂▁▂▂▁▂▂▂▁▂▁▂▁▁▂▂▁▂▁▂▃▁▄▂▂▂▂▁▁▂▁▂▂▂</td></tr><tr><td>val_loss_step</td><td>█▅▄▃▄▄▄▃▃▄▂▃▃▃▂▂▂▄▂▂▄▃▁▄▂▃▅▂▅▃▃▃▄▃▂▄▃▃▄▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.76964</td></tr><tr><td>train_auc</td><td>0.84706</td></tr><tr><td>train_f1</td><td>0.76868</td></tr><tr><td>train_loss_epoch</td><td>0.48686</td></tr><tr><td>train_loss_step</td><td>0.54182</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.75472</td></tr><tr><td>val_auc</td><td>0.85599</td></tr><tr><td>val_f1</td><td>0.72775</td></tr><tr><td>val_loss_epoch</td><td>0.48129</td></tr><tr><td>val_loss_step</td><td>0.47275</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_64_onehot_1</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/6g3mtc7f' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/6g3mtc7f</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240103_044113-6g3mtc7f\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1283f2e0238449f594f64613532f3a70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01693333333435779, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240103_051039-3pedd44w</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/3pedd44w' target=\"_blank\">MLP_3_64_onehot_1</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/3pedd44w' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/3pedd44w</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 9.2 K \n",
      "2  | head        | Sequential       | 2.1 K \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 4.2 K \n",
      "--------------------------------------------------\n",
      "15.6 K    Trainable params\n",
      "0         Non-trainable params\n",
      "15.6 K    Total params\n",
      "0.062     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▃▆▇▇▇█▇█▇████▇████▇████████████████████</td></tr><tr><td>train_auc</td><td>▁▃▇▇▇███████████████████████████████████</td></tr><tr><td>train_f1</td><td>▁▂▅▆▇▇▇▇█▇████▇█▇██▇████▇██▇█▇██████▇█▇▇</td></tr><tr><td>train_loss_epoch</td><td>█▇▄▃▂▂▁▂▁▁▁▁▁▁▂▂▂▂▁▂▁▁▁▁▁▁▁▁▂▁▁▂▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▇▅▃▄▃▄▃▃▃▃▃▂▃▃▂▂▄▃▄▂▄▂▄▄▃▂▄▃▃▄▂▄▂▄▁▃▃▃▄</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▇▇█▇██████▇███▇▇██▇▇██████▇▇██▇█████▇█▇</td></tr><tr><td>val_auc</td><td>▂▁▁▅▇▇██████████████████████████████████</td></tr><tr><td>val_f1</td><td>▁███▇██████▇███▇████▇▇█▇████▇██▇████████</td></tr><tr><td>val_loss_epoch</td><td>█▇▃▂▁▁▁▁▁▁▁▁▁▁▁▂▂▁▁▃▁▂▁▂▁▁▁▁▂▁▁▂▁▁▁▁▁▁▁▂</td></tr><tr><td>val_loss_step</td><td>█▇▆▂▁▂▃▁▃▃▂▂▂▁▂▂▄▂▄▄▂▂▂▂▁▃▃▂▄▂▃▃▃▃▃▃▃▁▂▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.77023</td></tr><tr><td>train_auc</td><td>0.84799</td></tr><tr><td>train_f1</td><td>0.7641</td></tr><tr><td>train_loss_epoch</td><td>0.48507</td></tr><tr><td>train_loss_step</td><td>0.51908</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.75708</td></tr><tr><td>val_auc</td><td>0.85443</td></tr><tr><td>val_f1</td><td>0.77363</td></tr><tr><td>val_loss_epoch</td><td>0.48812</td></tr><tr><td>val_loss_step</td><td>0.5152</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_64_onehot_1</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/3pedd44w' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/3pedd44w</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240103_051039-3pedd44w\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc127c517e6a436380b4cbee6195c03a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240103_054056-t5k4s9yq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/t5k4s9yq' target=\"_blank\">MLP_4_16_onehot_1</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/t5k4s9yq' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/t5k4s9yq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 1.0 K \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "1.2 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.2 K     Total params\n",
      "0.005     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac428a74cdc74f578f9c5bfcf1d12c3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▁▂▄▆▆▇█▇▇▇▇▇█▇▇▇█▇█▇▇▇██▇█▇█▇███████▇█</td></tr><tr><td>train_auc</td><td>▁▂▁▂▄▇▇▇████████▇▇███████▇██████████████</td></tr><tr><td>train_f1</td><td>▁▆▄▄▆▇▇███▇▇████████████████████████████</td></tr><tr><td>train_loss_epoch</td><td>███▇▆▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▂▁▂▂▂▂▁▂▁▂▁▂▁▁▂▂▁▁▁</td></tr><tr><td>train_loss_step</td><td>████▇▄▄▄▂▃▄▃▃▃▂▁▂▃▄▃▄▂▄▃▅▃▂▃▂▄▂▃▃▂▂▃▃▂▃▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▂▁▅▆▆██▇████▇██▇███▇███▇██████▇████████</td></tr><tr><td>val_auc</td><td>▁▅▅▆▆▆▇▇████████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▃▁▆▇▇██▇████▇██▇███▇███████████████████</td></tr><tr><td>val_loss_epoch</td><td>████▅▃▃▂▂▂▂▂▂▂▁▁▂▁▂▁▂▁▂▁▂▁▂▁▁▁▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_step</td><td>████▆▃▄▃▃▄▃▃▄▃▂▂▂▃▄▃▃▃▃▃▃▂▅▃▂▃▄▃▃▂▄▃▁▃▂▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.75724</td></tr><tr><td>train_auc</td><td>0.82155</td></tr><tr><td>train_f1</td><td>0.75492</td></tr><tr><td>train_loss_epoch</td><td>0.51904</td></tr><tr><td>train_loss_step</td><td>0.53929</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.76651</td></tr><tr><td>val_auc</td><td>0.85432</td></tr><tr><td>val_f1</td><td>0.75312</td></tr><tr><td>val_loss_epoch</td><td>0.48537</td></tr><tr><td>val_loss_step</td><td>0.50943</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_16_onehot_1</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/t5k4s9yq' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/t5k4s9yq</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240103_054056-t5k4s9yq\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "626a89f4c45e426484c27664afe0ed82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016933333330477276, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240103_061033-z39i2dfa</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/z39i2dfa' target=\"_blank\">MLP_4_16_onehot_1</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/z39i2dfa' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/z39i2dfa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 1.0 K \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "1.2 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.2 K     Total params\n",
      "0.005     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15f8a78fd43a4c6a9e137cd5680f7a56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▂▅▆▆▆▆▆▆▆▇▇▇▆▇▇▆▇██▇█▇▇▇█▇█▇██▇███▇███</td></tr><tr><td>train_auc</td><td>▁▂▂▃▅▄▄▅▅▄▄▅▅▅▄▆▅▆▆▇▆▆▆▇▆▆▇▇▇▇██▇███▇███</td></tr><tr><td>train_f1</td><td>▁▄▆▆▆▇▇▇▆▆▇▇▇▇▇▇▇▆▇██▇█▇▇▇█▇█▇██████▇███</td></tr><tr><td>train_loss_epoch</td><td>█▇▇▅▅▄▄▃▄▄▃▃▃▄▃▃▃▃▂▂▃▃▁▂▂▂▂▁▁▂▁▂▃▂▂▁▂▂▁▂</td></tr><tr><td>train_loss_step</td><td>██▇▆▆▆▅▆▄▆▅▄▃▄▃▄▄▄▅▅▆▅▄▃▅▄▂▅▃▄▁▆▄▄▃▅▃▅▃▄</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>▂▁▂▃▄▅▅▃▄▄▄▄▅▃▃▃▄▅▅▅█▇▅▆▇▇▇▅██▆▆▆▆▆▆▇██▆</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_epoch</td><td>█▇▇▅██▇▇▆▆▇▇▆▇▄▄▃▂▄▄▄▂▁▂▃▃▁▂▃▁▁▃▂▂▃▄▂▄▃▃</td></tr><tr><td>val_loss_step</td><td>██▇▅▆█▇▆▇▆▆▅▆▇▃▃▅▂▅█▆▄▄▅▆▅▃▅▅▄▅▇▅▅▆▇▁▇▅▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.67513</td></tr><tr><td>train_auc</td><td>0.70331</td></tr><tr><td>train_f1</td><td>0.66258</td></tr><tr><td>train_loss_epoch</td><td>0.62436</td></tr><tr><td>train_loss_step</td><td>0.62946</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.51179</td></tr><tr><td>val_auc</td><td>0.74829</td></tr><tr><td>val_f1</td><td>0.0</td></tr><tr><td>val_loss_epoch</td><td>0.68127</td></tr><tr><td>val_loss_step</td><td>0.68126</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_16_onehot_1</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/z39i2dfa' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/z39i2dfa</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240103_061033-z39i2dfa\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8ad612e10a04f8e9f361a17ff8cfbc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240103_064039-1fmdx4yd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/1fmdx4yd' target=\"_blank\">MLP_4_16_onehot_1</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/1fmdx4yd' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/1fmdx4yd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 1.0 K \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "1.2 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.2 K     Total params\n",
      "0.005     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "787e0172fa1549a1bbba3390822fb422",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▁▁▂▁▂▃▄▅▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇█</td></tr><tr><td>train_auc</td><td>▁▁▁▃▂▁▃▃▆▆▇▇▇▇██▇▇█████▇█▇█▇▇▇▇█▇▇▆▇▄▆▆▆</td></tr><tr><td>train_f1</td><td>▄▃▂▁▃▃▃▄▅▅▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇██▇█▇▇█▇█</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▅▄▃▃▃▃▃▂▂▂▂▂▁▃▁▂▂▂▁▁▂▁▂▂▂▁▁▁▁▁▁▁▂▁▂▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▂▁▂▂▃▄▇▇▇▆▇▇▇▆▇▇█▇▇▇█▇█▇█▇████▇▇█▇█▇▇█▇</td></tr><tr><td>val_auc</td><td>▂▁▂▂▅▆▅▅▇▇▆▇▇█▇█████████████████████████</td></tr><tr><td>val_f1</td><td>▆▆▂▃▁▄▅█▇▇██████▇██▇███████████▇██▇██▇██</td></tr><tr><td>val_loss_epoch</td><td>█▇▇▆▆▆▆▆▄▄▅▃▃▂▄▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▂▁▁▁</td></tr><tr><td>val_loss_step</td><td>█▇▆▆▆▆▆▆▄▄▄▃▄▂▄▂▂▂▂▂▂▂▂▂▂▂▃▂▁▂▂▁▁▁▂▂▁▂▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.76432</td></tr><tr><td>train_auc</td><td>0.64281</td></tr><tr><td>train_f1</td><td>0.77368</td></tr><tr><td>train_loss_epoch</td><td>0.52624</td></tr><tr><td>train_loss_step</td><td>0.54927</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.77594</td></tr><tr><td>val_auc</td><td>0.86852</td></tr><tr><td>val_f1</td><td>0.7619</td></tr><tr><td>val_loss_epoch</td><td>0.48123</td></tr><tr><td>val_loss_step</td><td>0.49503</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_16_onehot_1</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/1fmdx4yd' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/1fmdx4yd</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240103_064039-1fmdx4yd\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d93afa2daf5142b88e9f8f85102abec5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01691666666495924, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240103_071015-9bhxqi0b</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/9bhxqi0b' target=\"_blank\">MLP_4_16_onehot_1</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/9bhxqi0b' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/9bhxqi0b</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 1.0 K \n",
      "2  | head        | Sequential       | 154   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 17    \n",
      "--------------------------------------------------\n",
      "1.2 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.2 K     Total params\n",
      "0.005     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b46c3800b3c4a9f86446e6ede4552d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▁▁▁▄▅▇▆▇▇▇▇█▇▇▇████▇█▇███▇▇███▇█▇████▇</td></tr><tr><td>train_auc</td><td>▂▁▁▂▂▄▇▇▇▇█▇███▇████████████████████████</td></tr><tr><td>train_f1</td><td>▁▅▄▃▅▆▇▇▇▇█▇███▇████████████████████████</td></tr><tr><td>train_loss_epoch</td><td>█████▇▄▃▃▃▂▃▂▂▂▂▂▁▂▂▂▁▂▂▁▂▂▂▁▁▁▁▂▂▂▁▂▁▁▂</td></tr><tr><td>train_loss_step</td><td>█▇███▆▄▃▃▃▄▃▃▃▂▄▁▂▄▄▂▁▃▃▃▂▃▃▂▂▃▁▂▂▂▃▃▂▄▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▁▁▁▃▆▇██▇██▇█▇▇▇█▇██▇▇█▇▇▇▇█▇███▇▇█▇██▆</td></tr><tr><td>val_auc</td><td>▁▄▅▆▆▆▇▇████████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▃▇███▇██▇██▇▇█▇██▇████▇▇█▇███▇▇█▇██▇</td></tr><tr><td>val_loss_epoch</td><td>█████▆▄▂▂▂▁▁▂▁▁▁▁▁▂▁▁▂▂▁▁▁▂▂▁▂▁▁▁▁▂▁▁▁▁▃</td></tr><tr><td>val_loss_step</td><td>█████▆▄▃▂▂▁▂▃▂▂▂▂▁▁▂▃▂▂▃▂▂▃▃▂▄▃▃▂▂▃▂▁▂▃▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.74956</td></tr><tr><td>train_auc</td><td>0.81657</td></tr><tr><td>train_f1</td><td>0.76018</td></tr><tr><td>train_loss_epoch</td><td>0.53679</td></tr><tr><td>train_loss_step</td><td>0.55333</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.71226</td></tr><tr><td>val_auc</td><td>0.85287</td></tr><tr><td>val_f1</td><td>0.62577</td></tr><tr><td>val_loss_epoch</td><td>0.55249</td></tr><tr><td>val_loss_step</td><td>0.59207</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_16_onehot_1</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/9bhxqi0b' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/9bhxqi0b</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240103_071015-9bhxqi0b\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "804f00b8458c4b3d9f72483a0a2df52b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240103_074225-gaodei6y</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/gaodei6y' target=\"_blank\">MLP_4_16_onehot_1</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/gaodei6y' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/gaodei6y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 1.0 K \n",
      "2  | head        | Sequential       | 154   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 281   \n",
      "--------------------------------------------------\n",
      "1.5 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.5 K     Total params\n",
      "0.006     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a15769393099485590eed924d1d3287b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▁▂▂▅▇▇▇▇▇▇▇▇▇▇▇▇▇██▇██▇█▇▇████▇▇██████</td></tr><tr><td>train_auc</td><td>▁▁▁▁▃▆▇▇▇▇▇▇▇▇▇▇▇███████████████████████</td></tr><tr><td>train_f1</td><td>▁▄▃▄▅▇▇▇████████████████████████████████</td></tr><tr><td>train_loss_epoch</td><td>████▇▅▃▂▂▂▂▃▂▂▃▂▂▂▂▂▁▂▂▁▂▁▂▁▂▁▁▁▂▂▂▁▂▁▁▁</td></tr><tr><td>train_loss_step</td><td>███▇▇▄▄▃▃▃▃▃▃▂▄▃▃▂▂▂▂▃▃▂▂▃▄▄▄▂▂▂▂▂▅▃▃▁▁▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▁▁▁▆▇▇████████████████████▇███▇████▇▇█▇</td></tr><tr><td>val_auc</td><td>▁▄▅▅▇▇▇█████████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▆███████████████████████████████████</td></tr><tr><td>val_loss_epoch</td><td>████▇▃▂▂▂▂▂▂▁▁▂▂▁▁▂▁▁▁▁▁▁▂▂▂▁▂▁▂▁▁▁▁▂▂▁▁</td></tr><tr><td>val_loss_step</td><td>████▇▄▃▃▃▃▄▄▂▂▄▂▃▃▃▂▃▃▃▂▃▄▄▄▃▄▃▃▃▂▃▂▂▄▃▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.76314</td></tr><tr><td>train_auc</td><td>0.83047</td></tr><tr><td>train_f1</td><td>0.75564</td></tr><tr><td>train_loss_epoch</td><td>0.50721</td></tr><tr><td>train_loss_step</td><td>0.4793</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.75708</td></tr><tr><td>val_auc</td><td>0.85309</td></tr><tr><td>val_f1</td><td>0.77754</td></tr><tr><td>val_loss_epoch</td><td>0.47516</td></tr><tr><td>val_loss_step</td><td>0.39606</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_16_onehot_1</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/gaodei6y' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/gaodei6y</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240103_074225-gaodei6y\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4f5e26f347949fe9f59f91cc8d3501e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01693333333435779, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240103_081204-pp96d8xx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/pp96d8xx' target=\"_blank\">MLP_4_32_onehot_1</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/pp96d8xx' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/pp96d8xx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 3.6 K \n",
      "2 | head        | Sequential       | 562   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "4.2 K     Trainable params\n",
      "0         Non-trainable params\n",
      "4.2 K     Total params\n",
      "0.017     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01ae234174844c68bf1dd06c9b39887e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▃▆▆▇▇▇▇▇▇██▇▇▇▇▇▇▇▇██▇███▇▇▇▇▇███▇██▇█</td></tr><tr><td>train_auc</td><td>▁▁▃▆▇▇▇██▇████▇█████████████████████████</td></tr><tr><td>train_f1</td><td>▁▁▂▅▅▇▇▇▇▇▇▇█▇▇▇▇▇▇▇▇▇█▇█▇▇▇▇▇█▇███▇██▇▇</td></tr><tr><td>train_loss_epoch</td><td>███▅▄▃▃▂▂▂▂▂▂▂▃▂▂▂▂▂▂▁▂▂▂▂▂▂▂▁▂▂▂▂▁▂▁▂▂▁</td></tr><tr><td>train_loss_step</td><td>███▇▅▄▃▄▄▅▅▄▄▄▄▃▅▄▄▄▃▂▃▄▃▂▃▄▂▄▃▃▂▂▅▃▃▂▄▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▂▁█▅▇██▇███▇██▇▇▇█████████████▇██████▇██</td></tr><tr><td>val_auc</td><td>▁▂▃▂▅▆▇▇████████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▇█▆███▇██████▇█▇█████████████▇█████████</td></tr><tr><td>val_loss_epoch</td><td>██▇▅▃▂▂▂▂▁▁▁▂▁▂▂▂▁▁▁▁▂▁▂▁▁▁▁▁▁▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_step</td><td>██▇▅▃▃▃▃▃▃▂▂▄▁▂▃▂▂▁▂▂▃▁▃▂▁▂▃▃▂▃▂▂▂▄▃▂▁▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.76669</td></tr><tr><td>train_auc</td><td>0.84467</td></tr><tr><td>train_f1</td><td>0.76104</td></tr><tr><td>train_loss_epoch</td><td>0.4809</td></tr><tr><td>train_loss_step</td><td>0.40998</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.77594</td></tr><tr><td>val_auc</td><td>0.85514</td></tr><tr><td>val_f1</td><td>0.77958</td></tr><tr><td>val_loss_epoch</td><td>0.47853</td></tr><tr><td>val_loss_step</td><td>0.48179</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_32_onehot_1</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/pp96d8xx' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/pp96d8xx</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240103_081204-pp96d8xx\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e7b45fed07142aa9af735dea66cb3a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01691666666495924, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240103_084138-e45rtskr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/e45rtskr' target=\"_blank\">MLP_4_32_onehot_1</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/e45rtskr' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/e45rtskr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 3.6 K \n",
      "2 | head        | Sequential       | 562   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "4.2 K     Trainable params\n",
      "0         Non-trainable params\n",
      "4.2 K     Total params\n",
      "0.017     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55e18e382b34473bbd1328f338cd064a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▃▅▆▇▆▇▇▇▇▇▇▇▇▆▇▇▇▇▇▇▇███████▇▇▇▇█▇██▇█</td></tr><tr><td>train_auc</td><td>▁▁▃▅▆▇▆▇▆▇▇▇▇▆▇▇▇▇▇▇▇▇▇▇███▇██▇█▇██▇██▇█</td></tr><tr><td>train_f1</td><td>▁▁▂▄▆▆▄▆▆▆▇▇▇▇▇▆▇▇▆▇▆▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇██▇█</td></tr><tr><td>train_loss_epoch</td><td>██▇▆▅▄▅▄▄▃▃▃▃▃▄▃▃▃▃▃▂▂▂▂▂▂▁▂▂▁▃▂▂▁▁▂▁▁▃▁</td></tr><tr><td>train_loss_step</td><td>██▇▇▅▅▄▅▃▄▄▄▃▃▆▃▅▅▄▄▁▂▂▄▂▃▁▃▃▄▄▃▃▃▆▂▂▂▃▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▁████▁█████████▁▁▁▁▁▁▁▁▁▁▅▇████████████</td></tr><tr><td>val_auc</td><td>▁█▇▆▇▆▆▅▆▄▆▆▆▆▆▆▆▆▆▇▇▇▇▆▆▆▇▇█▇▆▇▆▆▆▇▆▆▇▆</td></tr><tr><td>val_f1</td><td>▁▁████▁█████████▁▁▁▁▁▁▁▁▁▁▅▇████████████</td></tr><tr><td>val_loss_epoch</td><td>█▇▇▅▅▄▆▄▅▅▅▅▄▄▄▄▄▆▆▆▇▇█▇██▇▅▅▄▃▃▄▂▃▂▂▁▁▃</td></tr><tr><td>val_loss_step</td><td>▇▇▆▅▅▅▆▄▅▆▅▅▅▄▄▅▄▆▆▆▇▇█▆▇▇▇▅▅▄▄▆▆▅▅▃▁▂▄▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.73892</td></tr><tr><td>train_auc</td><td>0.79085</td></tr><tr><td>train_f1</td><td>0.74656</td></tr><tr><td>train_loss_epoch</td><td>0.55267</td></tr><tr><td>train_loss_step</td><td>0.52224</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.73113</td></tr><tr><td>val_auc</td><td>0.76363</td></tr><tr><td>val_f1</td><td>0.772</td></tr><tr><td>val_loss_epoch</td><td>0.63599</td></tr><tr><td>val_loss_step</td><td>0.67262</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_32_onehot_1</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/e45rtskr' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/e45rtskr</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240103_084138-e45rtskr\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74176096adac44d8b7af9e8bae9629d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240103_091126-9bq3geyw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/9bq3geyw' target=\"_blank\">MLP_4_32_onehot_1</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/9bq3geyw' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/9bq3geyw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 3.6 K \n",
      "2 | head        | Sequential       | 562   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "4.2 K     Trainable params\n",
      "0         Non-trainable params\n",
      "4.2 K     Total params\n",
      "0.017     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eba5f3d92da04c3a81b082c7d21f1ac3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▁▁▂▃▅▆▆▆▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇▇████▇███▇▇█</td></tr><tr><td>train_auc</td><td>▆▆▅▆▆▆▇▇███▇▇▆▅▅▄▃▄▃▃▂▃▃▃▄▃▃▃▄▂▃▂▂▃▃▂▁▂▁</td></tr><tr><td>train_f1</td><td>▁▁▂▂▂▄▅▆▆▆▇▆▇▇▇▇▇▇▇▇▇▇▇▇█▇█▇▇▇████████▇█</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▂▂▃▂▄▆▄▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇█▇█▇▇██▇██▇█▇█▇▇█</td></tr><tr><td>val_auc</td><td>▅▅▅▅▆▅▇▆█▇█▇█▇▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_f1</td><td>▆▁▃▄▂▄▆▅▇▆▆▆▆█▇▇▇▇█▇▇█▇█▇█▇██████▇███▇██</td></tr><tr><td>val_loss_epoch</td><td>█████▇▅▅▃▃▂▃▃▁▂▂▁▂▁▁▁▂▂▂▂▁▁▁▁▁▁▁▁▁▂▁▁▁▃▁</td></tr><tr><td>val_loss_step</td><td>█████▇▅▆▄▄▃▃▄▂▂▂▁▂▁▂▂▃▂▂▂▂▂▂▂▂▂▁▂▁▄▂▂▂▃▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.78204</td></tr><tr><td>train_auc</td><td>0.3847</td></tr><tr><td>train_f1</td><td>0.78434</td></tr><tr><td>train_loss_epoch</td><td>0.48183</td></tr><tr><td>train_loss_step</td><td>0.3651</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.8066</td></tr><tr><td>val_auc</td><td>0.1307</td></tr><tr><td>val_f1</td><td>0.81279</td></tr><tr><td>val_loss_epoch</td><td>0.46887</td></tr><tr><td>val_loss_step</td><td>0.48753</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_32_onehot_1</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/9bq3geyw' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/9bq3geyw</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240103_091126-9bq3geyw\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "586a018251594c3bb048547bdb737ccb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240103_094200-xc1jaqz3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/xc1jaqz3' target=\"_blank\">MLP_4_32_onehot_1</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/xc1jaqz3' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/xc1jaqz3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 3.6 K \n",
      "2  | head        | Sequential       | 562   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 33    \n",
      "--------------------------------------------------\n",
      "4.2 K     Trainable params\n",
      "0         Non-trainable params\n",
      "4.2 K     Total params\n",
      "0.017     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9658f4c8c87f43c8b99e2604f2085251",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▂▅▆▆▇█████▇█▇██▇█▇█▇████▇▇██▇████████▇</td></tr><tr><td>train_auc</td><td>▁▁▂▅▆▇▇▇█▇██▇█▇██▇██████████████████████</td></tr><tr><td>train_f1</td><td>▃▁▄▄▇▅▇▇█▇▇█▇█▇██▇█▇█▇▇███▇▇██▇████████▇</td></tr><tr><td>train_loss_epoch</td><td>███▆▅▃▃▂▂▂▂▂▂▂▂▂▁▃▂▂▂▂▂▁▂▂▂▂▁▁▂▂▁▁▂▂▁▂▂▂</td></tr><tr><td>train_loss_step</td><td>███▅▅▆▃▃▂▃▁▂▂▃▃▂▂▅▃▃▂▁▂▁▄▂▃▃▄▂▁▃▃▂▃▂▂▂▂▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▄▇▆▅███▆██▇█▇██▇█████▇█▇▇██████▇▇██▇▇██</td></tr><tr><td>val_auc</td><td>▁▆▆▅▇▇▇█████████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▅█▇▆███▇██▇████▇███████▇▇██████▇████▇██</td></tr><tr><td>val_loss_epoch</td><td>███▅▄▃▂▂▂▂▁▂▂▂▂▂▂▂▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▁▁</td></tr><tr><td>val_loss_step</td><td>███▅▅▂▃▃▃▃▂▃▃▃▃▃▃▃▂▂▂▂▃▃▂▂▂▂▁▃▁▃▂▂▂▁▃▃▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.75369</td></tr><tr><td>train_auc</td><td>0.83161</td></tr><tr><td>train_f1</td><td>0.75282</td></tr><tr><td>train_loss_epoch</td><td>0.50786</td></tr><tr><td>train_loss_step</td><td>0.47228</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.7783</td></tr><tr><td>val_auc</td><td>0.85523</td></tr><tr><td>val_f1</td><td>0.7783</td></tr><tr><td>val_loss_epoch</td><td>0.47482</td></tr><tr><td>val_loss_step</td><td>0.46582</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_32_onehot_1</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/xc1jaqz3' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/xc1jaqz3</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240103_094200-xc1jaqz3\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e53e6340a864530adbdd4c854a07c09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240103_101219-km4ktry5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/km4ktry5' target=\"_blank\">MLP_4_32_onehot_1</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/km4ktry5' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/km4ktry5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 3.6 K \n",
      "2  | head        | Sequential       | 562   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 1.1 K \n",
      "--------------------------------------------------\n",
      "5.3 K     Trainable params\n",
      "0         Non-trainable params\n",
      "5.3 K     Total params\n",
      "0.021     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "293e22be85334746a9e5b0576a627d91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▂▃▆▇▇▇▇▇████▇██▇██████████████████████</td></tr><tr><td>train_auc</td><td>▁▁▁▃▇▇████▇█████████████████████████████</td></tr><tr><td>train_f1</td><td>▃▃▃▁▆▇█▇█▇████▇██▇██████████████████████</td></tr><tr><td>train_loss_epoch</td><td>███▇▄▃▂▂▂▂▂▁▂▂▂▂▁▂▁▂▁▂▁▁▂▁▁▁▁▁▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>████▄▂▃▂▄▄▂▅▃▃▂▂▄▃▁▄▂▂▃▄▄▂▃▃▃▂▄▃▃▃▂▂▄▃▃▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▂▁▂▄▇▇█▇█▇██████▇██▇████▇█▇███████▇█████</td></tr><tr><td>val_auc</td><td>▁▃▅▇▇▇▇▇████████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▇▇▅███████████████▇████████████████████</td></tr><tr><td>val_loss_epoch</td><td>███▇▂▂▂▂▁▂▁▂▂▂▁▂▁▁▂▂▂▂▁▁▂▁▂▁▁▁▁▁▁▂▂▁▁▁▂▁</td></tr><tr><td>val_loss_step</td><td>███▇▃▄▂▃▂▃▂▃▃▃▁▃▁▂▃▄▄▄▃▂▃▃▄▂▃▁▂▂▂▃▃▃▃▃▄▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.76196</td></tr><tr><td>train_auc</td><td>0.83719</td></tr><tr><td>train_f1</td><td>0.76224</td></tr><tr><td>train_loss_epoch</td><td>0.50225</td></tr><tr><td>train_loss_step</td><td>0.49655</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.76651</td></tr><tr><td>val_auc</td><td>0.85445</td></tr><tr><td>val_f1</td><td>0.76485</td></tr><tr><td>val_loss_epoch</td><td>0.4718</td></tr><tr><td>val_loss_step</td><td>0.44439</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_32_onehot_1</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/km4ktry5' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/km4ktry5</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240103_101219-km4ktry5\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a63765dde774b8fae96eb1f60bf697e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240103_104224-0cnpox4x</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/0cnpox4x' target=\"_blank\">MLP_4_64_onehot_1</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/0cnpox4x' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/0cnpox4x</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 13.4 K\n",
      "2 | head        | Sequential       | 2.1 K \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "15.5 K    Trainable params\n",
      "0         Non-trainable params\n",
      "15.5 K    Total params\n",
      "0.062     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▇▇▇▇▇▇▇▇██▇▇▇▇▇███▇▇█▇▇██▇████████████</td></tr><tr><td>train_auc</td><td>▁▂▇▇██▇█▇██████▇███████▇████████████████</td></tr><tr><td>train_f1</td><td>▁▃▇▇▇▇▇█▇▇█▇▇▇▇▆▇███▇▇█▇▇▇█▇▇███████████</td></tr><tr><td>train_loss_epoch</td><td>██▃▂▂▂▂▁▂▂▂▁▂▂▂▃▂▂▁▂▂▂▂▂▂▁▁▂▁▁▁▂▁▂▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>██▅▅▄▃▄▁▃▃▃▅▃▄▃▃▃▂▃▃▂▃▁▃▃▃▃▃▂▂▄▃▂▅▃▃▃▃▃▄</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▇▇▇█████████▇▇▇████▇▇██████▇██████▇█▇██</td></tr><tr><td>val_auc</td><td>▁▂▃▅▆▇▇▇███▇██▇▇▇▇██████████████████████</td></tr><tr><td>val_f1</td><td>▁█▅▄██▆▇▇▇██▇▅▄▇██▇▇▅▇▆▆▇█▇▆▇▇███▆▇▇▇█▆▇</td></tr><tr><td>val_loss_epoch</td><td>█▇▂▂▁▁▁▁▂▁▂▁▁▂▂▂▂▁▁▁▂▃▁▂▁▂▁▂▂▁▁▁▁▁▁▂▁▂▁▁</td></tr><tr><td>val_loss_step</td><td>█▇▂▄▂▁▂▂▃▂▃▁▂▃▄▃▄▂▃▂▃▄▂▃▂▃▂▄▃▁▃▁▃▃▂▂▃▄▂▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.772</td></tr><tr><td>train_auc</td><td>0.84636</td></tr><tr><td>train_f1</td><td>0.77241</td></tr><tr><td>train_loss_epoch</td><td>0.49211</td></tr><tr><td>train_loss_step</td><td>0.53292</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.76887</td></tr><tr><td>val_auc</td><td>0.85583</td></tr><tr><td>val_f1</td><td>0.76887</td></tr><tr><td>val_loss_epoch</td><td>0.48121</td></tr><tr><td>val_loss_step</td><td>0.51403</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_64_onehot_1</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/0cnpox4x' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/0cnpox4x</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240103_104224-0cnpox4x\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b09c27c40354b76a8ac5dce4fc00157",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240103_111316-y41qa7ea</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/y41qa7ea' target=\"_blank\">MLP_4_64_onehot_1</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/y41qa7ea' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/y41qa7ea</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 13.4 K\n",
      "2 | head        | Sequential       | 2.1 K \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "15.5 K    Trainable params\n",
      "0         Non-trainable params\n",
      "15.5 K    Total params\n",
      "0.062     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30bd01aab99944748b6fe5c76b4d9002",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▂▄▆▇▇▇▆▇▇█▇▇█▇▇▇▇█▇▇▇▇███▇▇█▇███▇██▇▇█</td></tr><tr><td>train_auc</td><td>▁▂▂▄▆▇▇▇▇▇▇█▇▇█▇▇███▇██▇███▇████████████</td></tr><tr><td>train_f1</td><td>▁▁▂▂▅▆▇▇▆▇▆█▇▇▇▇▇▇▇▇▇▇█▇███▇▇█▇█▇█▇██▇▇█</td></tr><tr><td>train_loss_epoch</td><td>██▇▇▅▄▃▃▃▃▃▂▂▂▃▂▂▂▂▁▂▂▂▂▁▂▁▂▂▁▂▁▂▂▁▂▁▂▂▂</td></tr><tr><td>train_loss_step</td><td>███▆▅▄▅▂▄▂▃▃▃▅▃▅▃▃▂▂▁▁▁▃▂▁▂▃▁▁▄▃▂▅▄▂▂▁▁▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>▁██▇▇██████████▇▇███▇▇███▇█▇▇▇▇▇█▇▇█▇█▇▇</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_epoch</td><td>▁▁▁▁▂▃▃▃▄▃▅▅▄▅▆▆▅▅▅▅▅▆▅▆▄▅▅▅▆█▇▇▆▅▅▅▆▅▄▄</td></tr><tr><td>val_loss_step</td><td>▁▁▁▁▂▂▃▂▃▃▄▃▃▄▅▄▅▅▃▅▄▅▅▅▃▅▄▄▅█▆▆▅▃▃▄▆▅▂▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.75192</td></tr><tr><td>train_auc</td><td>0.80119</td></tr><tr><td>train_f1</td><td>0.7619</td></tr><tr><td>train_loss_epoch</td><td>0.53949</td></tr><tr><td>train_loss_step</td><td>0.50929</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.48821</td></tr><tr><td>val_auc</td><td>0.76778</td></tr><tr><td>val_f1</td><td>0.6561</td></tr><tr><td>val_loss_epoch</td><td>0.88702</td></tr><tr><td>val_loss_step</td><td>0.91511</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_64_onehot_1</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/y41qa7ea' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/y41qa7ea</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240103_111316-y41qa7ea\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "096252b355ea4129bc297d7bd9396656",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01693333333435779, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240103_114519-g27mq9xr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/g27mq9xr' target=\"_blank\">MLP_4_64_onehot_1</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/g27mq9xr' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/g27mq9xr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 13.4 K\n",
      "2 | head        | Sequential       | 2.1 K \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "15.5 K    Trainable params\n",
      "0         Non-trainable params\n",
      "15.5 K    Total params\n",
      "0.062     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e52d554d6f64896ad3105fd9d28c14c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▁▂▄▆▇▇▇▇▇▇▇▇▇▆███▇▇█▇▇████▇█▇███████▇▇</td></tr><tr><td>train_auc</td><td>▁▁▁▂▃▆▇▆▇▇▇▇▇▇▇▇▇▇█▇▇▇▇▇█▇██▇████▇████▇▇</td></tr><tr><td>train_f1</td><td>▂▁▁▃▅▆▇▇▇▇▇▇▇▇▇▆▇██▇▇▇▇█▇███▇█▇███████▇▇</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▄▃▃▃▂▂▂▂▁▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▂▂▂▁▂▂▂▁▂▁▁▁▂▂▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▂▂▂▄▅▇▇▇▇▇▇▇▆▆▇▇▇▇▇▆██▆█▇▇▇▇████▇▇█▇███</td></tr><tr><td>val_auc</td><td>▁▇▂▃▆▇██████████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▅▄▃▅▆▇▇▇█▇▇▇▆▆█▇▇▇▇▇██▆██▇▇▇████▇▇█▇███</td></tr><tr><td>val_loss_epoch</td><td>█▆▆▆▅▃▂▂▂▁▂▁▂▂▃▃▂▂▁▁▂▂▁▃▁▂▁▁▁▁▁▁▁▁▁▁▁▂▁▂</td></tr><tr><td>val_loss_step</td><td>█▇▇▇▆▄▂▃▃▂▃▂▂▃▅▄▃▂▂▂▃▃▂▄▂▃▁▃▂▁▂▁▂▃▂▁▂▃▂▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.76255</td></tr><tr><td>train_auc</td><td>0.81943</td></tr><tr><td>train_f1</td><td>0.761</td></tr><tr><td>train_loss_epoch</td><td>0.50962</td></tr><tr><td>train_loss_step</td><td>0.53634</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.80896</td></tr><tr><td>val_auc</td><td>0.87682</td></tr><tr><td>val_f1</td><td>0.82353</td></tr><tr><td>val_loss_epoch</td><td>0.48466</td></tr><tr><td>val_loss_step</td><td>0.50249</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_64_onehot_1</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/g27mq9xr' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/g27mq9xr</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240103_114519-g27mq9xr\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c578a14f5e1b4f67b4a03e0632e651fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01693333333435779, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240103_121623-y31q3ct3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/y31q3ct3' target=\"_blank\">MLP_4_64_onehot_1</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/y31q3ct3' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/y31q3ct3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 13.4 K\n",
      "2  | head        | Sequential       | 2.1 K \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 65    \n",
      "--------------------------------------------------\n",
      "15.6 K    Trainable params\n",
      "0         Non-trainable params\n",
      "15.6 K    Total params\n",
      "0.062     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4bda1c65a9440dc95e7e298b3006f30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▃▆▇▇▇▇████▇████████████████████████████</td></tr><tr><td>train_auc</td><td>▁▃▇▇▇▇▇█████████████████████████████████</td></tr><tr><td>train_f1</td><td>▁▄▆▇▇▆▇█▇██▇████████████████████████████</td></tr><tr><td>train_loss_epoch</td><td>██▄▃▃▄▂▂▁▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▂</td></tr><tr><td>train_loss_step</td><td>██▃▃▃▅▂▃▁▂▃▂▂▄▃▄▂▂▃▂▂▃▂▂▁▄▃▃▃▄▄▁▃▃▃▂▂▁▃▄</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁██▇▇████▇██▇█████████▇█▇▇█▇█▇▇█▇▇▇███▇▇</td></tr><tr><td>val_auc</td><td>▁▃▄▆▆▆▆▇▇▇▇█████████████████████████████</td></tr><tr><td>val_f1</td><td>▁██▃▁▆▇▇▇▅██▄▇▇▇▇▇█▇▇▆▄▇▅▃▆▅▇▅▄█▅▃▅▆▆█▄▁</td></tr><tr><td>val_loss_epoch</td><td>█▇▂▂▂▂▂▂▂▂▂▂▂▁▁▁▂▂▁▁▁▂▃▁▂▂▂▂▁▂▂▁▂▃▁▂▂▁▂▃</td></tr><tr><td>val_loss_step</td><td>█▇▄▄▂▄▃▃▃▄▄▄▄▂▂▂▃▃▃▂▃▄▄▂▅▄▄▃▁▃▃▃▃▅▂▃▄▂▄▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.76373</td></tr><tr><td>train_auc</td><td>0.83339</td></tr><tr><td>train_f1</td><td>0.76879</td></tr><tr><td>train_loss_epoch</td><td>0.508</td></tr><tr><td>train_loss_step</td><td>0.52351</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.7217</td></tr><tr><td>val_auc</td><td>0.85236</td></tr><tr><td>val_f1</td><td>0.65294</td></tr><tr><td>val_loss_epoch</td><td>0.52928</td></tr><tr><td>val_loss_step</td><td>0.52354</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_64_onehot_1</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/y31q3ct3' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/y31q3ct3</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240103_121623-y31q3ct3\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a27b3fa61e954cc1bb59174bbf32fd5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240103_124845-z1or8n49</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/z1or8n49' target=\"_blank\">MLP_4_64_onehot_1</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/z1or8n49' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/z1or8n49</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 13.4 K\n",
      "2  | head        | Sequential       | 2.1 K \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 4.2 K \n",
      "--------------------------------------------------\n",
      "19.7 K    Trainable params\n",
      "0         Non-trainable params\n",
      "19.7 K    Total params\n",
      "0.079     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▇▇▇█████▇▇██▇▇███████████████████████▇</td></tr><tr><td>train_auc</td><td>▁▂▇▇▇███████████████████████████████████</td></tr><tr><td>train_f1</td><td>▁▁▇▇██████▇████▇███████████████████████▇</td></tr><tr><td>train_loss_epoch</td><td>██▄▃▂▂▂▁▁▂▂▂▁▂▂▂▁▁▂▁▁▁▁▁▂▁▁▁▁▁▂▁▁▁▂▁▁▂▁▂</td></tr><tr><td>train_loss_step</td><td>██▅▄▃▃▃▂▃▂▁▄▂▂▂▂▄▁▂▃▃▂▂▂▁▂▃▂▃▃▄▂▂▁▂▄▃▂▂▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▆████▇███▇▇█▇▇▇█████▇▇███████▇▇██▇█▇▇▇█</td></tr><tr><td>val_auc</td><td>▁▃▄▆▇▇▇█████████████████████████████████</td></tr><tr><td>val_f1</td><td>▃▁█▇█▇▃▇▇▇▇███▅▄█▇█▇█▇█▇▇██▇█▇▅▆▇▇▆█▇▆▇█</td></tr><tr><td>val_loss_epoch</td><td>█▇▃▂▂▂▃▂▁▂▁▂▂▂▂▃▂▂▁▂▂▁▂▂▁▂▂▁▂▂▂▂▂▂▂▁▁▁▂▂</td></tr><tr><td>val_loss_step</td><td>██▄▄▃▃▄▄▂▄▁▃▅▃▃▄▄▃▂▄▄▂▄▃▃▃▄▂▄▄▄▃▄▅▄▁▂▃▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.74956</td></tr><tr><td>train_auc</td><td>0.82602</td></tr><tr><td>train_f1</td><td>0.74882</td></tr><tr><td>train_loss_epoch</td><td>0.50921</td></tr><tr><td>train_loss_step</td><td>0.46535</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.77123</td></tr><tr><td>val_auc</td><td>0.85509</td></tr><tr><td>val_f1</td><td>0.78005</td></tr><tr><td>val_loss_epoch</td><td>0.47867</td></tr><tr><td>val_loss_step</td><td>0.46017</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_64_onehot_1</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/z1or8n49' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/z1or8n49</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240103_124845-z1or8n49\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9163d42af9964978ac55f5bdbd336207",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240103_131948-c146hrqb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/c146hrqb' target=\"_blank\">MLP_2_16_onehot_2</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/c146hrqb' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/c146hrqb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 496   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "650       Trainable params\n",
      "0         Non-trainable params\n",
      "650       Total params\n",
      "0.003     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71e611c95dd54cd0a5c594093fcd3130",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▂▅▇▇▇▇▇▇▇▇█▇███▇█▇██████▇███▇███▇▇▇███</td></tr><tr><td>train_auc</td><td>▁▁▁▄▇▇▇▇▇▇███▇██████████████████████████</td></tr><tr><td>train_f1</td><td>▁▅▄▇▇████████████▇████████▇█████████▇███</td></tr><tr><td>train_loss_epoch</td><td>███▇▄▃▃▂▂▂▁▂▂▂▂▂▂▂▁▂▁▁▁▂▁▁▁▁▁▁▂▂▂▁▂▂▂▁▁▂</td></tr><tr><td>train_loss_step</td><td>███▇▄▃▄▄▄▃▃▂▄▅▃▄▃▃▃▄▃▃▁▂▄▂▃▃▃▃▂▃▃▃▅▂▅▄▂▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▁▆▇██▇█▇███████████████████████████████</td></tr><tr><td>val_auc</td><td>▁▅▇▇▇▇██████████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▂▁▇█▇▅▇▆▇▇▆▇███▇█████▇▇▇████▇▇▇███▇█▇█▇</td></tr><tr><td>val_loss_epoch</td><td>███▇▄▂▂▂▁▂▁▁▂▂▂▂▁▁▂▂▁▂▁▁▁▂▁▁▂▁▁▁▂▁▁▁▁▂▁▁</td></tr><tr><td>val_loss_step</td><td>███▇▅▂▄▃▂▃▃▂▃▄▅▃▃▃▂▃▂▄▂▂▃▃▁▂▄▃▂▂▃▁▂▂▂▃▃▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.76092</td></tr><tr><td>train_auc</td><td>0.8242</td></tr><tr><td>train_f1</td><td>0.7532</td></tr><tr><td>train_loss_epoch</td><td>0.52245</td></tr><tr><td>train_loss_step</td><td>0.48658</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.77069</td></tr><tr><td>val_auc</td><td>0.86376</td></tr><tr><td>val_f1</td><td>0.74935</td></tr><tr><td>val_loss_epoch</td><td>0.45935</td></tr><tr><td>val_loss_step</td><td>0.41427</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_16_onehot_2</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/c146hrqb' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/c146hrqb</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240103_131948-c146hrqb\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31bf4250e6824f65afa4ee3f29e6d819",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240103_135034-3m7ir19m</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/3m7ir19m' target=\"_blank\">MLP_2_16_onehot_2</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/3m7ir19m' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/3m7ir19m</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 496   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "650       Trainable params\n",
      "0         Non-trainable params\n",
      "650       Total params\n",
      "0.003     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08cc1d9bd8f44bb9b1ce8e0435c46312",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▄▆▆▆▅▆▆▆▇▇█▆▇▇█▆▇▆█▇▇█▇█▇▇▇▇█▆▇▇▇▇▇▇▇█</td></tr><tr><td>train_auc</td><td>▁▂▂▄▅▅▅▆▆▆▇▇█▇▇██▆▇▇█▇████▇▇█▇█▇▇▇█▇████</td></tr><tr><td>train_f1</td><td>▁▃▄▅▆▅▅▆▆▆▇▇█▆▆██▆▇▆▇▇▇█▇▇▇▇▇▇█▆▇▇▇▇▇▇▇▇</td></tr><tr><td>train_loss_epoch</td><td>█▇▆▅▄▄▅▄▄▃▂▃▂▃▃▂▂▄▄▂▂▃▃▂▁▂▃▃▂▃▂▄▃▃▃▂▂▂▂▂</td></tr><tr><td>train_loss_step</td><td>█▇▆▅▄▆▆▄▄▃▅▅▃▅▇▂▄▄▃▃▃▄▆▃▅▂▂▅▃▃▄▇▂▄▄▁▄▂▁▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▄▄▄▄▄▄▄▄▅▄▅█▅▅▇▇▇██████████████████████</td></tr><tr><td>val_auc</td><td>▁▇██▇▇▇▇████████████████████████████████</td></tr><tr><td>val_f1</td><td>▇▁▁▁▁▁▁▁▁▃▁▃█▄▄▆▆▅██████████████████████</td></tr><tr><td>val_loss_epoch</td><td>█▇▆▄▃▃▃▂▂▂▂▂▁▂▂▂▁▂▃▂▂▃▂▃▂▃▂▃▃▂▄▃▄▃▃▄▄▃▄▄</td></tr><tr><td>val_loss_step</td><td>█▇▆▅▃▂▃▂▁▁▂▁▂▂▃▂▂▂▂▁▂▄▂▂▃▃▂▂▃▂▄▃▃▃▂▆▄▃▄▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.6889</td></tr><tr><td>train_auc</td><td>0.72005</td></tr><tr><td>train_f1</td><td>0.67489</td></tr><tr><td>train_loss_epoch</td><td>0.62087</td></tr><tr><td>train_loss_step</td><td>0.59416</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.71631</td></tr><tr><td>val_auc</td><td>0.82042</td></tr><tr><td>val_f1</td><td>0.74576</td></tr><tr><td>val_loss_epoch</td><td>0.6692</td></tr><tr><td>val_loss_step</td><td>0.66656</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_16_onehot_2</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/3m7ir19m' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/3m7ir19m</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240103_135034-3m7ir19m\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "280b3ee3849b4ecb95bd94cfa6721589",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240103_142221-bfosuts3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/bfosuts3' target=\"_blank\">MLP_2_16_onehot_2</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/bfosuts3' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/bfosuts3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 496   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "650       Trainable params\n",
      "0         Non-trainable params\n",
      "650       Total params\n",
      "0.003     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9cadf1659154997abe140a9c22cd72b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▃▄▄▅▆▆▆▇▇▇▇▇▇▇█▇▇███████████▇█████████</td></tr><tr><td>train_auc</td><td>▁▁▂▃▄▅▅▅▆▆▆▇▇▆▇▇▇▇▇▇████▇█▇███▇███▇████▇</td></tr><tr><td>train_f1</td><td>▁▂▂▃▄▅▆▆▆▇▇▇▇▇▇▇█▇██████████████▇███▇██▇</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▃▃▂▂▂▂▂▂▂▂▁▂▂▂▂▂▁▁▂▁▁▁▁▂▁▁▂▂▂▁▁▁▁▂▁▂▂▁▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▅▅▆▆▆▆▇▇▇▇▇▇██▇█████████████████▇▇████▇</td></tr><tr><td>val_auc</td><td>▁▄▅▆▆▆▇▇▇▇██████████████████████████████</td></tr><tr><td>val_f1</td><td>▄▅▁▅▄▅▅▆▆▇▆▆▇█▇▇▇██▇███████████▇████████</td></tr><tr><td>val_loss_epoch</td><td>█▄▃▃▃▃▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_step</td><td>█▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▁▂▂▁▂▂▂▁▁▂▁▂▂▁▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.74498</td></tr><tr><td>train_auc</td><td>0.78818</td></tr><tr><td>train_f1</td><td>0.74707</td></tr><tr><td>train_loss_epoch</td><td>0.52636</td></tr><tr><td>train_loss_step</td><td>0.55428</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.78487</td></tr><tr><td>val_auc</td><td>0.87669</td></tr><tr><td>val_f1</td><td>0.78487</td></tr><tr><td>val_loss_epoch</td><td>0.47606</td></tr><tr><td>val_loss_step</td><td>0.43156</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_16_onehot_2</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/bfosuts3' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/bfosuts3</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240103_142221-bfosuts3\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73cd8b2e80384113bc95021816d4a473",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240103_145521-tliqob8t</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/tliqob8t' target=\"_blank\">MLP_2_16_onehot_2</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/tliqob8t' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/tliqob8t</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 496   \n",
      "2  | head        | Sequential       | 154   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 17    \n",
      "--------------------------------------------------\n",
      "667       Trainable params\n",
      "0         Non-trainable params\n",
      "667       Total params\n",
      "0.003     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52388bd14e3c495eb2e4ba6dfc6636dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▃▅▆▆▇▇██▇▇▇█▇▇████████▇████████████▇██</td></tr><tr><td>train_auc</td><td>▁▁▂▅▆▆▇▇████████████████████████████████</td></tr><tr><td>train_f1</td><td>▁▆▆▆▇▇▇█████████████████████████████████</td></tr><tr><td>train_loss_epoch</td><td>██▇▆▄▄▃▃▂▂▂▂▂▂▂▂▂▂▁▁▂▂▂▁▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▂</td></tr><tr><td>train_loss_step</td><td>███▅▅▃▄▂▁▁▃▃▄▂▂▃▃▂▃▂▅▂▂▂▃▃▁▅▃▃▂▂▁▃▃▂▂▃▄▆</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▁▇▅▆▆▇▇███████████████████████████████▇</td></tr><tr><td>val_auc</td><td>▁▅▆▆▇▇▇█████████████████████████████████</td></tr><tr><td>val_f1</td><td>▄▄█▁▃▄▇▆█▆▇██▆▇█▇█▇▇█▇▇█▇▇▇██▇▇▇███▇▇██▆</td></tr><tr><td>val_loss_epoch</td><td>███▆▄▃▂▂▂▂▂▁▂▂▂▁▂▁▁▁▁▁▁▂▁▁▂▁▁▁▁▂▁▁▁▁▁▁▁▂</td></tr><tr><td>val_loss_step</td><td>███▇▄▄▃▃▂▂▃▂▃▃▂▂▄▂▂▃▂▁▃▃▂▃▄▂▂▂▁▃▂▂▂▁▂▂▃▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.7621</td></tr><tr><td>train_auc</td><td>0.82891</td></tr><tr><td>train_f1</td><td>0.77347</td></tr><tr><td>train_loss_epoch</td><td>0.51901</td></tr><tr><td>train_loss_step</td><td>0.62877</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.7565</td></tr><tr><td>val_auc</td><td>0.86363</td></tr><tr><td>val_f1</td><td>0.69971</td></tr><tr><td>val_loss_epoch</td><td>0.47906</td></tr><tr><td>val_loss_step</td><td>0.51427</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_16_onehot_2</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/tliqob8t' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/tliqob8t</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240103_145521-tliqob8t\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d58881e02984a379dd8f34dabb43d03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016933333330477276, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240103_152545-ymmodixf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/ymmodixf' target=\"_blank\">MLP_2_16_onehot_2</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/ymmodixf' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/ymmodixf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 496   \n",
      "2  | head        | Sequential       | 154   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 281   \n",
      "--------------------------------------------------\n",
      "931       Trainable params\n",
      "0         Non-trainable params\n",
      "931       Total params\n",
      "0.004     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "659516dfc969476d9f09ec3d652ffc90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▃▆▆▇▇▇███▇██▇█████████████████████████</td></tr><tr><td>train_auc</td><td>▁▁▂▅▆▇▇▇████████████████████████████████</td></tr><tr><td>train_f1</td><td>▁▅▅▆▆█▇▇██████▇█████████████████████████</td></tr><tr><td>train_loss_epoch</td><td>███▆▄▃▃▃▂▂▂▂▂▂▂▂▁▂▁▂▂▁▁▁▁▂▁▁▁▂▂▁▁▂▁▂▁▂▂▂</td></tr><tr><td>train_loss_step</td><td>███▆▆▄▃▂▃▃▄▁▃▂▁▂▄▂▄▃▄▂▂▂▂▁▁▁▃▂▂▂▃▄▂▃▃▂▃▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▇▇▇▆▇▇▇█▇██████████████████████████████</td></tr><tr><td>val_auc</td><td>▁▆▇▇▇▇▇█████████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▇▇▆▂▂▂▅▆▅▇▇▇▇▇▅▅▇▆▆▆▇▆▆▆▇████▇▇████▆▆▆▇</td></tr><tr><td>val_loss_epoch</td><td>███▆▃▃▃▂▂▂▁▂▁▁▂▁▁▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁</td></tr><tr><td>val_loss_step</td><td>███▆▄▄▄▃▂▃▂▃▁▃▃▂▂▃▃▃▃▃▂▂▂▂▃▃▂▃▃▂▂▁▄▂▃▃▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.7562</td></tr><tr><td>train_auc</td><td>0.82578</td></tr><tr><td>train_f1</td><td>0.7644</td></tr><tr><td>train_loss_epoch</td><td>0.51571</td></tr><tr><td>train_loss_step</td><td>0.52761</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.77305</td></tr><tr><td>val_auc</td><td>0.86335</td></tr><tr><td>val_f1</td><td>0.74737</td></tr><tr><td>val_loss_epoch</td><td>0.45517</td></tr><tr><td>val_loss_step</td><td>0.42638</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_16_onehot_2</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/ymmodixf' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/ymmodixf</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240103_152545-ymmodixf\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18470c1df5ce4042a654a08131c4dc33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01693333333435779, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240103_155613-so44bwsv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/so44bwsv' target=\"_blank\">MLP_2_32_onehot_2</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/so44bwsv' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/so44bwsv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 1.5 K \n",
      "2 | head        | Sequential       | 562   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "2.1 K     Trainable params\n",
      "0         Non-trainable params\n",
      "2.1 K     Total params\n",
      "0.008     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05788151808a42198830c4c2ae3be87c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▃▆▇▇▇▇████▇█▇▇███▇▇███▇▇██████▇█▇██████</td></tr><tr><td>train_auc</td><td>▁▃▇▇▇███████████████████████████████████</td></tr><tr><td>train_f1</td><td>▁▅▇▇█▇██████████████████████████████████</td></tr><tr><td>train_loss_epoch</td><td>█▇▄▃▂▂▂▁▂▁▂▁▁▂▂▂▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▇▆▃▄▅▃▁▃▁▃▃▂▃▂▃▁▃▃▃▄▁▂▂▂▂▁▃▁▂▃▂▁▁▃▃▅▁▂▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▆▇▇▇▇▇▇████████████████████████████████</td></tr><tr><td>val_auc</td><td>▁▁▂▄▅▆▇▇▇▇██████████████████████████████</td></tr><tr><td>val_f1</td><td>▂▁▆▇▅▆▆▆▇▇▇▇▇▇▇███▆██▇▇███▇▇█████▇▇██▇██</td></tr><tr><td>val_loss_epoch</td><td>█▇▄▃▂▂▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▂▁▁▁▁▁▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_step</td><td>█▇▅▃▃▂▁▂▁▂▃▂▂▂▃▂▂▃▂▂▁▂▁▃▂▃▂▂▁▃▃▂▂▃▂▂▃▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.77332</td></tr><tr><td>train_auc</td><td>0.84479</td></tr><tr><td>train_f1</td><td>0.77906</td></tr><tr><td>train_loss_epoch</td><td>0.49251</td></tr><tr><td>train_loss_step</td><td>0.50234</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.78723</td></tr><tr><td>val_auc</td><td>0.86587</td></tr><tr><td>val_f1</td><td>0.77387</td></tr><tr><td>val_loss_epoch</td><td>0.45998</td></tr><tr><td>val_loss_step</td><td>0.45603</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_32_onehot_2</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/so44bwsv' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/so44bwsv</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240103_155613-so44bwsv\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a330340759a74d2e8425c7ebab6abf9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01691666666883975, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240103_162601-yp3foqs1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/yp3foqs1' target=\"_blank\">MLP_2_32_onehot_2</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/yp3foqs1' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/yp3foqs1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 1.5 K \n",
      "2 | head        | Sequential       | 562   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "2.1 K     Trainable params\n",
      "0         Non-trainable params\n",
      "2.1 K     Total params\n",
      "0.008     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "219114e273eb4307af4bb498a52a5bd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▄▆▆▇▇▇▇▇▇▇▇█▇▇▇▇▇▇▇▇▇█▇▇▇▇▇▇▇▇▇████▇▇██</td></tr><tr><td>train_auc</td><td>▁▃▆▇▇▇▇▇▇▇▇▇█▇▇▇▇▇█▇▇██▇███▇███▇█████▇██</td></tr><tr><td>train_f1</td><td>▁▅▆▆▇▇▇▇▇▇█▇█▇▇▇▇▇▇▇▇▇▇▆▇▇▇▇▇▇▇▇████▇▇▇█</td></tr><tr><td>train_loss_epoch</td><td>█▆▅▄▃▂▃▂▂▃▂▃▂▂▂▂▂▂▁▂▂▂▂▂▁▂▂▂▂▁▂▂▁▁▂▂▂▃▁▁</td></tr><tr><td>train_loss_step</td><td>█▇▆▅▅▅▄▄▄▃▄▂▂▃▃▃▂▃▄▂▃▄▂▃▃▂▂▄▁▃▂▄▃▁▃▃▅▃▃▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▅███████▇█▃▃▃▃▃▃▇▃▃▃▃▂▃▃▃▂▂▃▃▃▂▂▃▃▁</td></tr><tr><td>val_auc</td><td>▁▃▇▆▆▆▂▂▂▂▄▅▅▅▅▅▅▆▆▆▅▅▆▆▆▆▆▆▆▆▆▆▄▄▆█▆▆▆▆</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▅█████████▇▇▇▇▇▇█▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>val_loss_epoch</td><td>▆▆▃▂▂▁▁▁▁▂▂▂▂▄▄▅▅▅▄▅▄▄▅▅▅▅▆▆▅▆▆▆▅▆▆▆█▇▇▇</td></tr><tr><td>val_loss_step</td><td>▆▅▃▂▂▂▁▁▁▂▂▂▂▅▄▅▅▄▃▅▃▄▄▃▄▄▇▇▃▆▅▄▃▅▅▄█▅▆▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.7379</td></tr><tr><td>train_auc</td><td>0.78571</td></tr><tr><td>train_f1</td><td>0.74629</td></tr><tr><td>train_loss_epoch</td><td>0.55514</td></tr><tr><td>train_loss_step</td><td>0.52778</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.56028</td></tr><tr><td>val_auc</td><td>0.81299</td></tr><tr><td>val_f1</td><td>0.66786</td></tr><tr><td>val_loss_epoch</td><td>0.69092</td></tr><tr><td>val_loss_step</td><td>0.65293</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_32_onehot_2</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/yp3foqs1' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/yp3foqs1</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240103_162601-yp3foqs1\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e9d4df3d230404cbad8b1312233c13b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01691666666495924, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240103_165635-ise0yhw8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/ise0yhw8' target=\"_blank\">MLP_2_32_onehot_2</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/ise0yhw8' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/ise0yhw8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 1.5 K \n",
      "2 | head        | Sequential       | 562   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "2.1 K     Trainable params\n",
      "0         Non-trainable params\n",
      "2.1 K     Total params\n",
      "0.008     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ddfc1f8f1e7408f8cceb8065c97d0c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▃▄▅▆▆▇█▇▇▇█▇▇▇█▇██▇▇██▇███▇███▇███▇████</td></tr><tr><td>train_auc</td><td>▁▁▄▆▆▇▇██▇▇▇▆▅▅▆▆▆▆▅▆▅▆▅▅▅▆▅▆▆▆▆▆▆▆▆▇█▆▆</td></tr><tr><td>train_f1</td><td>▁▃▃▄▆▆▇▇▇▇▇█▇▇▇▇▇██▇▇█▇▇███████▇█▇██████</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▅▅▃▄▃▃▂▂▁▂▂▁▂▃▂▂▂▂▂▃▁▂▂▂▂▁▂▁▂▂▂▁▂▂▁▂▂▃▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▄▇▆▆▇▇▇▇▇▇▇▇████▇█▇███████████▇████████</td></tr><tr><td>val_auc</td><td>▁▃▆▇▇▇▇████████▇████████████████████████</td></tr><tr><td>val_f1</td><td>▁▄▅▄▃▅▄▅▅▆▅▇▅▇▇▇█▆██▇▇▇▇█▆█████▇▇█▇██▇██</td></tr><tr><td>val_loss_epoch</td><td>█▆▄▄▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▂▁▁▁▂▁▁▁▂▂▁▁▂▁▁▁▁▂▁▁▁</td></tr><tr><td>val_loss_step</td><td>█▅▅▄▃▃▂▂▂▂▃▂▃▃▃▂▂▂▂▃▁▁▁▃▂▂▂▃▂▂▂▂▂▂▂▂▃▂▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.77155</td></tr><tr><td>train_auc</td><td>0.69363</td></tr><tr><td>train_f1</td><td>0.78123</td></tr><tr><td>train_loss_epoch</td><td>0.49275</td></tr><tr><td>train_loss_step</td><td>0.49787</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.81324</td></tr><tr><td>val_auc</td><td>0.88194</td></tr><tr><td>val_f1</td><td>0.79373</td></tr><tr><td>val_loss_epoch</td><td>0.44204</td></tr><tr><td>val_loss_step</td><td>0.4564</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_32_onehot_2</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/ise0yhw8' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/ise0yhw8</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240103_165635-ise0yhw8\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20c424df65204e78a7c3682917025b94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240103_172539-sf510qf4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/sf510qf4' target=\"_blank\">MLP_2_32_onehot_2</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/sf510qf4' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/sf510qf4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 1.5 K \n",
      "2  | head        | Sequential       | 562   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 33    \n",
      "--------------------------------------------------\n",
      "2.1 K     Trainable params\n",
      "0         Non-trainable params\n",
      "2.1 K     Total params\n",
      "0.008     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d564e7f8b3e42d28e778fad766e8489",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▄▆▅▅▆▇▇▇▇▇█▇███████▇▇▇█████▇██▇████████</td></tr><tr><td>train_auc</td><td>▁▃▆▆▆▇▇▇▇▇██████████████████████████████</td></tr><tr><td>train_f1</td><td>▁▄▆▅▅▆▇▇█▇▇█▇█████████▇████████▇████████</td></tr><tr><td>train_loss_epoch</td><td>█▇▅▄▄▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▂▂▂▁▂▁▁▁▁▁▂▂▂▁▁▁▂▁▂▁</td></tr><tr><td>train_loss_step</td><td>█▇▆▅▅▅▅▃▃▄▁▂▃▃▄▄▄▃▁▂▂▃▅▃▄▂▂▂▃▄▁▄▂▂▃▂▄▄▃▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▆▅▆▆▆▇█▇▇██████████████████████████████</td></tr><tr><td>val_auc</td><td>▃▂▁▁▁▂▄▅▇▇▇▇▇████▇███▇█▇████████████████</td></tr><tr><td>val_f1</td><td>▄▄▁▃▅▅▅▇▆▅█▇▆▇▇█▇▆█▆█▇█▇█▇▆▇█▇▇█▇▇▇▇▇▇▇█</td></tr><tr><td>val_loss_epoch</td><td>█▇▅▅▄▃▃▂▁▂▂▁▁▂▁▁▁▁▁▁▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_step</td><td>█▇▅▅▄▄▃▂▂▃▂▁▂▃▃▁▃▃▂▁▃▁▂▂▃▃▃▃▂▃▂▃▁▃▂▂▂▁▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.76269</td></tr><tr><td>train_auc</td><td>0.84367</td></tr><tr><td>train_f1</td><td>0.77002</td></tr><tr><td>train_loss_epoch</td><td>0.48715</td></tr><tr><td>train_loss_step</td><td>0.50295</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.77305</td></tr><tr><td>val_auc</td><td>0.86655</td></tr><tr><td>val_f1</td><td>0.76355</td></tr><tr><td>val_loss_epoch</td><td>0.44879</td></tr><tr><td>val_loss_step</td><td>0.43993</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_32_onehot_2</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/sf510qf4' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/sf510qf4</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240103_172539-sf510qf4\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "789a0d0e95b9409ca2b88831c4815246",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01693333333435779, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240103_175442-7u0bme10</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/7u0bme10' target=\"_blank\">MLP_2_32_onehot_2</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/7u0bme10' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/7u0bme10</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 1.5 K \n",
      "2  | head        | Sequential       | 562   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 1.1 K \n",
      "--------------------------------------------------\n",
      "3.1 K     Trainable params\n",
      "0         Non-trainable params\n",
      "3.1 K     Total params\n",
      "0.013     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d23e44bfd4b4d0eab4d140c7969d18e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▄▆▇▇▇▇██▇▇██▇█▇▇▇▇█▇█████████████████▇█</td></tr><tr><td>train_auc</td><td>▁▂▆▇▇▇▇█████████████████████████████████</td></tr><tr><td>train_f1</td><td>▁▅▆▇▇▇████▇██▇█▇██▇██████████████▇██████</td></tr><tr><td>train_loss_epoch</td><td>█▇▅▃▃▂▂▁▂▂▂▂▁▂▂▂▂▂▂▂▁▂▂▂▂▂▁▂▂▂▁▂▁▁▁▂▁▁▂▂</td></tr><tr><td>train_loss_step</td><td>██▅▄▄▃▃▄▄▂▃▂▅▄▃▃▂▃▃▃▃▃▄▂▃▃▃▃▃▁▃▄▃▃▄▅▂▂▂▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▇▆▇▇▇▇██████████▇▇█████████████████████</td></tr><tr><td>val_auc</td><td>▂▂▁▃▅▆▇▇▇▇▇▇████████████████████████████</td></tr><tr><td>val_f1</td><td>▂█▁▅▃▇▅▇█▇▆▇█▇███▅▆████▇██▇▇▇▇██▇█▇███▇▇</td></tr><tr><td>val_loss_epoch</td><td>█▇▄▃▂▂▂▁▂▁▁▁▁▁▂▂▂▂▁▁▂▂▂▁▁▁▁▁▂▁▁▁▁▁▁▂▁▁▁▁</td></tr><tr><td>val_loss_step</td><td>█▇▅▄▃▃▃▁▃▂▃▂▃▃▃▃▂▃▁▂▄▄▂▂▂▃▃▂▄▁▂▃▃▂▂▂▂▂▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.77096</td></tr><tr><td>train_auc</td><td>0.8391</td></tr><tr><td>train_f1</td><td>0.77624</td></tr><tr><td>train_loss_epoch</td><td>0.50116</td></tr><tr><td>train_loss_step</td><td>0.49066</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.78014</td></tr><tr><td>val_auc</td><td>0.86451</td></tr><tr><td>val_f1</td><td>0.75844</td></tr><tr><td>val_loss_epoch</td><td>0.45989</td></tr><tr><td>val_loss_step</td><td>0.46667</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_32_onehot_2</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/7u0bme10' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/7u0bme10</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240103_175442-7u0bme10\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09ce0392a69c4f018107f4d06b210e63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01691666666883975, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240103_182330-dsqjhhrm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/dsqjhhrm' target=\"_blank\">MLP_2_64_onehot_2</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/dsqjhhrm' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/dsqjhhrm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 5.1 K \n",
      "2 | head        | Sequential       | 2.1 K \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "7.2 K     Trainable params\n",
      "0         Non-trainable params\n",
      "7.2 K     Total params\n",
      "0.029     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afc7e09de3a64eea87d48993f19993a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▆▇▇▇▇███▇█████████████▇█▇██████████████</td></tr><tr><td>train_auc</td><td>▁▆▇▇▇▇██████████████████████████████████</td></tr><tr><td>train_f1</td><td>▁▄▇▇▇▇█▇▇▇███▇█████▇▇█▇▇█▇▇█████▇█▇▇██▇▇</td></tr><tr><td>train_loss_epoch</td><td>█▅▃▂▂▂▂▂▂▂▁▂▂▁▂▂▁▁▁▁▁▁▁▁▁▂▁▁▂▁▁▁▁▁▂▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▅▇▃▃▄▂▂▂▃▃▃▃▄▃▂▂▃▁▃▄▂▁▃▃▂▃▂▂▄▄▃▂▂▁▂▂▁▂▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▇▇███▇████████████████▇███████████████▇</td></tr><tr><td>val_auc</td><td>▁▃▅▇▇▇▇▇████████████████████████████████</td></tr><tr><td>val_f1</td><td>▁█▄▆▇▇▅▇▇▇█▇▇▇▇██████▇████▇█▆████▆██▇███</td></tr><tr><td>val_loss_epoch</td><td>█▅▂▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▂▁▁▁▁▁▁▁▁▂▁▁▂▁▁▁▂</td></tr><tr><td>val_loss_step</td><td>█▅▂▃▃▃▂▁▂▁▃▂▂▃▃▃▂▂▁▂▂▂▁▄▂▂▂▂▂▂▂▂▂▃▁▃▃▃▃▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.77155</td></tr><tr><td>train_auc</td><td>0.84826</td></tr><tr><td>train_f1</td><td>0.76951</td></tr><tr><td>train_loss_epoch</td><td>0.48967</td></tr><tr><td>train_loss_step</td><td>0.50231</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.76596</td></tr><tr><td>val_auc</td><td>0.86734</td></tr><tr><td>val_f1</td><td>0.7703</td></tr><tr><td>val_loss_epoch</td><td>0.4809</td></tr><tr><td>val_loss_step</td><td>0.46658</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_64_onehot_2</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/dsqjhhrm' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/dsqjhhrm</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240103_182330-dsqjhhrm\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83b84b748e344688a64e898f39f80bb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240103_185300-jjoodcdx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/jjoodcdx' target=\"_blank\">MLP_2_64_onehot_2</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/jjoodcdx' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/jjoodcdx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 5.1 K \n",
      "2 | head        | Sequential       | 2.1 K \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "7.2 K     Trainable params\n",
      "0         Non-trainable params\n",
      "7.2 K     Total params\n",
      "0.029     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▃▆▆▇▇▇▆▇▇█████▇▇▇█▇▇▇▇███▇█▇█▇▇█▇▇▇███▇</td></tr><tr><td>train_auc</td><td>▁▂▅▆▆▇▇▆▆▇▇█▇▇█▇▇▇██▇█████▇▇████▇███████</td></tr><tr><td>train_f1</td><td>▃▁▅▇▇▇▇▆▇▇▇██▇█▇▇▇█▇▇▇▇▇█▇▇█▇▇▇▇██▇▇███▇</td></tr><tr><td>train_loss_epoch</td><td>█▆▄▃▃▂▃▃▂▂▂▁▂▂▁▂▂▂▂▂▂▂▂▁▁▁▂▁▂▂▂▂▂▁▁▂▁▁▁▂</td></tr><tr><td>train_loss_step</td><td>█▇▆▄▄▅▄▄▅▄▃▄▃▁▃▃▂▄▂▄▄▃▂▃▃▅▃▂▃▄▃▂▃▃▂▅▂▃▃▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▁████▅▄▅▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>▁▇▇▇▇▇▇█▇▇▇▇▇▇███▇▇█████████████████▇█▇█</td></tr><tr><td>val_f1</td><td>▁▁████▄▄▄▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_epoch</td><td>▂▂▁▁▁▁▁▂▂▂▃▄▄▄▄▄▅▅▅▅▅▅▆█▇▇▆▆▆▆▆▆▆▆▆▇▇▇▇▇</td></tr><tr><td>val_loss_step</td><td>▂▂▁▁▁▁▁▂▃▂▃▄▄▄▄▄▆▅▅▄▅▅▅█▇▆▆▅▅▆▅▆▆▆▆▆█▅▇▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.72904</td></tr><tr><td>train_auc</td><td>0.79335</td></tr><tr><td>train_f1</td><td>0.72199</td></tr><tr><td>train_loss_epoch</td><td>0.54157</td></tr><tr><td>train_loss_step</td><td>0.52775</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.44444</td></tr><tr><td>val_auc</td><td>0.82853</td></tr><tr><td>val_f1</td><td>0.61538</td></tr><tr><td>val_loss_epoch</td><td>1.11731</td></tr><tr><td>val_loss_step</td><td>1.10998</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_64_onehot_2</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/jjoodcdx' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/jjoodcdx</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240103_185300-jjoodcdx\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12002425740c439a9eabca15d0d1a26a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01691666666883975, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240103_192430-vkixo8g8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/vkixo8g8' target=\"_blank\">MLP_2_64_onehot_2</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/vkixo8g8' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/vkixo8g8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 5.1 K \n",
      "2 | head        | Sequential       | 2.1 K \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "7.2 K     Trainable params\n",
      "0         Non-trainable params\n",
      "7.2 K     Total params\n",
      "0.029     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e348470139c43dbbc4686dc1646768f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▄▅▆▆▇▇▇▇▇█▇▇█▇██████████▇████▇█████▇███</td></tr><tr><td>train_auc</td><td>▇██▆▆▇▅▄▆▅▃▅▅▆▄▅▅▅▆▆▅▆▅▆▅▆▇▇▆▄▄▃▅▅▆▆▃▁▂▄</td></tr><tr><td>train_f1</td><td>▁▃▅▆▆▇▇▇▇▇█▇▇▇▇██▇█▇██▇██▇██▇█▇█████▇███</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▆▆▃▃▄▃▂▂▃▃▃▃▃▃▂▂▃▂▄▃▂▂▃▂▃▃▂▂▃▄▃▁▂▁▂▂▁▂▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▆▆▅▆▇▇▇▇▇▇▇█▇▇█▇▇█▇█▇▇▇▇▇▇▇▇█▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>val_auc</td><td>█▆▄▃▄▄▂▂▃▂▁▁▁▁▁▁▃▂▁▁▁▃▃▃▂▂▂▂▁▁▁▂▁▂▁▁▁▁▁▁</td></tr><tr><td>val_f1</td><td>▁█▇▆▇▇█▇▇█▇███████████████▇█████████████</td></tr><tr><td>val_loss_epoch</td><td>█▇▅▄▄▃▂▂▂▂▂▂▂▂▂▂▁▂▂▂▁▁▁▂▂▂▁▂▂▁▂▁▁▂▁▃▂▁▂▂</td></tr><tr><td>val_loss_step</td><td>▇█▅▅▅▅▃▄▃▃▃▃▃▅▅▃▂▄▄▄▃▃▁▅▃▃▁▃▄▂▄▃▃▅▂▅▃▄▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.77568</td></tr><tr><td>train_auc</td><td>0.40914</td></tr><tr><td>train_f1</td><td>0.78035</td></tr><tr><td>train_loss_epoch</td><td>0.4844</td></tr><tr><td>train_loss_step</td><td>0.48176</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.80142</td></tr><tr><td>val_auc</td><td>0.14735</td></tr><tr><td>val_f1</td><td>0.79104</td></tr><tr><td>val_loss_epoch</td><td>0.4449</td></tr><tr><td>val_loss_step</td><td>0.43896</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_64_onehot_2</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/vkixo8g8' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/vkixo8g8</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240103_192430-vkixo8g8\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "106a16876bab4cb4bb91ce0a92161334",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240103_195323-ri4rmib0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/ri4rmib0' target=\"_blank\">MLP_2_64_onehot_2</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/ri4rmib0' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/ri4rmib0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 5.1 K \n",
      "2  | head        | Sequential       | 2.1 K \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 65    \n",
      "--------------------------------------------------\n",
      "7.3 K     Trainable params\n",
      "0         Non-trainable params\n",
      "7.3 K     Total params\n",
      "0.029     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2189bde8db444da4a36c5407ada9de0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▆▆▇▇▇████████▇█▇████▇█████▇█▇██▇███████</td></tr><tr><td>train_auc</td><td>▁▅▇▇▇██████████████████████▇████████████</td></tr><tr><td>train_f1</td><td>▁▅▆▇▇▇█▇███▇██▇███▇██▇█████▇█▇▇█▇▇██████</td></tr><tr><td>train_loss_epoch</td><td>█▆▄▃▃▂▂▂▁▁▂▂▂▁▂▁▂▂▂▁▁▂▁▁▁▁▁▃▂▂▂▁▂▂▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▆▃▃▄▂▄▂▅▃▃▃▁▂▃▅▄▂▃▁▃▃▂▂▂▂▂▃▃▁▃▁▂▂▃▂▁▂▃▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▆▇▇▇███████████▇██████▇██▇█▇███▇███████</td></tr><tr><td>val_auc</td><td>▃▁▂▄▅▆▇▇▇▇█▇▇▇█▇▇▇██▇▇▇▇██▇█▆█▇▇▇███████</td></tr><tr><td>val_f1</td><td>▁▃▆▄▄▇▅██▇▆▆▆▇██▄▆███▆▆▆██▄█▆▆▇█▄█▆█▇▆██</td></tr><tr><td>val_loss_epoch</td><td>█▅▃▂▂▂▂▂▂▁▂▁▁▁▂▁▂▁▁▂▂▁▁▁▁▁▂▂▁▁▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_step</td><td>█▅▃▂▃▂▂▂▂▂▃▁▁▂▂▂▂▃▂▃▃▃▁▃▂▂▂▃▂▃▃▂▃▂▂▁▁▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.76978</td></tr><tr><td>train_auc</td><td>0.85059</td></tr><tr><td>train_f1</td><td>0.7756</td></tr><tr><td>train_loss_epoch</td><td>0.47467</td></tr><tr><td>train_loss_step</td><td>0.45476</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.78014</td></tr><tr><td>val_auc</td><td>0.86698</td></tr><tr><td>val_f1</td><td>0.77804</td></tr><tr><td>val_loss_epoch</td><td>0.44991</td></tr><tr><td>val_loss_step</td><td>0.42508</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_64_onehot_2</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/ri4rmib0' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/ri4rmib0</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240103_195323-ri4rmib0\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa385f2822ee46ff9b5a446c64d56d64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240103_202224-l5uxcwz3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/l5uxcwz3' target=\"_blank\">MLP_2_64_onehot_2</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/l5uxcwz3' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/l5uxcwz3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 5.1 K \n",
      "2  | head        | Sequential       | 2.1 K \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 4.2 K \n",
      "--------------------------------------------------\n",
      "11.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "11.4 K    Total params\n",
      "0.046     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6bcb88e8a124345a7a9fde083754558",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▆▇▇▇▇▇██████▇█████████▇██████▇█████████</td></tr><tr><td>train_auc</td><td>▁▅▇█▇▇██████████████████████████████████</td></tr><tr><td>train_f1</td><td>▁▅▇▇▇▇▇████▇█▇██▇█▇██████████▇██████████</td></tr><tr><td>train_loss_epoch</td><td>█▅▃▂▂▃▂▂▁▁▂▁▂▂▂▁▁▂▁▂▁▁▁▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▅▃▄▄▄▄▄▂▄▃▃▄▂▃▂▃▅▂▄▃▃▄▃▃▂▄▄▁▂▂▃▃▃▁▂▃▂▃▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▇▇▇▇▇▇▇▇▇█▇▇▇▇▇█▇▇███▇▇█████▇▇▇████████</td></tr><tr><td>val_auc</td><td>▁▂▄▆▇▇▇▇▇███████████████████████████████</td></tr><tr><td>val_f1</td><td>▁█▇██▇█▇█▆▇█▇▇█▇█▇█████▇█▇▇███▇█████████</td></tr><tr><td>val_loss_epoch</td><td>█▅▂▂▁▂▁▁▁▁▁▂▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_step</td><td>█▆▃▂▂▃▂▁▁▂▂▂▁▁▂▂▃▂▂▂▂▂▁▁▃▂▂▂▂▃▂▂▃▂▃▃▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.76269</td></tr><tr><td>train_auc</td><td>0.8427</td></tr><tr><td>train_f1</td><td>0.76976</td></tr><tr><td>train_loss_epoch</td><td>0.49213</td></tr><tr><td>train_loss_step</td><td>0.4814</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.7896</td></tr><tr><td>val_auc</td><td>0.867</td></tr><tr><td>val_f1</td><td>0.76011</td></tr><tr><td>val_loss_epoch</td><td>0.45285</td></tr><tr><td>val_loss_step</td><td>0.43696</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_64_onehot_2</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/l5uxcwz3' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/l5uxcwz3</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240103_202224-l5uxcwz3\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a4c8360f521498da753c270a5c66848",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016933333330477276, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240103_205137-9lsm6pgj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/9lsm6pgj' target=\"_blank\">MLP_3_16_onehot_2</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/9lsm6pgj' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/9lsm6pgj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 768   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "922       Trainable params\n",
      "0         Non-trainable params\n",
      "922       Total params\n",
      "0.004     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5778c0414b64419f914e101a60aea2d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▁▂▅▇▇▇▇▇██▇█▇▇██████▇██████▇▇█▇███████</td></tr><tr><td>train_auc</td><td>▁▁▁▂▅▇▇▇▇▇██████████████████████████████</td></tr><tr><td>train_f1</td><td>▅▁▄▃▆▇▇▇▇▇▇▇▇█▇▇▇▇▇█████▇██▇▇▇▇██▇██▇▇██</td></tr><tr><td>train_loss_epoch</td><td>███▇▆▃▃▃▂▂▂▂▂▂▂▂▁▁▂▁▂▁▂▁▁▂▁▂▁▁▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>████▇▃▃▆▅▄▄▄▃▂▃▃▃▃▄▃▃▃▃▃▁▂▃▃▃▃▂▃▄▂▅▂▂▄▂▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▁▁█▇▇▇█████████████████████████████████</td></tr><tr><td>val_auc</td><td>▁▆▇▇▇███████████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁█▅▅▅▇█████▆▆▆▆███▆▆▅▆▇▇▇███▇▇▇█▇▆▇▇▇▆</td></tr><tr><td>val_loss_epoch</td><td>████▆▂▂▂▂▂▁▂▂▁▁▁▁▂▂▂▂▁▂▂▁▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_step</td><td>████▆▃▂▃▃▂▂▄▃▂▂▂▁▂▃▂▃▂▃▄▃▃▃▃▃▂▂▁▂▃▁▂▂▄▂▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.77037</td></tr><tr><td>train_auc</td><td>0.82728</td></tr><tr><td>train_f1</td><td>0.78473</td></tr><tr><td>train_loss_epoch</td><td>0.51782</td></tr><tr><td>train_loss_step</td><td>0.45231</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.77541</td></tr><tr><td>val_auc</td><td>0.86419</td></tr><tr><td>val_f1</td><td>0.73684</td></tr><tr><td>val_loss_epoch</td><td>0.46743</td></tr><tr><td>val_loss_step</td><td>0.46664</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_16_onehot_2</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/9lsm6pgj' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/9lsm6pgj</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240103_205137-9lsm6pgj\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f442fc8d01a844eea206b089bdedbe46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240103_212235-3u0f90di</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/3u0f90di' target=\"_blank\">MLP_3_16_onehot_2</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/3u0f90di' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/3u0f90di</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 768   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "922       Trainable params\n",
      "0         Non-trainable params\n",
      "922       Total params\n",
      "0.004     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06c1ac81bcd44327b6bc123e353216ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▂▁▁▂▂▂▂▂▃▂▃▂▃▄▆▇▆▆▇▇▇▇▇█▇▇▇▇▇▇▇██▇▇▇▇▇█▇</td></tr><tr><td>train_auc</td><td>▁▁▁▂▁▂▂▂▃▂▃▂▃▅▆▇▆▆▇▇▇█▇██▇▇▇▇▇▇▇███▇▇███</td></tr><tr><td>train_f1</td><td>▄▁▃▁▄▂▃▄▁▂▂▅▆▅▆▇▅▅▅▆▆▆▇█▆▆▆▆▆▅▆▇▇▇▇▆▆▇█▇</td></tr><tr><td>train_loss_epoch</td><td>█▇▇▇▇▇▆▇▆▇▆▆▆▅▄▃▄▄▄▃▃▃▄▂▂▃▃▂▂▃▂▂▂▂▂▃▃▂▁▁</td></tr><tr><td>train_loss_step</td><td>█▇▇▇▇▇▆▇▇▆▆▆▆▆▆▆▆▆▃▄▃▅▅▁▂▅▃▃▁▄▂▁▆▂▄▃▃▃▂▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>▃▄▄▂▁▁▂▂▃▃▇▇▆▇█████▇▇███████▇▇█████▇█▇██</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_epoch</td><td>▂▂▂▂▂▂▃▃▃▃▄▄▅▃▁▁▁▁▂▂▁▃▃▃▄▄▄▄▅▄▄▄▄▅▆▅▅▆▇█</td></tr><tr><td>val_loss_step</td><td>▂▃▃▁▂▂▃▂▃▃▄▅▄▃▁▁▁▁▃▂▁▃▄▃▅▄▃▄▅▃▃▄▃▄▇▄▄▅██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.67592</td></tr><tr><td>train_auc</td><td>0.72382</td></tr><tr><td>train_f1</td><td>0.68026</td></tr><tr><td>train_loss_epoch</td><td>0.61408</td></tr><tr><td>train_loss_step</td><td>0.60011</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.44444</td></tr><tr><td>val_auc</td><td>0.79307</td></tr><tr><td>val_f1</td><td>0.61538</td></tr><tr><td>val_loss_epoch</td><td>0.77333</td></tr><tr><td>val_loss_step</td><td>0.79383</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_16_onehot_2</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/3u0f90di' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/3u0f90di</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240103_212235-3u0f90di\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a58195fea63544308cead38218abea19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.0171999999981684, max=1.0))…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240103_215309-qn5z22te</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/qn5z22te' target=\"_blank\">MLP_3_16_onehot_2</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/qn5z22te' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/qn5z22te</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 768   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "922       Trainable params\n",
      "0         Non-trainable params\n",
      "922       Total params\n",
      "0.004     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▂▃▄▅▆▆▆▆▇▇▆▇▇▇▇▇▇▇██▇▇█▇▇▇▇▇█▇▇████▇██</td></tr><tr><td>train_auc</td><td>▄▅▅▆▅▆▇██▇▇▇▇▇▆▄▅▄▄▄▃▃▃▃▃▂▃▂▂▂▂▃▁▂▃▁▂▁▁▁</td></tr><tr><td>train_f1</td><td>▁▁▂▄▄▄▆▆▆▅▇▆▆▇▆▆▇▆▇▇▇▇▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇██</td></tr><tr><td>train_loss_epoch</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▃▂▂▂▂▂▂▂▂▂▂▁▁▂▁▂▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▄▅▆▆▆▅▆▆▇▇▇▇▆▅▇▇███▆▇▆▇▇▇███▇▇███▇▇██▇▇</td></tr><tr><td>val_auc</td><td>▅▆▆▇▇▇████████▇▄█▅▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_f1</td><td>▁▅▄▆▅▄▃▄▅▇▆▆▇▅▃▆▇██▇▅▇▄▇▇▇▇██▇▇█▇█▆▅██▇▆</td></tr><tr><td>val_loss_epoch</td><td>█▇▆▆▅▅▄▃▃▃▃▂▂▂▃▂▁▂▂▂▃▂▂▂▁▂▁▂▁▁▂▁▁▁▂▂▁▂▁▂</td></tr><tr><td>val_loss_step</td><td>█▇▇▆▆▅▄▄▄▃▃▄▃▂▃▃▁▃▂▂▄▂▃▃▂▂▃▃▃▂▂▁▁▂▁▃▂▃▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.74321</td></tr><tr><td>train_auc</td><td>0.41965</td></tr><tr><td>train_f1</td><td>0.76701</td></tr><tr><td>train_loss_epoch</td><td>0.5431</td></tr><tr><td>train_loss_step</td><td>0.503</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.77778</td></tr><tr><td>val_auc</td><td>0.12033</td></tr><tr><td>val_f1</td><td>0.70253</td></tr><tr><td>val_loss_epoch</td><td>0.48923</td></tr><tr><td>val_loss_step</td><td>0.47142</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_16_onehot_2</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/qn5z22te' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/qn5z22te</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240103_215309-qn5z22te\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e321929fe2cd4e5cbb441ec6639122ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01691666666883975, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240103_222408-m2dm9g0u</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/m2dm9g0u' target=\"_blank\">MLP_3_16_onehot_2</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/m2dm9g0u' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/m2dm9g0u</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 768   \n",
      "2  | head        | Sequential       | 154   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 17    \n",
      "--------------------------------------------------\n",
      "939       Trainable params\n",
      "0         Non-trainable params\n",
      "939       Total params\n",
      "0.004     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97e35601b8664ad39101ff572fc10fb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▂▁▂▃▅▆▇▇▇▇▇▇▇▇▇██▇█████████▇███▇▇█████▇▇</td></tr><tr><td>train_auc</td><td>▁▁▁▂▅▆▇▇▇▇▇█▇███████████████████▇███████</td></tr><tr><td>train_f1</td><td>▄▁▄▄▆▅▆▇▆▇▇█▇▇▇▇█▇██████▇██▇███▇██████▇▇</td></tr><tr><td>train_loss_epoch</td><td>███▇▆▅▃▃▃▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▂▁▁▂▁▁▁▂▂▁▁▁▁▂▁▁</td></tr><tr><td>train_loss_step</td><td>███▇▆▅▅▃▅▄▂▂▂▂▃▄▂▄▃▃▄▂▄▂▂▃▃▇▄▁▃▃▃▂▄▂▃▂▂▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▁▁▇▅▆▇▇███▇████████████████████▇██▇████</td></tr><tr><td>val_auc</td><td>▁▇▇▇▇▇██████████████████████████████████</td></tr><tr><td>val_f1</td><td>▅▅▅█▁▂▇▅█▇▇▆▇▇▆▇▇▇▆▇▇▇▇▇█▇▆█▇▇▇▇▆▇▇▆▆▇▇▇</td></tr><tr><td>val_loss_epoch</td><td>████▆▅▃▃▂▂▂▂▂▁▂▁▁▁▂▁▁▁▁▁▁▁▂▁▁▁▁▁▂▁▂▂▂▁▁▂</td></tr><tr><td>val_loss_step</td><td>████▅▆▄▃▃▄▂▂▂▁▃▁▁▃▂▂▂▃▂▂▂▁▃▃▃▃▃▃▂▂▃▄▂▂▁▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.74793</td></tr><tr><td>train_auc</td><td>0.82456</td></tr><tr><td>train_f1</td><td>0.75558</td></tr><tr><td>train_loss_epoch</td><td>0.5219</td></tr><tr><td>train_loss_step</td><td>0.53441</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.76832</td></tr><tr><td>val_auc</td><td>0.8586</td></tr><tr><td>val_f1</td><td>0.72316</td></tr><tr><td>val_loss_epoch</td><td>0.47764</td></tr><tr><td>val_loss_step</td><td>0.50394</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_16_onehot_2</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/m2dm9g0u' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/m2dm9g0u</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240103_222408-m2dm9g0u\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12d88cf24eae421c816e7b547db0fb33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240103_225537-c1stpb82</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/c1stpb82' target=\"_blank\">MLP_3_16_onehot_2</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/c1stpb82' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/c1stpb82</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 768   \n",
      "2  | head        | Sequential       | 154   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 281   \n",
      "--------------------------------------------------\n",
      "1.2 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.2 K     Total params\n",
      "0.005     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eff46740ff81429b9fdfaecd28321626",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▁▂▆▆▇▇█▇▇▇▇█▇▇████▇▇██▇▇███▇████▇▇▇▇██</td></tr><tr><td>train_auc</td><td>▁▂▂▂▆▆▇▇█▇▇▇████████████▇███████████████</td></tr><tr><td>train_f1</td><td>▄▁▄▃▆▆▇▇▇▇▇▇▇█▇▇████▇▇██▇▇███▇████▇▇▇▇█▇</td></tr><tr><td>train_loss_epoch</td><td>█▇▇▇▅▄▃▃▂▂▂▃▂▂▂▂▁▂▂▂▂▂▁▁▂▂▂▁▂▁▁▂▂▂▂▂▂▂▁▂</td></tr><tr><td>train_loss_step</td><td>███▇▆▅▃▅▄▃▄▃▄▃▃▂▂▄▃▃▃▄▃▃▄▃▄▃▃▁▅▄▃▄▅▂▄▄▄▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▁▁▅▆▇▇▇████████████████████████████████</td></tr><tr><td>val_auc</td><td>▁▅▇▇▇▇██████████████████████████████████</td></tr><tr><td>val_f1</td><td>▂▂▂▆▁▄▄▅▇███▇▆████▇▆▇▇▇▆▅▇█▇▇██▇███▆▇███</td></tr><tr><td>val_loss_epoch</td><td>███▇▄▃▂▂▂▂▂▂▂▂▂▂▁▂▁▂▂▁▁▁▂▂▁▁▁▁▁▁▁▁▁▂▁▂▂▂</td></tr><tr><td>val_loss_step</td><td>███▇▅▄▃▃▃▄▃▂▃▄▃▃▂▄▂▃▃▃▁▂▄▃▃▂▂▂▃▃▃▂▂▃▂▃▄▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.76387</td></tr><tr><td>train_auc</td><td>0.82907</td></tr><tr><td>train_f1</td><td>0.76498</td></tr><tr><td>train_loss_epoch</td><td>0.51969</td></tr><tr><td>train_loss_step</td><td>0.52702</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.78251</td></tr><tr><td>val_auc</td><td>0.85991</td></tr><tr><td>val_f1</td><td>0.7665</td></tr><tr><td>val_loss_epoch</td><td>0.4715</td></tr><tr><td>val_loss_step</td><td>0.5019</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_16_onehot_2</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/c1stpb82' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/c1stpb82</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240103_225537-c1stpb82\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d56c41cebf340fa9107f88d7c724f92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01691666666495924, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240103_232758-amunj32b</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/amunj32b' target=\"_blank\">MLP_3_32_onehot_2</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/amunj32b' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/amunj32b</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 2.6 K \n",
      "2 | head        | Sequential       | 562   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "3.1 K     Trainable params\n",
      "0         Non-trainable params\n",
      "3.1 K     Total params\n",
      "0.012     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▆▇▆▇██▇███████████████████████████████</td></tr><tr><td>train_auc</td><td>▁▂▆▇▇▇██▇███████████████████████████████</td></tr><tr><td>train_f1</td><td>▁▅▇▇▅▇██▇███▇▇██████████████████████████</td></tr><tr><td>train_loss_epoch</td><td>██▄▃▃▃▂▁▂▂▂▁▁▁▁▂▁▂▂▁▁▁▁▁▁▁▂▁▂▁▁▁▁▁▁▁▂▁▁▁</td></tr><tr><td>train_loss_step</td><td>██▆▄▄▃▃▃▃▄▃▂▁▃▂▃▃▄▅▂▁▂▃▃▃▁▄▃▂▄▁▃▂▅▃▂▃▂▁▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▇▇▇████████████████████████████████████</td></tr><tr><td>val_auc</td><td>▂▁▁▄▅▆▆▇▇▇▇▇▇███████████████████████████</td></tr><tr><td>val_f1</td><td>▁▆▃▄██▇▇▇▇▇▇███▇▇▇▇▇█▆▇▇▇█▇██▅▇█▆▇██▇▇██</td></tr><tr><td>val_loss_epoch</td><td>█▇▃▂▃▂▂▁▁▂▂▁▂▂▁▁▁▁▁▁▂▁▁▂▁▂▁▂▁▁▁▂▁▁▂▂▁▁▁▂</td></tr><tr><td>val_loss_step</td><td>█▇▄▂▄▃▃▃▂▃▃▂▃▂▁▁▃▃▂▂▂▃▁▄▂▄▂▃▂▃▂▃▃▂▃▃▂▂▁▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.7621</td></tr><tr><td>train_auc</td><td>0.83543</td></tr><tr><td>train_f1</td><td>0.76419</td></tr><tr><td>train_loss_epoch</td><td>0.50288</td></tr><tr><td>train_loss_step</td><td>0.4973</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.77778</td></tr><tr><td>val_auc</td><td>0.86557</td></tr><tr><td>val_f1</td><td>0.77512</td></tr><tr><td>val_loss_epoch</td><td>0.47341</td></tr><tr><td>val_loss_step</td><td>0.46858</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_32_onehot_2</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/amunj32b' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/amunj32b</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240103_232758-amunj32b\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b222c5f92ebd4b67a91dd8836aee8c8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01693333333435779, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240103_235902-997tl8hg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/997tl8hg' target=\"_blank\">MLP_3_32_onehot_2</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/997tl8hg' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/997tl8hg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 2.6 K \n",
      "2 | head        | Sequential       | 562   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "3.1 K     Trainable params\n",
      "0         Non-trainable params\n",
      "3.1 K     Total params\n",
      "0.012     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▆▆▆▆▆▆▇▇▆▇▇▆▇▇▇▇▇█▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇▇▇</td></tr><tr><td>train_auc</td><td>▁▂▅▆▆▆▆▆▆▇▇▇▇▇▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇██▇█▇</td></tr><tr><td>train_f1</td><td>▁▃▆▅▅▅▆▅▆▆▆▇▇▆▆▆▇▇▇█▇█▇▇▇█▇▆▇█▇▆▇▇▇▇█▇▇▇</td></tr><tr><td>train_loss_epoch</td><td>█▇▆▅▃▄▃▃▃▃▃▃▂▂▂▃▂▂▂▂▂▃▂▃▂▂▂▃▂▃▂▂▂▁▂▂▁▂▁▂</td></tr><tr><td>train_loss_step</td><td>█▇▆▆▄▄▅▃▅▆▄▆▃▆▃▄▄▄▅▄▄▅▂▅▄▃▄▄▃▆▁▄▃▄▃▂▃▂▄▄</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁█▄▅▅█▅▅▅█████████▅▅▅▁▅▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>▁▃▅▆▄▄▅▄▅▇▆▇▃▅▇█▇▅▇█▇██▆▇▇▆▆▆▆█▇█▇█▇▇▆▇▆</td></tr><tr><td>val_f1</td><td>▇█▁▃▃█▃▃▃█████████▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>val_loss_epoch</td><td>▄▄▂▁▁▂▁▁▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▅▄▅▅▇▇██▇▇▇</td></tr><tr><td>val_loss_step</td><td>▄▄▂▁▁▂▁▂▂▁▂▂▂▂▂▂▂▃▃▃▃▄▂▃▄▃▄▃▄▅▄▃▄▇▅█▆▆▆▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.70484</td></tr><tr><td>train_auc</td><td>0.76107</td></tr><tr><td>train_f1</td><td>0.70998</td></tr><tr><td>train_loss_epoch</td><td>0.5728</td></tr><tr><td>train_loss_step</td><td>0.5786</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.44444</td></tr><tr><td>val_auc</td><td>0.80134</td></tr><tr><td>val_f1</td><td>0.61538</td></tr><tr><td>val_loss_epoch</td><td>0.76066</td></tr><tr><td>val_loss_step</td><td>0.76104</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_32_onehot_2</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/997tl8hg' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/997tl8hg</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240103_235902-997tl8hg\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b3e659154dd46698d40fa4debd4cf46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240104_002902-rfjdha4j</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/rfjdha4j' target=\"_blank\">MLP_3_32_onehot_2</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/rfjdha4j' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/rfjdha4j</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 2.6 K \n",
      "2 | head        | Sequential       | 562   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "3.1 K     Trainable params\n",
      "0         Non-trainable params\n",
      "3.1 K     Total params\n",
      "0.012     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "901e7ea73f94413bad3db178151c181e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▂▅▆▆▆▇▇▇▇█▇▇██▇▇▇▇█▇█████▇▇███▇▇█▇███▇</td></tr><tr><td>train_auc</td><td>▁▁▂▅▆▇████▇▇█▇▇▇▇▆▇▇▆▆▇▇▇█▇▇▆▇▆▇▆▆▆▆▆▆▆▆</td></tr><tr><td>train_f1</td><td>▁▃▃▄▅▆▆▇▇▆▇▇▇▇██▇▇▇████████▇▇███▇██████▇</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▄▄▃▃▃▂▃▃▃▃▂▁▂▂▂▃▃▂▁▁▂▂▂▂▁▂▂▂▃▁▂▂▃▂▂▂▂▁▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▁▅▄▅▆▆▇▆▇▆▇▇▇▇▇▇▇▇██▆▇▇▇█▇▇█▇██▇█▇█████</td></tr><tr><td>val_auc</td><td>▁▁▆▆▇▇████████████████████████████▇█████</td></tr><tr><td>val_f1</td><td>▁▂▆▅▇▇▇▇▇█▇▇▇█▇▇█▇███▇██▇████▇████▇█████</td></tr><tr><td>val_loss_epoch</td><td>█▆▆▄▄▃▂▂▂▂▃▂▂▁▂▂▂▂▁▁▂▃▁▂▂▁▁▂▁▂▁▁▁▁▂▁▁▁▁▁</td></tr><tr><td>val_loss_step</td><td>█▆▆▄▄▄▃▃▃▃▃▃▂▁▂▂▃▂▂▂▂▄▁▄▃▃▂▃▂▂▂▃▃▂▃▃▃▂▃▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.74085</td></tr><tr><td>train_auc</td><td>0.64272</td></tr><tr><td>train_f1</td><td>0.75839</td></tr><tr><td>train_loss_epoch</td><td>0.52329</td></tr><tr><td>train_loss_step</td><td>0.53083</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.80142</td></tr><tr><td>val_auc</td><td>0.8837</td></tr><tr><td>val_f1</td><td>0.76271</td></tr><tr><td>val_loss_epoch</td><td>0.43853</td></tr><tr><td>val_loss_step</td><td>0.42917</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_32_onehot_2</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/rfjdha4j' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/rfjdha4j</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240104_002902-rfjdha4j\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba7564eb6c114e4580891e83cb063552",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240104_005852-t9dqmu7s</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/t9dqmu7s' target=\"_blank\">MLP_3_32_onehot_2</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/t9dqmu7s' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/t9dqmu7s</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 2.6 K \n",
      "2  | head        | Sequential       | 562   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 33    \n",
      "--------------------------------------------------\n",
      "3.2 K     Trainable params\n",
      "0         Non-trainable params\n",
      "3.2 K     Total params\n",
      "0.013     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dde381b0e55142eaae13abebee5e9aa9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▃▅▆▇▇█▇██▇▇███▇▇████████████▇████████▇█</td></tr><tr><td>train_auc</td><td>▁▂▆▇▇▇█████▇████████████████████████████</td></tr><tr><td>train_f1</td><td>▁▅▅▅▇▇▇▇██▇▇██▇▇▇██▇███▇███▇█▇█▇██▇████▇</td></tr><tr><td>train_loss_epoch</td><td>██▅▄▂▂▂▂▂▁▂▂▂▁▁▂▂▁▁▂▁▁▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▂</td></tr><tr><td>train_loss_step</td><td>██▆▄▃▃▂▃▃▂▄▂▃▂▃▁▂▃▂▂▂▂▄▄▁▂▂▁▃▂▂▂▃▁▃▂▂▃▁▅</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁█▅▆▇██████▇█████████▇██████████████████</td></tr><tr><td>val_auc</td><td>▄▃▁▃▆▇▇█▇▇▇▇▇▇█▇▇▇██████████████████████</td></tr><tr><td>val_f1</td><td>▄█▁▅▆▇▇▇▇▇█▅▇▆█▇▆▇▆▇▇▆▇▇▇▇▇▇▆▇▇▇▇▇▇▇▇█▇█</td></tr><tr><td>val_loss_epoch</td><td>██▅▄▂▂▁▁▂▁▂▂▁▂▂▁▂▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▂</td></tr><tr><td>val_loss_step</td><td>██▅▅▄▄▃▁▃▂▃▃▂▂▂▃▃▂▃▃▁▃▁▂▃▁▂▃▂▂▃▃▂▂▃▂▃▃▂▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.75738</td></tr><tr><td>train_auc</td><td>0.82803</td></tr><tr><td>train_f1</td><td>0.75637</td></tr><tr><td>train_loss_epoch</td><td>0.51677</td></tr><tr><td>train_loss_step</td><td>0.58049</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.77541</td></tr><tr><td>val_auc</td><td>0.86358</td></tr><tr><td>val_f1</td><td>0.76998</td></tr><tr><td>val_loss_epoch</td><td>0.47459</td></tr><tr><td>val_loss_step</td><td>0.4979</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_32_onehot_2</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/t9dqmu7s' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/t9dqmu7s</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240104_005852-t9dqmu7s\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "875ed72ac0f345d6be9cf98d1d084270",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240104_012856-g7z6ulfn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/g7z6ulfn' target=\"_blank\">MLP_3_32_onehot_2</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/g7z6ulfn' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/g7z6ulfn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 2.6 K \n",
      "2  | head        | Sequential       | 562   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 1.1 K \n",
      "--------------------------------------------------\n",
      "4.2 K     Trainable params\n",
      "0         Non-trainable params\n",
      "4.2 K     Total params\n",
      "0.017     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7a8225e7e864962a9d6b914b03ef622",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▆▆▇█▇▇▇▇█████████████████▇█▇████████▇█</td></tr><tr><td>train_auc</td><td>▁▂▆▇███▇▇███████████████████████████████</td></tr><tr><td>train_f1</td><td>▁▄▅▅▇▇▇▇▆▇██▇██▇▇█▇███▇▇███▇█▇▇██▇█▇█▇██</td></tr><tr><td>train_loss_epoch</td><td>██▅▄▂▂▂▂▃▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▂▁▁▂▁▂▁▁▁▁▁▁▂▁</td></tr><tr><td>train_loss_step</td><td>██▅▄▃▂▄▄▄▃▁▄▄▅▄▃▃▃▃▄▄▃▄▄▃▃▅▄▄▂▃▄▃▂▃▃▂▅▅▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁█▆▇▇▇██████████████████████████████████</td></tr><tr><td>val_auc</td><td>▄▃▁▄▅▇▇▇▇▇▇▇████████████████████████████</td></tr><tr><td>val_f1</td><td>▁█▂▆▄▅▆▅▇▅▇▆▇▆▆▅█▆▆▇▇▆█▇▆█▆▇▆▆▇▆▇▇▆▇▆▇▄▇</td></tr><tr><td>val_loss_epoch</td><td>█▇▄▄▂▁▁▂▂▁▁▂▁▁▁▁▂▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁</td></tr><tr><td>val_loss_step</td><td>█▇▄▅▂▂▁▃▃▁▂▃▃▃▃▂▂▃▁▃▁▂▃▃▃▂▃▂▁▂▂▂▄▂▂▃▃▂▃▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.76741</td></tr><tr><td>train_auc</td><td>0.83765</td></tr><tr><td>train_f1</td><td>0.7774</td></tr><tr><td>train_loss_epoch</td><td>0.50165</td></tr><tr><td>train_loss_step</td><td>0.48454</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.77541</td></tr><tr><td>val_auc</td><td>0.86358</td></tr><tr><td>val_f1</td><td>0.74801</td></tr><tr><td>val_loss_epoch</td><td>0.45658</td></tr><tr><td>val_loss_step</td><td>0.44811</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_32_onehot_2</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/g7z6ulfn' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/g7z6ulfn</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240104_012856-g7z6ulfn\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "270e961d4bf14a30b7557c6538585b94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240104_015917-92t9bp9n</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/92t9bp9n' target=\"_blank\">MLP_3_64_onehot_2</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/92t9bp9n' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/92t9bp9n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 9.2 K \n",
      "2 | head        | Sequential       | 2.1 K \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "11.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "11.4 K    Total params\n",
      "0.045     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe63e86f1d554f3087a0785a1b9d6684",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▃▇▇█████████████████████████████████▇██</td></tr><tr><td>train_auc</td><td>▁▄▇▇████████████████████████████████████</td></tr><tr><td>train_f1</td><td>▁▅▆▇▇▇███▇▇█▇█▇██▇█▇█▇█████▇▇████████▇██</td></tr><tr><td>train_loss_epoch</td><td>█▇▃▂▂▂▂▁▁▂▂▂▁▂▂▂▂▂▂▁▂▂▂▁▁▂▁▁▁▂▁▁▂▂▁▁▁▂▂▁</td></tr><tr><td>train_loss_step</td><td>█▇▃▂▃▄▃▃▃▃▄▄▄▃▃▁▁▃▁▄▁▂▂▁▂▃▂▃▃▁▁▂▂▃▂▃▃▁▂▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▄▇▇▇▇▇▇█▇▇█▇▇██▇▇▇██▇▇▇█▇█▇████████████</td></tr><tr><td>val_auc</td><td>▃▁▄▅▆▇▇█▇███████▇▇▇█████████████████████</td></tr><tr><td>val_f1</td><td>▅▁█▆▆██████▇█▇██▇█▇█▇█▇███▇████▇████████</td></tr><tr><td>val_loss_epoch</td><td>█▆▃▂▂▂▁▁▁▂▂▁▂▁▂▁▁▂▁▁▁▂▁▂▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_step</td><td>█▆▃▃▂▄▂▃▂▃▄▁▃▂▃▁▂▂▂▂▂▂▂▃▂▂▄▂▁▂▂▂▃▃▃▂▂▃▂▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.76505</td></tr><tr><td>train_auc</td><td>0.84259</td></tr><tr><td>train_f1</td><td>0.77514</td></tr><tr><td>train_loss_epoch</td><td>0.49078</td></tr><tr><td>train_loss_step</td><td>0.49544</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.78487</td></tr><tr><td>val_auc</td><td>0.86641</td></tr><tr><td>val_f1</td><td>0.76607</td></tr><tr><td>val_loss_epoch</td><td>0.45853</td></tr><tr><td>val_loss_step</td><td>0.46022</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_64_onehot_2</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/92t9bp9n' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/92t9bp9n</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240104_015917-92t9bp9n\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0a30d264ec24424ac92a9928f340e49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01693333333435779, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240104_022841-joe4vdev</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/joe4vdev' target=\"_blank\">MLP_3_64_onehot_2</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/joe4vdev' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/joe4vdev</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 9.2 K \n",
      "2 | head        | Sequential       | 2.1 K \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "11.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "11.4 K    Total params\n",
      "0.045     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54925b3c0c33424c869a0344aab4e599",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▅▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇███▇█▇█▇███▇▇▇████</td></tr><tr><td>train_auc</td><td>▁▂▅▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇██▇▇█████▇██████▇▇▇██▇</td></tr><tr><td>train_f1</td><td>▁▄▅▅▆▆▇▆▇▇▇▇▆█▇▇▇▇▇▇▇▇▇█▇▇█▇█▇██▇▇▇▇███▇</td></tr><tr><td>train_loss_epoch</td><td>█▇▅▄▄▄▃▃▃▃▂▃▂▃▂▂▂▂▂▂▃▂▂▂▂▂▂▂▁▂▂▂▁▂▂▂▂▁▂▂</td></tr><tr><td>train_loss_step</td><td>██▇▄▅▅▅▅▅▃▄▄▅▄▅▁▃▅▃▄▃▃▂▃▄▃▃▄▄▃▄▃▃▃▅▃▃▂▂▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▄██████▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>▁▅▇▇▆▇█▇▇█▇███▇██▇▇▇▇▇███▇██▇▇▇▇█▇██▇█▇▇</td></tr><tr><td>val_f1</td><td>▁██████▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>val_loss_epoch</td><td>▂▂▁▁▁▁▂▂▂▃▄▃▄▃▄▅▅▅▅▅▅▆▆▆▆▇▆▆▇▆▇▆█▇▆▇▆██▇</td></tr><tr><td>val_loss_step</td><td>▂▂▁▁▁▁▁▂▂▃▄▃▄▄▄▅▄▅▄▅▅▆▇▆▅▆▆▅▇▅▆▆▇▇▆▆▅▇██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.74557</td></tr><tr><td>train_auc</td><td>0.80128</td></tr><tr><td>train_f1</td><td>0.75525</td></tr><tr><td>train_loss_epoch</td><td>0.53341</td></tr><tr><td>train_loss_step</td><td>0.53583</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.44444</td></tr><tr><td>val_auc</td><td>0.81227</td></tr><tr><td>val_f1</td><td>0.61538</td></tr><tr><td>val_loss_epoch</td><td>0.9093</td></tr><tr><td>val_loss_step</td><td>0.9477</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_64_onehot_2</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/joe4vdev' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/joe4vdev</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240104_022841-joe4vdev\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ef6bdc4260a4e66beec595d08c39bae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.017183333332650364, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240104_025746-9fb12pdg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/9fb12pdg' target=\"_blank\">MLP_3_64_onehot_2</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/9fb12pdg' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/9fb12pdg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 9.2 K \n",
      "2 | head        | Sequential       | 2.1 K \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "11.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "11.4 K    Total params\n",
      "0.045     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dfb75e0a00c42f7ac85c9ba360959fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▂▄▆▆▇▇▇▇▇▇▇██▇██▇█▇██▇███████████████▇</td></tr><tr><td>train_auc</td><td>▁▁▃▄▆▇▇▇▇▇█▇▇██████▇███▇██████████▇█████</td></tr><tr><td>train_f1</td><td>▃▁▂▄▆▆▇▇▇▇▇▇▇██▇████▇███████████████████</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▄▂▂▂▂▂▂▁▂▂▂▂▂▁▁▂▂▁▂▁▂▁▁▁▁▁▂▁▁▁▁▁▁▁▂▂▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▅▅▅▆▇▆▇▇▇▇▇▇▇█▇█▇▇▇█▇██▇██▇█▇▇█▇▇▇▇▇█▇█</td></tr><tr><td>val_auc</td><td>▁▃▇▇▇▇▇██████████████▇██████████████████</td></tr><tr><td>val_f1</td><td>▁▇▇▆▇█▇█████████████████████████████████</td></tr><tr><td>val_loss_epoch</td><td>█▄▄▃▂▂▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_step</td><td>█▄▄▃▂▃▂▂▂▂▃▁▂▁▂▁▁▁▁▁▂▂▂▂▂▁▂▂▁▁▁▁▂▂▂▁▁▂▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.75679</td></tr><tr><td>train_auc</td><td>0.81595</td></tr><tr><td>train_f1</td><td>0.76484</td></tr><tr><td>train_loss_epoch</td><td>0.50403</td></tr><tr><td>train_loss_step</td><td>0.46429</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.80378</td></tr><tr><td>val_auc</td><td>0.88135</td></tr><tr><td>val_f1</td><td>0.78987</td></tr><tr><td>val_loss_epoch</td><td>0.45911</td></tr><tr><td>val_loss_step</td><td>0.45328</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_64_onehot_2</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/9fb12pdg' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/9fb12pdg</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240104_025746-9fb12pdg\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff556bc74a5d4851b77942f3c26dcfcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.017183333332650364, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240104_032720-78c8jtba</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/78c8jtba' target=\"_blank\">MLP_3_64_onehot_2</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/78c8jtba' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/78c8jtba</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 9.2 K \n",
      "2  | head        | Sequential       | 2.1 K \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 65    \n",
      "--------------------------------------------------\n",
      "11.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "11.4 K    Total params\n",
      "0.046     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9c05b0142ea4d83b941702f2fac6fcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▆▆▇▇▇██▇████████████▇█▇▇██▇▇████▇███▇██</td></tr><tr><td>train_auc</td><td>▁▆▇▇████████████████████████████████████</td></tr><tr><td>train_f1</td><td>▁▅▅▇▇▇██▇███▇█▇▇█▇▇██▇█▇▇██▇▇████▇▇██▇█▇</td></tr><tr><td>train_loss_epoch</td><td>█▆▄▃▂▂▁▂▂▁▁▁▁▁▁▂▁▁▁▁▁▂▂▂▁▁▁▂▂▁▁▁▁▂▁▁▂▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▇▅▄▄▃▂▂▄▄▄▂▁▁▃▂▄▁▃▁▄▃▄▃▄▄▃▂▂▅▃▃▄▃▂▃▄▄▂▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▅▇▇▇█▇███████████████▇▇█▇▇█████▇███▇███</td></tr><tr><td>val_auc</td><td>▃▁▄▆▆▇▇▇█▇██████████████████████████████</td></tr><tr><td>val_f1</td><td>▄▁▆▅▅▇▆▆▇▇▇▆▇█▇▇▇█▇▇▆█▆▅█▇▅▇█▆▇█▆▇█▆▅█▇█</td></tr><tr><td>val_loss_epoch</td><td>█▄▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▂▂▂▁▂▁▁▁▁▂▁▁▁▁▂▁▁▁</td></tr><tr><td>val_loss_step</td><td>█▅▃▃▄▂▂▂▁▃▃▃▂▂▂▁▃▁▃▃▁▃▃▄▃▃▁▂▃▂▁▃▂▂▂▁▂▁▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.76328</td></tr><tr><td>train_auc</td><td>0.84481</td></tr><tr><td>train_f1</td><td>0.76994</td></tr><tr><td>train_loss_epoch</td><td>0.48857</td></tr><tr><td>train_loss_step</td><td>0.45789</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.77778</td></tr><tr><td>val_auc</td><td>0.86471</td></tr><tr><td>val_f1</td><td>0.76382</td></tr><tr><td>val_loss_epoch</td><td>0.45487</td></tr><tr><td>val_loss_step</td><td>0.43787</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_64_onehot_2</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/78c8jtba' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/78c8jtba</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240104_032720-78c8jtba\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be87114eb92943a99e69fad5773402d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01691666666495924, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240104_035628-gmpabeuf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/gmpabeuf' target=\"_blank\">MLP_3_64_onehot_2</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/gmpabeuf' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/gmpabeuf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 9.2 K \n",
      "2  | head        | Sequential       | 2.1 K \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 4.2 K \n",
      "--------------------------------------------------\n",
      "15.6 K    Trainable params\n",
      "0         Non-trainable params\n",
      "15.6 K    Total params\n",
      "0.062     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c6d2e9713414cf7997d2ce475d9e372",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▆▇▇▇██▇▇▇████████▇███▇▇████████████████</td></tr><tr><td>train_auc</td><td>▁▆▇█▇█████████████▇█████████████████████</td></tr><tr><td>train_f1</td><td>▁▅▆▇▇█▇█▇▇████████▇████▇▇████████▇██████</td></tr><tr><td>train_loss_epoch</td><td>█▅▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▂▁▁▁▂▂▁▂▂▁▁▁▁▁▁▂▁▂▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▅▄▅▃▅▃▂▄▂▃▂▃▆▄▄▄▄▃▃▃▃▃▂▄▁▃▃▃▄▃▃▂▃▄▄▃▃▃▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▇▇▇█▇████████████████▇███▇██▇██████████</td></tr><tr><td>val_auc</td><td>▃▁▄▆▆▇▇▇▇▇▇█████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▅▇█▇▆▇▆▇█▆▇▇█▆▇▆▇▆▆▆▇▅▆▇▇▇▇▆▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>val_loss_epoch</td><td>█▄▃▄▂▁▂▁▁▂▁▁▁▃▁▁▁▂▂▁▁▁▂▁▂▁▂▁▁▂▁▁▁▂▁▁▁▁▂▁</td></tr><tr><td>val_loss_step</td><td>█▅▄▄▂▂▃▂▁▃▂▁▃▃▂▂▂▂▄▂▃▃▃▂▂▁▂▂▂▃▃▂▄▄▂▃▂▂▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.77096</td></tr><tr><td>train_auc</td><td>0.8476</td></tr><tr><td>train_f1</td><td>0.77675</td></tr><tr><td>train_loss_epoch</td><td>0.48102</td></tr><tr><td>train_loss_step</td><td>0.45298</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.78487</td></tr><tr><td>val_auc</td><td>0.86641</td></tr><tr><td>val_f1</td><td>0.76486</td></tr><tr><td>val_loss_epoch</td><td>0.45895</td></tr><tr><td>val_loss_step</td><td>0.47044</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_64_onehot_2</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/gmpabeuf' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/gmpabeuf</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240104_035628-gmpabeuf\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cafb098bbb48405eb2b608c3d1aa2786",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240104_042516-quwiehqb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/quwiehqb' target=\"_blank\">MLP_4_16_onehot_2</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/quwiehqb' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/quwiehqb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 1.0 K \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "1.2 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.2 K     Total params\n",
      "0.005     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f3c0a0c9bdd4dfc9a5224ccbae27e7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▂▂▄▆▆▇▇▇▇▇▇▇██████████████▇█████████▇█</td></tr><tr><td>train_auc</td><td>▁▁▂▂▄▆▇▇▇██▇▇▇▇█████████████████████████</td></tr><tr><td>train_f1</td><td>▁▆▅▅▇▇▆▇▇▇█▇████████████████████████████</td></tr><tr><td>train_loss_epoch</td><td>██▇▇▇▄▄▃▃▂▂▂▂▂▂▁▂▂▁▂▁▁▂▂▁▂▁▁▂▂▁▂▁▁▂▂▁▁▂▁</td></tr><tr><td>train_loss_step</td><td>████▇▅▅▅▅▃▃▅▄▃▅▃▁▃▄▄▂▃▅▄▃▅▃▃▂▄▃▂▃▃▃▃▄▂▃▄</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▃▁▃▆▇▅▇███████████████████▇▇████████████</td></tr><tr><td>val_auc</td><td>▁▅▆▆▆▆▆▇▇▇▇█████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▇▁▆█▅▇██████████▇▇████████▇████████▇███</td></tr><tr><td>val_loss_epoch</td><td>████▇▅▄▃▂▂▂▂▂▁▁▁▂▁▁▁▂▁▂▂▁▁▁▁▁▂▁▂▁▁▁▁▁▂▂▁</td></tr><tr><td>val_loss_step</td><td>████▇▅▅▄▃▃▃▃▃▂▁▂▃▁▂▂▃▂▃▃▂▃▁▂▂▂▂▂▂▁▁▂▂▃▃▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.75856</td></tr><tr><td>train_auc</td><td>0.82305</td></tr><tr><td>train_f1</td><td>0.76124</td></tr><tr><td>train_loss_epoch</td><td>0.52072</td></tr><tr><td>train_loss_step</td><td>0.55348</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.76832</td></tr><tr><td>val_auc</td><td>0.8596</td></tr><tr><td>val_f1</td><td>0.75253</td></tr><tr><td>val_loss_epoch</td><td>0.47596</td></tr><tr><td>val_loss_step</td><td>0.47491</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_16_onehot_2</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/quwiehqb' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/quwiehqb</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240104_042516-quwiehqb\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f0f28c8429f4e5e8929d7a2dc8a5284",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240104_045353-n16bw9zu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/n16bw9zu' target=\"_blank\">MLP_4_16_onehot_2</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/n16bw9zu' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/n16bw9zu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 1.0 K \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "1.2 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.2 K     Total params\n",
      "0.005     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▁▃▆▆▄▆▆▇▇▄▇▆▇▆▆▇▇▇█▇▇▇▇▇▇▇▇▇▇▇▇█▇▇████</td></tr><tr><td>train_auc</td><td>▂▁▂▃▅▆▅▆▆▆▆▄▅▅▄▅▅▅▆▆▇▆▇▇▇▇▆▇▇▆▆▇▇▇▇▇█▆█▇</td></tr><tr><td>train_f1</td><td>▁▄▄▅▆▆▅▆▆▇▇▄▇▇▇▇▇▇▇▇█▇▇▇▇▇▇█▇▇▇▇██▇▇██▇█</td></tr><tr><td>train_loss_epoch</td><td>█▇▇▆▅▅▅▄▄▄▃▅▃▄▃▂▃▃▂▂▂▃▂▂▁▂▃▂▂▂▂▂▂▁▂▂▂▁▁▂</td></tr><tr><td>train_loss_step</td><td>███▆▅▆▆▅▆▄▄▅▄▃▄▅▃▄▄▄▄▂▅▄▂▅▄▂▂▅▄▃▄▃▃▃▁▁▃▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁████████████████</td></tr><tr><td>val_auc</td><td>▁▆▆▇▇▇▇▇▆▇▇▇▇▇▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇▇▇█▇▇</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁████████████████</td></tr><tr><td>val_loss_epoch</td><td>███▇▅▆▆▅▅▄▂▄▄▃▂▁▂▂▂▂▂▂▂▂▃▃▄▂▄▄▄▄▄▄▄▄▃▄▃▄</td></tr><tr><td>val_loss_step</td><td>█▆▇▇▅▅▇▄▅▄▃▄▅▅▁▃▃▁▃▂▃▂▃▃▃▃▄▂▄▄▅▅▅▄▅▄▃▄▃▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.70248</td></tr><tr><td>train_auc</td><td>0.68348</td></tr><tr><td>train_f1</td><td>0.70213</td></tr><tr><td>train_loss_epoch</td><td>0.60818</td></tr><tr><td>train_loss_step</td><td>0.60269</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.72813</td></tr><tr><td>val_auc</td><td>0.76242</td></tr><tr><td>val_f1</td><td>0.75269</td></tr><tr><td>val_loss_epoch</td><td>0.66459</td></tr><tr><td>val_loss_step</td><td>0.6627</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_16_onehot_2</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/n16bw9zu' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/n16bw9zu</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240104_045353-n16bw9zu\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "295167c0530c4b428895b8434551ff28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01691666666883975, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240104_052544-11zkms9z</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/11zkms9z' target=\"_blank\">MLP_4_16_onehot_2</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/11zkms9z' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/11zkms9z</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 1.0 K \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "1.2 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.2 K     Total params\n",
      "0.005     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d916bf913230466ab4311278b86c8b7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▁▂▂▃▄▅▅▆▇▇▇▆▆▇▇▇▇▇▇▇▇██▇█▇▇▇▇█▇██▇▇███</td></tr><tr><td>train_auc</td><td>▂▁▃▂▃▄▅▆▇▇██▇▇▇█▆▇▆▇▇▇▆▆▇▆▆▆▆▅▆▅▆▆▆▇▆▄▆▇</td></tr><tr><td>train_f1</td><td>▃▂▁▁▂▄▄▅▆▇▇▇▇▇▇█▇▇▇▇▇███████▇███████████</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▁▂▂▂▂▂▂▂▁▂▁▁▁▂▂▂▂▁▁▂▂▁▂▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▃▃▄▅▅▆▇▇▇▇▇▇▇▇▇▇██▇█▇▇█████▇██▇█████▇█▇</td></tr><tr><td>val_auc</td><td>▃▁▄▅▆▇▇▇▇▇▇▇█▇▇█████████████▇███████████</td></tr><tr><td>val_f1</td><td>▅▄▁▃▄▁▆▇████████████████████████████████</td></tr><tr><td>val_loss_epoch</td><td>█▆▆▆▆▅▄▄▄▄▃▄▂▂▂▂▂▁▂▂▂▂▂▁▂▁▁▁▃▂▁▂▁▂▁▁▁▂▂▁</td></tr><tr><td>val_loss_step</td><td>█▇▇▇▆▅▅▅▄▅▄▄▃▃▃▂▃▁▂▃▃▃▃▂▃▂▂▂▃▂▂▃▂▂▁▂▁▃▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.74793</td></tr><tr><td>train_auc</td><td>0.67142</td></tr><tr><td>train_f1</td><td>0.7567</td></tr><tr><td>train_loss_epoch</td><td>0.52358</td></tr><tr><td>train_loss_step</td><td>0.56241</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.7896</td></tr><tr><td>val_auc</td><td>0.88101</td></tr><tr><td>val_f1</td><td>0.77468</td></tr><tr><td>val_loss_epoch</td><td>0.46458</td></tr><tr><td>val_loss_step</td><td>0.46239</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_16_onehot_2</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/11zkms9z' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/11zkms9z</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240104_052544-11zkms9z\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7d95fed16df4e3ca87d65f9aa8ab51b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01693333333435779, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240104_055412-141wbj54</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/141wbj54' target=\"_blank\">MLP_4_16_onehot_2</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/141wbj54' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/141wbj54</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 1.0 K \n",
      "2  | head        | Sequential       | 154   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 17    \n",
      "--------------------------------------------------\n",
      "1.2 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.2 K     Total params\n",
      "0.005     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▁▁▂▅▆▇▇▇▇▇▇▇▇███▇█████▇███████████████</td></tr><tr><td>train_auc</td><td>▂▁▁▁▂▆▇▇▇▇▇▇▇▇██████████████████████████</td></tr><tr><td>train_f1</td><td>▁▅▄▅▅▆▇▇▇██▇██▇█████████████████████████</td></tr><tr><td>train_loss_epoch</td><td>████▇▅▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▁▁▁▂▂▁▂▁▂▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█████▆▄▆▃▂▃▃▄▄▅▃▂▄▃▂▃▃▃▃▄▂▃▃▂▁▃▃▃▂▄▂▂▄▄▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▁▁▇▇▄██▇▇▇▇▇▆▇▇▇▆▆▆▇▇▇▇▇▇▇▆▇▇▇▆▇▇▇▇▇▇█▇</td></tr><tr><td>val_auc</td><td>▁▄▅▆▆▆▇█████████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁█▇▅██▇▇█▇▇▇█▇█▆▆▇▇▇▇▇▇▇▇▆▇▇▇▆▇▇▇▇▇▇█▇</td></tr><tr><td>val_loss_epoch</td><td>█████▅▃▂▂▂▁▁▂▃▁▂▁▃▃▂▂▁▂▃▂▂▂▃▂▂▂▃▂▂▂▁▂▂▁▃</td></tr><tr><td>val_loss_step</td><td>█████▅▄▃▂▃▃▃▃▅▄▂▂▃▄▂▂▁▄▂▂▃▄▄▃▄▂▄▃▂▄▂▃▃▃▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.75561</td></tr><tr><td>train_auc</td><td>0.83148</td></tr><tr><td>train_f1</td><td>0.76897</td></tr><tr><td>train_loss_epoch</td><td>0.51039</td></tr><tr><td>train_loss_step</td><td>0.48069</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.72813</td></tr><tr><td>val_auc</td><td>0.85878</td></tr><tr><td>val_f1</td><td>0.61279</td></tr><tr><td>val_loss_epoch</td><td>0.53504</td></tr><tr><td>val_loss_step</td><td>0.58114</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_16_onehot_2</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/141wbj54' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/141wbj54</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240104_055412-141wbj54\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69eeaac2a33e4549859338684750bfe0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240104_062229-110quyb3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/110quyb3' target=\"_blank\">MLP_4_16_onehot_2</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/110quyb3' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/110quyb3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 1.0 K \n",
      "2  | head        | Sequential       | 154   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 281   \n",
      "--------------------------------------------------\n",
      "1.5 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.5 K     Total params\n",
      "0.006     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36c1ea66861540968cee7a23b7ac54a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▁▂▅▇▇▇▇▇▇██▇█████████████████████████▇</td></tr><tr><td>train_auc</td><td>▁▂▂▂▄▇▇▇▇▇▇▇████████████████████████████</td></tr><tr><td>train_f1</td><td>▁▆▄▅▆███████████████████████████████████</td></tr><tr><td>train_loss_epoch</td><td>███▇▆▃▃▃▂▃▂▂▂▂▂▂▂▂▂▂▁▂▂▁▂▂▁▂▂▁▂▂▂▁▁▁▂▁▁▂</td></tr><tr><td>train_loss_step</td><td>███▇▆▄▄▃▃▃▃▄▃▃▃▃▄▄▃▄▃▂▃▄▂▂▃▃▄▂▃▄▁▂▃▃▃▂▄▄</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▃▁▃▄▇▇▇▇██▇█████████████████████████████</td></tr><tr><td>val_auc</td><td>▁▄▅▆▇▇▇▇▇▇▇▇▇███████████████████████████</td></tr><tr><td>val_f1</td><td>▁▇▁▂█▇▇▇██▇█████████████████████████▇███</td></tr><tr><td>val_loss_epoch</td><td>███▇▅▂▂▂▂▂▂▁▂▂▁▁▁▁▂▁▁▂▂▁▁▁▁▁▂▁▁▁▁▂▁▁▂▂▂▂</td></tr><tr><td>val_loss_step</td><td>███▇▅▁▃▄▃▂▂▂▃▂▂▂▂▁▃▁▂▃▃▁▁▁▂▂▄▁▂▂▂▄▁▂▃▃▂▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.74498</td></tr><tr><td>train_auc</td><td>0.82145</td></tr><tr><td>train_f1</td><td>0.74255</td></tr><tr><td>train_loss_epoch</td><td>0.52461</td></tr><tr><td>train_loss_step</td><td>0.57376</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.76123</td></tr><tr><td>val_auc</td><td>0.86179</td></tr><tr><td>val_f1</td><td>0.76457</td></tr><tr><td>val_loss_epoch</td><td>0.48328</td></tr><tr><td>val_loss_step</td><td>0.49887</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_16_onehot_2</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/110quyb3' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/110quyb3</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240104_062229-110quyb3\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "540ff79c3a5c46eeb5a92037ce18ec50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01693333333435779, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240104_065036-kcw8422s</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/kcw8422s' target=\"_blank\">MLP_4_32_onehot_2</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/kcw8422s' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/kcw8422s</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 3.6 K \n",
      "2 | head        | Sequential       | 562   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "4.2 K     Trainable params\n",
      "0         Non-trainable params\n",
      "4.2 K     Total params\n",
      "0.017     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c03d70a8ffe64d6ba6be782614a6bbc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▂▄▇▇████▇▇███▇▇█████████████▇█████████</td></tr><tr><td>train_auc</td><td>▁▂▂▅▇▇▇███▇▇████████████████████████████</td></tr><tr><td>train_f1</td><td>▁▃▂▅▆▇▇▇▇█▇▇█▇▇▇█▇▇█▇██▇██▇███▇█████▇██▇</td></tr><tr><td>train_loss_epoch</td><td>███▆▄▂▂▂▂▂▂▂▂▂▂▁▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▂▂▂▁▂▁▁▁▂</td></tr><tr><td>train_loss_step</td><td>███▅▄▂▄▃▂▃▃▄▃▄▃▃▃▂▂▃▄▅▁▃▂▁▅▃▃▂▄▃▄▃▅▄▃▄▄▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▃▁▁▆▇█▇██▇▇█▇███████████████████████████</td></tr><tr><td>val_auc</td><td>▁▄▅▅▆▇▇▇▇▇▇▇▇▇▇▇████████████████████████</td></tr><tr><td>val_f1</td><td>▁▇▇▆██████▇█▇██████▇████████████████████</td></tr><tr><td>val_loss_epoch</td><td>███▅▃▂▂▂▁▁▂▂▂▁▂▃▁▂▂▁▂▁▂▁▁▁▂▂▁▁▁▁▁▁▁▁▁▁▂▂</td></tr><tr><td>val_loss_step</td><td>███▅▄▃▂▄▁▂▃▂▂▂▂▄▁▂▃▂▂▂▃▃▁▃▃▃▃▁▃▂▁▂▂▃▁▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.75443</td></tr><tr><td>train_auc</td><td>0.82693</td></tr><tr><td>train_f1</td><td>0.75179</td></tr><tr><td>train_loss_epoch</td><td>0.51419</td></tr><tr><td>train_loss_step</td><td>0.52376</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.76832</td></tr><tr><td>val_auc</td><td>0.86433</td></tr><tr><td>val_f1</td><td>0.76995</td></tr><tr><td>val_loss_epoch</td><td>0.47459</td></tr><tr><td>val_loss_step</td><td>0.45127</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_32_onehot_2</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/kcw8422s' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/kcw8422s</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240104_065036-kcw8422s\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b0eee5d048e49788d4557c10251ff96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240104_071837-rr84qisl</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/rr84qisl' target=\"_blank\">MLP_4_32_onehot_2</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/rr84qisl' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/rr84qisl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 3.6 K \n",
      "2 | head        | Sequential       | 562   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "4.2 K     Trainable params\n",
      "0         Non-trainable params\n",
      "4.2 K     Total params\n",
      "0.017     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bcff2ff2b7a4a4387653c4796accc85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▂▃▆▆▇▆▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇██▇█▇▇▇▇█▇</td></tr><tr><td>train_auc</td><td>▁▂▂▄▆▆▇▇▇▇▇▇▇▇▇▇█▇▇▇▇▇▇▇▇▇█▇███████▇▇███</td></tr><tr><td>train_f1</td><td>▁▄▄▆▅▅▆▆▆▆▆▇▇▇▇▆▆▆▆▇▇▇▇▇▇▇▇▆▇▇██▇█▇▇▇▇█▇</td></tr><tr><td>train_loss_epoch</td><td>█▇▇▇▆▅▄▄▄▄▃▃▃▄▃▃▂▂▃▂▂▂▂▂▂▂▁▂▂▁▂▁▂▂▂▂▂▂▁▂</td></tr><tr><td>train_loss_step</td><td>███▇▅▄▄█▅▄▅▄▄▆▃▄▂▂▃▅▂▅▂▄▂▂▄▃▃▁▂▂▃▁▄▃▃▃▃▅</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▁▁▁████▅▅██▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁████</td></tr><tr><td>val_auc</td><td>▁██▆▅▆▇▅▅▆▆▆▆▆▆▆▇▆▇▇▇▇▇█▇▇▆▇▇█▆▆▇▆▆▆▅▆▆▆</td></tr><tr><td>val_f1</td><td>▁▁▁▁████▄▄██▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▁▁▁████</td></tr><tr><td>val_loss_epoch</td><td>▄▃▃▃▁▁▁▂▁▂▁▁▂▂▂▄▄▅▄▅▆▇▇█▆▄▅▆▆▆▅▅▃▅▆▄▄▄▃▅</td></tr><tr><td>val_loss_step</td><td>▄▃▃▃▂▂▁▂▁▁▁▁▂▁▁▄▅▄▄▄▆▅██▅▄▅▅▆▇▅▄▁▅▄▂▃▃▁▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.71488</td></tr><tr><td>train_auc</td><td>0.77074</td></tr><tr><td>train_f1</td><td>0.71771</td></tr><tr><td>train_loss_epoch</td><td>0.57576</td></tr><tr><td>train_loss_step</td><td>0.61355</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.7305</td></tr><tr><td>val_auc</td><td>0.80388</td></tr><tr><td>val_f1</td><td>0.75536</td></tr><tr><td>val_loss_epoch</td><td>0.72585</td></tr><tr><td>val_loss_step</td><td>0.71833</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_32_onehot_2</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/rr84qisl' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/rr84qisl</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240104_071837-rr84qisl\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22c820e0b189402e98a535d7e1b5d67b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240104_074821-zd37w7iq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/zd37w7iq' target=\"_blank\">MLP_4_32_onehot_2</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/zd37w7iq' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/zd37w7iq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 3.6 K \n",
      "2 | head        | Sequential       | 562   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "4.2 K     Trainable params\n",
      "0         Non-trainable params\n",
      "4.2 K     Total params\n",
      "0.017     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "317d03bf91294cd0a2614dfacf3045db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▂▂▂▄▅▆▆▆▆▆▇▇▇█▇▇██▇█▇██▇██████▇█▇█████</td></tr><tr><td>train_auc</td><td>▁▁▂▂▂▂▄▅▅▅▅▅▆▇▅▆▆▅▆▆▆▆▆▆▆▅▇▆▇▇▇█▇█▇█████</td></tr><tr><td>train_f1</td><td>▁▁▃▅▅▅▅▆▆▆▆▆▇▇▇█▇▇██▇█▇██▇██████████████</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▄▃▃▃▂▃▃▂▂▃▂▂▂▂▂▂▁▂▂▂▂▁▂▂▁▂▂▂▁▂▁▂▂▂▁▁▂▂▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▃▄▄▆▅▅▅▆▆▇▅▆▇█▇▆█▆█▇▇██▆███▇█▇███▇█████</td></tr><tr><td>val_auc</td><td>▃▃▁▁▃▆▆▆▇▇▇▇████████████████████████████</td></tr><tr><td>val_f1</td><td>▆▁▃▄▇▆▅▄▆▇▇▅▆▇█▇▆█▆█▇▇██▆███▇█▇█▇█▇█▇███</td></tr><tr><td>val_loss_epoch</td><td>█▇▇▇▇▆▅▅▄▃▄▄▃▂▂▂▂▂▃▂▂▂▂▁▃▂▁▂▂▁▂▁▂▁▂▁▂▁▂▁</td></tr><tr><td>val_loss_step</td><td>██▇▇▇▆▆▆▅▄▄▅▄▃▃▂▂▂▄▂▃▃▂▂▃▄▂▃▃▁▃▂▂▂▃▂▂▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.74852</td></tr><tr><td>train_auc</td><td>0.70196</td></tr><tr><td>train_f1</td><td>0.75768</td></tr><tr><td>train_loss_epoch</td><td>0.51784</td></tr><tr><td>train_loss_step</td><td>0.51503</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.81324</td></tr><tr><td>val_auc</td><td>0.88115</td></tr><tr><td>val_f1</td><td>0.78237</td></tr><tr><td>val_loss_epoch</td><td>0.43811</td></tr><tr><td>val_loss_step</td><td>0.40159</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_32_onehot_2</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/zd37w7iq' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/zd37w7iq</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240104_074821-zd37w7iq\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d1a67693db346a1a210c26bcc542199",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01693333333435779, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240104_081630-aczat7l0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/aczat7l0' target=\"_blank\">MLP_4_32_onehot_2</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/aczat7l0' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/aczat7l0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 3.6 K \n",
      "2  | head        | Sequential       | 562   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 33    \n",
      "--------------------------------------------------\n",
      "4.2 K     Trainable params\n",
      "0         Non-trainable params\n",
      "4.2 K     Total params\n",
      "0.017     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae2f50f78b724a4599ff47b01f497ab6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▃▅▆▇▇▇▇▇█▇█▇▇█▇█▇▇▇█▇▇██████▇███▇▇▇███</td></tr><tr><td>train_auc</td><td>▁▂▃▅▇▇▇▇▇██▇████▇█████▇█████████████████</td></tr><tr><td>train_f1</td><td>▁▄▄▃▅▆▇▇▆▇█▇█▇▇█▇█▇▇▇█▇▇█▇▇▇██▇██▇▇▇▇███</td></tr><tr><td>train_loss_epoch</td><td>███▆▄▂▂▂▃▂▂▃▂▂▂▂▂▂▁▂▂▁▃▂▁▂▁▂▂▁▂▁▁▂▁▂▂▁▂▁</td></tr><tr><td>train_loss_step</td><td>███▆▅▅▃▅▄▃▅▅▁▅▂▄▃▄▃▂▄▃▃▄▂▂▃▂▂▂▄▃▁▄▃▄▄▄▃▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▃▁▄▆▇███████▇███████████████████████████</td></tr><tr><td>val_auc</td><td>▁▃▃▃▅▆▇▇▇▇▇▇████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▇▇▆████████▇███████████▇███████▇█▇█████</td></tr><tr><td>val_loss_epoch</td><td>██▇▆▄▂▂▂▂▂▂▂▂▁▁▁▃▂▂▂▁▁▁▂▂▁▂▁▁▂▂▁▂▁▂▂▁▂▁▂</td></tr><tr><td>val_loss_step</td><td>███▆▅▄▃▄▃▃▃▃▃▂▂▂▅▃▄▃▃▁▂▄▃▃▃▃▃▃▃▂▃▂▂▃▂▄▂▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.77332</td></tr><tr><td>train_auc</td><td>0.84155</td></tr><tr><td>train_f1</td><td>0.78132</td></tr><tr><td>train_loss_epoch</td><td>0.49506</td></tr><tr><td>train_loss_step</td><td>0.51451</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.77541</td></tr><tr><td>val_auc</td><td>0.86292</td></tr><tr><td>val_f1</td><td>0.76071</td></tr><tr><td>val_loss_epoch</td><td>0.47418</td></tr><tr><td>val_loss_step</td><td>0.54466</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_32_onehot_2</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/aczat7l0' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/aczat7l0</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240104_081630-aczat7l0\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d09be4ebd7a41838c588d6b10862c5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.017183333332650364, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240104_084457-qgndeprt</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/qgndeprt' target=\"_blank\">MLP_4_32_onehot_2</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/qgndeprt' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/qgndeprt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 3.6 K \n",
      "2  | head        | Sequential       | 562   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 1.1 K \n",
      "--------------------------------------------------\n",
      "5.3 K     Trainable params\n",
      "0         Non-trainable params\n",
      "5.3 K     Total params\n",
      "0.021     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7a98291a9f64ea491ba777b30f018d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▃▆▇▇▇▇▇█▇▇▇██████████████████▇███▇███▇</td></tr><tr><td>train_auc</td><td>▁▁▄▆▇▇█▇████████████████████████████████</td></tr><tr><td>train_f1</td><td>▅▁▆▆▇▇█▇▇███▇██████████████████████▇████</td></tr><tr><td>train_loss_epoch</td><td>██▇▅▄▃▂▃▂▁▂▂▂▂▂▂▂▂▁▁▂▁▁▁▁▁▂▁▂▂▁▂▁▁▁▁▁▁▁▂</td></tr><tr><td>train_loss_step</td><td>███▅▅▃▂▃▃▄▁▃▃▃▃▃▃▂▂▂▂▂▄▂▃▃▁▂▃▂▃▁▁▃▂▂▃▃▃▄</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▇▇▆▆▇█▇▇▇███▇████▇████████████▇████████</td></tr><tr><td>val_auc</td><td>▁▂▃▂▅▆▇▇▇▇██████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▇▇▇▇▇█████████████████████████▇████████</td></tr><tr><td>val_loss_epoch</td><td>██▇▄▃▂▂▂▂▂▁▁▁▂▁▂▁▁▁▁▂▁▂▁▁▁▁▂▁▁▁▂▁▁▁▂▁▁▁▂</td></tr><tr><td>val_loss_step</td><td>██▇▄▃▃▃▄▃▃▂▂▂▂▂▂▁▃▁▂▃▂▄▃▁▃▂▃▂▃▁▂▁▃▄▃▂▂▁▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.75443</td></tr><tr><td>train_auc</td><td>0.83217</td></tr><tr><td>train_f1</td><td>0.75786</td></tr><tr><td>train_loss_epoch</td><td>0.51018</td></tr><tr><td>train_loss_step</td><td>0.55598</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.76832</td></tr><tr><td>val_auc</td><td>0.86424</td></tr><tr><td>val_f1</td><td>0.76555</td></tr><tr><td>val_loss_epoch</td><td>0.47932</td></tr><tr><td>val_loss_step</td><td>0.51469</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_32_onehot_2</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/qgndeprt' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/qgndeprt</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240104_084457-qgndeprt\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8890e34a030d46e191e0e20a9fc0593f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016933333330477276, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240104_091404-15vqsti8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/15vqsti8' target=\"_blank\">MLP_4_64_onehot_2</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/15vqsti8' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/15vqsti8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 13.4 K\n",
      "2 | head        | Sequential       | 2.1 K \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "15.5 K    Trainable params\n",
      "0         Non-trainable params\n",
      "15.5 K    Total params\n",
      "0.062     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▇▇▇▇▇█▇█▇▇██▇█▇████▇███▇███▇█▇████▇███</td></tr><tr><td>train_auc</td><td>▁▂▆▇████████████████████████████████████</td></tr><tr><td>train_f1</td><td>▁▄▇▇▇▇▇█▇█▇▇██▇█▇█▇██▇██▇▇███▇█▇██▇█▇███</td></tr><tr><td>train_loss_epoch</td><td>██▄▂▁▂▂▂▂▁▁▂▁▂▁▂▁▁▁▁▂▂▂▁▂▂▁▁▁▂▁▂▁▁▁▂▂▁▁▁</td></tr><tr><td>train_loss_step</td><td>██▂▃▃▂▄▄▁▂▃▂▄▃▃▂▂▂▄▁▂▃▂▂▁▃▄▃▃▂▂▃▂▂▃▁▃▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▇▇█████████████████████████████████████</td></tr><tr><td>val_auc</td><td>▁▃▃▄▅▆▆▇▇▇▇▇▇▇▇▇████████▇▇▇█████████████</td></tr><tr><td>val_f1</td><td>▁▄▂▇█▆▇▇▇▇▇▇▆▆██▇██▇▇▇▇▆██▇▆▇█▇███▇▇██▇▇</td></tr><tr><td>val_loss_epoch</td><td>█▇▂▂▂▂▂▁▁▁▁▁▁▁▂▁▂▂▁▁▁▁▁▁▂▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_step</td><td>█▇▃▃▄▃▂▂▂▂▁▂▂▁▃▂▃▂▃▂▂▃▁▁▃▂▂▁▁▃▁▁▂▂▂▃▂▃▃▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.77096</td></tr><tr><td>train_auc</td><td>0.84125</td></tr><tr><td>train_f1</td><td>0.77727</td></tr><tr><td>train_loss_epoch</td><td>0.4898</td></tr><tr><td>train_loss_step</td><td>0.45395</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.78251</td></tr><tr><td>val_auc</td><td>0.86752</td></tr><tr><td>val_f1</td><td>0.75661</td></tr><tr><td>val_loss_epoch</td><td>0.45144</td></tr><tr><td>val_loss_step</td><td>0.42508</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_64_onehot_2</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/15vqsti8' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/15vqsti8</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240104_091404-15vqsti8\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "638593e174c24955ba98b18cd561491f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01693333333435779, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240104_094259-2pke0kab</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/2pke0kab' target=\"_blank\">MLP_4_64_onehot_2</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/2pke0kab' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/2pke0kab</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 13.4 K\n",
      "2 | head        | Sequential       | 2.1 K \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "15.5 K    Trainable params\n",
      "0         Non-trainable params\n",
      "15.5 K    Total params\n",
      "0.062     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▂▄▇▇▇▇▇▇█▇██▇▇█▇████▇▇█████▇█████▇███▇</td></tr><tr><td>train_auc</td><td>▂▁▂▄▇▇▇▇▇▇▇▇█▇▇▇▇██████▇█████▇██████████</td></tr><tr><td>train_f1</td><td>▁▃▁▃▆▆▇▆▇▇█▇▇▇▆▇█▇████▇▇▇███▇▇███▇█▇██▇▇</td></tr><tr><td>train_loss_epoch</td><td>███▇▄▄▃▃▃▃▂▃▁▂▃▃▂▁▂▁▂▂▂▂▂▂▂▂▁▂▁▂▁▁▁▂▁▂▁▂</td></tr><tr><td>train_loss_step</td><td>▇█▇▆▅▅▄▄▃▂▅▄▃▄▃▃▃▂▃▁▃▃▂▂▂▄▂▃▁▄▁▄▃▄▂▂▂▃▂▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>▁▄▇▆▇▇▆▇▇█████▇██▇█▇▇█▇▇▇▇▇▇▆▆▆▆▆▆▆▆▆▇▆▆</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_epoch</td><td>▁▁▁▁▁▁▁▂▃▄▅▄▅▅▄▅▅▇▅▇▆▆▇▅▆▅▆▆▆▇█▇█▇▆▇█▅▆█</td></tr><tr><td>val_loss_step</td><td>▁▂▁▁▁▂▁▃▃▅▅▄▅▅▄▅▅▆▅█▆▅▇▄▆▃▅▄▅▆█▇▆▇▅▇█▅▆▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.72668</td></tr><tr><td>train_auc</td><td>0.79697</td></tr><tr><td>train_f1</td><td>0.72971</td></tr><tr><td>train_loss_epoch</td><td>0.53181</td></tr><tr><td>train_loss_step</td><td>0.50232</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.44444</td></tr><tr><td>val_auc</td><td>0.78233</td></tr><tr><td>val_f1</td><td>0.61538</td></tr><tr><td>val_loss_epoch</td><td>0.98653</td></tr><tr><td>val_loss_step</td><td>0.99387</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_64_onehot_2</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/2pke0kab' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/2pke0kab</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240104_094259-2pke0kab\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f97063b61ed4833ae3ede4a619b9f49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240104_101128-qq3mkse3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/qq3mkse3' target=\"_blank\">MLP_4_64_onehot_2</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/qq3mkse3' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/qq3mkse3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 13.4 K\n",
      "2 | head        | Sequential       | 2.1 K \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "15.5 K    Trainable params\n",
      "0         Non-trainable params\n",
      "15.5 K    Total params\n",
      "0.062     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afce46a650ff4e39920c40ee8b7d73b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▂▁▂▁▂▃▆▆▇▇▇▇██▇███▇█▇██▇▇█▇▇█▇██████████</td></tr><tr><td>train_auc</td><td>▁▁▁▁▂▃▆▇▇▇████▇██▇█▇█▇▇███▇██▇█▇▇█▇█▇▇▇▇</td></tr><tr><td>train_f1</td><td>▂▁▂▃▃▃▅▆▇▇▇▇██▇▇█▇▇█▇▇▇▇▇▇▇▇█▇█▇█▇██████</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▅▄▄▄▃▃▂▂▂▂▂▂▂▂▂▁▂▂▁▂▂▂▂▁▂▂▂▁▁▂▂▁▂▂▁▂▁▁▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▁▃▄▅▇▇▇▇▇▇▇███▇█▇▇██▇▇██▇▇█▇▇▇▇▇▇▇▇████</td></tr><tr><td>val_auc</td><td>▂▁▁▇▆▇▇██▇██████████▇▇██████████████████</td></tr><tr><td>val_f1</td><td>▃▃▁▂▄▇▅▇▆▇▇█▇███████████████▇███████████</td></tr><tr><td>val_loss_epoch</td><td>█▇▆▆▆▅▃▂▂▂▂▂▁▁▂▃▁▂▃▁▂▂▂▁▂▃▂▁▁▂▂▁▂▁▂▂▁▁▁▁</td></tr><tr><td>val_loss_step</td><td>█▇▆▆▆▅▄▃▃▃▂▃▂▂▃▃▃▂▄▂▃▃▂▁▂▃▂▂▁▃▃▁▂▂▃▃▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.78099</td></tr><tr><td>train_auc</td><td>0.74831</td></tr><tr><td>train_f1</td><td>0.78715</td></tr><tr><td>train_loss_epoch</td><td>0.49796</td></tr><tr><td>train_loss_step</td><td>0.50107</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.80378</td></tr><tr><td>val_auc</td><td>0.88121</td></tr><tr><td>val_f1</td><td>0.7888</td></tr><tr><td>val_loss_epoch</td><td>0.43997</td></tr><tr><td>val_loss_step</td><td>0.42063</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_64_onehot_2</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/qq3mkse3' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/qq3mkse3</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240104_101128-qq3mkse3\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ea9fb9f16b64c86ab0c2692cf6b4a34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01693333333435779, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240104_104040-i6x3jnhg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/i6x3jnhg' target=\"_blank\">MLP_4_64_onehot_2</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/i6x3jnhg' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/i6x3jnhg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 13.4 K\n",
      "2  | head        | Sequential       | 2.1 K \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 65    \n",
      "--------------------------------------------------\n",
      "15.6 K    Trainable params\n",
      "0         Non-trainable params\n",
      "15.6 K    Total params\n",
      "0.062     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8f6542aedc640cdbe1f4337536b27f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▄▆▇▇▇▇▇▇██▇██▇██████▇███████▇▇█▇███████</td></tr><tr><td>train_auc</td><td>▁▄▆▇█▇▇▇████████████████████████████████</td></tr><tr><td>train_f1</td><td>▁▅▆▇▇▇▇▇▇▇█▇██▇███████▇█████▇▇▇█▇███████</td></tr><tr><td>train_loss_epoch</td><td>█▇▄▃▂▃▂▃▂▂▂▂▂▁▂▂▁▁▁▂▂▂▂▂▁▁▁▁▂▁▂▁▁▁▂▁▂▂▁▁</td></tr><tr><td>train_loss_step</td><td>█▇▅▄▄▅▃▄▄▂▂▄▃▃▂▃▄▂▅▂▄▄▃▅▃▁▃▃▄▃▃▄▂▁▃▂▂▃▃▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▆▇███▇█▇███████▇████████▇██████████████</td></tr><tr><td>val_auc</td><td>▁▂▃▃▅▆▆▆▇▇▇▇▇██████████████████▇████████</td></tr><tr><td>val_f1</td><td>▃▆▃▆▇█▁█▄▇█▇▆▆▇▇▅▇▇▇█▆▇▆▇▅▇▇██▆█▇▆▇██▆▅▆</td></tr><tr><td>val_loss_epoch</td><td>█▇▃▂▂▂▃▂▂▂▂▂▁▂▁▂▂▁▁▁▁▂▁▂▁▂▂▁▂▂▂▂▁▂▂▂▁▂▂▂</td></tr><tr><td>val_loss_step</td><td>█▇▄▃▃▃▃▂▂▃▃▃▂▄▃▃▃▃▁▂▁▂▁▄▂▄▃▂▄▃▃▄▂▃▃▃▂▂▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.7686</td></tr><tr><td>train_auc</td><td>0.83789</td></tr><tr><td>train_f1</td><td>0.78076</td></tr><tr><td>train_loss_epoch</td><td>0.49637</td></tr><tr><td>train_loss_step</td><td>0.47785</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.77305</td></tr><tr><td>val_auc</td><td>0.86311</td></tr><tr><td>val_f1</td><td>0.73034</td></tr><tr><td>val_loss_epoch</td><td>0.47182</td></tr><tr><td>val_loss_step</td><td>0.50059</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_64_onehot_2</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/i6x3jnhg' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/i6x3jnhg</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240104_104040-i6x3jnhg\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5b1d575e1b24765947d9a7383e20bfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01693333333435779, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240104_111723-xcolgahz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/xcolgahz' target=\"_blank\">MLP_4_64_onehot_2</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/xcolgahz' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/xcolgahz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 13.4 K\n",
      "2  | head        | Sequential       | 2.1 K \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 4.2 K \n",
      "--------------------------------------------------\n",
      "19.7 K    Trainable params\n",
      "0         Non-trainable params\n",
      "19.7 K    Total params\n",
      "0.079     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▃▆▇▇█▇▇▇▇████▇███████▇███████████▇███▇█</td></tr><tr><td>train_auc</td><td>▁▃▆▇██▇▇████████████████████████████████</td></tr><tr><td>train_f1</td><td>▁▁▆▇▇█▇▇█▇████████████████████████████▇█</td></tr><tr><td>train_loss_epoch</td><td>█▇▄▃▂▂▂▂▂▂▂▂▂▁▂▁▁▂▁▁▂▁▂▂▂▁▁▂▂▁▁▁▂▂▂▁▁▂▂▂</td></tr><tr><td>train_loss_step</td><td>█▇▆▄▃▃▄▅▄▂▃▃▂▁▄▃▄▃▂▂▃▂▁▂▃▂▃▂▂▂▃▁▄▃▃▂▃▃▄▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▂█▇▇█▇█████████████████████████████████</td></tr><tr><td>val_auc</td><td>▁▁▂▃▄▆▆▆▆▇▇▇▇█▇▇██▇████████████████████▇</td></tr><tr><td>val_f1</td><td>▁▂█▃▅▆▃▇▇▇▆▆▇▇▇▇▇▇▇▇▇▇▆▇▆▅▆█▇▇▇▇▇▇▇▇▆▆▆█</td></tr><tr><td>val_loss_epoch</td><td>█▇▄▃▂▂▂▂▂▂▁▁▁▁▂▂▂▂▁▂▁▁▂▂▁▂▁▂▁▁▁▂▂▁▁▁▂▂▂▂</td></tr><tr><td>val_loss_step</td><td>█▇▅▃▃▃▃▃▃▄▃▃▂▁▄▃▄▄▂▃▃▃▄▃▂▂▃▃▂▃▃▃▄▂▃▂▃▃▃▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.76564</td></tr><tr><td>train_auc</td><td>0.83555</td></tr><tr><td>train_f1</td><td>0.76824</td></tr><tr><td>train_loss_epoch</td><td>0.50002</td></tr><tr><td>train_loss_step</td><td>0.51682</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.77069</td></tr><tr><td>val_auc</td><td>0.86374</td></tr><tr><td>val_f1</td><td>0.77283</td></tr><tr><td>val_loss_epoch</td><td>0.47172</td></tr><tr><td>val_loss_step</td><td>0.4346</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_64_onehot_2</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/xcolgahz' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/xcolgahz</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240104_111723-xcolgahz\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e0f0625bffb46c4acb41a3a7c43c065",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240104_114813-zykoay7a</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/zykoay7a' target=\"_blank\">MLP_2_16_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/zykoay7a' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/zykoay7a</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 496   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "650       Trainable params\n",
      "0         Non-trainable params\n",
      "650       Total params\n",
      "0.003     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▂▅▇▇████▇█▇█████▇███████▇█████████████</td></tr><tr><td>train_auc</td><td>▁▁▂▅▇▇██████████████████████████████████</td></tr><tr><td>train_f1</td><td>▁▄▂▆▇█████████████████████▇█████████████</td></tr><tr><td>train_loss_epoch</td><td>███▆▃▃▂▂▁▂▂▁▂▁▂▁▂▁▁▁▁▁▁▁▂▁▁▂▂▁▂▁▂▁▁▁▂▁▁▁</td></tr><tr><td>train_loss_step</td><td>██▇▆▄▅▁▃▃▃▂▃▄▄▂▅▃▂▁▃▁▂▂▂▄▁▂▂▁▃▁▁▄▂▂▁▃▂▃▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▂▄▇▇▇▇█▇██████████████████▇██▇▇██▇▇████</td></tr><tr><td>val_auc</td><td>▁▅▆▇▇███████████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▂▅███▇█████████████████████████████████</td></tr><tr><td>val_loss_epoch</td><td>███▆▃▂▂▂▂▂▁▁▂▂▂▂▂▁▂▁▁▂▁▁▁▂▁▂▁▁▂▂▁▂▁▁▁▂▁▁</td></tr><tr><td>val_loss_step</td><td>███▆▄▃▄▃▃▃▃▃▃▃▃▃▃▂▄▂▂▄▃▂▃▄▃▄▃▃▄▃▃▄▁▃▁▃▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.76919</td></tr><tr><td>train_auc</td><td>0.84039</td></tr><tr><td>train_f1</td><td>0.76056</td></tr><tr><td>train_loss_epoch</td><td>0.49702</td></tr><tr><td>train_loss_step</td><td>0.51235</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.76123</td></tr><tr><td>val_auc</td><td>0.83336</td></tr><tr><td>val_f1</td><td>0.76235</td></tr><tr><td>val_loss_epoch</td><td>0.50346</td></tr><tr><td>val_loss_step</td><td>0.51205</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_16_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/zykoay7a' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/zykoay7a</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240104_114813-zykoay7a\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f84881799694ae09705e06325c708fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240104_121959-xmmig78w</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/xmmig78w' target=\"_blank\">MLP_2_16_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/xmmig78w' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/xmmig78w</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 496   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "650       Trainable params\n",
      "0         Non-trainable params\n",
      "650       Total params\n",
      "0.003     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▅▆▆▆▆▇▇▇▇▇▇▇▇██▇▇▇█▇▆█▇▇▇█▇▇█▇▆▇▇▇▇▇▇█</td></tr><tr><td>train_auc</td><td>▁▂▂▅▆▆▆▇▇▇█▇█▇▇██▇▇███▇███▇▇█▇██▇▇██████</td></tr><tr><td>train_f1</td><td>▁▃▅▅▆▆▅▆▇▇▇▇▇▇▆██▆▇▇▇▇▆█▇▇▇█▇▇█▇▆▇▇▇▇▇▇█</td></tr><tr><td>train_loss_epoch</td><td>█▇▆▅▄▄▄▃▂▂▂▂▂▂▂▂▂▃▃▂▁▂▂▁▁▁▂▂▂▂▂▁▂▃▂▂▂▁▂▁</td></tr><tr><td>train_loss_step</td><td>█▇▆▅▇▅▆▄▄▂▃▆▃▅▅▃▄▃▂▂▁▄▄▂▄▁▁▄▃▅▃▄▄▃▄▁▄▁▃▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▂▂▂▂▂▂▂▂▂▂▄█████████████████████▅██▅▁▁▁</td></tr><tr><td>val_auc</td><td>▁▇▇██▇███████████████▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>val_f1</td><td>▇▁▁▁▁▁▁▁▁▁▁▃█████████████████████████▇▇▇</td></tr><tr><td>val_loss_epoch</td><td>█▇▆▄▄▄▄▃▂▂▂▁▁▂▂▁▁▂▂▂▂▂▃▂▂▃▂▄▃▃▄▃▄▅▃▄▄▅▅▆</td></tr><tr><td>val_loss_step</td><td>█▇▆▅▅▃▅▄▂▁▂▂▂▃▃▂▃▃▂▃▂▃▃▂▁▅▂▄▅▂▅▃▅▅▂▆▃▅▅▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.7137</td></tr><tr><td>train_auc</td><td>0.74536</td></tr><tr><td>train_f1</td><td>0.69969</td></tr><tr><td>train_loss_epoch</td><td>0.60373</td></tr><tr><td>train_loss_step</td><td>0.56647</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.487</td></tr><tr><td>val_auc</td><td>0.71072</td></tr><tr><td>val_f1</td><td>0.65501</td></tr><tr><td>val_loss_epoch</td><td>0.682</td></tr><tr><td>val_loss_step</td><td>0.68953</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_16_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/xmmig78w' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/xmmig78w</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240104_121959-xmmig78w\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1de78c848ec0401588096c02ad4fd910",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01691666666883975, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240104_125159-dg01et6g</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/dg01et6g' target=\"_blank\">MLP_2_16_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/dg01et6g' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/dg01et6g</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 496   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "650       Trainable params\n",
      "0         Non-trainable params\n",
      "650       Total params\n",
      "0.003     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1e938a23322482097b8f9874005c11a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▃▄▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇██▇▇█▇█▇█▇███████</td></tr><tr><td>train_auc</td><td>▁▂▃▄▅▆▆▇▇▇▇▇▇▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇█▇▇█▇███▇███</td></tr><tr><td>train_f1</td><td>▁▂▃▃▄▅▅▅▆▆▇▇▆▇▇▇▇▇▇▇▇▇▇██▇▇█▇█▇█▇███▇███</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▃▂▂▂▂▂▂▂▂▁▂▂▂▁▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▅▄▆▆▆▆▇▇██▇████████████████████████████</td></tr><tr><td>val_auc</td><td>▁▄▅▆▆▇▇▇█▇██████████████████████████████</td></tr><tr><td>val_f1</td><td>▅▃▁▅▅▄▅▇▇█▇▆▇█▇█▇▇██▇███▇███▇▇▇▇█▇▇▇████</td></tr><tr><td>val_loss_epoch</td><td>█▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_step</td><td>█▃▃▃▃▃▃▂▂▃▂▂▂▂▂▂▂▁▂▂▁▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.76387</td></tr><tr><td>train_auc</td><td>0.75695</td></tr><tr><td>train_f1</td><td>0.76798</td></tr><tr><td>train_loss_epoch</td><td>0.50616</td></tr><tr><td>train_loss_step</td><td>0.50748</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.78251</td></tr><tr><td>val_auc</td><td>0.84743</td></tr><tr><td>val_f1</td><td>0.79091</td></tr><tr><td>val_loss_epoch</td><td>0.49908</td></tr><tr><td>val_loss_step</td><td>0.50004</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_16_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/dg01et6g' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/dg01et6g</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240104_125159-dg01et6g\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7230cec179e448f694be0efce3b62677",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240104_132347-139hacxz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/139hacxz' target=\"_blank\">MLP_2_16_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/139hacxz' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/139hacxz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 496   \n",
      "2  | head        | Sequential       | 154   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 17    \n",
      "--------------------------------------------------\n",
      "667       Trainable params\n",
      "0         Non-trainable params\n",
      "667       Total params\n",
      "0.003     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1262c2fedebb47038b7dc256c23fe2e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▃▅▆▇▇▇▇▇█▇▇▇███▇██▇█▇▇███▇▇███████████</td></tr><tr><td>train_auc</td><td>▁▁▂▆▆▇▇▇▇███▇███████████████████████████</td></tr><tr><td>train_f1</td><td>▁▅▅▆▇▇▇▇▇███████████████████▇███████████</td></tr><tr><td>train_loss_epoch</td><td>███▆▄▄▃▂▂▂▂▂▂▂▂▂▁▁▁▁▂▂▂▂▂▂▁▂▂▁▂▂▁▁▁▂▂▁▁▁</td></tr><tr><td>train_loss_step</td><td>███▅▄▄▄▄▁▃▃▃▄▄▃▂▃▁▂▁▃▁▂▃▄▃▂▃▄▂▁▄▂▂▃▁▃▃▃▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▄▆▄▅▆▇▇▇▇▇█▇▇▇█▇█▇██▇▇█▇▇▇████▇▇▇▇▇▇▇▇▇</td></tr><tr><td>val_auc</td><td>▁▅▆▆▆▇▇█████████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▅▇▆▆▇█▇█████▇███████▇▇███▇████████▇████</td></tr><tr><td>val_loss_epoch</td><td>██▇▆▅▄▃▃▂▂▂▂▂▂▂▂▁▂▁▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▂▁▂▂▂</td></tr><tr><td>val_loss_step</td><td>███▇▅▄▄▅▅▄▄▃▅▄▄▃▂▄▃▃▃▃▅▄▄▃▃▃▃▃▄▃▂▁▃▃▂▄▄▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.77863</td></tr><tr><td>train_auc</td><td>0.84705</td></tr><tr><td>train_f1</td><td>0.7798</td></tr><tr><td>train_loss_epoch</td><td>0.49451</td></tr><tr><td>train_loss_step</td><td>0.47182</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.74941</td></tr><tr><td>val_auc</td><td>0.83186</td></tr><tr><td>val_f1</td><td>0.73762</td></tr><tr><td>val_loss_epoch</td><td>0.51524</td></tr><tr><td>val_loss_step</td><td>0.56572</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_16_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/139hacxz' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/139hacxz</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240104_132347-139hacxz\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d81e68231614698b9dc6ef6f228e0b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01693333333435779, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240104_135515-ywd2oo5u</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/ywd2oo5u' target=\"_blank\">MLP_2_16_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/ywd2oo5u' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/ywd2oo5u</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 496   \n",
      "2  | head        | Sequential       | 154   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 281   \n",
      "--------------------------------------------------\n",
      "931       Trainable params\n",
      "0         Non-trainable params\n",
      "931       Total params\n",
      "0.004     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4305566da3a4d0797c77c561ba4d08f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▃▆▆▇▇▇▇▇▇▇▇█▇███████▇█▇██▇▇▇███▇█▇████</td></tr><tr><td>train_auc</td><td>▁▁▂▅▆▇▇█▇▇█▇██▇█████████████████████████</td></tr><tr><td>train_f1</td><td>▁▄▅▆▇▇▇▇▇████████████████████▇██████████</td></tr><tr><td>train_loss_epoch</td><td>███▆▄▃▃▂▂▂▂▂▂▂▂▂▂▁▂▂▁▂▂▁▂▂▂▂▁▂▁▁▂▂▁▂▁▂▂▂</td></tr><tr><td>train_loss_step</td><td>███▅▅▄▂▂▃▃▃▁▃▂▂▃▂▁▂▄▂▃▁▃▃▂▁▂▃▂▂▁▂▃▂▂▁▂▁▄</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▃▅▆▆▇▇██▇██▇██▇▇▇█▇██▇▇▇████████▇██▇█▇█</td></tr><tr><td>val_auc</td><td>▁▆▇▇▇▇██████████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▄▆█▇█▇████████████████▇████████████████</td></tr><tr><td>val_loss_epoch</td><td>██▇▅▄▃▂▂▂▂▂▂▂▂▁▂▂▂▁▂▂▁▁▂▂▂▂▁▂▁▁▁▂▂▁▁▁▂▂▂</td></tr><tr><td>val_loss_step</td><td>███▆▄▅▄▄▃▃▃▃▂▄▃▃▂▃▃▄▄▃▂▃▃▄▄▂▃▃▃▂▄▃▃▁▃▃▂▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.76623</td></tr><tr><td>train_auc</td><td>0.83871</td></tr><tr><td>train_f1</td><td>0.76706</td></tr><tr><td>train_loss_epoch</td><td>0.50391</td></tr><tr><td>train_loss_step</td><td>0.56343</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.7565</td></tr><tr><td>val_auc</td><td>0.8326</td></tr><tr><td>val_f1</td><td>0.75061</td></tr><tr><td>val_loss_epoch</td><td>0.50543</td></tr><tr><td>val_loss_step</td><td>0.52668</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_16_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/ywd2oo5u' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/ywd2oo5u</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240104_135515-ywd2oo5u\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3f1b816ed4642e5af1fa870ba6fd3a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01693333333435779, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240104_142702-ui3m764s</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/ui3m764s' target=\"_blank\">MLP_2_32_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/ui3m764s' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/ui3m764s</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 1.5 K \n",
      "2 | head        | Sequential       | 562   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "2.1 K     Trainable params\n",
      "0         Non-trainable params\n",
      "2.1 K     Total params\n",
      "0.008     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▄▆▇▇▇▇▇█▇▇███▇██▇█▇▇██████▇███▇████████</td></tr><tr><td>train_auc</td><td>▁▄▇▇████████████████████████████████████</td></tr><tr><td>train_f1</td><td>▁▅▇▇█▇██████████████████████████████████</td></tr><tr><td>train_loss_epoch</td><td>█▇▄▃▂▂▂▂▁▁▂▁▂▁▂▁▂▂▁▁▁▂▂▁▁▂▂▂▁▁▁▁▂▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▇▆▄▃▃▃▄▃▃▃▂▃▂▂▃▂▂▃▃▄▃▁▃▃▃▂▃▂▂▂▃▄▂▂▃▂▃▃▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▅▆▇▇▇███████████▇████████████▇█████████</td></tr><tr><td>val_auc</td><td>▁▁▂▅▆▇▇▇▇▇██████████████████████████████</td></tr><tr><td>val_f1</td><td>▄▁▆▇▅▇█▇▇▇▇▇▇▇▇▇█▇▇▇█▇▇██▇▇██▇▇█▇▇▇▇▇▇█▇</td></tr><tr><td>val_loss_epoch</td><td>█▇▅▃▂▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▂▁▁▂▂▁▁▂▁▂▁▁▂▁▂▁</td></tr><tr><td>val_loss_step</td><td>█▇▆▅▂▄▂▃▃▄▃▂▅▂▃▃▁▂▃▂▂▄▂▁▂▂▂▃▃▂▃▄▃▄▃▄▅▃▄▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.7686</td></tr><tr><td>train_auc</td><td>0.84434</td></tr><tr><td>train_f1</td><td>0.77236</td></tr><tr><td>train_loss_epoch</td><td>0.49275</td></tr><tr><td>train_loss_step</td><td>0.48438</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.75414</td></tr><tr><td>val_auc</td><td>0.83502</td></tr><tr><td>val_f1</td><td>0.74879</td></tr><tr><td>val_loss_epoch</td><td>0.49781</td></tr><tr><td>val_loss_step</td><td>0.49347</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_32_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/ui3m764s' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/ui3m764s</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240104_142702-ui3m764s\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3de9e6425464e09a9f5221891db5c47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240104_145843-pl5rcxgw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/pl5rcxgw' target=\"_blank\">MLP_2_32_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/pl5rcxgw' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/pl5rcxgw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 1.5 K \n",
      "2 | head        | Sequential       | 562   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "2.1 K     Trainable params\n",
      "0         Non-trainable params\n",
      "2.1 K     Total params\n",
      "0.008     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a6a9f9000b54a5e82112ec9223045c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▄▆▆▇▇▇▇▇▇█▇██▇▇▇▇▇▇▇▇█▇▇███▇██▇█▇████▇█</td></tr><tr><td>train_auc</td><td>▁▃▆▇▇▇▇▇▇▇▇▇█▇▇█▇▇█▇▇▇███▇█▇███▇██████▇▇</td></tr><tr><td>train_f1</td><td>▁▅▆▆▇▇▇▇▇▇█▇██▇▇▆▇▇▇▇▇█▇▇▇█▇▇▇█▇▇▇████▇█</td></tr><tr><td>train_loss_epoch</td><td>█▇▅▄▃▂▃▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▂▂▂▂▁▂▂▁▂▂▂▁▁▁▁▂▂▂</td></tr><tr><td>train_loss_step</td><td>█▇▆▆▄▅▄▅▄▄▄▃▃▃▃▃▃▃▅▄▄▄▂▅▅▂▂▄▁▃▃▄▆▃▅▃▃▃▃▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▂▂▂▂▂████████▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>▁▃▄▅▆▇▆▆▆▆▇▇▇▇▆███▇▇▆▆████▇▇▇▇▇▇▇████▇▆▇</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁████████▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>val_loss_epoch</td><td>▄▄▂▂▂▁▁▁▂▂▂▃▄▅▅▆▆▅▆▆▅▅▆▆█▅▆▇▇██▇▆▇▆███▇▇</td></tr><tr><td>val_loss_step</td><td>▄▄▃▂▂▂▂▂▂▃▁▃▄▆▄▆▄▅▅▆▄▄▆▄▇▄▇▆▇█▆▅▃▆▅▆█▇▆▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.73436</td></tr><tr><td>train_auc</td><td>0.78035</td></tr><tr><td>train_f1</td><td>0.73715</td></tr><tr><td>train_loss_epoch</td><td>0.5596</td></tr><tr><td>train_loss_step</td><td>0.53024</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.487</td></tr><tr><td>val_auc</td><td>0.75359</td></tr><tr><td>val_f1</td><td>0.65501</td></tr><tr><td>val_loss_epoch</td><td>0.73351</td></tr><tr><td>val_loss_step</td><td>0.70874</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_32_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/pl5rcxgw' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/pl5rcxgw</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240104_145843-pl5rcxgw\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ea1c0bda1414cfb95b4fd5b7225ed47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016933333330477276, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240104_152958-qr89ta39</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/qr89ta39' target=\"_blank\">MLP_2_32_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/qr89ta39' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/qr89ta39</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 1.5 K \n",
      "2 | head        | Sequential       | 562   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "2.1 K     Trainable params\n",
      "0         Non-trainable params\n",
      "2.1 K     Total params\n",
      "0.008     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▃▅▆▆▆▇▇▇▇▇▇█▇▇▇▇█▇▇▇▇██▇█▇████▇▇▇▇▇████</td></tr><tr><td>train_auc</td><td>▁▂▄▆▆▆▇▇▇▇▇█▇▇▆█▇█▇██▇▇█▇█▇▇▇█▇▇▇█▇▇▇██▇</td></tr><tr><td>train_f1</td><td>▁▃▄▅▆▅▇▇▇▇▇▇█▇▇▇▇█▇▇▇▇████▇████▇▇▇█▇▇█▇█</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▅▅▄▃▃▃▃▃▃▂▂▃▂▂▂▂▂▂▂▃▂▁▂▃▃▃▂▁▂▂▂▃▂▃▂▂▂▃▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▅▅▆▆▇▆█▇▇▇▇▇▇▇██▇████▇▇█▇▇██▇▇███▇▇████</td></tr><tr><td>val_auc</td><td>▁▅▇▇▇▇▇▇█▇█████▇█████████████████▇██████</td></tr><tr><td>val_f1</td><td>▃▄▂▄▁▆▃▇▄▇▅▇▅▇▆▇▇▅▇▇▇▆▆▇▇▅▇▇▇▆▇█▆█▅▆█▆█▆</td></tr><tr><td>val_loss_epoch</td><td>█▆▅▄▃▃▂▂▂▃▂▂▂▂▂▂▁▁▂▁▁▂▁▁▁▂▁▂▁▁▁▂▂▂▁▂▂▁▂▁</td></tr><tr><td>val_loss_step</td><td>█▆▅▄▃▄▃▃▃▄▃▂▄▂▃▃▁▂▃▁▂▃▁▁▁▃▁▂▂▂▃▄▃▄▂▄▄▁▄▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.77922</td></tr><tr><td>train_auc</td><td>0.78784</td></tr><tr><td>train_f1</td><td>0.78702</td></tr><tr><td>train_loss_epoch</td><td>0.48119</td></tr><tr><td>train_loss_step</td><td>0.43773</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.78251</td></tr><tr><td>val_auc</td><td>0.85994</td></tr><tr><td>val_f1</td><td>0.77</td></tr><tr><td>val_loss_epoch</td><td>0.47593</td></tr><tr><td>val_loss_step</td><td>0.46566</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_32_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/qr89ta39' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/qr89ta39</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240104_152958-qr89ta39\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7939fe241984e738677700393762a76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01693333333435779, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240104_160119-0m1v9gq0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/0m1v9gq0' target=\"_blank\">MLP_2_32_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/0m1v9gq0' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/0m1v9gq0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 1.5 K \n",
      "2  | head        | Sequential       | 562   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 33    \n",
      "--------------------------------------------------\n",
      "2.1 K     Trainable params\n",
      "0         Non-trainable params\n",
      "2.1 K     Total params\n",
      "0.008     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a55077d23a5a4cd1ada1b53d88598980",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▄▆▆▇▇▇▇▇▇█▇████████████▇████████▇██████</td></tr><tr><td>train_auc</td><td>▁▃▆▇▇▇▇▇▇██▇████████████████████████████</td></tr><tr><td>train_f1</td><td>▁▄▇▆▇▇▇▇███▇████████████████████████████</td></tr><tr><td>train_loss_epoch</td><td>█▇▅▄▃▄▃▃▃▂▂▂▂▂▂▁▁▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▂▁▁▂▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▇▄▅▅▅▃▄▃▃▃▄▂▃▅▄▃▂▂▃▃▃▃▃▂▂▃▃▄▃▁▄▄▃▁▄▃▃▃▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▅▅▅▅▆▆▇▇█▇█▇██████▇▇██▇████████████████</td></tr><tr><td>val_auc</td><td>▄▂▁▁▁▂▃▅▆▆▇▇▇▇▇█▇▇▇▇█▇▇▇████████▇███▇███</td></tr><tr><td>val_f1</td><td>▄▁▁▃▄▄▅▇▇▇▆█▅███▇▇█▆█▇█▇██▇██▇██▇██▇▇███</td></tr><tr><td>val_loss_epoch</td><td>█▇▆▅▄▄▃▃▂▂▂▁▃▂▂▂▂▂▁▂▂▂▂▂▂▂▂▂▂▂▁▂▂▁▁▁▂▁▁▂</td></tr><tr><td>val_loss_step</td><td>█▇▆▇▄▅▄▃▅▃▃▂▄▄▃▃▄▄▁▂▃▃▄▅▄▄▅▄▄▄▃▄▄▃▁▂▂▃▂▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.7686</td></tr><tr><td>train_auc</td><td>0.84705</td></tr><tr><td>train_f1</td><td>0.77367</td></tr><tr><td>train_loss_epoch</td><td>0.47945</td></tr><tr><td>train_loss_step</td><td>0.45789</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.75414</td></tr><tr><td>val_auc</td><td>0.83813</td></tr><tr><td>val_f1</td><td>0.75926</td></tr><tr><td>val_loss_epoch</td><td>0.50281</td></tr><tr><td>val_loss_step</td><td>0.52875</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_32_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/0m1v9gq0' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/0m1v9gq0</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240104_160119-0m1v9gq0\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99b5b2b5a63f40a4a246b28070f8a377",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240104_163221-sgxli1z8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/sgxli1z8' target=\"_blank\">MLP_2_32_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/sgxli1z8' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/sgxli1z8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 1.5 K \n",
      "2  | head        | Sequential       | 562   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 1.1 K \n",
      "--------------------------------------------------\n",
      "3.1 K     Trainable params\n",
      "0         Non-trainable params\n",
      "3.1 K     Total params\n",
      "0.013     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd80a7184d3c495498b00d395b8b6660",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▃▆▆▇▇▇█████████▇█▇█████████████████████</td></tr><tr><td>train_auc</td><td>▁▃▇▇▇███████████▇███████████████████████</td></tr><tr><td>train_f1</td><td>▁▄▆▇▇███████▇███▇███████████████████████</td></tr><tr><td>train_loss_epoch</td><td>█▇▄▃▃▂▂▂▂▁▂▂▂▁▁▁▂▂▂▁▁▂▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▂▁</td></tr><tr><td>train_loss_step</td><td>██▅▄▄▂▄▄▄▂▂▂▃▅▃▃▄▁▅▂▃▃▃▂▃▂▃▄▁▂▂▄▃▂▃▄▂▂▃▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▆▆▇▆█▇█▇█▇██████▇███████████▇▇███▇███▇█</td></tr><tr><td>val_auc</td><td>▃▂▁▄▆▆▇▇▇███████████████████████████████</td></tr><tr><td>val_f1</td><td>▂▅▁▆▂▇▄▇▅▇▅▇▇▇▇██▅▆▇████▇▇▇▇▇▅▅▇▇▇▅██▇▅█</td></tr><tr><td>val_loss_epoch</td><td>█▇▅▄▃▂▂▂▂▁▂▂▂▂▁▂▂▂▁▂▂▂▂▂▁▁▂▁▁▂▂▂▁▁▂▁▁▁▂▂</td></tr><tr><td>val_loss_step</td><td>█▇▆▅▄▅▃▃▃▃▃▃▃▄▂▃▂▃▃▄▃▂▂▄▃▂▃▂▃▄▄▄▃▃▄▂▃▁▄▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.77627</td></tr><tr><td>train_auc</td><td>0.84979</td></tr><tr><td>train_f1</td><td>0.76932</td></tr><tr><td>train_loss_epoch</td><td>0.48705</td></tr><tr><td>train_loss_step</td><td>0.4944</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.76596</td></tr><tr><td>val_auc</td><td>0.83403</td></tr><tr><td>val_f1</td><td>0.77136</td></tr><tr><td>val_loss_epoch</td><td>0.50604</td></tr><tr><td>val_loss_step</td><td>0.50455</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_32_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/sgxli1z8' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/sgxli1z8</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240104_163221-sgxli1z8\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9904c6df8e8a42c2b26e561a057c1343",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.017183333332650364, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240104_170254-yof1134i</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/yof1134i' target=\"_blank\">MLP_2_64_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/yof1134i' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/yof1134i</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 5.1 K \n",
      "2 | head        | Sequential       | 2.1 K \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "7.2 K     Trainable params\n",
      "0         Non-trainable params\n",
      "7.2 K     Total params\n",
      "0.029     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bddbdc186a142b0b6a8242ef6e084d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▅▇▇█▇▇██▇█▇████████████████████████████</td></tr><tr><td>train_auc</td><td>▁▆▇▇█▇██████████████████████████████████</td></tr><tr><td>train_f1</td><td>▁▅▇▇█████▇███████████████████▇██████████</td></tr><tr><td>train_loss_epoch</td><td>█▅▂▂▂▂▂▂▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▆▄▃▄▅▄▃▄▂▄▄▂▃▄▂▃▃▄▄▂▄▁▂▁▃▃▃▃▄▄▃▄▁▂▃▂▃▄▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▄▅█▇▇▅▇█▇▇▇▇▇▇▇███▇▇▇▇██▇▇▇▇▇▆▇▇▇▇█████</td></tr><tr><td>val_auc</td><td>▁▃▅▆▇▇▇▇▇▇██████████████████████████████</td></tr><tr><td>val_f1</td><td>▄▅▂▇▆▅▁▆▇▇▆▆▆▅▇▅▇▇▇▅▆▅▅█▇▅▆▇▅▇▅▆▇▅▅█▇▇▇▇</td></tr><tr><td>val_loss_epoch</td><td>█▄▃▂▂▂▂▂▂▂▁▂▁▁▂▁▁▁▂▁▁▂▁▂▁▁▁▂▂▁▁▂▂▁▁▂▁▁▂▁</td></tr><tr><td>val_loss_step</td><td>█▄▄▃▃▃▃▅▃▂▃▄▂▂▃▂▂▃▄▂▂▄▁▃▂▃▁▃▄▂▁▄▄▁▃▄▃▂▄▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.77627</td></tr><tr><td>train_auc</td><td>0.85721</td></tr><tr><td>train_f1</td><td>0.77427</td></tr><tr><td>train_loss_epoch</td><td>0.47684</td></tr><tr><td>train_loss_step</td><td>0.46447</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.77069</td></tr><tr><td>val_auc</td><td>0.8365</td></tr><tr><td>val_f1</td><td>0.77283</td></tr><tr><td>val_loss_epoch</td><td>0.4991</td></tr><tr><td>val_loss_step</td><td>0.492</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_64_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/yof1134i' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/yof1134i</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240104_170254-yof1134i\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84a6367193604eb09b2c9c31db5d247e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01693333333435779, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240104_173329-7uhm6ra3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/7uhm6ra3' target=\"_blank\">MLP_2_64_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/7uhm6ra3' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/7uhm6ra3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 5.1 K \n",
      "2 | head        | Sequential       | 2.1 K \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "7.2 K     Trainable params\n",
      "0         Non-trainable params\n",
      "7.2 K     Total params\n",
      "0.029     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f98e54e16255446e868ff00ff08c113e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▃▆▇▇▇█▇▇▇█▇▇▇▇█▇▇▇████▇▇███▇██▇▇█▇█▇▇██</td></tr><tr><td>train_auc</td><td>▁▂▅▅▆▅▆▄▅▄▆▇▅▇▆▇▆▆▆▇▇▆▇▇▇▆▇▇▇▇▇▇▇▇██▇▇██</td></tr><tr><td>train_f1</td><td>▁▂▅▆▇▆█▇▇▇▇▇▇▇▇█▇▇▇████▇▇██▇▇██▇▇▇██▇▇▇▇</td></tr><tr><td>train_loss_epoch</td><td>█▆▄▃▃▃▃▂▂▂▁▂▂▂▂▂▂▂▂▂▁▁▂▂▁▂▂▁▂▁▂▁▂▁▁▁▂▁▂▁</td></tr><tr><td>train_loss_step</td><td>█▇▆▄▂▇▅▅▅▂▄▅▄▄▄▃▄▆▄▅▂▄▁▁▂▃▃▄▃▄▅▂▃▂▃▅▃▄▄▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▁███▇█▇▅▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>▁▆██████▇▇▇█▇██▇█▇▇▇███▇▇█▇█▇▇█▇███▇▇███</td></tr><tr><td>val_f1</td><td>▁▁███▇█▇▅▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_epoch</td><td>▂▂▁▁▁▂▁▂▂▃▃▅▅▅▅▄▅▅▅▅▇▆▆▇▇▆▆▇▆▇▆▇▆▇▇█▇▆▆▇</td></tr><tr><td>val_loss_step</td><td>▂▂▁▁▂▂▁▂▃▂▂▅▄▅▅▄▆▆▅▄▆▆▅▇▆▅▆▇▅▆▅▆▅▆▆▆█▄▅▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.76092</td></tr><tr><td>train_auc</td><td>0.79631</td></tr><tr><td>train_f1</td><td>0.75792</td></tr><tr><td>train_loss_epoch</td><td>0.49663</td></tr><tr><td>train_loss_step</td><td>0.51692</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.487</td></tr><tr><td>val_auc</td><td>0.76376</td></tr><tr><td>val_f1</td><td>0.65501</td></tr><tr><td>val_loss_epoch</td><td>1.09667</td></tr><tr><td>val_loss_step</td><td>1.0745</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_64_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/7uhm6ra3' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/7uhm6ra3</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240104_173329-7uhm6ra3\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abfc1a0f09374313b7d69510615b7bf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01693333333435779, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240104_180400-m6y3ix42</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/m6y3ix42' target=\"_blank\">MLP_2_64_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/m6y3ix42' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/m6y3ix42</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 5.1 K \n",
      "2 | head        | Sequential       | 2.1 K \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "7.2 K     Trainable params\n",
      "0         Non-trainable params\n",
      "7.2 K     Total params\n",
      "0.029     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3d32bb129f446a3870cb5a21a73b25a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▄▆▇▆▇▇▇█▇██▇█▇▇█████▇▇██████▇██████████</td></tr><tr><td>train_auc</td><td>█▇▇▄▃▃▂▂▁▂▂▂▂▂▂▂▂▂▂▂▃▂▂▂▂▃▂▂▂▄▃▂▂▁▁▂▂▃▃▃</td></tr><tr><td>train_f1</td><td>▁▅▅▆▆▇▇▆█▇█▇▇▇▇▇█████▇▇██████▇██████████</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▄▄▃▃▄▃▂▃▂▂▄▂▂▃▁▂▂▃▃▃▃▁▂▁▂▂▂▂▂▃▂▃▁▁▂▂▃▃▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▂▆▇▇▅▂▆▇▇▅▆▆▇▆▆▆▆▇▇▇▇▇▇█▇▇▇▇▇▇▇▇▇▇▇▇▇▆█</td></tr><tr><td>val_auc</td><td>█▄▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_f1</td><td>▁▂▇▇▇▆▁▇▇▇▅▇▆█▆▇▇▇▇▆▇▇▆▇█▇▇█▇▇█▆▇▇▇█▇▇▇█</td></tr><tr><td>val_loss_epoch</td><td>██▇▅▄▄▇▇▃▄▃▄▂▄▂▁▁▁▃▁▂▃▁▂▂▂▁▄▂▂▂▃▂▁▂▅▂▂▂▂</td></tr><tr><td>val_loss_step</td><td>▄▅▆▅▄▅▇█▄▄▅▅▂▄▄▂▁▂▅▂▃▅▂▃▂▄▂▄▅▄▂▆▄▃▄▅▃▂▅▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.78276</td></tr><tr><td>train_auc</td><td>0.27267</td></tr><tr><td>train_f1</td><td>0.78043</td></tr><tr><td>train_loss_epoch</td><td>0.47537</td></tr><tr><td>train_loss_step</td><td>0.52992</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.79433</td></tr><tr><td>val_auc</td><td>0.13923</td></tr><tr><td>val_f1</td><td>0.80624</td></tr><tr><td>val_loss_epoch</td><td>0.48633</td></tr><tr><td>val_loss_step</td><td>0.4697</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_64_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/m6y3ix42' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/m6y3ix42</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240104_180400-m6y3ix42\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fd979f05e9c4a6d9e1e63cdd5d6e327",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01693333333435779, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240104_183438-m0tjzj2m</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/m0tjzj2m' target=\"_blank\">MLP_2_64_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/m0tjzj2m' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/m0tjzj2m</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 5.1 K \n",
      "2  | head        | Sequential       | 2.1 K \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 65    \n",
      "--------------------------------------------------\n",
      "7.3 K     Trainable params\n",
      "0         Non-trainable params\n",
      "7.3 K     Total params\n",
      "0.029     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a1e5c631cd94d3f9481a7bbbc774f1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▆▇▇▇█▇███████▇███████▇████▇█▇█████▇████</td></tr><tr><td>train_auc</td><td>▁▅▇▇▇█▇████████████████████▇████████████</td></tr><tr><td>train_f1</td><td>▁▆▇▇▇█▇▇████▇█▇███████▇████▇████████████</td></tr><tr><td>train_loss_epoch</td><td>█▆▃▂▃▂▂▂▂▁▂▂▂▁▂▁▂▂▁▁▁▁▂▁▁▁▂▃▁▂▁▁▂▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▆▅▅▃▃▄▂▄▃▄▃▃▅▃▄▄▃▄▂▄▃▄▃▃▃▂▄▃▁▂▁▄▂▄▂▂▁▃▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▄▁▅██▆█▇▇▆▇████▆█▇██▇▇████▇▅█▇▇▆▅▆▇▇████</td></tr><tr><td>val_auc</td><td>▄▁▄▆▆▇▇▇▇▇▇▇█▇█▇▇▇████▇▇██▇▇▇███▇▇▇█████</td></tr><tr><td>val_f1</td><td>▇▁▇▇▇▅█▇▆█▆▇▇██▇▇▆▇█▇██▇▇▇▆▇▇▆▇▇▄▇▆▇██▇▇</td></tr><tr><td>val_loss_epoch</td><td>█▄▄▂▁▂▂▂▂▂▂▂▂▂▁▃▁▂▂▂▂▂▂▂▁▁▂▃▂▂▂▂▃▁▂▂▁▁▂▁</td></tr><tr><td>val_loss_step</td><td>█▅▅▃▁▄▄▂▃▃▄▄▅▃▃▇▂▄▄▄▄▄▄▄▂▃▃▅▃▃▃▃▆▂▄▄▂▃▄▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.77509</td></tr><tr><td>train_auc</td><td>0.85814</td></tr><tr><td>train_f1</td><td>0.78166</td></tr><tr><td>train_loss_epoch</td><td>0.45967</td></tr><tr><td>train_loss_step</td><td>0.39595</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.75887</td></tr><tr><td>val_auc</td><td>0.83761</td></tr><tr><td>val_f1</td><td>0.75943</td></tr><tr><td>val_loss_epoch</td><td>0.48701</td></tr><tr><td>val_loss_step</td><td>0.44427</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_64_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/m0tjzj2m' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/m0tjzj2m</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240104_183438-m0tjzj2m\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c18ae26de74f4911b8514d1b93b82cf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240104_190526-9knxuwg7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/9knxuwg7' target=\"_blank\">MLP_2_64_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/9knxuwg7' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/9knxuwg7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 5.1 K \n",
      "2  | head        | Sequential       | 2.1 K \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 4.2 K \n",
      "--------------------------------------------------\n",
      "11.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "11.4 K    Total params\n",
      "0.046     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38453f97fb2549caa046dadd6a93a648",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▆▇██████▇████████████████▇█████████████</td></tr><tr><td>train_auc</td><td>▁▆▇█████████████████████████████████████</td></tr><tr><td>train_f1</td><td>▁▅▇██▇█▇███▇██████████████▇█▇███████████</td></tr><tr><td>train_loss_epoch</td><td>█▅▃▂▂▂▂▂▂▂▂▁▂▁▁▁▁▂▁▂▂▁▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▂</td></tr><tr><td>train_loss_step</td><td>█▅▅▂▂▃▂▄▃▄▃▄▄▃▃▅▃▄▃▄▃▂▃▃▂▂▄▄▃▃▃▂▄▁▃▃▃▂▄▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▅▇█▆█▆▇█▆▆▇▇▇█▇▇██▇▇▇▇██▇▇▇▇▇▇▇▇▇▇███▇▇</td></tr><tr><td>val_auc</td><td>▂▁▅▆▇▇▇▇▇▇▇█████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▇██▆▇▇██▇▇█▇▇█▇▇██▇▇▇▇██▇█▇█▇▇█▇▇▇███▇▇</td></tr><tr><td>val_loss_epoch</td><td>█▄▃▂▂▂▂▂▂▁▂▂▂▁▁▂▂▂▁▂▁▂▁▁▁▁▂▁▂▂▁▁▂▁▁▂▂▁▁▁</td></tr><tr><td>val_loss_step</td><td>█▅▄▃▂▃▄▂▃▂▄▃▄▂▂▆▃▄▂▃▂▅▃▃▂▃▄▁▄▄▂▁▄▃▃▄▄▃▂▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.77273</td></tr><tr><td>train_auc</td><td>0.84795</td></tr><tr><td>train_f1</td><td>0.77629</td></tr><tr><td>train_loss_epoch</td><td>0.48263</td></tr><tr><td>train_loss_step</td><td>0.46528</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.75414</td></tr><tr><td>val_auc</td><td>0.83531</td></tr><tr><td>val_f1</td><td>0.7451</td></tr><tr><td>val_loss_epoch</td><td>0.49938</td></tr><tr><td>val_loss_step</td><td>0.50154</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_64_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/9knxuwg7' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/9knxuwg7</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240104_190526-9knxuwg7\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b585d81db2b94faea7ee3411a6c44dcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01693333333435779, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240104_193620-kodlex5i</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/kodlex5i' target=\"_blank\">MLP_3_16_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/kodlex5i' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/kodlex5i</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 768   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "922       Trainable params\n",
      "0         Non-trainable params\n",
      "922       Total params\n",
      "0.004     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21ac21e3681d46ae8e5caf6f5fa88444",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▁▂▅▇▇▇▇▇█▇▇██▇█▇███████▇██▇█▇█████████</td></tr><tr><td>train_auc</td><td>▁▁▁▂▅▇▇▇▇▇██████████████████████████████</td></tr><tr><td>train_f1</td><td>▅▁▄▃▆▇▇▇▇▇█▇▇██▇█▇▇██████▇▇▇▇█▇██▇███▇██</td></tr><tr><td>train_loss_epoch</td><td>███▇▆▃▃▂▂▂▂▂▂▁▂▂▁▁▁▁▂▁▂▁▁▂▁▁▂▂▂▁▂▁▁▁▂▂▁▂</td></tr><tr><td>train_loss_step</td><td>███▇▆▃▄▆▃▄▃▄▂▄▂▅▄▃▄▂▃▅▃▃▂▅▃▄▄▃▅▁▄▄▄▃▄▃▃▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▁▁▆▇▇███▇████████████▇█████▇███████████</td></tr><tr><td>val_auc</td><td>▁▆▇▇▇▇██████████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▄▆▅▆▇▇▇▇▇▆▆▇▇▇▇█▅▆▅▆▇▇▇▇▇██▇▇▇▇▇▇▇▇▆</td></tr><tr><td>val_loss_epoch</td><td>███▇▅▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▂▁▂▁▁▁▁▁▂▂▁▁▁▁▁▁▁▂▁▁</td></tr><tr><td>val_loss_step</td><td>████▆▄▃▃▅▃▄▃▃▃▃▃▄▁▃▁▃▃▃▃▂▂▂▁▁▃▂▁▂▂▃▃▂▃▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.76151</td></tr><tr><td>train_auc</td><td>0.82555</td></tr><tr><td>train_f1</td><td>0.7763</td></tr><tr><td>train_loss_epoch</td><td>0.52287</td></tr><tr><td>train_loss_step</td><td>0.49512</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.74941</td></tr><tr><td>val_auc</td><td>0.83066</td></tr><tr><td>val_f1</td><td>0.74146</td></tr><tr><td>val_loss_epoch</td><td>0.51126</td></tr><tr><td>val_loss_step</td><td>0.53479</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_16_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/kodlex5i' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/kodlex5i</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240104_193620-kodlex5i\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8095f248d94f4e44865c6cb1308672d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01691666666495924, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240104_200702-vvcrqgzc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/vvcrqgzc' target=\"_blank\">MLP_3_16_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/vvcrqgzc' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/vvcrqgzc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 768   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "922       Trainable params\n",
      "0         Non-trainable params\n",
      "922       Total params\n",
      "0.004     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc4b9ffa0d0343e8bb3840852dbe0e7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▂▁▁▂▁▂▃▃▄▃▃▅▄▄▆▇▆▆▇▇█▇▇▇█▇▇▇▇▇▇▇▇▇▇▇▇▇▇█</td></tr><tr><td>train_auc</td><td>▁▁▁▂▁▂▂▃▃▃▃▅▄▄▆▇▆▇▇▇██▇▇█▇▇▇▇▇▇████▇█▇██</td></tr><tr><td>train_f1</td><td>▆▃▄▁▃▁▄▄▄▄▃▆▇▆██▇▆▇█▇█▇▇█▇▇▇▆▇▇█▇▆▇█▇▇██</td></tr><tr><td>train_loss_epoch</td><td>█▇▇▇▇▆▆▆▆▆▆▅▆▅▃▂▃▃▂▃▁▂▂▂▂▃▃▂▂▃▂▂▂▁▂▂▂▂▁▂</td></tr><tr><td>train_loss_step</td><td>█▇▇▇▇▇▇▇▇▆▆▅▆▆▅▅▆▆▃▄▄▅▃▄▃▆▄▃▃▃▄▁▄▄▄▁▃▄▁▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>▂▁▆▄▄▄▃▃▃▃▃▇▇▇▇▇▇████▇▇███████████▇▇▇▇▇▇</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_epoch</td><td>▂▂▂▁▁▁▂▂▃▃▄▅▇▇▅▃▄▃▃▃▂▃▄▄▃▄▄▅▅▄▅▆▆▆▆▆▇▇██</td></tr><tr><td>val_loss_step</td><td>▂▃▂▁▁▁▃▁▃▃▄▅▅▇▃▂▃▂▅▂▂▃▄▃▃▃▃▄▄▄▂▆▂▄▇▅▆▅█▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.67946</td></tr><tr><td>train_auc</td><td>0.71827</td></tr><tr><td>train_f1</td><td>0.67813</td></tr><tr><td>train_loss_epoch</td><td>0.62385</td></tr><tr><td>train_loss_step</td><td>0.6192</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.487</td></tr><tr><td>val_auc</td><td>0.71797</td></tr><tr><td>val_f1</td><td>0.65501</td></tr><tr><td>val_loss_epoch</td><td>0.74851</td></tr><tr><td>val_loss_step</td><td>0.76235</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_16_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/vvcrqgzc' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/vvcrqgzc</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240104_200702-vvcrqgzc\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff273b593d1b463ea3b444be9bbdb36e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240104_204038-a2sptky6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/a2sptky6' target=\"_blank\">MLP_3_16_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/a2sptky6' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/a2sptky6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 768   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "922       Trainable params\n",
      "0         Non-trainable params\n",
      "922       Total params\n",
      "0.004     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9e207e446af451d854ae3e59ae7fcca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▂▃▄▅▅▆▆▆▆▇▇▇█▇▇█▇▇▇███▇▇██▇█▇█▇███████</td></tr><tr><td>train_auc</td><td>▄▅▄▅▅▆▇▇▇▇▇█▇▇▅▆▅▄▄▄▄▃▃▂▃▃▃▂▂▁▂▂▂▂▂▂▃▂▂▁</td></tr><tr><td>train_f1</td><td>▁▁▂▃▄▄▅▅▆▅▆▇▇▇▇▇▇█▆▇▇▇▇▇▇▇▇▇▇███▇▇▇▇▇███</td></tr><tr><td>train_loss_epoch</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▃▂▂▂▂▂▂▁▁▁▂▁▂▁▂▂▂▁▁▁▁▁▁▁▂▁▂▂▁▂▁▂▂▁▁▂▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▃▆▆▆▅▆█▇██▇█▆██▇▇▇█▆▇▇█▇▇███▇▆▇▇██▇██▆▇</td></tr><tr><td>val_auc</td><td>▅▅▆▇▇▇██████████▅▆▆▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_f1</td><td>▁▇▇▇▇▆▆█▇██▇█▆██▇▇▇█▆▇▇█▇▇▇██▇▆▇▇██▇██▆▇</td></tr><tr><td>val_loss_epoch</td><td>█▆▆▅▅▄▃▃▃▂▂▂▂▂▂▂▂▂▁▁▂▂▂▁▁▂▁▁▁▁▂▂▂▁▁▁▁▁▂▂</td></tr><tr><td>val_loss_step</td><td>█▆▆▅▅▅▄▄▄▃▃▃▃▃▃▃▄▂▂▂▄▂▂▂▂▂▂▂▁▂▂▁▃▁▂▂▁▃▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.73554</td></tr><tr><td>train_auc</td><td>0.39216</td></tr><tr><td>train_f1</td><td>0.76145</td></tr><tr><td>train_loss_epoch</td><td>0.54875</td></tr><tr><td>train_loss_step</td><td>0.51438</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.75887</td></tr><tr><td>val_auc</td><td>0.1476</td></tr><tr><td>val_f1</td><td>0.71348</td></tr><tr><td>val_loss_epoch</td><td>0.51892</td></tr><tr><td>val_loss_step</td><td>0.53829</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_16_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/a2sptky6' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/a2sptky6</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240104_204038-a2sptky6\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38b23fa5e1e346ef9687f5e4f492b5cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240104_211221-m61fytpn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/m61fytpn' target=\"_blank\">MLP_3_16_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/m61fytpn' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/m61fytpn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 768   \n",
      "2  | head        | Sequential       | 154   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 17    \n",
      "--------------------------------------------------\n",
      "939       Trainable params\n",
      "0         Non-trainable params\n",
      "939       Total params\n",
      "0.004     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bef0e162df3e4f5b822f33af2c1fefd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▂▁▂▄▆▆▇▇▇██▇████████████████████████████</td></tr><tr><td>train_auc</td><td>▁▁▁▃▆▇▇▇▇███████████████████████████████</td></tr><tr><td>train_f1</td><td>▅▁▃▄▆▆▇▇▇▇▇████▇████████████████████████</td></tr><tr><td>train_loss_epoch</td><td>██▇▇▅▄▃▂▂▂▂▂▂▂▂▂▂▂▁▁▂▁▁▂▁▂▁▁▁▁▁▁▂▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>███▇▆▄▃▄▂▃▃▄▃▄▂▃▄▂▄▃▃▄▃▄▃▂▁▂▄▃▅▂▃▃▄▃▂▃▄▄</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▁▁▆▄▇▇▇▇▇▇▇▇▇▇█▇▇▆▇▇▇█▇█▇▇██▇▇█▇▇▇▇▇▇▇▇</td></tr><tr><td>val_auc</td><td>▁▆▇▇▇▇██████████████████████████████████</td></tr><tr><td>val_f1</td><td>▆▆▆▆▁▇▇▇▆▇█▇▇▆▇▇▇▇▅▆▇▇▇▇▇▇▇██▇▇▇▆▇▇▆▇▇▇▇</td></tr><tr><td>val_loss_epoch</td><td>███▇▆▄▃▂▂▂▂▂▂▂▂▁▁▂▃▂▂▂▁▂▁▁▂▁▁▂▂▁▂▂▂▂▂▂▂▂</td></tr><tr><td>val_loss_step</td><td>████▆▆▅▅▄▄▄▃▄▄▃▂▁▄▄▃▅▄▃▄▄▃▅▄▄▃▄▄▄▄▄▄▃▃▄▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.76033</td></tr><tr><td>train_auc</td><td>0.83504</td></tr><tr><td>train_f1</td><td>0.7609</td></tr><tr><td>train_loss_epoch</td><td>0.509</td></tr><tr><td>train_loss_step</td><td>0.5496</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.73759</td></tr><tr><td>val_auc</td><td>0.82518</td></tr><tr><td>val_f1</td><td>0.7204</td></tr><tr><td>val_loss_epoch</td><td>0.51891</td></tr><tr><td>val_loss_step</td><td>0.54267</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_16_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/m61fytpn' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/m61fytpn</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240104_211221-m61fytpn\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0aed212cbbf3413bbd0e8cacc6b17f64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01693333333435779, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240104_214337-gekftu5x</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/gekftu5x' target=\"_blank\">MLP_3_16_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/gekftu5x' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/gekftu5x</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 768   \n",
      "2  | head        | Sequential       | 154   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 281   \n",
      "--------------------------------------------------\n",
      "1.2 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.2 K     Total params\n",
      "0.005     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd2ee7348ed0439e80d378242cd09a84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▂▃▆▆▇▇█▇████████████▇██████▇██████████</td></tr><tr><td>train_auc</td><td>▁▂▂▂▆▇▇▇▇███████████████████████████████</td></tr><tr><td>train_f1</td><td>▅▁▄▃▆▆▇▇█▇▇██████▇██▇█▇██████▇███▇██████</td></tr><tr><td>train_loss_epoch</td><td>█▇▇▇▅▄▃▃▂▂▂▂▂▂▂▂▂▂▁▂▂▂▂▂▂▁▁▂▁▂▁▂▂▂▂▁▁▁▂▂</td></tr><tr><td>train_loss_step</td><td>███▇▆▅▄▄▃▄▄▄▂▄▄▃▃▃▃▄▃▅▃▃▃▁▄▄▃▄▄▄▄▄▃▃▅▄▃▄</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▄▁▆▅▆▇█▇████▇█████▇█▇███▇██▇█▇████▇████</td></tr><tr><td>val_auc</td><td>▁▆▇▇▇▇██████████████████████████████████</td></tr><tr><td>val_f1</td><td>▆▁▆▇▅▅▇█▇████▇███▇▇▇█▇██▇▇██▇▇▇████▇████</td></tr><tr><td>val_loss_epoch</td><td>███▇▄▃▂▂▂▂▂▂▂▂▁▂▂▁▁▁▂▂▂▂▂▁▁▁▁▂▂▁▁▂▁▂▂▂▁▁</td></tr><tr><td>val_loss_step</td><td>███▇▅▄▃▃▄▄▃▃▃▃▂▄▃▃▁▃▃▂▄▃▃▂▂▃▂▄▃▃▂▄▂▃▄▄▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.7621</td></tr><tr><td>train_auc</td><td>0.82731</td></tr><tr><td>train_f1</td><td>0.76501</td></tr><tr><td>train_loss_epoch</td><td>0.51851</td></tr><tr><td>train_loss_step</td><td>0.54622</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.74941</td></tr><tr><td>val_auc</td><td>0.82846</td></tr><tr><td>val_f1</td><td>0.73892</td></tr><tr><td>val_loss_epoch</td><td>0.49926</td></tr><tr><td>val_loss_step</td><td>0.46306</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_16_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/gekftu5x' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/gekftu5x</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240104_214337-gekftu5x\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82557f671cef4532bcb4e9798b75bb3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01693333333435779, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240104_221516-z2s6nqjn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/z2s6nqjn' target=\"_blank\">MLP_3_32_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/z2s6nqjn' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/z2s6nqjn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 2.6 K \n",
      "2 | head        | Sequential       | 562   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "3.1 K     Trainable params\n",
      "0         Non-trainable params\n",
      "3.1 K     Total params\n",
      "0.012     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▇▇▇████▇████▇█████▇█████████████▇█████</td></tr><tr><td>train_auc</td><td>▁▂▆▇▇███████████████████████████████████</td></tr><tr><td>train_f1</td><td>▁▅▇▇▇████▇████▇█████▇█████████████▇█▇███</td></tr><tr><td>train_loss_epoch</td><td>██▄▃▂▂▂▂▂▁▂▂▂▁▁▁▂▁▁▂▂▂▂▁▂▁▂▁▂▁▁▁▁▂▂▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>██▆▄▄▂▄▂▃▃▃▅▂▂▂▂▃▁▅▃▂▃▂▃▂▃▂▃▁▃▂▃▃▂▁▂▂▂▁▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▆▆▇▇▇███████████████▇██▇███████████████</td></tr><tr><td>val_auc</td><td>▃▁▂▅▆▇▇▇▇███████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▃▆▇▇▆▇▇▇▆█▆█▇▇▇▇▇▇▇▄▆▇▄▇█▇▇▆█▇▆▇█▆▇███</td></tr><tr><td>val_loss_epoch</td><td>█▇▄▂▃▂▂▂▁▂▁▁▁▂▁▁▁▁▁▁▁▂▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▂▂▁</td></tr><tr><td>val_loss_step</td><td>█▇▅▄▄▄▃▃▁▄▃▂▂▅▂▂▃▃▃▃▂▃▂▂▁▃▄▂▄▃▃▃▃▁▃▃▃▃▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.77214</td></tr><tr><td>train_auc</td><td>0.84551</td></tr><tr><td>train_f1</td><td>0.76941</td></tr><tr><td>train_loss_epoch</td><td>0.49064</td></tr><tr><td>train_loss_step</td><td>0.51009</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.76359</td></tr><tr><td>val_auc</td><td>0.83242</td></tr><tr><td>val_f1</td><td>0.76852</td></tr><tr><td>val_loss_epoch</td><td>0.49678</td></tr><tr><td>val_loss_step</td><td>0.45245</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_32_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/z2s6nqjn' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/z2s6nqjn</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240104_221516-z2s6nqjn\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c09025be01da4b708dc72e5e5793c5da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01693333333435779, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240104_224652-08i85vcw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/08i85vcw' target=\"_blank\">MLP_3_32_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/08i85vcw' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/08i85vcw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 2.6 K \n",
      "2 | head        | Sequential       | 562   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "3.1 K     Trainable params\n",
      "0         Non-trainable params\n",
      "3.1 K     Total params\n",
      "0.012     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69d1ae0f9ad342ee9ee6fddec2d85d89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▆▆▆▆▆▆▇▇▆▇▇▇▇▇██▇█▇████▇█▇▇▇▇▇██▇▇▇█▇▇</td></tr><tr><td>train_auc</td><td>▁▂▅▆▆▆▆▇▆▇▇▇▇▇▇▇▇▇▇█▇████▇█▇█▇▇█████████</td></tr><tr><td>train_f1</td><td>▁▂▆▆▆▅▆▆▇▆▆▇▇▇▆▇█▇▇█▇████▇█▇▇▇▇▇▇█▇▇▇█▇█</td></tr><tr><td>train_loss_epoch</td><td>█▇▅▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▁▂▁▂▁▂▂▂▁▁▂▁▂▁▁▁</td></tr><tr><td>train_loss_step</td><td>███▆▅▅▅▄▆▅▄▆▄▃▄▆▄▄▅▅▃▄▂▄▃▃▃▆▄▆▁▅▅▃▂▃▃▂▂▄</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁█▃▃████████████▁▁▁▁▁▁▅▁▅▅▅▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>▁▅▅▅▇▇▅▇▇▇█▇█▇▇▇▇▇▇▇▇▇██▇█▇▇██████▇▇█▇█▇</td></tr><tr><td>val_f1</td><td>▇█▁▁████████████▇▇▇▇▇▇█▇███▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>val_loss_epoch</td><td>▄▄▂▂▁▁▁▁▁▁▂▂▂▂▂▃▃▃▄▄▄▄▂▃▃▃▅▄▅▅▆▆▆▇▆█▇█▆▆</td></tr><tr><td>val_loss_step</td><td>▄▄▂▂▂▂▂▁▁▂▃▂▃▃▂▂▄▄▃▃▅▅▁▂▂▂▆▃▆▆▇▄▄▇▄█▆▇▃▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.7379</td></tr><tr><td>train_auc</td><td>0.79095</td></tr><tr><td>train_f1</td><td>0.74306</td></tr><tr><td>train_loss_epoch</td><td>0.54494</td></tr><tr><td>train_loss_step</td><td>0.55407</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.487</td></tr><tr><td>val_auc</td><td>0.746</td></tr><tr><td>val_f1</td><td>0.65501</td></tr><tr><td>val_loss_epoch</td><td>0.71489</td></tr><tr><td>val_loss_step</td><td>0.71701</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_32_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/08i85vcw' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/08i85vcw</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240104_224652-08i85vcw\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da81763d0dd049608dca488e832a4ae8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240104_231849-a043tia8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/a043tia8' target=\"_blank\">MLP_3_32_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/a043tia8' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/a043tia8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 2.6 K \n",
      "2 | head        | Sequential       | 562   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "3.1 K     Trainable params\n",
      "0         Non-trainable params\n",
      "3.1 K     Total params\n",
      "0.012     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "053d742fe0224d9f909b988f10146878",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▄▅▆▇▆▆▇▇▇▇▇█▇▇▇▇▇██▇▇▇▇█▇█▇▇█▇█▇██▇▇█▇</td></tr><tr><td>train_auc</td><td>▁▂▂▄▅▆▆▅▆▆▆▆▅▇▇▆▇▇▆▇▇▇▆▇▇▇██▇▇█▇▇▇█▇▇▇▇█</td></tr><tr><td>train_f1</td><td>▁▂▄▄▅▆▆▆▇▇▇▇▆█▇▇▇▇██▇█▇█▇█▇█▇▇███▇██▇███</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▄▄▄▃▂▂▂▂▂▂▃▂▂▂▂▁▁▂▁▁▂▁▂▂▂▂▂▂▂▁▂▂▂▁▁▁▁▁▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▁▄▄▆▇▇▆▇▇▇▇▇▇▇▇██▇██▆▇█▇██▇██▇▇██▆▇█▇██</td></tr><tr><td>val_auc</td><td>▁▅▇▆▇██████▇████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▂▅▅▇▇▇▆▇▇▇▇██▇▇██▇██▆██▇██▇███▇██▆█████</td></tr><tr><td>val_loss_epoch</td><td>█▇▅▅▄▃▂▃▂▂▂▂▂▂▂▂▁▂▂▁▁▃▁▁▁▁▁▁▂▁▁▂▁▂▃▁▁▁▁▁</td></tr><tr><td>val_loss_step</td><td>█▆▆▅▄▄▂▄▂▃▂▁▂▃▂▂▂▃▂▂▂▄▁▂▁▃▃▂▃▃▂▃▂▂▅▃▂▃▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.76269</td></tr><tr><td>train_auc</td><td>0.75961</td></tr><tr><td>train_f1</td><td>0.77314</td></tr><tr><td>train_loss_epoch</td><td>0.50794</td></tr><tr><td>train_loss_step</td><td>0.54234</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.77541</td></tr><tr><td>val_auc</td><td>0.85206</td></tr><tr><td>val_f1</td><td>0.75325</td></tr><tr><td>val_loss_epoch</td><td>0.48278</td></tr><tr><td>val_loss_step</td><td>0.44896</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_32_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/a043tia8' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/a043tia8</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240104_231849-a043tia8\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfda080515984a20b3e409b577e2b764",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01691666666495924, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240104_235027-svupx0eq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/svupx0eq' target=\"_blank\">MLP_3_32_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/svupx0eq' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/svupx0eq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 2.6 K \n",
      "2  | head        | Sequential       | 562   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 33    \n",
      "--------------------------------------------------\n",
      "3.2 K     Trainable params\n",
      "0         Non-trainable params\n",
      "3.2 K     Total params\n",
      "0.013     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25c07c95353e4b588e682ac3a1fe0286",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▃▆▇▇▇▇███▇█▇▇▇██▇████▇██▇████████▇█████</td></tr><tr><td>train_auc</td><td>▁▃▆▇▇▇██████████████████████████████████</td></tr><tr><td>train_f1</td><td>▁▅▅▇▇▇▇█▇█▇█▇▇▇██▇████▇██▇██████████████</td></tr><tr><td>train_loss_epoch</td><td>██▅▄▃▂▂▂▂▂▁▃▂▂▂▂▂▂▂▂▁▂▁▁▁▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>██▆▃▃▄▄▂▃▄▄▃▃▂▄▁▁▄▂▁▃▃▃▃▂▄▂▂▄▂▃▃▂▃▃▄▃▂▃▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▃▄▁▇▇▆▆▇▇▆▆▆██▆▇▇▇▆▇█▇██▇▇▇▇▆▇█▇█▇▇▇███▇</td></tr><tr><td>val_auc</td><td>▅▃▁▅▆▇▇▇▇▇▇▇▇▇██████████████████████████</td></tr><tr><td>val_f1</td><td>▂▅▁▇▇▇▇▇█▇▇▆██▇▇▆█▆▇█▇██▇█▇▇▆▇█▇▇▇▇▇███▇</td></tr><tr><td>val_loss_epoch</td><td>█▇▅▃▂▂▂▂▂▁▂▂▁▂▂▁▁▂▁▁▁▂▂▂▁▂▁▁▁▂▁▁▁▁▁▁▂▁▂▁</td></tr><tr><td>val_loss_step</td><td>█▇▅▄▃▅▄▃▄▃▄▃▂▃▂▃▂▂▂▃▃▄▄▃▃▄▃▃▂▅▃▃▃▂▃▂▄▁▂▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.78453</td></tr><tr><td>train_auc</td><td>0.85332</td></tr><tr><td>train_f1</td><td>0.78791</td></tr><tr><td>train_loss_epoch</td><td>0.48068</td></tr><tr><td>train_loss_step</td><td>0.50685</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.7565</td></tr><tr><td>val_auc</td><td>0.83426</td></tr><tr><td>val_f1</td><td>0.75534</td></tr><tr><td>val_loss_epoch</td><td>0.50058</td></tr><tr><td>val_loss_step</td><td>0.50298</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_32_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/svupx0eq' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/svupx0eq</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240104_235027-svupx0eq\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f28fd38b7a4b4835bc1fca962b6c0ce5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240105_002237-t7o0c9mf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/t7o0c9mf' target=\"_blank\">MLP_3_32_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/t7o0c9mf' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/t7o0c9mf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 2.6 K \n",
      "2  | head        | Sequential       | 562   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 1.1 K \n",
      "--------------------------------------------------\n",
      "4.2 K     Trainable params\n",
      "0         Non-trainable params\n",
      "4.2 K     Total params\n",
      "0.017     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b279af2d93ba463e96f016724001a10a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▃▆▆▇█▇█▇▇██▇██████████▇█████▇████████▇█</td></tr><tr><td>train_auc</td><td>▁▃▆▇████████████████████████████████████</td></tr><tr><td>train_f1</td><td>▁▄▆▆▇▇▇▇▇▇██▇█████▇████▇██▇███████████▇█</td></tr><tr><td>train_loss_epoch</td><td>██▄▃▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▂▁▂▂▁▂▁▁▂▁▁▁▁▁▂▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▇▄▃▅▃▃▄▅▄▃▄▃▂▃▃▃▂▃▃▂▃▄▁▃▂▃▃▃▂▂▃▄▂▂▃▃▄▄▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▃▄▁▅▄▇▇▇▆▆▇▅▇▇▇█▇▆█████▇▇▇▇█▇▅▇▇▇█▇█▇██▇</td></tr><tr><td>val_auc</td><td>▄▃▁▄▆▇▇▇▇▇▇▇▇███████████████████████████</td></tr><tr><td>val_f1</td><td>▅▄▂▅▁▆▅▇▄▇▆▃▆▆▆█▆▅█▇▇▇▇██▆▇▇▆▂▆▇▇█▅█▆▇█▆</td></tr><tr><td>val_loss_epoch</td><td>█▇▅▃▃▂▂▂▂▂▂▂▂▁▁▂▂▁▂▂▁▁▁▂▂▁▁▁▁▃▁▁▂▂▁▁▁▂▁▁</td></tr><tr><td>val_loss_step</td><td>█▇▆▅▄▃▂▄▃▄▃▄▃▂▃▃▃▃▃▃▁▃▂▅▄▂▃▃▂▄▂▂▄▃▂▂▂▃▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.7745</td></tr><tr><td>train_auc</td><td>0.84978</td></tr><tr><td>train_f1</td><td>0.77687</td></tr><tr><td>train_loss_epoch</td><td>0.48105</td></tr><tr><td>train_loss_step</td><td>0.46939</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.74704</td></tr><tr><td>val_auc</td><td>0.83063</td></tr><tr><td>val_f1</td><td>0.73839</td></tr><tr><td>val_loss_epoch</td><td>0.49855</td></tr><tr><td>val_loss_step</td><td>0.4688</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_32_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/t7o0c9mf' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/t7o0c9mf</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240105_002237-t7o0c9mf\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dce737bea604530b8c54446d64a186d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.0171999999981684, max=1.0))…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240105_005528-8srxqodb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/8srxqodb' target=\"_blank\">MLP_3_64_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/8srxqodb' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/8srxqodb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 9.2 K \n",
      "2 | head        | Sequential       | 2.1 K \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "11.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "11.4 K    Total params\n",
      "0.045     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d5ae041f1b146328153ab4d1713dd28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▄▇▇▇▇██████████▇▇███▇▇▇█▇██████████▇▇██</td></tr><tr><td>train_auc</td><td>▁▄▇▇▇███████████████████████████████████</td></tr><tr><td>train_f1</td><td>▁▄▆▇▇▇█▇███▇█▇██▇▇███▇▇▇█▇▇▇▇██▇▇▇█▇█▇██</td></tr><tr><td>train_loss_epoch</td><td>█▇▃▃▃▂▂▂▁▁▁▂▁▂▁▁▂▂▁▂▁▂▁▂▁▂▂▁▂▁▂▂▁▂▁▁▂▂▁▁</td></tr><tr><td>train_loss_step</td><td>█▆▆▃▄▄▄▂▃▃▂▄▂▃▄▂▃▁▂▃▂▂▃▃▃▂▄▂▂▂▂▃▂▂▄▁▃▂▃▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▆▆▆▇███████████▇█▇███▇█▇▇▇█████▇█▇█▇▇██</td></tr><tr><td>val_auc</td><td>▅▁▃▅▅▆▇▇█▇██████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▇█▇██████████████████▇█▇█▇███████▇█▇███</td></tr><tr><td>val_loss_epoch</td><td>█▅▄▃▂▂▂▁▁▂▂▁▂▂▂▁▃▂▁▁▁▁▂▂▂▂▂▂▂▁▁▂▁▂▁▂▂▂▂▂</td></tr><tr><td>val_loss_step</td><td>█▅▄▄▄▃▄▂▂▄▄▂▂▄▄▁▃▄▂▃▂▃▄▂▂▄▂▃▃▁▁▄▃▄▂▂▅▂▄▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.77627</td></tr><tr><td>train_auc</td><td>0.85472</td></tr><tr><td>train_f1</td><td>0.77978</td></tr><tr><td>train_loss_epoch</td><td>0.46982</td></tr><tr><td>train_loss_step</td><td>0.44991</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.75414</td></tr><tr><td>val_auc</td><td>0.83361</td></tr><tr><td>val_f1</td><td>0.75238</td></tr><tr><td>val_loss_epoch</td><td>0.51301</td></tr><tr><td>val_loss_step</td><td>0.56106</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_64_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/8srxqodb' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/8srxqodb</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240105_005528-8srxqodb\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cebc6ba227974452a32b2804302a20ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016933333330477276, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240105_012822-x8x61udm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/x8x61udm' target=\"_blank\">MLP_3_64_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/x8x61udm' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/x8x61udm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 9.2 K \n",
      "2 | head        | Sequential       | 2.1 K \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "11.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "11.4 K    Total params\n",
      "0.045     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇█▇▇▇▇██▇▇▇██▇▇█▇███▇██</td></tr><tr><td>train_auc</td><td>▁▂▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇█▇▇▇▇██▇▇█▇█▇██▇████▇█</td></tr><tr><td>train_f1</td><td>▁▄▅▆▆▆▆▇▆▇▇▆▆▇▇▇▇▇█▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇█▇██</td></tr><tr><td>train_loss_epoch</td><td>█▇▅▅▄▄▃▃▂▂▂▂▃▂▂▂▂▂▁▂▂▂▂▂▁▂▂▁▁▁▂▂▂▂▁▁▂▂▂▁</td></tr><tr><td>train_loss_step</td><td>██▇▅▅▆▅▄▄▃▃▅▅▃▅▂▃▃▃▂▂▂▂▂▅▃▃▄▄▃▃▃▃▃▄▃▂▃▂▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▂████████▅▅▁▁▅▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>▃▃▂▁▂▆█▇▆▆▇▆▇▆▆▆▆▆▇▆▆▆▆▆▆██▆▆▆▆▆▆▆▆▆▆▆▆▆</td></tr><tr><td>val_f1</td><td>▁██████████▇▇█▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>val_loss_epoch</td><td>▂▂▁▁▁▁▁▁▁▂▂▂▃▂▃▃▃▄▅▅▅▅▅▅▆▆▅▇▇▇▆▇▇▇▇█▆▇▆▇</td></tr><tr><td>val_loss_step</td><td>▂▂▁▁▁▁▁▁▁▂▃▃▃▃▃▃▃▄▄▅▅▅▆▆▅▇▅▆▇▅▅▇▆█▇▇▆▆▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.76682</td></tr><tr><td>train_auc</td><td>0.81153</td></tr><tr><td>train_f1</td><td>0.77416</td></tr><tr><td>train_loss_epoch</td><td>0.49553</td></tr><tr><td>train_loss_step</td><td>0.45445</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.487</td></tr><tr><td>val_auc</td><td>0.74082</td></tr><tr><td>val_f1</td><td>0.65501</td></tr><tr><td>val_loss_epoch</td><td>0.94326</td></tr><tr><td>val_loss_step</td><td>0.9703</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_64_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/x8x61udm' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/x8x61udm</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240105_012822-x8x61udm\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdafda4efa3943e0a7dc89c0b1ab77dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016933333330477276, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240105_015900-w86z0zos</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/w86z0zos' target=\"_blank\">MLP_3_64_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/w86z0zos' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/w86z0zos</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 9.2 K \n",
      "2 | head        | Sequential       | 2.1 K \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "11.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "11.4 K    Total params\n",
      "0.045     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d89ee252838429abbed6af3b0325f0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▂▅▆▆▇▇▇▇█▇█████▇▇██▇██████████████████</td></tr><tr><td>train_auc</td><td>▁▁▃▅▇▇▇▇▇▇█▇████▇█▇████▇█▇█████████▇████</td></tr><tr><td>train_f1</td><td>▃▁▂▅▆▇▇▇▇▇▇▇█████▇██████████████████████</td></tr><tr><td>train_loss_epoch</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▄▃▂▂▂▂▂▂▂▁▂▂▂▂▁▂▁▁▂▂▁▂▂▂▂▂▁▁▁▁▁▂▁▂▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▁▃▄▅▆▇▆▆▇▇▇█▇▇▇█▇▇▇▇█▇█████████████████</td></tr><tr><td>val_auc</td><td>▁▅▆▆▇▇▇▇█▇▇▇▇▇▇▇▇▇██▇█████▇▇▇███████▇▇█▇</td></tr><tr><td>val_f1</td><td>▁▂▄▅▆▇▇▇▇▇███▇█████▇██▇█████████████████</td></tr><tr><td>val_loss_epoch</td><td>█▃▃▂▂▂▂▁▂▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁</td></tr><tr><td>val_loss_step</td><td>█▄▄▃▂▂▂▂▂▂▂▂▁▂▂▁▂▃▁▂▁▂▂▁▁▂▁▂▂▁▁▂▂▂▁▂▂▁▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.77863</td></tr><tr><td>train_auc</td><td>0.80949</td></tr><tr><td>train_f1</td><td>0.7798</td></tr><tr><td>train_loss_epoch</td><td>0.47021</td></tr><tr><td>train_loss_step</td><td>0.47716</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.77305</td></tr><tr><td>val_auc</td><td>0.85083</td></tr><tr><td>val_f1</td><td>0.77033</td></tr><tr><td>val_loss_epoch</td><td>0.49952</td></tr><tr><td>val_loss_step</td><td>0.56778</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_64_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/w86z0zos' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/w86z0zos</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240105_015900-w86z0zos\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "100dc209564d4646a544c14fc8b15b66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01691666666883975, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240105_023216-7rgri4jv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/7rgri4jv' target=\"_blank\">MLP_3_64_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/7rgri4jv' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/7rgri4jv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 9.2 K \n",
      "2  | head        | Sequential       | 2.1 K \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 65    \n",
      "--------------------------------------------------\n",
      "11.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "11.4 K    Total params\n",
      "0.046     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8216d70ca6548bf81c9689f3678b3b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▆▆▇▇▇██▇█▇██▇█▇████▇██▇█▇▇█████▇▇▇█████</td></tr><tr><td>train_auc</td><td>▁▆▇▇▇▇███████████████████████████▇██████</td></tr><tr><td>train_f1</td><td>▁▆▆▇▇▇██▇█▇██▇█▇████▇██▇█▇▇█████▇▇▇█████</td></tr><tr><td>train_loss_epoch</td><td>█▅▄▃▂▃▂▂▂▂▂▂▂▂▁▂▂▁▂▁▂▂▁▂▂▂▂▁▁▁▂▁▂▂▂▁▁▂▁▁</td></tr><tr><td>train_loss_step</td><td>█▅▄▄▃▂▃▂▂▃▃▃▃▂▂▂▄▁▂▂▃▁▅▂▄▃▃▂▃▁▃▃▃▁▃▃▂▁▂▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▂▇▄█▇█▆▅▇█▇▇▆███▇▇▆▇▆█▇▇▇▆▆▇█▆▆█▇▆▆▆▇▇▇</td></tr><tr><td>val_auc</td><td>▃▁▅▆▆▆▇▇▇▇▇▇▇█████████████▇▇█████▇█▇▇▇▇▇</td></tr><tr><td>val_f1</td><td>▁▄▇▅█▇█▆▆▇█▇██████▇▇▇██▇▇█▇▆▇█▇██▇█▇▇▇██</td></tr><tr><td>val_loss_epoch</td><td>█▄▃▃▂▂▂▂▂▂▂▂▂▃▂▂▁▂▁▂▁▃▁▁▂▂▂▂▂▁▂▃▁▁▂▂▂▁▁▂</td></tr><tr><td>val_loss_step</td><td>█▅▃▄▄▃▄▄▃▄▃▃▂▄▄▃▃▃▂▃▃▅▂▂▃▄▃▄▄▂▃▅▂▂▂▂▃▁▃▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.78512</td></tr><tr><td>train_auc</td><td>0.8495</td></tr><tr><td>train_f1</td><td>0.7896</td></tr><tr><td>train_loss_epoch</td><td>0.47539</td></tr><tr><td>train_loss_step</td><td>0.4276</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.74941</td></tr><tr><td>val_auc</td><td>0.82799</td></tr><tr><td>val_f1</td><td>0.74882</td></tr><tr><td>val_loss_epoch</td><td>0.51644</td></tr><tr><td>val_loss_step</td><td>0.55659</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_64_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/7rgri4jv' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/7rgri4jv</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240105_023216-7rgri4jv\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7f7147d997e439bb94c8e350ac9ecae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016933333330477276, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240105_030155-dyhxov4d</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/dyhxov4d' target=\"_blank\">MLP_3_64_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/dyhxov4d' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/dyhxov4d</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 9.2 K \n",
      "2  | head        | Sequential       | 2.1 K \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 4.2 K \n",
      "--------------------------------------------------\n",
      "15.6 K    Trainable params\n",
      "0         Non-trainable params\n",
      "15.6 K    Total params\n",
      "0.062     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0c24e6f690b448b9939f700a45019ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▆▇▇▇██▇█████████████▇█████▇█▇██████████</td></tr><tr><td>train_auc</td><td>▁▆▇█████████████████████████████████████</td></tr><tr><td>train_f1</td><td>▁▆▇█▇███████████████████████████████████</td></tr><tr><td>train_loss_epoch</td><td>█▅▃▂▂▂▁▂▁▁▂▁▁▂▁▁▁▁▂▁▁▂▁▁▁▂▂▂▁▂▁▂▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▅▄▂▃▄▁▁▂▃▃▃▃▃▃▃▁▃▂▃▂▂▃▄▂▃▃▄▂▄▂▂▁▃▂▃▂▂▃▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▆▄▅▆▇▇▇▆▇▇▇▇▅▇▇▇▇▇▇▆█▇▆▇█▇▆▇▇▇█▇▇█▆▇█▇▇</td></tr><tr><td>val_auc</td><td>▄▁▃▅▆▇▇▇▇▇▇▇██▇▇▇█████████████▇█████████</td></tr><tr><td>val_f1</td><td>▂▃▄▅▅▃▅▅▁▆▅▂▄▅▂▅▃▄▄▆▂▆▅▁▅█▇▁▅▇▄▇▄▄▇▁▅█▇▅</td></tr><tr><td>val_loss_epoch</td><td>█▃▃▃▂▁▂▁▂▂▂▂▁▃▂▁▂▁▁▁▁▂▁▁▂▂▂▂▁▂▂▂▂▂▂▂▂▂▂▁</td></tr><tr><td>val_loss_step</td><td>█▄▄▅▄▁▃▂▃▃▄▄▁▄▄▂▃▂▂▂▂▄▁▁▄▃▂▃▂▂▄▄▄▃▄▅▄▃▃▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.77509</td></tr><tr><td>train_auc</td><td>0.85032</td></tr><tr><td>train_f1</td><td>0.77732</td></tr><tr><td>train_loss_epoch</td><td>0.48194</td></tr><tr><td>train_loss_step</td><td>0.47329</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.75414</td></tr><tr><td>val_auc</td><td>0.8335</td></tr><tr><td>val_f1</td><td>0.7512</td></tr><tr><td>val_loss_epoch</td><td>0.49864</td></tr><tr><td>val_loss_step</td><td>0.48922</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_64_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/dyhxov4d' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/dyhxov4d</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240105_030155-dyhxov4d\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f36cb6105c946ee993252ffe9a4d056",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016933333330477276, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240105_033138-tr2pdk7x</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/tr2pdk7x' target=\"_blank\">MLP_4_16_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/tr2pdk7x' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/tr2pdk7x</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 1.0 K \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "1.2 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.2 K     Total params\n",
      "0.005     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "917057e9c10c4212b26b992d3279dec3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▁▂▃▇▆▇▇▇▇▇██▇█▇███▇█▇███████████████▇█</td></tr><tr><td>train_auc</td><td>▁▁▂▂▃▆▇▇▇██▇████████████████████████████</td></tr><tr><td>train_f1</td><td>▁▅▄▄▆▇▆▇▇▇▇▇██████████████████████████▇█</td></tr><tr><td>train_loss_epoch</td><td>███▇▇▅▄▃▃▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▂▂▂▂▂▂▂</td></tr><tr><td>train_loss_step</td><td>████▇▅▅▃▃▄▃▃▄▂▄▄▄▁▂▄▃▃▃▃▄▃▃▃▂▄▂▃▃▃▃▃▃▃▂▄</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▁▁▂▆▅▆▇▇▇█▇█████████████████▇█████▇▇███</td></tr><tr><td>val_auc</td><td>▁▄▅▅▅▄▆▇▇▇▇▇▇▇▇█████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▂█▆▇█████████████████████████████████</td></tr><tr><td>val_loss_epoch</td><td>████▇▅▄▃▂▂▂▂▂▁▂▁▂▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁</td></tr><tr><td>val_loss_step</td><td>████▇▅▅▄▂▃▂▁▃▂▂▂▂▂▃▂▃▂▂▃▂▃▃▂▁▁▃▃▃▃▂▂▁▂▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.75266</td></tr><tr><td>train_auc</td><td>0.81321</td></tr><tr><td>train_f1</td><td>0.75222</td></tr><tr><td>train_loss_epoch</td><td>0.53573</td></tr><tr><td>train_loss_step</td><td>0.56007</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.75414</td></tr><tr><td>val_auc</td><td>0.82667</td></tr><tr><td>val_f1</td><td>0.74879</td></tr><tr><td>val_loss_epoch</td><td>0.51609</td></tr><tr><td>val_loss_step</td><td>0.54554</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_16_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/tr2pdk7x' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/tr2pdk7x</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240105_033138-tr2pdk7x\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ab09375b24343348f3220700f4d149c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01691666666495924, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240105_040309-v4wbig20</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/v4wbig20' target=\"_blank\">MLP_4_16_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/v4wbig20' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/v4wbig20</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 1.0 K \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "1.2 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.2 K     Total params\n",
      "0.005     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▂▄▅▆▅▅▆▆▇▅▇▇▇▇▆▇▇▇▇▇▇▆▇▇▇▇▇▇▇▇▇█▇▇▇▇▇█</td></tr><tr><td>train_auc</td><td>▁▁▂▃▅▆▆▆▇▇▇▆▇▆▆▇▆▆▆▆▆▇▇▆▇▇▇▇▇▆▇▇▇█▆▇▇▇██</td></tr><tr><td>train_f1</td><td>▁▄▄▅▅▇▆▆▆▇▇▅▇▇█▇▇█▇▇██▇▇▇▇▇▇▇▇▇▇▇█▇▇▇█▇█</td></tr><tr><td>train_loss_epoch</td><td>█▇▇▆▅▅▄▄▄▄▂▃▂▃▂▂▃▂▂▂▃▂▂▂▂▂▃▃▃▂▂▂▃▁▂▂▃▂▂▂</td></tr><tr><td>train_loss_step</td><td>███▆▆▆▅▄▅▅▆▅▅▃▄▆▄▂▄▃▄▂▅▄▃▃▅▃▁▄▃▃▄▄▄▃▄▁▃▅</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>▃▆▃▄▄▄▁▄▄▄▆▄▄▄▇▇▆▅▅▅▅▇▅▇▅▅▅███▇█▇█▇▇▇▇▇▇</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_epoch</td><td>█▇▇▆▅▆▅▆▇▅▂▄▃▃▁▂▃▃▃▂▃▂▂▄▃▂▄▂▂▃▃▄▄▂▃▃▃▃▄▃</td></tr><tr><td>val_loss_step</td><td>█▅▇▇▅▅█▆▇▅▃▃▄▅▁▄▄▁▃▁▄▁▁▄▄▄▅▁▃▅▃▅▅▃▃▂▃▄▅▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.71665</td></tr><tr><td>train_auc</td><td>0.7116</td></tr><tr><td>train_f1</td><td>0.70624</td></tr><tr><td>train_loss_epoch</td><td>0.6051</td></tr><tr><td>train_loss_step</td><td>0.64263</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.513</td></tr><tr><td>val_auc</td><td>0.70042</td></tr><tr><td>val_f1</td><td>0.0</td></tr><tr><td>val_loss_epoch</td><td>0.67613</td></tr><tr><td>val_loss_step</td><td>0.67639</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_16_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/v4wbig20' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/v4wbig20</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240105_040309-v4wbig20\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89a58ea480cc4c45b7d9cc1fb47a1623",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01693333333435779, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240105_043234-bxtn8fkf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/bxtn8fkf' target=\"_blank\">MLP_4_16_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/bxtn8fkf' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/bxtn8fkf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 1.0 K \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "1.2 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.2 K     Total params\n",
      "0.005     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "058d114c02e14169be3d9a77a64dbaff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▁▂▂▄▄▅▅▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇███▇████▇▇▇█</td></tr><tr><td>train_auc</td><td>▁▁▂▃▂▅▆▆▇▇█▇██▇▇▆▇▆▆▇▇▆▅▅▆▅▅▅▆▆▆▆▅▄▅▅▄▄▅</td></tr><tr><td>train_f1</td><td>▄▃▃▁▂▄▄▆▆▆▇▇▇▇▇▇▇██▇██▇▇█▇▇▇███▇████▇█▇█</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▅▄▃▃▃▃▂▂▃▂▂▂▁▃▂▂▁▁▂▁▂▂▂▂▂▁▁▂▂▁▁▂▂▁▁▁▁▁▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▃▂▄▅▅▅▇▇▆▇▆██▇▇▇██▇██▅█████▇▇█████████▇</td></tr><tr><td>val_auc</td><td>▄▁▂▄▆▇▆▇▇▇▇▇▇▇▇▇▇▇█▇██▇▇███▇▇▇██████▇▇██</td></tr><tr><td>val_f1</td><td>▅▁▄▄▆▅▇█▇▇█▇██████████▇█▇███████████████</td></tr><tr><td>val_loss_epoch</td><td>█▇▇▇▇▅▆▄▄▄▃▄▃▂▃▃▃▂▂▃▂▂▅▃▁▂▁▁▃▂▂▂▁▂▁▁▂▂▂▃</td></tr><tr><td>val_loss_step</td><td>█▇▇▇▇▆▆▅▄▅▃▃▄▂▃▃▃▃▃▄▃▃▅▄▂▃▂▂▂▂▄▃▃▃▁▃▂▂▃▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.75738</td></tr><tr><td>train_auc</td><td>0.63542</td></tr><tr><td>train_f1</td><td>0.75666</td></tr><tr><td>train_loss_epoch</td><td>0.53659</td></tr><tr><td>train_loss_step</td><td>0.56721</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.75887</td></tr><tr><td>val_auc</td><td>0.83175</td></tr><tr><td>val_f1</td><td>0.7875</td></tr><tr><td>val_loss_epoch</td><td>0.55378</td></tr><tr><td>val_loss_step</td><td>0.57377</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_16_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/bxtn8fkf' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/bxtn8fkf</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240105_043234-bxtn8fkf\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc50c3e3e1c44e2c8069bcc6fea0e002",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240105_050117-wccuci3k</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/wccuci3k' target=\"_blank\">MLP_4_16_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/wccuci3k' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/wccuci3k</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 1.0 K \n",
      "2  | head        | Sequential       | 154   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 17    \n",
      "--------------------------------------------------\n",
      "1.2 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.2 K     Total params\n",
      "0.005     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae82b7ca72194936ba8f321ce17a99d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▁▃▅▇▇▇▇▇█████▇██▇▇████████████████████</td></tr><tr><td>train_auc</td><td>▁▁▁▂▅▇▇▇█▇████████▇█████████████████████</td></tr><tr><td>train_f1</td><td>▁▅▃▅▆▇█▇▇███████████████████████████████</td></tr><tr><td>train_loss_epoch</td><td>████▆▃▂▃▂▂▂▂▁▂▂▂▂▂▂▂▂▁▂▁▂▂▁▁▁▁▁▁▁▁▂▂▁▂▁▁</td></tr><tr><td>train_loss_step</td><td>████▆▅▄▄▂▃▃▄▄▃▃▂▂▃▄▂▁▃▁▂▃▃▁▁▃▂▃▅▆▁▁▃▄▄▂▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▁▁▂▇▇██▇▇▇█▇▇█▆▇▆▇▇▇▇▆▇▇▇▆▇▇▇▇▇▇▇▇▆▇▇▇▆</td></tr><tr><td>val_auc</td><td>▁▄▅▆▇▇██████████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▂█▇██▇▇██▇▇█▇█▆▇▇▇▇▇█▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇</td></tr><tr><td>val_loss_epoch</td><td>████▅▁▁▁▁▂▁▁▂▂▂▃▁▄▂▁▂▂▃▂▃▄▃▂▃▃▂▃▂▂▃▃▃▃▁▄</td></tr><tr><td>val_loss_step</td><td>████▅▂▃▂▁▅▃▃▂▃▃▄▃▃▄▂▃▄▅▃▅▅▅▂▄▆▄▄▁▂▅▃▇▅▃▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.75384</td></tr><tr><td>train_auc</td><td>0.83648</td></tr><tr><td>train_f1</td><td>0.76103</td></tr><tr><td>train_loss_epoch</td><td>0.50814</td></tr><tr><td>train_loss_step</td><td>0.50801</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.69504</td></tr><tr><td>val_auc</td><td>0.82283</td></tr><tr><td>val_f1</td><td>0.61493</td></tr><tr><td>val_loss_epoch</td><td>0.5917</td></tr><tr><td>val_loss_step</td><td>0.59856</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_16_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/wccuci3k' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/wccuci3k</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240105_050117-wccuci3k\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "706e188315944522b1cd0b5d0156a351",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.017183333332650364, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240105_053243-8k7hzgyz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/8k7hzgyz' target=\"_blank\">MLP_4_16_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/8k7hzgyz' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/8k7hzgyz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 1.0 K \n",
      "2  | head        | Sequential       | 154   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 281   \n",
      "--------------------------------------------------\n",
      "1.5 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.5 K     Total params\n",
      "0.006     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0adfb385c5d7431e830d26c36c11fa5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▁▂▄▇▇▇▇▇▇█▇███████████████████████████</td></tr><tr><td>train_auc</td><td>▁▂▂▂▅▇▇▇▇███████████████████████████████</td></tr><tr><td>train_f1</td><td>▁▅▄▄▆▇█▇████████████████████████████████</td></tr><tr><td>train_loss_epoch</td><td>███▇▆▃▂▂▂▂▂▂▁▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▂▁▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>███▇▆▃▃▄▂▄▂▃▃▂▃▂▂▄▂▂▃▂▂▃▁▁▃▂▃▂▃▃▂▁▂▃▂▁▄▄</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▁▁▁▆█▇▇▇█▇▇█▇█▇▇█▇▇▇▇████▇█▇▇██████▇█▇█</td></tr><tr><td>val_auc</td><td>▁▅▆▆▇▇██████████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▇█▇█▇█▇█████████████████████████████</td></tr><tr><td>val_loss_epoch</td><td>████▅▂▂▂▁▁▂▁▂▂▁▁▂▁▁▁▂▂▁▁▁▂▂▂▁▁▁▁▂▁▂▁▂▁▂▁</td></tr><tr><td>val_loss_step</td><td>████▆▄▃▃▂▁▃▂▄▃▂▂▃▂▃▃▄▃▃▂▂▃▄▄▂▂▂▁▅▃▃▃▃▁▅▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.75856</td></tr><tr><td>train_auc</td><td>0.83267</td></tr><tr><td>train_f1</td><td>0.75046</td></tr><tr><td>train_loss_epoch</td><td>0.50791</td></tr><tr><td>train_loss_step</td><td>0.56072</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.75177</td></tr><tr><td>val_auc</td><td>0.82314</td></tr><tr><td>val_f1</td><td>0.75524</td></tr><tr><td>val_loss_epoch</td><td>0.51475</td></tr><tr><td>val_loss_step</td><td>0.51842</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_16_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/8k7hzgyz' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/8k7hzgyz</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240105_053243-8k7hzgyz\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "613b0ce7445d420e8680681144e76fc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240105_060121-d2tvf4gh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/d2tvf4gh' target=\"_blank\">MLP_4_32_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/d2tvf4gh' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/d2tvf4gh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 3.6 K \n",
      "2 | head        | Sequential       | 562   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "4.2 K     Trainable params\n",
      "0         Non-trainable params\n",
      "4.2 K     Total params\n",
      "0.017     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9eb22eaa2fea40a191424f7b31443bd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▂▆▇▇██▇██████████▇▇███████████████████</td></tr><tr><td>train_auc</td><td>▁▂▂▆▇▇██▇██████████▇████████████████████</td></tr><tr><td>train_f1</td><td>▁▂▁▆▇▇█▇▇█▇▇██▇██▇▇▇▇████▇▇██▇████▇█▇███</td></tr><tr><td>train_loss_epoch</td><td>███▅▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▂▂▂▂▁▂▁▁▂▁▁▁▁▂▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>███▃▃▂▂▃▃▃▂▂▂▃▃▁▃▃▃▄▃▃▃▃▂▃▃▃▁▃▃▁▄▂▃▄▃▂▂▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▁▆▆▇▇█▇▇▇▇█▇██████▇█▇██▇▇████▇▇███▇████</td></tr><tr><td>val_auc</td><td>▆▆▁▆▇▇██████████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▇█████████████████████▇█████▇█████████</td></tr><tr><td>val_loss_epoch</td><td>███▅▃▃▂▂▂▂▂▁▁▁▂▂▁▂▁▁▁▁▁▁▂▁▁▁▁▂▂▁▁▁▁▁▁▁▁▂</td></tr><tr><td>val_loss_step</td><td>███▆▅▅▄▃▃▄▄▃▂▂▂▃▂▄▃▂▂▂▁▂▃▃▃▂▂▃▄▂▂▂▂▂▃▂▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.76623</td></tr><tr><td>train_auc</td><td>0.83902</td></tr><tr><td>train_f1</td><td>0.76316</td></tr><tr><td>train_loss_epoch</td><td>0.49541</td></tr><tr><td>train_loss_step</td><td>0.49756</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.76359</td></tr><tr><td>val_auc</td><td>0.83124</td></tr><tr><td>val_f1</td><td>0.77477</td></tr><tr><td>val_loss_epoch</td><td>0.51641</td></tr><tr><td>val_loss_step</td><td>0.50367</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_32_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/d2tvf4gh' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/d2tvf4gh</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240105_060121-d2tvf4gh\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cde55fac48c44f18501bd663a43e423",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016933333330477276, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240105_062933-1y9x8jto</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/1y9x8jto' target=\"_blank\">MLP_4_32_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/1y9x8jto' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/1y9x8jto</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 3.6 K \n",
      "2 | head        | Sequential       | 562   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "4.2 K     Trainable params\n",
      "0         Non-trainable params\n",
      "4.2 K     Total params\n",
      "0.017     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7984722d9ed542a58d6a9c11f47134af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▃▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇█▇▇███▇██▇▇██</td></tr><tr><td>train_auc</td><td>▁▂▃▆▆▆▇▇▇▇▇▇▇▇▇▇█▇▇█▇▇▇▇▇█▇██████▇██████</td></tr><tr><td>train_f1</td><td>▁▃▄▆▆▅▆▆▇▆▇▆▇▇▆▆▆▆▆▇▇▇▇▇▇█▇▇▇▇▇██▇▇█▇▇▇▇</td></tr><tr><td>train_loss_epoch</td><td>█▇▇▅▄▄▄▄▃▄▃▂▃▃▃▂▂▂▂▂▂▂▂▂▂▁▂▂▁▁▁▁▁▂▁▂▂▂▁▁</td></tr><tr><td>train_loss_step</td><td>███▅▅▄▅▄▅▅▃▅▄▅▃▅▄▄▃▃▃▅▄▁▄▄▄▂▁▃▃▂▄▄▄▃▄▄▄▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▁█▂▂▃▃█████████████████████████████████</td></tr><tr><td>val_auc</td><td>▁▅▇▇▃▂▅▅▄▄▄▇▆█▆▆▇▅▆▆█▆▇▇▅▅▅▅▇▅▇▇▆▅▅▅▅▅▅▅</td></tr><tr><td>val_f1</td><td>▇▇█▁▁▂▂█████████████████████████████████</td></tr><tr><td>val_loss_epoch</td><td>██▇▅▅▄▅▄▄▄▄▄▄▄▄▅▄▅▄▄▄▄▄▄▃▄▃▄▅▄▃▁▂▃▄▁▂▂▂▄</td></tr><tr><td>val_loss_step</td><td>██▇▇▇▆▆▆▆▆▆▆▆▆▆▇▇▆▇▆▅▅▇▆▆▇▅▅█▇▅▄▃█▄▁▄▅▄▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.74026</td></tr><tr><td>train_auc</td><td>0.79824</td></tr><tr><td>train_f1</td><td>0.73171</td></tr><tr><td>train_loss_epoch</td><td>0.5445</td></tr><tr><td>train_loss_step</td><td>0.49318</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.68558</td></tr><tr><td>val_auc</td><td>0.71751</td></tr><tr><td>val_f1</td><td>0.74374</td></tr><tr><td>val_loss_epoch</td><td>0.65355</td></tr><tr><td>val_loss_step</td><td>0.66022</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_32_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/1y9x8jto' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/1y9x8jto</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240105_062933-1y9x8jto\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c8a1a5f59524bf8abf33f773875340a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016933333330477276, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240105_065758-32obut5v</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/32obut5v' target=\"_blank\">MLP_4_32_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/32obut5v' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/32obut5v</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 3.6 K \n",
      "2 | head        | Sequential       | 562   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "4.2 K     Trainable params\n",
      "0         Non-trainable params\n",
      "4.2 K     Total params\n",
      "0.017     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "178d66a826d24b82819697126448380e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▂▂▃▅▆▅▆▇▆▆▇▇▇▇▇▇▇▇▇▇███▇█▇██▇███▇█▇██▇</td></tr><tr><td>train_auc</td><td>▁▂▂▂▃▄▅▆▅▆▆▅▇▇▇▇▇▇▇▇▆▇▇▇█▇██████████▇███</td></tr><tr><td>train_f1</td><td>▁▁▃▄▅▅▆▅▇▆▆▆▇▇▇▇▇▇▇▇▇████▇████▇█████████</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▂▂▂▂▁▂▂▁▂▂▁▂▂▂▂▁▂▁▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▄▁▁▁▄▄▃▇▅█▇▇▇▇▇▇▇▇▇▇▇▇▇▇▆▇▇▇▇▇▇▇▇███▇█▇█</td></tr><tr><td>val_auc</td><td>▅▁▁▃▅▅▆▇▇▇█▇▇█▇▇▇██████████████████▇███▇</td></tr><tr><td>val_f1</td><td>▆▁▂▁▅▅▄█▅█▇█▇▇█▇▇▇▇█▇▇█▇▇█▇█▇█▇▇████████</td></tr><tr><td>val_loss_epoch</td><td>▇█▇▇▇▆▆▄▄▃▃▃▂▂▂▂▁▂▂▂▂▂▁▁▂▂▁▁▁▁▂▁▁▂▁▂▁▁▁▂</td></tr><tr><td>val_loss_step</td><td>▇█▇█▇▇▇▆▅▅▄▄▂▂▂▃▁▄▃▂▂▃▁▂▃▃▃▂▁▂▄▂▂▃▃▂▂▁▁▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.7562</td></tr><tr><td>train_auc</td><td>0.77323</td></tr><tr><td>train_f1</td><td>0.7572</td></tr><tr><td>train_loss_epoch</td><td>0.49966</td></tr><tr><td>train_loss_step</td><td>0.49687</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.79196</td></tr><tr><td>val_auc</td><td>0.83209</td></tr><tr><td>val_f1</td><td>0.80444</td></tr><tr><td>val_loss_epoch</td><td>0.51251</td></tr><tr><td>val_loss_step</td><td>0.50587</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_32_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/32obut5v' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/32obut5v</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240105_065758-32obut5v\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67e91e63a48742f58c7de98e465b7358",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01693333333435779, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240105_072627-8zbnus3o</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/8zbnus3o' target=\"_blank\">MLP_4_32_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/8zbnus3o' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/8zbnus3o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 3.6 K \n",
      "2  | head        | Sequential       | 562   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 33    \n",
      "--------------------------------------------------\n",
      "4.2 K     Trainable params\n",
      "0         Non-trainable params\n",
      "4.2 K     Total params\n",
      "0.017     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "139378b109874370a1c14b6c3acdca5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▃▆▆▇▇▇▇▇▇▇▇██████▇██▇█████████████▇███</td></tr><tr><td>train_auc</td><td>▁▁▃▆▇▇▇█▇███████████████████████████████</td></tr><tr><td>train_f1</td><td>▁▂▁▄▆▇▇▇▇▇▇▇▇▇▇▇▇█▇▇▇█▇▇█▇▇█▇███████▇▇██</td></tr><tr><td>train_loss_epoch</td><td>███▆▄▃▂▂▃▂▂▂▂▂▂▂▁▂▁▁▂▁▂▂▁▂▂▂▂▂▁▁▁▂▁▁▂▂▁▁</td></tr><tr><td>train_loss_step</td><td>███▅▄▄▂▃▃▂▁▃▂▄▂▃▂▅▄▂▂▃▂▂▂▂▄▂▂▁▃▃▃▅▂▁▄▂▂▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▂▁▇▅▇▇█▇▇▇▇█▇██▇▇██▇█▇▇█▇██▇██████▇█████</td></tr><tr><td>val_auc</td><td>▅▆▁▆▇▇██████████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▇█▅████▇█▇█▇██████▇████▇███████▇███████</td></tr><tr><td>val_loss_epoch</td><td>██▇▆▃▃▂▃▂▃▂▂▂▂▂▁▂▁▂▂▂▂▁▂▃▁▂▂▁▂▁▁▂▂▂▁▂▂▂▁</td></tr><tr><td>val_loss_step</td><td>███▇▄▅▂▃▃▃▄▄▄▃▃▃▃▁▃▄▃▄▁▅▅▂▃▄▂▃▂▃▃▄▄▂▄▃▄▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.78217</td></tr><tr><td>train_auc</td><td>0.85077</td></tr><tr><td>train_f1</td><td>0.78408</td></tr><tr><td>train_loss_epoch</td><td>0.48212</td></tr><tr><td>train_loss_step</td><td>0.46571</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.75414</td></tr><tr><td>val_auc</td><td>0.82638</td></tr><tr><td>val_f1</td><td>0.75472</td></tr><tr><td>val_loss_epoch</td><td>0.50854</td></tr><tr><td>val_loss_step</td><td>0.50381</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_32_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/8zbnus3o' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/8zbnus3o</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240105_072627-8zbnus3o\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19a1fba52a904a9e8e5131d9c9f6ce32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01691666666883975, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240105_075443-cv0fsgh2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/cv0fsgh2' target=\"_blank\">MLP_4_32_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/cv0fsgh2' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/cv0fsgh2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 3.6 K \n",
      "2  | head        | Sequential       | 562   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 1.1 K \n",
      "--------------------------------------------------\n",
      "5.3 K     Trainable params\n",
      "0         Non-trainable params\n",
      "5.3 K     Total params\n",
      "0.021     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▄▆▇▇▇▇███▇▇█████▇▇██████████████████▇█</td></tr><tr><td>train_auc</td><td>▁▁▅▆▇██▇████████████████████████████████</td></tr><tr><td>train_f1</td><td>▆▁▆▆███▇████████████████████████████████</td></tr><tr><td>train_loss_epoch</td><td>██▇▅▃▂▂▃▂▂▂▂▂▂▂▂▁▁▂▂▂▂▂▁▂▁▁▁▂▁▁▁▁▁▁▂▁▁▁▂</td></tr><tr><td>train_loss_step</td><td>███▅▄▃▃▅▄▂▄▄▃▃▃▄▄▄▃▃▃▂▃▃▄▁▂▂▃▄▃▄▁▃▃▃▃▃▂▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▁▅▆▇██▇█████▇██████▇██████████████▇▇███</td></tr><tr><td>val_auc</td><td>▁▃▁▂▅▆▇▇▇███████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▂▇▇▇███████████████▇███████████████████</td></tr><tr><td>val_loss_epoch</td><td>██▆▄▃▂▂▃▂▂▂▂▁▂▂▁▁▁▂▂▃▂▁▂▂▁▁▂▂▂▂▁▂▁▁▃▁▂▁▁</td></tr><tr><td>val_loss_step</td><td>██▇▃▄▃▄▅▄▃▃▄▃▃▃▂▂▃▂▃▄▃▂▄▄▁▃▃▄▃▅▁▄▂▃▃▃▄▂▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.77273</td></tr><tr><td>train_auc</td><td>0.8472</td></tr><tr><td>train_f1</td><td>0.77912</td></tr><tr><td>train_loss_epoch</td><td>0.49426</td></tr><tr><td>train_loss_step</td><td>0.49582</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.75887</td></tr><tr><td>val_auc</td><td>0.82978</td></tr><tr><td>val_f1</td><td>0.75829</td></tr><tr><td>val_loss_epoch</td><td>0.5096</td></tr><tr><td>val_loss_step</td><td>0.5212</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_32_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/cv0fsgh2' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/cv0fsgh2</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240105_075443-cv0fsgh2\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eda19c538cfa46ef924f06239a3a8132",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01691666666883975, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240105_082318-vwx0oe4y</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/vwx0oe4y' target=\"_blank\">MLP_4_64_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/vwx0oe4y' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/vwx0oe4y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 13.4 K\n",
      "2 | head        | Sequential       | 2.1 K \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "15.5 K    Trainable params\n",
      "0         Non-trainable params\n",
      "15.5 K    Total params\n",
      "0.062     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db9f566498d0452a85db9e3e31c20e46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▃▇▇█████▇█▇█████▇█▇█████▇██████▇███████</td></tr><tr><td>train_auc</td><td>▁▃▇█████████████████████████████████████</td></tr><tr><td>train_f1</td><td>▁▄█▇█████▇▇▇█████▇█▇█████▇██████▇███████</td></tr><tr><td>train_loss_epoch</td><td>█▇▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▂▁▂▁▁▂▂▁▂▁▁▁▂▁▁▂▁▁▁▁▂▁▁</td></tr><tr><td>train_loss_step</td><td>█▇▄▃▃▂▄▅▃▃▄▄▄▂▃▂▃▃▃▃▁▂▂▃▂▃▁▃▃▅▂▃▂▅▂▃▂▄▃▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▁▆▆▆█▇▇▇█▆▇▇▇██▇█▇▇██▇██▇█▇██▇▇██████▇█</td></tr><tr><td>val_auc</td><td>▁▃▃▅▆▆▇▇▇▇▇▇▇▇▇█████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁████▇▇▇██▇█▇██▇█████▇████▇████████████</td></tr><tr><td>val_loss_epoch</td><td>█▆▃▃▃▂▁▂▂▂▃▁▂▁▂▂▁▁▁▁▂▁▁▂▂▁▂▂▂▂▁▁▁▂▁▁▂▂▂▂</td></tr><tr><td>val_loss_step</td><td>█▆▅▃▄▃▂▄▄▄▃▃▅▂▂▄▁▁▃▃▃▁▂▃▃▂▄▂▃▂▃▁▂▂▂▂▃▃▅▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.77509</td></tr><tr><td>train_auc</td><td>0.8527</td></tr><tr><td>train_f1</td><td>0.77062</td></tr><tr><td>train_loss_epoch</td><td>0.47792</td></tr><tr><td>train_loss_step</td><td>0.42382</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.75887</td></tr><tr><td>val_auc</td><td>0.83104</td></tr><tr><td>val_f1</td><td>0.77333</td></tr><tr><td>val_loss_epoch</td><td>0.52669</td></tr><tr><td>val_loss_step</td><td>0.55714</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_64_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/vwx0oe4y' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/vwx0oe4y</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240105_082318-vwx0oe4y\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed8dfbe1df8b4b358db30bf4b08543f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.017183333332650364, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240105_085217-920epzzp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/920epzzp' target=\"_blank\">MLP_4_64_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/920epzzp' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/920epzzp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 13.4 K\n",
      "2 | head        | Sequential       | 2.1 K \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "15.5 K    Trainable params\n",
      "0         Non-trainable params\n",
      "15.5 K    Total params\n",
      "0.062     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7567f3b3c01455db95c143a6a3d4121",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▄▆▆▆▆▇▇▇▇▇▇▇█▇█▇▇▇██▇▇█▇█▇████▇▇██▇▇█▇</td></tr><tr><td>train_auc</td><td>▁▁▄▆▆▇▇▇▇▇▇▆▇▇▇▇█▇▇▇████████████████████</td></tr><tr><td>train_f1</td><td>▁▃▄▆▆▆▆▇▇▇▆▆▇▇█▇█▇▇▇██▇▇█▇██▇███▇▇██▇▇█▇</td></tr><tr><td>train_loss_epoch</td><td>██▆▄▄▃▄▃▃▃▃▄▃▂▂▂▂▃▂▂▂▂▂▂▁▂▁▂▁▁▁▁▂▂▁▂▂▂▁▁</td></tr><tr><td>train_loss_step</td><td>██▇▅▅▅▅▅▅▃▆▄▅▃▄▃▃▄▂▄▂▂▃▂▃▂▂▃▅▅▁▂▄▅▄▁▃▄▃▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▁██▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▅█▁▁████▅▁▁▅▁▁▁██</td></tr><tr><td>val_auc</td><td>▁▇▇▇▆█████████▇▇▇▇▇▇▇▇▇▇▇▇▆▇▆▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>val_f1</td><td>▁▁██▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▆█▁▁████▆▁▁▆▁▁▁██</td></tr><tr><td>val_loss_epoch</td><td>▂▂▁▁▂▂▃▃▅▇▅▄▅▅▅▇▇▇███▆▇▅▅▆▇▄▆▅▆▆▆▆▇▆▆▆▅▅</td></tr><tr><td>val_loss_step</td><td>▂▂▁▁▁▂▃▃▄█▄▄▅▅▄▆▆▆█▇▇▅▇▅▅▄▆▂▅▄▅▆▄▆▇▅▆▆▆▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.75325</td></tr><tr><td>train_auc</td><td>0.81441</td></tr><tr><td>train_f1</td><td>0.75922</td></tr><tr><td>train_loss_epoch</td><td>0.50644</td></tr><tr><td>train_loss_step</td><td>0.42808</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.68558</td></tr><tr><td>val_auc</td><td>0.7181</td></tr><tr><td>val_f1</td><td>0.74374</td></tr><tr><td>val_loss_epoch</td><td>0.85768</td></tr><tr><td>val_loss_step</td><td>0.88918</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_64_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/920epzzp' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/920epzzp</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240105_085217-920epzzp\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "752a0e7fd47b4b049ff34a2685ac51a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016933333330477276, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240105_092351-ubcysdpa</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/ubcysdpa' target=\"_blank\">MLP_4_64_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/ubcysdpa' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/ubcysdpa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 13.4 K\n",
      "2 | head        | Sequential       | 2.1 K \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "15.5 K    Trainable params\n",
      "0         Non-trainable params\n",
      "15.5 K    Total params\n",
      "0.062     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▁▂▄▆▆▇▇▇▇▇▇████████████▇▇█████████▇▇▇█</td></tr><tr><td>train_auc</td><td>▁▁▁▁▃▆▆▇▇▇▇▇▇▇██▇▇██▇█████▇█▇███████▇▇██</td></tr><tr><td>train_f1</td><td>▂▁▂▂▄▆▆▇▇▇▇▇▇████████████▇▇█████████▇█▇█</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▄▃▃▃▂▂▂▂▂▁▂▂▂▂▁▂▂▁▁▁▁▁▁▁▂▁▁▂▂▁▁▂▂▂▁▁▂▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▂▁▃▃▄▄▆▇▅▇█▆▇▇▇█▇▇▇▇▇█▇▇▇██▇▇███▇▇███▇██</td></tr><tr><td>val_auc</td><td>▃▁▅▅▇▇██████████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▇▅▃▅▄▆▇▅▇█▆▇▇▇█▇▇▇▇▇██▇▇██▇▇███▇▇▇█████</td></tr><tr><td>val_loss_epoch</td><td>█▇▇▇▆▆▃▃▄▂▂▃▂▂▂▂▁▁▁▂▁▁▁▂▂▁▁▂▁▂▁▁▂▁▁▁▂▃▂▁</td></tr><tr><td>val_loss_step</td><td>██▇▇▆▆▄▄▅▃▂▃▄▂▂▃▁▁▂▃▃▁▁▄▃▁▃▃▂▂▃▂▄▂▂▁▃▄▄▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.77096</td></tr><tr><td>train_auc</td><td>0.81017</td></tr><tr><td>train_f1</td><td>0.76766</td></tr><tr><td>train_loss_epoch</td><td>0.48311</td></tr><tr><td>train_loss_step</td><td>0.46016</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.77305</td></tr><tr><td>val_auc</td><td>0.85133</td></tr><tr><td>val_f1</td><td>0.77033</td></tr><tr><td>val_loss_epoch</td><td>0.50224</td></tr><tr><td>val_loss_step</td><td>0.53745</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_64_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/ubcysdpa' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/ubcysdpa</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240105_092351-ubcysdpa\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e623806fe6a04d3bb7c4e8a2cfa16949",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240105_095440-6p5qi0w6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/6p5qi0w6' target=\"_blank\">MLP_4_64_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/6p5qi0w6' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/6p5qi0w6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 13.4 K\n",
      "2  | head        | Sequential       | 2.1 K \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 65    \n",
      "--------------------------------------------------\n",
      "15.6 K    Trainable params\n",
      "0         Non-trainable params\n",
      "15.6 K    Total params\n",
      "0.062     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▄▇▇▇██▇████▇████▇███████▇██▇▇██████████</td></tr><tr><td>train_auc</td><td>▁▄▇▇▇███████████████████████████████████</td></tr><tr><td>train_f1</td><td>▁▃▇▆▇██▇████▇████▇███████▇██▇▇██████████</td></tr><tr><td>train_loss_epoch</td><td>█▇▃▃▂▂▂▂▂▁▂▁▂▁▁▂▁▁▁▁▁▁▁▁▁▂▁▂▁▁▁▁▁▁▁▂▂▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▇▄▄▄▄▃▄▅▄▂▂▃▃▃▄▁▁▄▃▃▃▂▄▂▄▂▂▂▃▃▄▂▂▃▃▅▃▃▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▆▅▇▇█▇▇███▇▇██▇▇█▇███████▇█████▇█████▇█</td></tr><tr><td>val_auc</td><td>▅▁▃▂▃▅▆▆▆▇▇▇▇▇▇███████████▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>val_f1</td><td>▅█▁█▆▇▆▇▇██▆▇▇▇▆▆▇▆▇▇▇█▇▇█▇▇█▇▇█▆▇█▇█▇▆▇</td></tr><tr><td>val_loss_epoch</td><td>█▆▅▃▂▂▂▂▁▂▁▂▁▁▁▂▂▂▃▁▁▂▁▁▂▁▁▂▁▁▂▂▂▁▂▁▁▁▃▁</td></tr><tr><td>val_loss_step</td><td>█▇▄▄▂▃▃▃▃▃▃▂▃▄▃▃▃▄▄▁▂▂▃▃▄▃▁▃▃▃▅▄▂▂▄▃▃▁▆▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.77686</td></tr><tr><td>train_auc</td><td>0.85294</td></tr><tr><td>train_f1</td><td>0.78375</td></tr><tr><td>train_loss_epoch</td><td>0.47866</td></tr><tr><td>train_loss_step</td><td>0.45894</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.74232</td></tr><tr><td>val_auc</td><td>0.8243</td></tr><tr><td>val_f1</td><td>0.72953</td></tr><tr><td>val_loss_epoch</td><td>0.5057</td></tr><tr><td>val_loss_step</td><td>0.46193</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_64_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/6p5qi0w6' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/6p5qi0w6</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240105_095440-6p5qi0w6\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af7c0b78e3604b1ebd74b135b4d4e84d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01691666666495924, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240105_102525-fhs3l2yw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/fhs3l2yw' target=\"_blank\">MLP_4_64_onehot_3</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/fhs3l2yw' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/fhs3l2yw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 13.4 K\n",
      "2  | head        | Sequential       | 2.1 K \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 4.2 K \n",
      "--------------------------------------------------\n",
      "19.7 K    Trainable params\n",
      "0         Non-trainable params\n",
      "19.7 K    Total params\n",
      "0.079     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e9a68539b5b4c089c2e3c6fe9c1af1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▇▇▇▇█▇████▇████████▇██▇██████████▇████</td></tr><tr><td>train_auc</td><td>▁▂▇▇████████████████████████████████████</td></tr><tr><td>train_f1</td><td>▃▁▇▇█▇██████████████████████████████████</td></tr><tr><td>train_loss_epoch</td><td>██▄▃▂▂▂▂▂▂▂▁▂▂▁▁▁▂▁▁▁▁▁▁▂▂▁▁▁▁▁▂▁▂▁▂▁▁▁▂</td></tr><tr><td>train_loss_step</td><td>██▃▃▄▃▄▄▂▂▂▂▄▁▂▄▃▃▂▂▂▁▁▂▂▂▂▃▁▃▃▄▂▃▂▃▃▃▂▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▆▇█▇▇▇██▇▇██▇████▇████▇████▇███▇▇▇▇█▇█▇</td></tr><tr><td>val_auc</td><td>▄▁▁▂▃▄▅▅▆▇▇▇▇▇▇███▇██████▇▇▇████████▇▇██</td></tr><tr><td>val_f1</td><td>▁▆▇█▂▆▃██▆▇▇▇▆▇███▆█▇▇▇▅▇█▇▇▇███▆▅█▇▆▅▇▇</td></tr><tr><td>val_loss_epoch</td><td>█▇▃▂▃▃▂▂▂▁▂▁▁▂▁▁▂▁▁▁▁▁▂▁▁▂▁▁▂▂▂▂▂▁▂▂▁▁▁▂</td></tr><tr><td>val_loss_step</td><td>█▇▄▄▆▅▄▃▃▂▂▃▁▄▂▂▄▃▂▃▂▃▄▃▂▂▂▂▂▄▃▃▃▂▄▃▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.76505</td></tr><tr><td>train_auc</td><td>0.84326</td></tr><tr><td>train_f1</td><td>0.7582</td></tr><tr><td>train_loss_epoch</td><td>0.49339</td></tr><tr><td>train_loss_step</td><td>0.48565</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.74232</td></tr><tr><td>val_auc</td><td>0.8301</td></tr><tr><td>val_f1</td><td>0.76253</td></tr><tr><td>val_loss_epoch</td><td>0.51957</td></tr><tr><td>val_loss_step</td><td>0.50086</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_64_onehot_3</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/fhs3l2yw' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/fhs3l2yw</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240105_102525-fhs3l2yw\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d38cf43fcaed41fa9e0a313dfec6fbd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01693333333435779, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240105_105627-z1thc6nl</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/z1thc6nl' target=\"_blank\">MLP_2_16_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/z1thc6nl' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/z1thc6nl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 496   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "650       Trainable params\n",
      "0         Non-trainable params\n",
      "650       Total params\n",
      "0.003     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16290bc6f5fe4ce68aee2aad18d51aad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▂▅▇█▇██▇▇██████████████████▇██████████</td></tr><tr><td>train_auc</td><td>▁▁▂▅▇▇▇▇█▇██████████████████████████████</td></tr><tr><td>train_f1</td><td>▁▄▂▆▇████████████████████████▇██████████</td></tr><tr><td>train_loss_epoch</td><td>███▆▃▃▂▂▂▂▂▂▁▂▁▁▁▁▂▁▂▂▂▂▂▁▂▂▁▂▂▁▂▂▂▁▁▂▂▂</td></tr><tr><td>train_loss_step</td><td>███▆▄▄▃▄▃▂▃▄▃▃▃▄▃▃▁▃▁▃▄▃▅▃▃▄▃▅▁▂▃▃▁▃▄▂▃▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▁▃▇█▇▇█████████████████████████████████</td></tr><tr><td>val_auc</td><td>▁▆▇▇▇███████████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▄██▇██████████████████████████████████</td></tr><tr><td>val_loss_epoch</td><td>███▆▂▂▂▂▂▁▁▂▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_step</td><td>███▆▄▃▄▃▂▃▃▃▃▂▄▃▃▂▃▂▁▃▃▂▂▃▁▁▃▃▂▂▄▃▁▃▃▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.76328</td></tr><tr><td>train_auc</td><td>0.83122</td></tr><tr><td>train_f1</td><td>0.73426</td></tr><tr><td>train_loss_epoch</td><td>0.51473</td></tr><tr><td>train_loss_step</td><td>0.46528</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.78251</td></tr><tr><td>val_auc</td><td>0.85463</td></tr><tr><td>val_f1</td><td>0.79556</td></tr><tr><td>val_loss_epoch</td><td>0.47264</td></tr><tr><td>val_loss_step</td><td>0.45808</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_16_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/z1thc6nl' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/z1thc6nl</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240105_105627-z1thc6nl\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "588366b295544b8ebd6898c23a51a37d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016933333330477276, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240105_112752-verkdcbi</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/verkdcbi' target=\"_blank\">MLP_2_16_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/verkdcbi' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/verkdcbi</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 496   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "650       Trainable params\n",
      "0         Non-trainable params\n",
      "650       Total params\n",
      "0.003     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▃▅▅▆▆▅▇▇▇▇▇▇▇▇▇█▇▇▇▇▇██▇█▇▇█▇█▇▇█▇▇▇▇▇█</td></tr><tr><td>train_auc</td><td>▁▂▂▄▅▅▆▆▇▇▇▇▇▇▇▇█▇▇▇▇▇██▇██▇███▇███▇█▇▇█</td></tr><tr><td>train_f1</td><td>▁▂▄▄▅▅▅▆▇▆▆▇▇▆▆▇█▆▆▇▇▇▇█▇▇▇▇█▇█▇▇█▇▇▇▇▇▇</td></tr><tr><td>train_loss_epoch</td><td>█▇▆▅▅▅▄▄▃▃▃▃▂▃▃▃▂▃▃▂▂▃▂▁▁▁▁▂▂▂▁▂▁▁▂▂▁▂▂▁</td></tr><tr><td>train_loss_step</td><td>█▇▆▄▆▆▅▅▅▄▃▅▄▅▅▃▆▄▂▃▂▄▄▃▅▄▁▅▃▄▂▃▁▂▃▁▃▃▃▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁█▂█████████████████████████████</td></tr><tr><td>val_auc</td><td>▁▇▇██████▇▇██▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁█▂█████████████████████████████</td></tr><tr><td>val_loss_epoch</td><td>██▇▇▇▆▇▅▄▄▄▃▃▃▃▃▂▃▃▂▂▂▂▂▂▂▁▂▂▁▂▂▂▁▂▂▂▁▃▂</td></tr><tr><td>val_loss_step</td><td>██▇█▇▆▇▅▄▄▄▃▃▃▄▄▂▃▄▃▂▂▂▃▁▄▁▂▂▂▂▃▂▁▂▂▂▂▃▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.69421</td></tr><tr><td>train_auc</td><td>0.7376</td></tr><tr><td>train_f1</td><td>0.6601</td></tr><tr><td>train_loss_epoch</td><td>0.60415</td></tr><tr><td>train_loss_step</td><td>0.5753</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.74941</td></tr><tr><td>val_auc</td><td>0.74458</td></tr><tr><td>val_f1</td><td>0.8037</td></tr><tr><td>val_loss_epoch</td><td>0.65137</td></tr><tr><td>val_loss_step</td><td>0.65231</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_16_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/verkdcbi' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/verkdcbi</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240105_112752-verkdcbi\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a158c52f5ab14abb9535f6b2dc88693c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01693333333435779, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240105_115851-e9vdnpu8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/e9vdnpu8' target=\"_blank\">MLP_2_16_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/e9vdnpu8' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/e9vdnpu8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 496   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "650       Trainable params\n",
      "0         Non-trainable params\n",
      "650       Total params\n",
      "0.003     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▃▄▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇█▇▇▇▇█▇▇████▇▇▇█▇██▇</td></tr><tr><td>train_auc</td><td>▁▁▂▃▄▅▅▄▅▄▄▅▅▆▆▆▇▇▆▇▇▇▇▇▇█▇▇████████████</td></tr><tr><td>train_f1</td><td>▁▁▃▃▃▄▅▅▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇█▇█▇▇█▇██▇▇▇█▇██▇</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▃▃▂▂▂▂▂▁▂▂▂▁▂▂▁▁▂▁▁▁▁▁▁▂▁▁▁▁▂▁▁▁▁▁▂▂▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▄▂▅▄▄▆▇▇▇▇▇▇▇▇▇▇▇██▇▇▇▇▇██▇▇█▇▇▇████▇██</td></tr><tr><td>val_auc</td><td>▁▄▆▆▆▇▇▇▇▇▇█████████████████████████████</td></tr><tr><td>val_f1</td><td>▆▅▁▅▄▄▆▇▇█▇▇▇▇▇▇▇▇██▇▇▇▇▇██▇▇█▇▆▇████▇██</td></tr><tr><td>val_loss_epoch</td><td>█▄▃▃▃▃▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_step</td><td>█▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▂▁▁▂▁▁▂▁▁▁▂▁▂▁▂▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.7438</td></tr><tr><td>train_auc</td><td>0.78383</td></tr><tr><td>train_f1</td><td>0.72739</td></tr><tr><td>train_loss_epoch</td><td>0.51891</td></tr><tr><td>train_loss_step</td><td>0.48996</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.81087</td></tr><tr><td>val_auc</td><td>0.8721</td></tr><tr><td>val_f1</td><td>0.82684</td></tr><tr><td>val_loss_epoch</td><td>0.45782</td></tr><tr><td>val_loss_step</td><td>0.43925</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_16_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/e9vdnpu8' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/e9vdnpu8</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240105_115851-e9vdnpu8\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1a61035c2a7435ebc262ffd77827730",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01691666666495924, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240105_122922-a8h622zi</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/a8h622zi' target=\"_blank\">MLP_2_16_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/a8h622zi' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/a8h622zi</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 496   \n",
      "2  | head        | Sequential       | 154   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 17    \n",
      "--------------------------------------------------\n",
      "667       Trainable params\n",
      "0         Non-trainable params\n",
      "667       Total params\n",
      "0.003     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df6e2ade6ea7436fb6dc378b597eee36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▃▅▆▇▇█▇▇▇██▇█▇██▇███▇█▇███▇███████████</td></tr><tr><td>train_auc</td><td>▁▁▂▅▇▇▇▇█▇██▇███████████████████████████</td></tr><tr><td>train_f1</td><td>▁▄▄▆▇▇██████████████████████████████████</td></tr><tr><td>train_loss_epoch</td><td>██▇▆▄▄▃▂▂▃▂▂▂▂▂▂▂▁▂▂▁▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▁▂▂▂</td></tr><tr><td>train_loss_step</td><td>███▆▄▃▅▅▁▄▃▄▄▃▃▃▂▃▅▄▃▃▅▅▃▃▃▅▄▃▄▄▂▄▃▃▂▃▄▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▁▄▅▆▇▇▇████████████████████████████████</td></tr><tr><td>val_auc</td><td>▁▅▇▇▇▇██████████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▆▆▇███████████████████████████████████</td></tr><tr><td>val_loss_epoch</td><td>██▇▅▄▃▂▂▂▂▂▂▂▂▂▁▁▂▂▁▁▂▂▁▂▂▂▁▁▁▂▂▁▁▁▁▁▁▁▂</td></tr><tr><td>val_loss_step</td><td>██▇▆▃▄▃▄▃▃▃▃▃▃▂▁▂▄▂▃▂▁▂▂▂▂▃▃▃▂▄▃▃▁▂▁▁▃▁▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.75325</td></tr><tr><td>train_auc</td><td>0.83144</td></tr><tr><td>train_f1</td><td>0.73973</td></tr><tr><td>train_loss_epoch</td><td>0.50948</td></tr><tr><td>train_loss_step</td><td>0.51588</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.78723</td></tr><tr><td>val_auc</td><td>0.854</td></tr><tr><td>val_f1</td><td>0.79167</td></tr><tr><td>val_loss_epoch</td><td>0.49327</td></tr><tr><td>val_loss_step</td><td>0.50897</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_16_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/a8h622zi' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/a8h622zi</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240105_122922-a8h622zi\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78f22eba2afa4a77a04003d5cc219d64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240105_125917-ginl6mlt</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/ginl6mlt' target=\"_blank\">MLP_2_16_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/ginl6mlt' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/ginl6mlt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 496   \n",
      "2  | head        | Sequential       | 154   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 281   \n",
      "--------------------------------------------------\n",
      "931       Trainable params\n",
      "0         Non-trainable params\n",
      "931       Total params\n",
      "0.004     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e3bb25973ff44b2afbd8b91d4e9aa7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▃▅▆▇▇▇███▇▇██████████████▇▇███████████</td></tr><tr><td>train_auc</td><td>▁▁▂▅▇▇█▇███▇████████████████████████████</td></tr><tr><td>train_f1</td><td>▁▃▄▅▇▇█████▇████████████████████████████</td></tr><tr><td>train_loss_epoch</td><td>██▇▆▄▃▂▂▂▂▂▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▂▂▁▁▁▂▁▁▁▁▁▁▂▂</td></tr><tr><td>train_loss_step</td><td>███▆▆▅▂▄▃▃▃▂▄▂▄▃▁▃▂▃▂▄▂▂▃▃▂▃▁▃▂▃▃▃▁▃▂▃▃▄</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▁▄▆▆▆▇█████████████████████████████████</td></tr><tr><td>val_auc</td><td>▁▆▇▇▇███████████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▅▇▇▇██████████████████████████████████</td></tr><tr><td>val_loss_epoch</td><td>██▇▅▃▃▂▁▁▂▂▁▂▁▂▂▂▁▁▁▁▁▂▁▁▂▁▁▁▁▂▂▂▁▂▂▂▂▂▁</td></tr><tr><td>val_loss_step</td><td>██▇▆▅▄▄▂▂▄▃▂▄▃▃▂▃▂▂▃▃▂▄▁▃▄▂▂▃▂▄▃▃▂▄▃▂▄▄▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.75974</td></tr><tr><td>train_auc</td><td>0.82494</td></tr><tr><td>train_f1</td><td>0.75438</td></tr><tr><td>train_loss_epoch</td><td>0.52423</td></tr><tr><td>train_loss_step</td><td>0.56555</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.78251</td></tr><tr><td>val_auc</td><td>0.85376</td></tr><tr><td>val_f1</td><td>0.79091</td></tr><tr><td>val_loss_epoch</td><td>0.47976</td></tr><tr><td>val_loss_step</td><td>0.47456</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_16_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/ginl6mlt' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/ginl6mlt</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240105_125917-ginl6mlt\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80ca47d1337248c689ea64c3c2fccd3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240105_132916-lfcavz6n</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/lfcavz6n' target=\"_blank\">MLP_2_32_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/lfcavz6n' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/lfcavz6n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 1.5 K \n",
      "2 | head        | Sequential       | 562   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "2.1 K     Trainable params\n",
      "0         Non-trainable params\n",
      "2.1 K     Total params\n",
      "0.008     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aafe948d970249e9a07638d238feb235",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▃▆▇▇▇▇▇▇▇████▇█▇████▇▇▇████████████████</td></tr><tr><td>train_auc</td><td>▁▃▇▇▇███████████████████████████████████</td></tr><tr><td>train_f1</td><td>▁▄▇▇████████████████████████████████████</td></tr><tr><td>train_loss_epoch</td><td>█▇▄▃▂▂▂▂▂▁▂▂▂▂▂▁▂▁▁▂▁▂▁▁▂▂▁▂▂▁▁▁▁▁▂▂▁▁▁▂</td></tr><tr><td>train_loss_step</td><td>█▇▅▄▄▄▂▃▂▄▃▃▃▂▃▂▂▂▄▂▁▃▃▃▄▄▂▃▂▂▂▂▃▂▃▃▂▂▁▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▂▇▇███████████▇███████████████▇████████</td></tr><tr><td>val_auc</td><td>▁▂▃▅▆▇▇▇▇███████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▃█▇████████████████████████████████████</td></tr><tr><td>val_loss_epoch</td><td>█▇▃▂▂▂▂▂▂▂▂▂▁▁▁▂▁▂▂▁▁▂▂▁▂▂▂▁▁▂▁▁▁▂▁▁▁▁▁▁</td></tr><tr><td>val_loss_step</td><td>█▇▃▄▃▄▃▄▄▃▂▃▃▂▂▃▁▄▂▃▂▄▃▃▃▃▃▃▁▃▃▃▁▄▃▂▃▂▃▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.77037</td></tr><tr><td>train_auc</td><td>0.83947</td></tr><tr><td>train_f1</td><td>0.76439</td></tr><tr><td>train_loss_epoch</td><td>0.50051</td></tr><tr><td>train_loss_step</td><td>0.526</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.78723</td></tr><tr><td>val_auc</td><td>0.85592</td></tr><tr><td>val_f1</td><td>0.80349</td></tr><tr><td>val_loss_epoch</td><td>0.46384</td></tr><tr><td>val_loss_step</td><td>0.4368</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_32_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/lfcavz6n' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/lfcavz6n</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240105_132916-lfcavz6n\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "418b45015a184b9f8ee3701f46d32849",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01691666666495924, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240105_135905-cngxo3h8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/cngxo3h8' target=\"_blank\">MLP_2_32_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/cngxo3h8' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/cngxo3h8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 1.5 K \n",
      "2 | head        | Sequential       | 562   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "2.1 K     Trainable params\n",
      "0         Non-trainable params\n",
      "2.1 K     Total params\n",
      "0.008     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "882b495f4360405aa083ac0f41ebb1bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▄▆▅▇▆▆▇▇▇▇▇█▇▇▇▇▇█▇▇▇██▇▇████████▇█▇▇██</td></tr><tr><td>train_auc</td><td>▁▃▆▆▇▇▇▇▇▇▇▇█▇▇▇▇▇█▇▇▇█▇█▇████████▇█████</td></tr><tr><td>train_f1</td><td>▁▅▆▅▇▇▇▇▇▇███▇▇▇▇▇█▇▇███▇▇█████▇██▇█▇▇▇█</td></tr><tr><td>train_loss_epoch</td><td>█▇▅▄▃▃▃▃▃▂▃▃▂▂▂▂▂▂▁▂▂▂▁▂▂▂▂▁▁▁▁▂▁▂▂▂▂▂▁▁</td></tr><tr><td>train_loss_step</td><td>█▇▆▅▅▆▄▄▄▅▅▄▄▃▂▄▄▃▆▂▂▄▃▅▅▃▁▃▃▂▂▃▄▂▄▄▃▃▃▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁████████▆▆▅▅▅▅▃▃▃▅▃▃▃▄▄▃▄▃▃▅▆▄▄▃▃▅▃</td></tr><tr><td>val_auc</td><td>▆▁██▇▃▄▆▆▆▆▆▆█▆▆██▇█▇▆█▇▇▇▇▇▇▇▇▇▇▇▇▇▆▆▆▆</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁███████████▇██▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇▇▇▇▇▇</td></tr><tr><td>val_loss_epoch</td><td>█▇▃▄▃▂▃▂▁▁▁▁▁▂▂▃▃▃▃▄▃▃▃▄▄▄▄▃▄▃▄▄▂▂▃▃▄▃▃▃</td></tr><tr><td>val_loss_step</td><td>█▇▅▅▄▅▄▄▄▄▃▄▄▆▄▆▄▄▄▆▃▅▅▄▆▄▆▅▆▅▄▃▁▅▅▃▆▄▅▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.73259</td></tr><tr><td>train_auc</td><td>0.77895</td></tr><tr><td>train_f1</td><td>0.72955</td></tr><tr><td>train_loss_epoch</td><td>0.55496</td></tr><tr><td>train_loss_step</td><td>0.54528</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.54137</td></tr><tr><td>val_auc</td><td>0.7769</td></tr><tr><td>val_f1</td><td>0.70245</td></tr><tr><td>val_loss_epoch</td><td>0.64675</td></tr><tr><td>val_loss_step</td><td>0.62035</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_32_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/cngxo3h8' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/cngxo3h8</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240105_135905-cngxo3h8\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e01b2d47b8074971b7992902152a9093",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01691666666495924, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240105_142907-fey59lys</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/fey59lys' target=\"_blank\">MLP_2_32_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/fey59lys' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/fey59lys</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 1.5 K \n",
      "2 | head        | Sequential       | 562   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "2.1 K     Trainable params\n",
      "0         Non-trainable params\n",
      "2.1 K     Total params\n",
      "0.008     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▄▅▆▆▇▇▇▇▇▇█▇▇▇▇█▇███▇█▇████████████▇██▇</td></tr><tr><td>train_auc</td><td>▁▂▄▅▆▆▆▆▆▇▇▇▇▇██▇▇██▇▇▇███▇▇██████▇▇▇███</td></tr><tr><td>train_f1</td><td>▁▄▄▅▆▆▇▇▇█▇▇█▇█▇█▇█████▇████████████▇███</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▅▅▄▄▃▂▃▃▄▃▂▃▃▃▃▂▂▃▂▁▃▂▂▄▃▃▂▂▁▁▂▃▂▃▂▂▂▂▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▂▁▅▆▅▇▅▇▇▆▇▇▇█▇█▇▇▇▇██▇██████▇█████▇█▇██</td></tr><tr><td>val_auc</td><td>▁▆▇▇▇▇█▇███████▇████████████████████████</td></tr><tr><td>val_f1</td><td>▃▁▆▆▆▇▅▇▇▆▇▇▇█▇█▇▇▇▇▇▇▇██▇███▇██▇█▇▇█▇██</td></tr><tr><td>val_loss_epoch</td><td>██▆▅▄▄▄▃▃▃▂▂▂▂▂▂▂▂▂▂▁▂▂▁▂▂▁▂▁▁▁▁▁▂▂▂▂▂▂▁</td></tr><tr><td>val_loss_step</td><td>▇█▆▆▅▇▄▄▅▄▂▃▃▃▂▃▂▄▁▂▂▄▂▃▅▃▃▄▂▂▁▃▂▄▂▂▄▃▄▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.76269</td></tr><tr><td>train_auc</td><td>0.82339</td></tr><tr><td>train_f1</td><td>0.75695</td></tr><tr><td>train_loss_epoch</td><td>0.49679</td></tr><tr><td>train_loss_step</td><td>0.46791</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.80142</td></tr><tr><td>val_auc</td><td>0.87946</td></tr><tr><td>val_f1</td><td>0.8125</td></tr><tr><td>val_loss_epoch</td><td>0.44996</td></tr><tr><td>val_loss_step</td><td>0.43831</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_32_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/fey59lys' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/fey59lys</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240105_142907-fey59lys\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1557465100ea4355840fa0a67560160c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01693333333435779, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240105_145846-7kss46cj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/7kss46cj' target=\"_blank\">MLP_2_32_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/7kss46cj' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/7kss46cj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 1.5 K \n",
      "2  | head        | Sequential       | 562   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 33    \n",
      "--------------------------------------------------\n",
      "2.1 K     Trainable params\n",
      "0         Non-trainable params\n",
      "2.1 K     Total params\n",
      "0.008     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c718700a903424daf2291fcace1863d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▃▅▆▆▇▇▇▇▇██▇█▇█████▇▇▇███▇▇███▇▇███████</td></tr><tr><td>train_auc</td><td>▁▄▆▇▇▇▇▇▇███████████████████████████████</td></tr><tr><td>train_f1</td><td>▁▃▆▆▆▇▇▇██████▇█████████████████████████</td></tr><tr><td>train_loss_epoch</td><td>█▇▅▄▄▃▃▂▂▂▁▂▂▂▂▁▁▂▂▁▁▂▂▂▂▁▂▂▁▁▁▂▂▂▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▇▅▅▅▄▃▄▃▃▂▃▅▃▂▂▃▂▂▃▂▂▂▄▂▂▃▃▄▄▂▃▃▂▁▄▂▂▅▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▆▁▂▂▅▄▇█▇████▇▇█████▇█▇▇▇███▇███████████</td></tr><tr><td>val_auc</td><td>▂▂▁▂▃▄▆▇▇▇▇▇█▇▇▇██████▇▇▇███████████████</td></tr><tr><td>val_f1</td><td>▇▁▂▃▆▄▇█▇▇█▇▇▇██▇█▇▇███▇▇██▇█▇▇█▇▇██▇▇██</td></tr><tr><td>val_loss_epoch</td><td>█▇▆▅▃▄▂▂▂▂▁▁▂▃▂▂▂▁▁▂▁▂▂▂▂▁▂▂▁▂▂▁▂▁▁▁▂▂▁▁</td></tr><tr><td>val_loss_step</td><td>█▇▆▆▄▅▃▂▄▂▂▁▄▄▃▃▃▂▂▃▂▂▃▃▃▁▃▄▃▃▂▁▂▁▃▂▄▃▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.76682</td></tr><tr><td>train_auc</td><td>0.84285</td></tr><tr><td>train_f1</td><td>0.75481</td></tr><tr><td>train_loss_epoch</td><td>0.48865</td></tr><tr><td>train_loss_step</td><td>0.46413</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.77541</td></tr><tr><td>val_auc</td><td>0.85781</td></tr><tr><td>val_f1</td><td>0.79744</td></tr><tr><td>val_loss_epoch</td><td>0.46718</td></tr><tr><td>val_loss_step</td><td>0.48394</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_32_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/7kss46cj' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/7kss46cj</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240105_145846-7kss46cj\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1ca15b20e1148dabeccaf58ce8ed942",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.017183333336530875, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240105_152913-op32axkz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/op32axkz' target=\"_blank\">MLP_2_32_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/op32axkz' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/op32axkz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 1.5 K \n",
      "2  | head        | Sequential       | 562   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 1.1 K \n",
      "--------------------------------------------------\n",
      "3.1 K     Trainable params\n",
      "0         Non-trainable params\n",
      "3.1 K     Total params\n",
      "0.013     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31c966d5f37b4970a8632eb227140d8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▃▆▆▇█▇▇▇██████▇███▇████████████████████</td></tr><tr><td>train_auc</td><td>▁▃▇▇██▇█████████████████████████████████</td></tr><tr><td>train_f1</td><td>▁▄▆▆▇██▇████████████████████████████████</td></tr><tr><td>train_loss_epoch</td><td>█▇▄▃▂▂▂▂▂▂▂▂▂▁▂▂▂▁▂▁▁▂▁▂▂▁▂▂▁▁▂▂▁▂▁▂▁▂▁▂</td></tr><tr><td>train_loss_step</td><td>██▅▄▃▃▂▃▄▂▃▂▂▄▁▃▃▃▂▃▃▃▃▂▃▄▄▄▃▄▃▃▂▂▃▂▂▁▂▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▆▅▁▅▅▇▆██▇█████▇█▇█▇█▇▇███▇█▇▇█████▇▇███</td></tr><tr><td>val_auc</td><td>▁▂▁▄▅▆▇▇▇▇▇▇▇███████████████████████████</td></tr><tr><td>val_f1</td><td>▇▆▁▆▆▇▅██▇███▇███▇██████████▇▇▇███████▇▇</td></tr><tr><td>val_loss_epoch</td><td>█▇▄▃▂▂▃▂▁▂▂▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▂▂▂▁▂▁▁▁▁▁▂▁</td></tr><tr><td>val_loss_step</td><td>█▇▄▄▃▄▃▃▃▃▄▂▃▂▃▄▃▂▂▃▃▂▃▃▃▂▄▂▃▃▃▃▄▃▂▃▃▂▄▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.76978</td></tr><tr><td>train_auc</td><td>0.84287</td></tr><tr><td>train_f1</td><td>0.76335</td></tr><tr><td>train_loss_epoch</td><td>0.49857</td></tr><tr><td>train_loss_step</td><td>0.513</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.78487</td></tr><tr><td>val_auc</td><td>0.85572</td></tr><tr><td>val_f1</td><td>0.78886</td></tr><tr><td>val_loss_epoch</td><td>0.4725</td></tr><tr><td>val_loss_step</td><td>0.41302</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_32_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/op32axkz' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/op32axkz</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240105_152913-op32axkz\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8abc968c33ba4e4fb9484c9a84abe84b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016933333330477276, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240105_155853-pxt5lkue</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/pxt5lkue' target=\"_blank\">MLP_2_64_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/pxt5lkue' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/pxt5lkue</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 5.1 K \n",
      "2 | head        | Sequential       | 2.1 K \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "7.2 K     Trainable params\n",
      "0         Non-trainable params\n",
      "7.2 K     Total params\n",
      "0.029     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1eba8775f69490bbcca22647e3b1d33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▅▇▇▇▇▇███████▇█████████████████████████</td></tr><tr><td>train_auc</td><td>▁▆▇▇▇███████████████████████████████████</td></tr><tr><td>train_f1</td><td>▁▄▇▇▇▇██████████████████████████████████</td></tr><tr><td>train_loss_epoch</td><td>█▆▃▂▂▂▂▁▂▁▁▂▁▁▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▆▄▃▃▄▄▄▄▂▃▅▂▁▃▃▄▃▃▃▂▂▂▂▂▂▂▃▃▃▂▃▄▃▃▃▂▃▃▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▇▆▇██▇▇███████████████████████████▇████</td></tr><tr><td>val_auc</td><td>▁▃▄▆▇▇▇▇▇▇██████████████████████████████</td></tr><tr><td>val_f1</td><td>▁█▆███▇█████████████████████████████████</td></tr><tr><td>val_loss_epoch</td><td>█▄▃▂▂▂▂▂▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▁▁▁▁▁▁▂▁</td></tr><tr><td>val_loss_step</td><td>█▅▄▃▃▄▂▃▂▁▃▃▂▁▁▃▃▂▁▃▂▂▂▃▃▂▂▂▃▁▂▂▂▂▂▂▂▂▄▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.76978</td></tr><tr><td>train_auc</td><td>0.84616</td></tr><tr><td>train_f1</td><td>0.75926</td></tr><tr><td>train_loss_epoch</td><td>0.48693</td></tr><tr><td>train_loss_step</td><td>0.49611</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.78014</td></tr><tr><td>val_auc</td><td>0.85713</td></tr><tr><td>val_f1</td><td>0.80338</td></tr><tr><td>val_loss_epoch</td><td>0.46125</td></tr><tr><td>val_loss_step</td><td>0.43459</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_64_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/pxt5lkue' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/pxt5lkue</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240105_155853-pxt5lkue\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e472ee9fd45d4bb9aad08ed0254ca68b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01693333333435779, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240105_162753-yhgb9g2a</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/yhgb9g2a' target=\"_blank\">MLP_2_64_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/yhgb9g2a' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/yhgb9g2a</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 5.1 K \n",
      "2 | head        | Sequential       | 2.1 K \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "7.2 K     Trainable params\n",
      "0         Non-trainable params\n",
      "7.2 K     Total params\n",
      "0.029     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88f8066a1e324a29980a04ba6547a322",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▃▅▆▇▇▇▇▇▇▇▇▇▇▇▇█▇█████▇█████▇▇▇██▇▇████</td></tr><tr><td>train_auc</td><td>▁▂▅▇▇▇▇▇▆█▇██▇▇▆▇▇▆▇▇▇▇▇▇▇▆▇▇▇██▇▇█▇██▇▇</td></tr><tr><td>train_f1</td><td>▂▁▅▆▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇█▇█▇▇█▇█▇█▇█▇▇█▇▇███▇</td></tr><tr><td>train_loss_epoch</td><td>█▆▅▄▃▃▃▂▂▂▂▂▂▂▂▂▁▂▂▁▁▁▂▁▁▂▂▂▂▂▁▁▂▁▂▂▁▁▂▁</td></tr><tr><td>train_loss_step</td><td>█▇▆▄▃▆▄▂▂▃▃▃▂▂▄▃▃▃▃▂▃▂▃▂▃▃▂▂▃▃▂▃▄▁▂▅▂▂▃▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▁█████▅▄▅▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>▁▇▇▇▇▇██▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇██████████</td></tr><tr><td>val_f1</td><td>▁▁█████▄▄▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_epoch</td><td>▃▃▁▁▁▁▁▂▂▂▃▃▃▅▅▃▅▅▅▅▆▅▆█▇█▆▇▇▇▇▇█▇▇███▇▇</td></tr><tr><td>val_loss_step</td><td>▃▂▁▁▂▁▁▂▂▁▂▃▃▄▄▃▅▅▄▃▄▄▅▆▅▆▅▅▄▆▇▅▆▆▄▅█▅▄▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.74321</td></tr><tr><td>train_auc</td><td>0.73002</td></tr><tr><td>train_f1</td><td>0.72624</td></tr><tr><td>train_loss_epoch</td><td>0.5227</td></tr><tr><td>train_loss_step</td><td>0.53255</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.54137</td></tr><tr><td>val_auc</td><td>0.79933</td></tr><tr><td>val_f1</td><td>0.70245</td></tr><tr><td>val_loss_epoch</td><td>0.85754</td></tr><tr><td>val_loss_step</td><td>0.82118</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_64_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/yhgb9g2a' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/yhgb9g2a</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240105_162753-yhgb9g2a\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ecd7a3866fc44319c0da76a5beb4db0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240105_165705-v3330xiv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/v3330xiv' target=\"_blank\">MLP_2_64_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/v3330xiv' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/v3330xiv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 5.1 K \n",
      "2 | head        | Sequential       | 2.1 K \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "7.2 K     Trainable params\n",
      "0         Non-trainable params\n",
      "7.2 K     Total params\n",
      "0.029     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d655f1a33384d43aea7a4f632334d5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▄▆▇▇▇▇▇▇▇█▇█████████████████▇█████████▇</td></tr><tr><td>train_auc</td><td>██▇▆▅▅▄▄▃▃▂▂▂▂▂▁▂▁▂▂▂▂▂▂▃▁▂▂▂▁▁▂▂▂▂▂▂▂▂▂</td></tr><tr><td>train_f1</td><td>▁▃▆▇▆▇▆▇▇▇█▇██▇▇▇▇███████████▇█████████▇</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▅▄▃▃▄▄▃▃▂▂▃▂▁▂▃▂▂▂▂▂▁▁▂▂▂▁▂▂▂▂▂▂▂▂▂▁▂▂▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▇▇▆▇▆▇█▇█▇▇▇█▇▇██▇█████████████████████</td></tr><tr><td>val_auc</td><td>█▆▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_f1</td><td>▁█▇▇█▇▇█▇███████████████████████████████</td></tr><tr><td>val_loss_epoch</td><td>█▄▃▃▂▂▂▂▂▁▂▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_step</td><td>█▄▄▃▃▃▃▂▂▁▂▃▂▁▁▃▂▂▂▃▂▁▂▃▃▂▂▂▂▁▂▂▁▂▂▂▁▂▃▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.7497</td></tr><tr><td>train_auc</td><td>0.2679</td></tr><tr><td>train_f1</td><td>0.73892</td></tr><tr><td>train_loss_epoch</td><td>0.49847</td></tr><tr><td>train_loss_step</td><td>0.53239</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.80142</td></tr><tr><td>val_auc</td><td>0.13116</td></tr><tr><td>val_f1</td><td>0.82996</td></tr><tr><td>val_loss_epoch</td><td>0.45683</td></tr><tr><td>val_loss_step</td><td>0.43361</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_64_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/v3330xiv' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/v3330xiv</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240105_165705-v3330xiv\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4510371bed8641c4bf5df6899cbb3c4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01693333333435779, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240105_172604-jgpwef2d</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/jgpwef2d' target=\"_blank\">MLP_2_64_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/jgpwef2d' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/jgpwef2d</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 5.1 K \n",
      "2  | head        | Sequential       | 2.1 K \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 65    \n",
      "--------------------------------------------------\n",
      "7.3 K     Trainable params\n",
      "0         Non-trainable params\n",
      "7.3 K     Total params\n",
      "0.029     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fd8f7059cf146db94ce2b8e7173c302",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▆▆▇████▇██▇███████████████▇█▇██████▇███</td></tr><tr><td>train_auc</td><td>▁▆▇▇███████████████████████▇█▇██████████</td></tr><tr><td>train_f1</td><td>▁▆▆▇▇█▇█▇██▇██████▇████▇███▇█▇██████▇███</td></tr><tr><td>train_loss_epoch</td><td>█▆▄▃▂▁▂▁▂▁▂▂▂▁▁▁▂▁▂▁▁▂▂▂▁▁▁▂▁▂▂▁▂▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▅▅▃▃▃▄▃▃▃▄▃▅▂▄▄▄▃▃▅▂▃▃▂▃▁▁▃▄▃▃▂▃▂▄▃▃▃▃▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▅▇▇▇██████████████▇████████▇███████████</td></tr><tr><td>val_auc</td><td>▂▁▃▄▆▇▇▇▇██▇██████████▇█████▇███████████</td></tr><tr><td>val_f1</td><td>▁▆█▇▇█▇▇██▇██████▇████▇███▇███▇█████████</td></tr><tr><td>val_loss_epoch</td><td>█▄▄▃▂▂▃▂▂▂▃▂▂▁▂▂▂▂▁▂▂▂▃▂▂▂▃▂▂▂▂▁▂▂▂▂▂▂▂▁</td></tr><tr><td>val_loss_step</td><td>█▅▅▃▄▅▄▃▂▃▄▃▂▂▂▃▃▃▂▃▃▃▃▄▃▃▃▄▃▂▃▂▃▃▃▃▂▃▄▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.76151</td></tr><tr><td>train_auc</td><td>0.84338</td></tr><tr><td>train_f1</td><td>0.75952</td></tr><tr><td>train_loss_epoch</td><td>0.48163</td></tr><tr><td>train_loss_step</td><td>0.4397</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.77305</td></tr><tr><td>val_auc</td><td>0.85988</td></tr><tr><td>val_f1</td><td>0.79487</td></tr><tr><td>val_loss_epoch</td><td>0.44369</td></tr><tr><td>val_loss_step</td><td>0.38512</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_64_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/jgpwef2d' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/jgpwef2d</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240105_172604-jgpwef2d\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29412bfdbef24ed088457f1bf1e22536",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01693333333435779, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240105_175506-7f13sjlq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/7f13sjlq' target=\"_blank\">MLP_2_64_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/7f13sjlq' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/7f13sjlq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 5.1 K \n",
      "2  | head        | Sequential       | 2.1 K \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 4.2 K \n",
      "--------------------------------------------------\n",
      "11.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "11.4 K    Total params\n",
      "0.046     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d375a06efe5438f8bfc05c0852ec995",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▆▇▇█▇██▇███████████████████████████████</td></tr><tr><td>train_auc</td><td>▁▅▇▇████████████████████████████████████</td></tr><tr><td>train_f1</td><td>▁▅▇▇█▇██▇█████████████▇█████████████████</td></tr><tr><td>train_loss_epoch</td><td>█▅▃▂▂▂▁▂▂▂▁▁▁▁▁▁▁▁▁▁▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▂</td></tr><tr><td>train_loss_step</td><td>█▅▃▄▅▅▃▃▃▃▂▃▅▄▄▃▂▂▃▃▁▃▃▂▃▃▃▃▄▂▄▂▃▂▁▂▁▂▄▅</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▇██▇███▇▆██████████████████████████████</td></tr><tr><td>val_auc</td><td>▁▁▄▆▇▇▇▇▇▇▇▇████████████████████████████</td></tr><tr><td>val_f1</td><td>▁███▇████▇██████████████████████████████</td></tr><tr><td>val_loss_epoch</td><td>█▄▂▂▃▂▁▂▂▄▂▁▂▁▁▂▁▂▁▂▁▂▁▁▂▁▁▁▁▁▂▁▁▂▁▁▁▁▁▁</td></tr><tr><td>val_loss_step</td><td>█▅▂▃▃▃▂▃▃▅▃▂▂▂▁▃▂▃▂▃▁▃▂▁▄▁▂▂▂▁▂▃▃▃▁▃▂▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.77804</td></tr><tr><td>train_auc</td><td>0.84447</td></tr><tr><td>train_f1</td><td>0.77157</td></tr><tr><td>train_loss_epoch</td><td>0.49364</td></tr><tr><td>train_loss_step</td><td>0.55192</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.78723</td></tr><tr><td>val_auc</td><td>0.85666</td></tr><tr><td>val_f1</td><td>0.80263</td></tr><tr><td>val_loss_epoch</td><td>0.46354</td></tr><tr><td>val_loss_step</td><td>0.4352</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_2_64_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/7f13sjlq' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/7f13sjlq</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240105_175506-7f13sjlq\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ba4876bff1b4822a17fb860905066d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01691666666883975, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240105_182439-0rjk5iy6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/0rjk5iy6' target=\"_blank\">MLP_3_16_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/0rjk5iy6' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/0rjk5iy6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 768   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "922       Trainable params\n",
      "0         Non-trainable params\n",
      "922       Total params\n",
      "0.004     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▂▂▁▂▄▆▇▇▇██▇▇████▇███▇███▇▇█▇████▇██████</td></tr><tr><td>train_auc</td><td>▁▁▁▂▂▆▇▇▇███████████████████▇███████████</td></tr><tr><td>train_f1</td><td>▆▁▃▁▅▆▇▇███▇▇██▇█▇███▇█▇▇▇▇▇▆▇▇██▇██████</td></tr><tr><td>train_loss_epoch</td><td>███▇▇▄▃▂▂▂▂▂▂▂▂▁▂▂▂▁▁▂▂▂▂▂▂▂▂▂▂▂▁▂▁▂▂▁▁▂</td></tr><tr><td>train_loss_step</td><td>████▇▃▂▆▄▄▂▄▂▅▄▂▅▂▃▂▃▄▃▃▄▄▂▃▂▂▃▁▄▃▃▁▃▂▄▄</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▃▁▄▁▇▇▇█████████████████████████████████</td></tr><tr><td>val_auc</td><td>▁▆▇▇████████████████████████████████████</td></tr><tr><td>val_f1</td><td>▇▁▇▁████████████████████████████████████</td></tr><tr><td>val_loss_epoch</td><td>████▇▃▂▂▂▁▁▂▂▂▂▂▁▁▂▁▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_step</td><td>████▇▄▂▃▃▂▁▃▃▂▂▄▃▁▃▃▃▂▃▃▂▂▁▂▁▂▂▂▃▁▁▃▁▂▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.76328</td></tr><tr><td>train_auc</td><td>0.82467</td></tr><tr><td>train_f1</td><td>0.75564</td></tr><tr><td>train_loss_epoch</td><td>0.52294</td></tr><tr><td>train_loss_step</td><td>0.57589</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.78251</td></tr><tr><td>val_auc</td><td>0.85193</td></tr><tr><td>val_f1</td><td>0.78899</td></tr><tr><td>val_loss_epoch</td><td>0.48423</td></tr><tr><td>val_loss_step</td><td>0.46653</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_16_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/0rjk5iy6' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/0rjk5iy6</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240105_182439-0rjk5iy6\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08d0ea254486437aa3b7226319f30e54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016933333330477276, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240105_185312-7t50nh15</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/7t50nh15' target=\"_blank\">MLP_3_16_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/7t50nh15' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/7t50nh15</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 768   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "922       Trainable params\n",
      "0         Non-trainable params\n",
      "922       Total params\n",
      "0.004     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c37900ac7e8a4d0fa44780252f2d0052",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▂▁▁▂▂▂▅▃▄▅▄▄▆▅▇██▆▇▇█▇▇▇▇▇▇▇▇▇▇█▇██▇▇▇██</td></tr><tr><td>train_auc</td><td>▁▁▁▂▁▂▃▄▄▃▃▄▅▅▇█▇▆▇▇▇█▇▇▇▇▇▇▇█▆▇███▇▇██▇</td></tr><tr><td>train_f1</td><td>▇▃▄▁▃▁▆▅▅▅▆▅▆▆▇██▆▇▆█▇▇▆▇▆▇▆▇▆▇▇▇▇█▇▇▇██</td></tr><tr><td>train_loss_epoch</td><td>█▇▆▆▆▆▅▅▆▅▅▅▄▄▂▁▂▄▃▃▂▂▃▃▂▄▃▃▂▂▃▃▂▂▂▃▃▂▂▃</td></tr><tr><td>train_loss_step</td><td>█▆▇▆▇▅▅▆▆▅▄▄▃▄▃▂▅▆▃▃▄▃▅▄▃▄▂▃▂▃▅▁▄▃▄▁▂▂▄▄</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>▁▄▃▄▄▃▃▃▃▇▇█████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_epoch</td><td>▆▇▇▆▆▆▇▆█▇▇▇▅▅▂▁▁▃▂▂▁▂▂▃▂▂▂▂▂▂▂▃▂▁▃▂▂▂▃▄</td></tr><tr><td>val_loss_step</td><td>▅▆▆▅▄▅█▃█▆▇▇▅▆▃▃▁▃▃▄▂▃▃▂▃▂▃▄▂▂▂▅▁▁▅▃▃▂▅▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.66116</td></tr><tr><td>train_auc</td><td>0.69035</td></tr><tr><td>train_f1</td><td>0.63205</td></tr><tr><td>train_loss_epoch</td><td>0.64301</td></tr><tr><td>train_loss_step</td><td>0.64664</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.54137</td></tr><tr><td>val_auc</td><td>0.75333</td></tr><tr><td>val_f1</td><td>0.70245</td></tr><tr><td>val_loss_epoch</td><td>0.68143</td></tr><tr><td>val_loss_step</td><td>0.68711</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_16_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/7t50nh15' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/7t50nh15</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240105_185312-7t50nh15\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c257aec308574f5eb80156b4bc3e4dae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016933333330477276, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240105_192135-eret6xfp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/eret6xfp' target=\"_blank\">MLP_3_16_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/eret6xfp' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/eret6xfp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 768   \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "922       Trainable params\n",
      "0         Non-trainable params\n",
      "922       Total params\n",
      "0.004     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "835937f523394718abbe6ccd4e83dfc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▃▃▄▅▆▆▆▆▇▇▆▆▇▇▇▇▇▇█▇█▇▇▇█▇▇██▇▇▇██████</td></tr><tr><td>train_auc</td><td>▄▅▅▆▆▇██▇▆▇▇▆▅▅▆▄▄▄▄▄▄▃▂▃▃▂▁▂▂▂▃▂▂▁▂▂▂▂▁</td></tr><tr><td>train_f1</td><td>▁▁▂▄▄▅▅▆▅▅▆▆▆▇▇▇▇▇▇▇█▆▇▆▆▇▇▇▆▇▇▇▇▆▇▇▇▇██</td></tr><tr><td>train_loss_epoch</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▃▃▂▂▂▁▂▂▂▁▂▁▂▂▁▂▂▂▁▂▂▂▁▁▁▁▁▁▁▁▁▂▂▂▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▆▄▆▅▅▅▅▇▇▆▆█▇▇▇▆▆▆▇▆▇▆█▇█▇▇█▇▇▇▆██▇▇▇▇▆</td></tr><tr><td>val_auc</td><td>▆▆▆▇▇████████▆▆▇▂▃▄▂▃▂▂▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_f1</td><td>▁▇▄▆▆▅▆▆▇▇▇▇█▇▇▇▇▇▇█▆▇▇█▇████▇▇▇▇██▇▇▇▇▇</td></tr><tr><td>val_loss_epoch</td><td>█▆▆▅▅▄▃▃▂▂▂▂▂▂▂▂▂▂▂▁▃▂▂▂▁▁▁▁▁▁▁▂▂▁▁▂▂▁▁▂</td></tr><tr><td>val_loss_step</td><td>█▆▆▅▆▅▃▄▃▂▂▃▃▃▂▄▃▃▂▂▃▂▃▃▂▂▁▂▁▁▂▂▃▁▁▃▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.73672</td></tr><tr><td>train_auc</td><td>0.41742</td></tr><tr><td>train_f1</td><td>0.74427</td></tr><tr><td>train_loss_epoch</td><td>0.54296</td></tr><tr><td>train_loss_step</td><td>0.5456</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.72813</td></tr><tr><td>val_auc</td><td>0.13285</td></tr><tr><td>val_f1</td><td>0.7013</td></tr><tr><td>val_loss_epoch</td><td>0.5125</td></tr><tr><td>val_loss_step</td><td>0.50782</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_16_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/eret6xfp' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/eret6xfp</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240105_192135-eret6xfp\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ef88ad577224d96a790aa86a9aca950",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01691666666495924, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240105_195006-sjvhgbxc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/sjvhgbxc' target=\"_blank\">MLP_3_16_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/sjvhgbxc' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/sjvhgbxc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 768   \n",
      "2  | head        | Sequential       | 154   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 17    \n",
      "--------------------------------------------------\n",
      "939       Trainable params\n",
      "0         Non-trainable params\n",
      "939       Total params\n",
      "0.004     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "725cbe7beed54803bf4e9f90458a62f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▂▃▆▆▇▇▇█▇▇▇▇▇█▇███████████████████████</td></tr><tr><td>train_auc</td><td>▁▁▁▂▆▆▇▇▇▇▇▇█▇██▇███████████████████████</td></tr><tr><td>train_f1</td><td>▅▁▃▂▆▅▆▇▇▇▇▆▇▇▇▇▇███▇▇▇█▇█▇▇▇██████████▇</td></tr><tr><td>train_loss_epoch</td><td>██▇▇▅▄▃▃▂▂▃▃▂▂▂▁▂▁▂▁▂▁▁▂▂▁▁▂▁▁▂▂▁▂▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>███▇▆▄▃▃▄▃▄▄▃▄▂▂▄▂▃▂▂▃▃▃▂▃▂▃▃▅▃▃▃▂▂▃▂▁▂▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▃▁▂▃▄▇▇▆█▇██▇▇▇██▇▇▇█▇█▇█▇▇███▇█▇▇▇▇▆███</td></tr><tr><td>val_auc</td><td>▁▇▇█████████████████████████████████████</td></tr><tr><td>val_f1</td><td>▇▁▂▄▅▇█▇█▇███▇████▇████▇██████▇██▇█▇▇███</td></tr><tr><td>val_loss_epoch</td><td>████▆▄▃▄▂▃▂▂▂▃▃▂▁▂▃▂▂▂▂▂▂▂▂▁▂▂▃▁▂▃▃▃▄▁▂▂</td></tr><tr><td>val_loss_step</td><td>████▆▅▄▄▃▄▃▄▃▅▄▄▁▃▃▂▄▃▃▃▄▄▄▃▃▄▄▂▃▅▄▄▅▁▂▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.74734</td></tr><tr><td>train_auc</td><td>0.82277</td></tr><tr><td>train_f1</td><td>0.74186</td></tr><tr><td>train_loss_epoch</td><td>0.52206</td></tr><tr><td>train_loss_step</td><td>0.52736</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.77305</td></tr><tr><td>val_auc</td><td>0.8502</td></tr><tr><td>val_f1</td><td>0.77358</td></tr><tr><td>val_loss_epoch</td><td>0.50503</td></tr><tr><td>val_loss_step</td><td>0.49777</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_16_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/sjvhgbxc' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/sjvhgbxc</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240105_195006-sjvhgbxc\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f2113eb8d194440a35a4109b71a3965",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.017183333336530875, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240105_201847-o6rdk6hr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/o6rdk6hr' target=\"_blank\">MLP_3_16_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/o6rdk6hr' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/o6rdk6hr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 768   \n",
      "2  | head        | Sequential       | 154   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 281   \n",
      "--------------------------------------------------\n",
      "1.2 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.2 K     Total params\n",
      "0.005     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d435fa9469d543bc8b1d91e694bf3e83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▂▂▅▆▇▇▇▇██████████████████████████████</td></tr><tr><td>train_auc</td><td>▁▂▂▂▅▆▇▇▇▇██████████████████████████████</td></tr><tr><td>train_f1</td><td>▅▁▃▂▆▆▇█▇▇▇█████████████████████████████</td></tr><tr><td>train_loss_epoch</td><td>█▇▇▇▆▄▂▂▂▂▂▂▂▂▁▁▁▁▁▂▁▁▁▂▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>████▇▄▄▃▄▄▄▂▃▃▃▂▁▂▄▃▂▄▂▂▃▃▃▃▄▂▄▃▂▄▃▂▄▃▃▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▃▁▄▄▄▅▆▇▇████▇██████████████████████████</td></tr><tr><td>val_auc</td><td>▁▆▇▇▇▇██████████████████████████████████</td></tr><tr><td>val_f1</td><td>▇▁▅▅▆▆▇▇█████▇██████████████████████████</td></tr><tr><td>val_loss_epoch</td><td>████▆▄▃▂▂▁▁▁▂▃▂▂▁▂▂▁▂▂▁▁▂▂▂▂▂▁▁▁▁▁▁▂▂▁▁▂</td></tr><tr><td>val_loss_step</td><td>████▆▆▂▃▃▂▁▃▂▄▂▃▁▄▂▁▃▄▂▃▃▂▂▃▃▂▁▁▂▂▂▂▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.75974</td></tr><tr><td>train_auc</td><td>0.82951</td></tr><tr><td>train_f1</td><td>0.75585</td></tr><tr><td>train_loss_epoch</td><td>0.50945</td></tr><tr><td>train_loss_step</td><td>0.50066</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.78251</td></tr><tr><td>val_auc</td><td>0.85011</td></tr><tr><td>val_f1</td><td>0.78899</td></tr><tr><td>val_loss_epoch</td><td>0.48963</td></tr><tr><td>val_loss_step</td><td>0.48965</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_16_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/o6rdk6hr' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/o6rdk6hr</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240105_201847-o6rdk6hr\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f0b400eda5743fa835cc9c9a0528a23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01693333333435779, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240105_204807-mo3hy7se</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/mo3hy7se' target=\"_blank\">MLP_3_32_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/mo3hy7se' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/mo3hy7se</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 2.6 K \n",
      "2 | head        | Sequential       | 562   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "3.1 K     Trainable params\n",
      "0         Non-trainable params\n",
      "3.1 K     Total params\n",
      "0.012     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▆▇▇▇█▇▇▇██████████████████████████████</td></tr><tr><td>train_auc</td><td>▁▂▆▇▇███████████████████████████████████</td></tr><tr><td>train_f1</td><td>▁▅▇▇▇▇██████████████████████████████████</td></tr><tr><td>train_loss_epoch</td><td>██▅▃▂▂▂▂▂▂▂▁▁▁▁▁▁▂▂▁▁▂▂▁▁▁▂▁▂▁▁▂▁▁▁▁▂▁▁▁</td></tr><tr><td>train_loss_step</td><td>██▆▅▄▃▂▃▂▃▅▂▁▂▃▃▃▃▃▃▂▅▃▂▂▃▂▃▃▄▂▂▂▁▃▁▂▃▃▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▅▆▇█████████████████▇██▇███████████████</td></tr><tr><td>val_auc</td><td>▁▁▂▅▆▆▇▇▇▇██▇███████████████████████████</td></tr><tr><td>val_f1</td><td>▁▆▇▇█████████████████▇██▇███████████████</td></tr><tr><td>val_loss_epoch</td><td>█▇▃▂▂▂▂▁▂▁▂▁▂▁▁▁▁▁▂▁▁▃▂▁▃▁▁▁▁▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_step</td><td>█▇▅▃▄▃▄▂▃▂▄▃▅▃▂▂▃▂▂▂▃▄▃▁▃▃▂▂▂▃▄▃▂▂▂▃▃▃▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.76564</td></tr><tr><td>train_auc</td><td>0.84041</td></tr><tr><td>train_f1</td><td>0.75659</td></tr><tr><td>train_loss_epoch</td><td>0.49264</td></tr><tr><td>train_loss_step</td><td>0.49979</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.78014</td></tr><tr><td>val_auc</td><td>0.85504</td></tr><tr><td>val_f1</td><td>0.8</td></tr><tr><td>val_loss_epoch</td><td>0.46496</td></tr><tr><td>val_loss_step</td><td>0.43954</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_32_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/mo3hy7se' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/mo3hy7se</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240105_204807-mo3hy7se\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0479768194194bd59e431d60db4c3c57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01691666666883975, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240105_211853-qqd2tc59</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/qqd2tc59' target=\"_blank\">MLP_3_32_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/qqd2tc59' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/qqd2tc59</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 2.6 K \n",
      "2 | head        | Sequential       | 562   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "3.1 K     Trainable params\n",
      "0         Non-trainable params\n",
      "3.1 K     Total params\n",
      "0.012     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▃▆▆▇▆▇▆▇▇▇▇▇▇▇▇█▇▇▇▇▇████▇▇▇▇█▇▇█▇█▇▇▇█</td></tr><tr><td>train_auc</td><td>▁▂▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇▇▇▇█▇█▇██▇█▇█▇█</td></tr><tr><td>train_f1</td><td>▁▄▅▅▆▄▆▆▆▆▆▇▇▆▆▇▇▇▇▇▇▇████▇▇▇▇█▆▇▇▇▇▇▇▆▇</td></tr><tr><td>train_loss_epoch</td><td>█▇▅▄▃▄▃▃▂▂▃▂▂▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▂▁▁▂▁▂▁▂▂</td></tr><tr><td>train_loss_step</td><td>█▇▆▅▅▅▅▄▆▄▃▄▃▃▂▄▄▅▆▄▁▇▁▂▃▃▃▄▃▅▁▄▃▂▁▃▃▂▅▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▃▁▁▂█████████▆▆▆▆▃▃▃▃▃▃▃▆▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃</td></tr><tr><td>val_auc</td><td>▃▄██▆▅▃▁▇▁▁▁▁▃▄▄▅▅▄▅▅▅▅▅▅▆▅▆▆▅▅▇▆▅▅▇█▆▆█</td></tr><tr><td>val_f1</td><td>▇▁▁▂█████████████▇▇▇▇▇▇▇█▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>val_loss_epoch</td><td>█▇▅▅▂▃▁▃▂▁▂▁▂▂▂▁▂▂▂▁▃▂▂▁▂▂▃▂▃▄▄▅▄▆▅▅▅▄▃▃</td></tr><tr><td>val_loss_step</td><td>█▇▅▅▃▅▄▄▃▁▄▃▄▃▃▂▃▄▄▁▇▃▄▂▃▃▄▂▄▆▆▃▂▇▆▄▆▅▂▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.72432</td></tr><tr><td>train_auc</td><td>0.77055</td></tr><tr><td>train_f1</td><td>0.71226</td></tr><tr><td>train_loss_epoch</td><td>0.56758</td></tr><tr><td>train_loss_step</td><td>0.57103</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.54137</td></tr><tr><td>val_auc</td><td>0.79147</td></tr><tr><td>val_f1</td><td>0.70245</td></tr><tr><td>val_loss_epoch</td><td>0.65231</td></tr><tr><td>val_loss_step</td><td>0.66068</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_32_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/qqd2tc59' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/qqd2tc59</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240105_211853-qqd2tc59\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69603cc69b264c2a9f5605a9f0e3d575",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01693333333435779, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240105_214922-0ut2fk52</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/0ut2fk52' target=\"_blank\">MLP_3_32_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/0ut2fk52' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/0ut2fk52</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 2.6 K \n",
      "2 | head        | Sequential       | 562   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "3.1 K     Trainable params\n",
      "0         Non-trainable params\n",
      "3.1 K     Total params\n",
      "0.012     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06b6bd3f30e44d74b6b3d74156b42b1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▂▄▆▆▇▇▇▇▇▇▇▇█▇▇▇▇▇▇▇█▇▇█▇▇▇▇█▇█▇██▇▇█▇</td></tr><tr><td>train_auc</td><td>▃▂▁▄▆▆████▇▇▆▅▅▅▅▃▄▅▄▄▅▄▇▆▆▇▅▅▇▆█▇▇▇▇█▆█</td></tr><tr><td>train_f1</td><td>▁▂▂▂▄▅▇▆▇▆▇▇▇▇▇▇▇▇▇▇▇█▇█▇▇▇▇▇▇█▇█▇██▇▇█▇</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▄▄▄▃▃▂▃▂▃▂▂▁▂▂▂▃▂▂▁▁▂▂▂▁▂▂▁▂▂▁▂▂▁▂▂▁▂▂▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▁▅▄▅▆▅▆▆▇▄▇▇▇▇▆▇█▆▇█▅█▆▆█▇██▇▇█▇█▆█████</td></tr><tr><td>val_auc</td><td>▄▂▁▇▇███████████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▆▅▆▇▆▆▆█▅██▇▇▇▇█▇██▆█▇▇█▇███████▇█████</td></tr><tr><td>val_loss_epoch</td><td>█▄▄▃▃▂▃▂▂▁▃▁▂▁▂▂▂▁▂▁▁▃▁▂▂▁▂▁▁▁▁▁▁▁▂▁▁▁▁▁</td></tr><tr><td>val_loss_step</td><td>█▄▄▃▃▃▃▃▃▂▄▂▃▂▂▂▂▁▂▁▂▃▂▂▂▂▂▁▂▂▂▂▂▁▂▂▂▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.75738</td></tr><tr><td>train_auc</td><td>0.62134</td></tr><tr><td>train_f1</td><td>0.75166</td></tr><tr><td>train_loss_epoch</td><td>0.51273</td></tr><tr><td>train_loss_step</td><td>0.53472</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.79905</td></tr><tr><td>val_auc</td><td>0.86963</td></tr><tr><td>val_f1</td><td>0.80899</td></tr><tr><td>val_loss_epoch</td><td>0.45283</td></tr><tr><td>val_loss_step</td><td>0.44222</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_32_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/0ut2fk52' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/0ut2fk52</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240105_214922-0ut2fk52\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d11c763f4f3545dca1f07b801766afd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01691666666883975, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240105_221826-3y3wjei5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/3y3wjei5' target=\"_blank\">MLP_3_32_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/3y3wjei5' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/3y3wjei5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 2.6 K \n",
      "2  | head        | Sequential       | 562   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 33    \n",
      "--------------------------------------------------\n",
      "3.2 K     Trainable params\n",
      "0         Non-trainable params\n",
      "3.2 K     Total params\n",
      "0.013     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4589b7d48d54df9a0e87d1a860620e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▃▆▇▇▇▇█▇▇█▇████▇█▇█▇█▇██████████▇▇█████</td></tr><tr><td>train_auc</td><td>▁▃▆▇█▇█████▇████████████████████████████</td></tr><tr><td>train_f1</td><td>▁▅▆▇▇▇▇█▇█▇▇██████▇█▇█▇██████▇███▇██████</td></tr><tr><td>train_loss_epoch</td><td>██▅▃▂▂▂▁▂▂▂▂▂▁▁▁▁▁▂▁▂▂▂▁▁▁▁▁▂▁▁▁▁▂▂▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>██▇▄▅▃▄▃▄▃▅▄▄▁▃▄▂▃▂▂▂▃▃▄▃▄▄▂▃▃▄▄▂▃▃▃▂▂▃▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▅▅▇███▇█▇▇█████████████████▇████████▇██</td></tr><tr><td>val_auc</td><td>▂▂▁▅▆▇▇▇▇▇▇▇▇▇▇█████████████████████████</td></tr><tr><td>val_f1</td><td>▁▆▆▇█████▇██████████████████████████████</td></tr><tr><td>val_loss_epoch</td><td>█▇▅▃▂▂▂▂▂▂▂▂▁▁▂▂▁▁▂▁▁▂▁▁▁▁▁▁▂▂▁▁▁▁▁▁▂▁▁▁</td></tr><tr><td>val_loss_step</td><td>█▇▅▃▃▂▄▂▄▃▄▃▁▃▃▂▁▂▃▂▁▂▂▂▁▃▂▂▂▄▃▁▂▁▁▃▄▃▃▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.76269</td></tr><tr><td>train_auc</td><td>0.83884</td></tr><tr><td>train_f1</td><td>0.75155</td></tr><tr><td>train_loss_epoch</td><td>0.49663</td></tr><tr><td>train_loss_step</td><td>0.47809</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.77778</td></tr><tr><td>val_auc</td><td>0.8547</td></tr><tr><td>val_f1</td><td>0.80335</td></tr><tr><td>val_loss_epoch</td><td>0.47072</td></tr><tr><td>val_loss_step</td><td>0.46783</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_32_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/3y3wjei5' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/3y3wjei5</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240105_221826-3y3wjei5\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c04d66e057d4eb6972443f502063314",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016933333330477276, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240105_224753-4ergs8a2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/4ergs8a2' target=\"_blank\">MLP_3_32_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/4ergs8a2' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/4ergs8a2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 2.6 K \n",
      "2  | head        | Sequential       | 562   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 1.1 K \n",
      "--------------------------------------------------\n",
      "4.2 K     Trainable params\n",
      "0         Non-trainable params\n",
      "4.2 K     Total params\n",
      "0.017     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f81c1586be0b43fca9fc76b8d910091f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▃▆▇▇▇▇▇▇▇█▇███████████▇████████████████</td></tr><tr><td>train_auc</td><td>▁▂▆▇▇▇██▇███████████████████████████████</td></tr><tr><td>train_f1</td><td>▁▄▆▇▇▇▇▇▇▇█▇███▇█▇▇█▇██▇▇█▇█████████▇███</td></tr><tr><td>train_loss_epoch</td><td>██▅▃▂▂▂▂▂▂▂▁▁▂▂▁▂▁▂▂▂▁▁▂▂▁▂▁▁▂▁▁▁▁▁▁▂▁▂▁</td></tr><tr><td>train_loss_step</td><td>█▇▅▃▂▂▂▂▄▂▁▂▂▃▂▂▂▂▁▁▂▁▄▂▃▁▂▃▁▂▃▂▃▁▂▃▂▁▄▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▆▇█▇███████████████████████████████████</td></tr><tr><td>val_auc</td><td>▁▂▁▅▆▆▇▇▇▇▇▇▇███████████████████████████</td></tr><tr><td>val_f1</td><td>▁▇▇█████████████████████████████████████</td></tr><tr><td>val_loss_epoch</td><td>█▇▃▃▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▂▁▁▁▁▁▂▁▁▂▂▁▂▁▁▂▂▁▁▁▂</td></tr><tr><td>val_loss_step</td><td>█▇▂▄▂▂▂▃▃▃▂▃▃▃▄▃▃▂▃▄▂▂▃▃▁▃▂▂▃▃▃▂▂▂▂▄▁▁▁▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.76564</td></tr><tr><td>train_auc</td><td>0.83924</td></tr><tr><td>train_f1</td><td>0.76355</td></tr><tr><td>train_loss_epoch</td><td>0.4982</td></tr><tr><td>train_loss_step</td><td>0.50417</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.77778</td></tr><tr><td>val_auc</td><td>0.85315</td></tr><tr><td>val_f1</td><td>0.7783</td></tr><tr><td>val_loss_epoch</td><td>0.51236</td></tr><tr><td>val_loss_step</td><td>0.53993</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_32_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/4ergs8a2' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/4ergs8a2</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240105_224753-4ergs8a2\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1a7a88d4899464782bd48f6b57b01a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01693333333435779, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240105_232005-zdyrz9df</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/zdyrz9df' target=\"_blank\">MLP_3_64_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/zdyrz9df' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/zdyrz9df</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 9.2 K \n",
      "2 | head        | Sequential       | 2.1 K \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "11.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "11.4 K    Total params\n",
      "0.045     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c8c84e939a7402f81a2336b006dccc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▅▇██▇█▇██████▇▇████████████████████████</td></tr><tr><td>train_auc</td><td>▁▅▇█████████████████████████████████████</td></tr><tr><td>train_f1</td><td>▁▅▇██▇█▇██▇███▇▇█████▇█████▇████████▇███</td></tr><tr><td>train_loss_epoch</td><td>█▆▃▂▂▂▁▂▁▁▂▁▂▂▂▂▁▁▁▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▇▄▄▃▄▃▄▃▁▅▄▂▄▄▂▂▄▄▃▁▂▂▃▄▄▃▃▂▁▄▃▂▄▃▂▃▂▃▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▇▇▇██████████████████████▇█████████████</td></tr><tr><td>val_auc</td><td>▁▁▃▅▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇███▇█████████████████</td></tr><tr><td>val_f1</td><td>▁▇█▇████████████████████████████████████</td></tr><tr><td>val_loss_epoch</td><td>█▄▂▃▂▂▁▂▂▂▂▁▂▂▁▂▂▁▂▁▂▂▂▁▂▁▂▁▂▁▁▂▁▂▂▁▂▁▂▁</td></tr><tr><td>val_loss_step</td><td>█▄▃▄▃▃▂▃▃▂▃▂▃▃▂▃▃▂▄▁▃▄▃▂▃▃▃▂▄▃▂▃▂▃▄▃▃▂▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.7686</td></tr><tr><td>train_auc</td><td>0.84936</td></tr><tr><td>train_f1</td><td>0.75921</td></tr><tr><td>train_loss_epoch</td><td>0.48379</td></tr><tr><td>train_loss_step</td><td>0.47592</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.7896</td></tr><tr><td>val_auc</td><td>0.85596</td></tr><tr><td>val_f1</td><td>0.8044</td></tr><tr><td>val_loss_epoch</td><td>0.4724</td></tr><tr><td>val_loss_step</td><td>0.47662</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_64_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/zdyrz9df' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/zdyrz9df</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240105_232005-zdyrz9df\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fcbd98241694e93ab0120929326255a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240105_235010-nhvlgpi0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/nhvlgpi0' target=\"_blank\">MLP_3_64_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/nhvlgpi0' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/nhvlgpi0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 9.2 K \n",
      "2 | head        | Sequential       | 2.1 K \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "11.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "11.4 K    Total params\n",
      "0.045     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3f37ed4edb14a83b0985913f6c9afdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▅▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇██▇▇▇███▇▇█▇▇██▇▇▇█▇██</td></tr><tr><td>train_auc</td><td>▁▁▄▆▇▆▇▇▇▇▇▇▇▇▇▇▇▇██▇█▇████▇███████▇████</td></tr><tr><td>train_f1</td><td>▁▂▃▆▆▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇▇▇▇▇█▇█▇▇█▇██</td></tr><tr><td>train_loss_epoch</td><td>█▇▆▄▃▄▃▃▂▃▃▃▂▂▂▂▂▂▂▁▂▂▂▁▁▁▁▂▂▁▁▁▁▂▂▂▁▂▁▁</td></tr><tr><td>train_loss_step</td><td>██▇▄▄▅▄▄▄▃▆▄▄▄▅▃▃▂▃▃▂▂▃▃▃▄▂▂▂▃▃▃▃▄▄▁▂▂▂▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▁████▆▆▆▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃</td></tr><tr><td>val_auc</td><td>▁▃▄▃▅▇▇▇▇█▇███▇█▇██▇█▇▇███▇██▇█▇█▇█▇▇▇▇█</td></tr><tr><td>val_f1</td><td>▁▁███████▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>val_loss_epoch</td><td>▆▅▄▃▁▃▁▂▂▃▄▄▅▄▅▅▃▄▅▅▄▅▄▇▆▆▅▄▆▅▆▇▇▅██▇▆▇█</td></tr><tr><td>val_loss_step</td><td>▅▄▃▃▁▃▁▂▁▄▃▃▄▅▄▅▃▂▃▃▃▄▃▅▄▆▄▂▆▃▃▅▄▃▆▅▅▄▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.73967</td></tr><tr><td>train_auc</td><td>0.80341</td></tr><tr><td>train_f1</td><td>0.73353</td></tr><tr><td>train_loss_epoch</td><td>0.53662</td></tr><tr><td>train_loss_step</td><td>0.54253</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.54137</td></tr><tr><td>val_auc</td><td>0.79982</td></tr><tr><td>val_f1</td><td>0.70245</td></tr><tr><td>val_loss_epoch</td><td>0.73369</td></tr><tr><td>val_loss_step</td><td>0.77242</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_64_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/nhvlgpi0' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/nhvlgpi0</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240105_235010-nhvlgpi0\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6676897060b47e2b432fde9efc9ba1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01693333333435779, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240106_002033-a7ic6p30</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/a7ic6p30' target=\"_blank\">MLP_3_64_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/a7ic6p30' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/a7ic6p30</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 9.2 K \n",
      "2 | head        | Sequential       | 2.1 K \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "11.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "11.4 K    Total params\n",
      "0.045     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▂▄▆▆▇▆▇▇▇▇▇▇▇▇▇▇▇█▇██▇▇█▇██▇▇████▇▇▇██</td></tr><tr><td>train_auc</td><td>▁▁▃▅▇▇▇▇██▇▇█▇▇██▇██████▇████████████▇▇█</td></tr><tr><td>train_f1</td><td>▃▁▃▄▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇▇██▇▇█▇█████████████</td></tr><tr><td>train_loss_epoch</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▄▃▂▂▂▁▂▁▁▂▂▁▂▁▁▂▂▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▃▅▆▅▇▇▇▇▇█▇█▇████▇███▇███▇█▇██▇██▇█████</td></tr><tr><td>val_auc</td><td>▆▁▇▆▇▇▆▇█▇▇▇▇██▇▇██▇█████████▇██████████</td></tr><tr><td>val_f1</td><td>▁▄▆▇▆▇██▇███████████████████▇███████████</td></tr><tr><td>val_loss_epoch</td><td>█▅▄▃▃▂▂▂▂▂▂▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▂▁▁▁▂▁</td></tr><tr><td>val_loss_step</td><td>█▅▅▄▃▂▂▃▃▁▂▂▃▂▂▃▂▁▂▁▁▃▂▂▂▂▂▁▃▂▁▂▁▂▃▂▁▂▃▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.77981</td></tr><tr><td>train_auc</td><td>0.81404</td></tr><tr><td>train_f1</td><td>0.77018</td></tr><tr><td>train_loss_epoch</td><td>0.48778</td></tr><tr><td>train_loss_step</td><td>0.49874</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.81324</td></tr><tr><td>val_auc</td><td>0.87444</td></tr><tr><td>val_f1</td><td>0.82863</td></tr><tr><td>val_loss_epoch</td><td>0.44786</td></tr><tr><td>val_loss_step</td><td>0.43396</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_64_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/a7ic6p30' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/a7ic6p30</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240106_002033-a7ic6p30\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25bd0cecb3c94115a556a123ddcc10c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240106_004944-t5gk8zei</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/t5gk8zei' target=\"_blank\">MLP_3_64_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/t5gk8zei' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/t5gk8zei</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 9.2 K \n",
      "2  | head        | Sequential       | 2.1 K \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 65    \n",
      "--------------------------------------------------\n",
      "11.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "11.4 K    Total params\n",
      "0.046     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7e97782484f411aba6df725c5a342d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▅▆▆▇▇▇▇▇█▇▇▇▇▇▇█▇▇▇▇▇██▇█▇▇▇▇██▇██▇█▇▇▇</td></tr><tr><td>train_auc</td><td>▁▅▇▇▇███████████████████████████▇███████</td></tr><tr><td>train_f1</td><td>▁▅▇▇▇▇▇█▇█▇▇▇█▇▇█▇█▇▇███▇█▇▇▇▇███▇███▇██</td></tr><tr><td>train_loss_epoch</td><td>█▆▄▃▂▂▂▂▂▂▂▁▂▂▂▂▁▁▂▂▂▂▁▂▁▁▁▂▂▂▁▂▂▂▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▅▄▄▃▃▂▄▃▁▃▁▂▂▂▃▂▂▁▁▄▂▃▃▂▄▄▂▁▃▂▂▃▃▃▄▃▃▃▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▂▅▁███▆▇▇██▇██▇█▇▇████▆█▇▇█▇▇█▆▃▇▇██▆█▇</td></tr><tr><td>val_auc</td><td>▃▁▄▆▆▆▇▇▇▇▇█████████▇▇██████████▇▇▇█████</td></tr><tr><td>val_f1</td><td>▁▄▅▁█▇▇▆▇██▇███▇▇█▇███▇▆▇█▇▇██▇█▃█▇▇▇▆▇▇</td></tr><tr><td>val_loss_epoch</td><td>█▄▃▅▂▂▂▃▂▁▁▁▁▁▁▁▂▁▂▁▁▁▂▃▂▁▁▂▂▁▂▂▄▁▁▂▁▂▂▁</td></tr><tr><td>val_loss_step</td><td>█▆▃▆▃▃▃▃▄▃▃▁▁▂▃▃▃▂▃▃▃▃▄▃▄▁▂▄▄▃▄▃▄▂▂▃▂▂▂▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.76033</td></tr><tr><td>train_auc</td><td>0.8469</td></tr><tr><td>train_f1</td><td>0.75513</td></tr><tr><td>train_loss_epoch</td><td>0.48316</td></tr><tr><td>train_loss_step</td><td>0.45139</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.77069</td></tr><tr><td>val_auc</td><td>0.85393</td></tr><tr><td>val_f1</td><td>0.7914</td></tr><tr><td>val_loss_epoch</td><td>0.47291</td></tr><tr><td>val_loss_step</td><td>0.48596</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_64_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/t5gk8zei' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/t5gk8zei</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240106_004944-t5gk8zei\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07c6a0cd7393405182647a19ad5a92bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01691666666883975, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240106_011950-hwrfp4kl</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/hwrfp4kl' target=\"_blank\">MLP_3_64_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/hwrfp4kl' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/hwrfp4kl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 9.2 K \n",
      "2  | head        | Sequential       | 2.1 K \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 4.2 K \n",
      "--------------------------------------------------\n",
      "15.6 K    Trainable params\n",
      "0         Non-trainable params\n",
      "15.6 K    Total params\n",
      "0.062     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▆▇▇▇██▇██▇██▇▇██████████▇█████▇████████</td></tr><tr><td>train_auc</td><td>▁▅▇▇████████████████████████████████████</td></tr><tr><td>train_f1</td><td>▁▇▇█▇███████████████████████████████████</td></tr><tr><td>train_loss_epoch</td><td>█▅▃▂▂▂▂▂▂▁▂▂▁▂▂▂▂▂▂▁▂▂▂▁▁▂▁▂▁▂▁▂▁▂▁▁▁▂▂▁</td></tr><tr><td>train_loss_step</td><td>█▅▅▂▄▃▂▂▃▃▄▂▃▃▄▃▂▃▄▂▃▄▂▅▃▁▄▁▃▆▄▂▁▃▂▄▂▂▄▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▄▇▇▇▆▇█▇▇██▇▆█▆▆█▇▇███▇▇▇▆▇█▇▇██▇▇▇▇█▇▇</td></tr><tr><td>val_auc</td><td>▁▁▄▆▇▇▇▇▇███████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▅▇████▇█▇█▇██▇▇▆███▇██▇███▇███████▆████</td></tr><tr><td>val_loss_epoch</td><td>█▃▂▂▂▁▂▂▁▂▂▂▁▂▂▁▂▁▁▁▂▁▂▂▂▂▁▁▂▁▁▁▂▁▁▂▂▂▁▁</td></tr><tr><td>val_loss_step</td><td>█▅▂▄▂▁▄▃▂▃▃▃▂▃▃▂▃▂▃▂▃▃▄▃▅▄▂▂▃▂▃▃▄▄▃▄▅▄▃▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.77863</td></tr><tr><td>train_auc</td><td>0.85477</td></tr><tr><td>train_f1</td><td>0.77092</td></tr><tr><td>train_loss_epoch</td><td>0.47537</td></tr><tr><td>train_loss_step</td><td>0.42653</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.78251</td></tr><tr><td>val_auc</td><td>0.85416</td></tr><tr><td>val_f1</td><td>0.80342</td></tr><tr><td>val_loss_epoch</td><td>0.4774</td></tr><tr><td>val_loss_step</td><td>0.51012</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_3_64_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/hwrfp4kl' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/hwrfp4kl</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240106_011950-hwrfp4kl\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "097e32a2f3e841fe8d07a8b96c741225",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01693333333435779, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240106_014938-jgepbq0f</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/jgepbq0f' target=\"_blank\">MLP_4_16_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/jgepbq0f' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/jgepbq0f</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 1.0 K \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "1.2 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.2 K     Total params\n",
      "0.005     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▂▂▃▆▇▇▇▇▇▇█▇█▇▇▇███▇▇▇▇▇██████████████</td></tr><tr><td>train_auc</td><td>▁▁▂▂▃▆▇▇▇▇█▇█▇█▇▇▇▇███▇▇▇▇██████████████</td></tr><tr><td>train_f1</td><td>▁▄▃▃▅▇▇▇▇▇█▇███████████▇████████████████</td></tr><tr><td>train_loss_epoch</td><td>████▇▄▄▃▃▃▂▃▂▃▂▃▂▃▂▂▂▂▂▂▂▃▂▂▁▁▂▂▂▁▂▂▂▁▂▂</td></tr><tr><td>train_loss_step</td><td>█████▄▅▃▄▅▄▃▅▁▄▃▃▄▅▄▄▃▃▅▄▃▃▄▄▃▃▄▄▁▄▄▃▄▃▄</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▁▁▁▄▆██████████████████████████████████</td></tr><tr><td>val_auc</td><td>▁▅▅▅▇▇▇▇▇▇██████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▅▇██████████████████████████████████</td></tr><tr><td>val_loss_epoch</td><td>████▇▃▂▂▂▂▂▂▂▁▂▂▁▂▂▁▂▂▁▂▁▁▂▁▁▁▁▁▂▁▁▁▂▁▁▁</td></tr><tr><td>val_loss_step</td><td>████▇▄▃▃▂▂▃▂▂▂▃▃▂▂▃▂▂▂▂▃▁▂▃▂▂▂▂▂▃▁▂▂▂▂▃▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.76033</td></tr><tr><td>train_auc</td><td>0.82185</td></tr><tr><td>train_f1</td><td>0.74206</td></tr><tr><td>train_loss_epoch</td><td>0.51882</td></tr><tr><td>train_loss_step</td><td>0.52021</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.77778</td></tr><tr><td>val_auc</td><td>0.85047</td></tr><tr><td>val_f1</td><td>0.79654</td></tr><tr><td>val_loss_epoch</td><td>0.47951</td></tr><tr><td>val_loss_step</td><td>0.48922</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_16_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/jgepbq0f' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/jgepbq0f</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240106_014938-jgepbq0f\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "713fd8419e47441bbf842e1e79ff5d6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240106_021854-dc28cs7q</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/dc28cs7q' target=\"_blank\">MLP_4_16_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/dc28cs7q' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/dc28cs7q</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 1.0 K \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "1.2 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.2 K     Total params\n",
      "0.005     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e327f03f26e24e06bcb0e8714d07e992",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▁▂▁▁▃▆▆▆▆▆▆▆▇▇▆█▇█▇█▆▇▇█▇▇▇▇▇▇▇▇▇▇▇█▇█</td></tr><tr><td>train_auc</td><td>▁▁▂▁▂▁▃▆▆▅▆▆▇▇▇█▇█▇██▇▇▇▇██▇▇▇▇█▇▇█▇▇███</td></tr><tr><td>train_f1</td><td>▁▂▂▁▁▃▃▇▇▆▇▆▇▇▇▇▆█████▆▇▇██▇▇▇▇█▇▇▇█▇█▇█</td></tr><tr><td>train_loss_epoch</td><td>█▇▇▇▇▇▆▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▃▂▂▃▁▂▂▂▂▂▁▂▁</td></tr><tr><td>train_loss_step</td><td>██▇███▇▆▅▆▅▆▅▃▅▄▄▄▆▃▆▃▄▇▆▁▅▃▃▂▅▃▄▂▅▃▅▁▃▄</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_auc</td><td>▁▆▆▇▇▇▇█▇▇▇▇█▇▇▇▇▇▇▇▇▇▇▇▇▇█▇▇▇▇▇█▇▇▇█▇▇▇</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_epoch</td><td>▆▆▅▅▄▄▄▆██▆▄▅▄▃▃▅▄▂▁▃▁▂▃▄▁▁▁▁▃▃▂▂▃▃▂▃▂▃▂</td></tr><tr><td>val_loss_step</td><td>▆▅▅▆▅▅▆▆██▅▄▆▆▃▆▇▃▃▂▄▁▂▄▅▃▃▁▂▄▃▅▃▄▃▁▃▄▅▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.67532</td></tr><tr><td>train_auc</td><td>0.72088</td></tr><tr><td>train_f1</td><td>0.64424</td></tr><tr><td>train_loss_epoch</td><td>0.61997</td></tr><tr><td>train_loss_step</td><td>0.63121</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.45863</td></tr><tr><td>val_auc</td><td>0.74684</td></tr><tr><td>val_f1</td><td>0.0</td></tr><tr><td>val_loss_epoch</td><td>0.68957</td></tr><tr><td>val_loss_step</td><td>0.68667</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_16_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/dc28cs7q' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/dc28cs7q</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240106_021854-dc28cs7q\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc45a8f4951d4637961692e65d8da430",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016933333330477276, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240106_024817-qkzr8th5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/qkzr8th5' target=\"_blank\">MLP_4_16_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/qkzr8th5' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/qkzr8th5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 1.0 K \n",
      "2 | head        | Sequential       | 154   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "1.2 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.2 K     Total params\n",
      "0.005     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0110b06483d9445595241a65802fca86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▂▂▂▃▄▅▆▆▆▇▇▇▇▇▇▇▇██▇▇▇█▇█████████▇▇███</td></tr><tr><td>train_auc</td><td>▁▁▂▂▃▄▅▆▇▇▇████▇██▇██▇▆▆▆▆▆▅▅▅▅▅▅▅▅▅▆▄▅▅</td></tr><tr><td>train_f1</td><td>▄▃▂▁▂▃▃▅▆▇▇▇▇▇▇▇▇█▇██▇▇█████████████████</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▅▄▃▃▃▃▃▃▃▂▂▂▁▂▂▂▂▂▂▂▂▂▂▂▁▁▂▂▁▂▂▁▁▂▂▁▂▁▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▂▁▁▂▃▃▆▇▆█▇█▇███████▇███▇██▇███▇▇██▆████</td></tr><tr><td>val_auc</td><td>▅▁▄▄▆▇▇▇▆▇█▇▇▇██▇█████▇█████▇▇██████████</td></tr><tr><td>val_f1</td><td>▆▁▂▄▄▄▇███▇█████████▇███▇██▇███████▇████</td></tr><tr><td>val_loss_epoch</td><td>███▇▇▆▅▄▅▄▃▃▂▂▃▂▃▂▂▂▂▂▂▂▂▂▂▂▁▁▂▂▂▁▁▃▁▁▂▁</td></tr><tr><td>val_loss_step</td><td>████▇▆▆▄▅▄▄▂▃▂▃▃▂▂▃▂▃▁▃▃▁▂▂▂▁▂▂▂▃▁▂▂▁▁▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.74321</td></tr><tr><td>train_auc</td><td>0.65049</td></tr><tr><td>train_f1</td><td>0.73427</td></tr><tr><td>train_loss_epoch</td><td>0.53075</td></tr><tr><td>train_loss_step</td><td>0.56675</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.79196</td></tr><tr><td>val_auc</td><td>0.87291</td></tr><tr><td>val_f1</td><td>0.8009</td></tr><tr><td>val_loss_epoch</td><td>0.48148</td></tr><tr><td>val_loss_step</td><td>0.488</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_16_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/qkzr8th5' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/qkzr8th5</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240106_024817-qkzr8th5\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "354fda59b5644d73ad1226be32bb5332",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01691666666883975, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240106_031740-dcuf1xh1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/dcuf1xh1' target=\"_blank\">MLP_4_16_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/dcuf1xh1' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/dcuf1xh1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 1.0 K \n",
      "2  | head        | Sequential       | 154   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 17    \n",
      "--------------------------------------------------\n",
      "1.2 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.2 K     Total params\n",
      "0.005     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13005fafc9124a489189eb39e9a573f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▁▂▄▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇█▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█</td></tr><tr><td>train_auc</td><td>▁▁▁▂▅▇▇▇▇▇▇█▇█▇▇██▇▇█▇███▇███▇█████▇████</td></tr><tr><td>train_f1</td><td>▁▄▂▃▆▇▇▇▇▇▇███▇█████████████████▇█████▇█</td></tr><tr><td>train_loss_epoch</td><td>████▆▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▂▁</td></tr><tr><td>train_loss_step</td><td>████▆▆▅▂▄▃▃▂▃▃▄▃▁▃▂▂▂▃▂▂▃▁▄▃▄▁▃▃▄▄▁▄▃▅▃▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▁▁▁▇▅██▅▇██▇▅█▆▇▅▇▆▇▇▅▆▇▅▆▇▆▆▇▅▆▇▆▆▆▆█▆</td></tr><tr><td>val_auc</td><td>▁▅▅▇████████████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▇▆██▆███▇▆█▇▇▆▇▇▇█▆▇▇▆▇▇▇▇▇▆▇▇▇▇▇▇█▆</td></tr><tr><td>val_loss_epoch</td><td>████▅▅▁▂▄▂▁▂▂▅▁▄▃▅▃▄▂▂▅▃▃▅▄▃▅▃▂▅▃▂▄▄▅▄▁▅</td></tr><tr><td>val_loss_step</td><td>████▆▅▂▃▅▃▃▄▄▆▂▄▅▅▄▅▂▂▇▂▅▅▅▅█▄▄▅▄▃▄▄█▆▁▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.78276</td></tr><tr><td>train_auc</td><td>0.84672</td></tr><tr><td>train_f1</td><td>0.7848</td></tr><tr><td>train_loss_epoch</td><td>0.49484</td></tr><tr><td>train_loss_step</td><td>0.49923</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.67139</td></tr><tr><td>val_auc</td><td>0.84655</td></tr><tr><td>val_f1</td><td>0.61281</td></tr><tr><td>val_loss_epoch</td><td>0.62216</td></tr><tr><td>val_loss_step</td><td>0.67728</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_16_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/dcuf1xh1' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/dcuf1xh1</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240106_031740-dcuf1xh1\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faa4804c1ce14fbf8804ed8af7b42597",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01691666666883975, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240106_034719-06156yfd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/06156yfd' target=\"_blank\">MLP_4_16_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/06156yfd' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/06156yfd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 1.0 K \n",
      "2  | head        | Sequential       | 154   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 281   \n",
      "--------------------------------------------------\n",
      "1.5 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.5 K     Total params\n",
      "0.006     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf6b816a262a4cda94cf87e326b41292",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▁▂▄▇▇▇▇▇█▇██████████████▇█▇███████████</td></tr><tr><td>train_auc</td><td>▁▂▂▂▄▇▇▇█▇▇▇████████████████████████████</td></tr><tr><td>train_f1</td><td>▁▅▃▃▆██▇█▇██████████████████████████████</td></tr><tr><td>train_loss_epoch</td><td>████▆▂▂▂▂▃▂▂▁▂▁▂▁▁▁▁▁▂▁▂▁▂▁▁▂▂▂▂▁▂▁▁▁▂▁▁</td></tr><tr><td>train_loss_step</td><td>███▇▇▄▃▃▃▃▄▄▁▂▄▂▃▁▂▄▃▂▁▄▄▃▅▃▆▂▃▁▁▂▂▂▂▃▃▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▁▁▁▆█▇███▇█████████████████████████████</td></tr><tr><td>val_auc</td><td>▁▄▅▆▇▇██████████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▇█▇█████████████████████████████████</td></tr><tr><td>val_loss_epoch</td><td>████▅▁▃▂▂▁▂▂▁▁▁▂▁▁▁▂▂▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▂▁▂▁</td></tr><tr><td>val_loss_step</td><td>████▆▂▄▃▂▂▃▃▂▂▁▂▂▂▁▂▂▂▁▂▂▁▃▂▂▂▂▂▂▁▁▂▃▂▃▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.76151</td></tr><tr><td>train_auc</td><td>0.82833</td></tr><tr><td>train_f1</td><td>0.74527</td></tr><tr><td>train_loss_epoch</td><td>0.50828</td></tr><tr><td>train_loss_step</td><td>0.51928</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.78014</td></tr><tr><td>val_auc</td><td>0.84685</td></tr><tr><td>val_f1</td><td>0.80338</td></tr><tr><td>val_loss_epoch</td><td>0.47967</td></tr><tr><td>val_loss_step</td><td>0.4803</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_16_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/06156yfd' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/06156yfd</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240106_034719-06156yfd\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f67dfed42f1f4e038d71add5747bc36d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01693333333435779, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240106_041601-lc6xoyby</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/lc6xoyby' target=\"_blank\">MLP_4_32_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/lc6xoyby' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/lc6xoyby</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 3.6 K \n",
      "2 | head        | Sequential       | 562   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "4.2 K     Trainable params\n",
      "0         Non-trainable params\n",
      "4.2 K     Total params\n",
      "0.017     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e0017464dc04c7488f233c2d55ae8a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▂▃▇▇▇▇█▇▇████████▇▇█████▇███▇▇███▇████</td></tr><tr><td>train_auc</td><td>▁▂▂▃▇▇██████████████████████████████████</td></tr><tr><td>train_f1</td><td>▄▂▁▄▇▇█▇██▇█████████████████████████████</td></tr><tr><td>train_loss_epoch</td><td>████▄▂▂▂▁▂▂▂▂▁▁▁▁▁▁▂▁▂▂▁▂▁▁▁▁▁▂▂▁▂▁▂▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>████▄▃▃▂▂▂▃▂▃▃▁▃▂▂▃▃▁▄▂▁▂▁▂▄▂▂▃▂▄▃▁▂▂▂▂▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▁▁▇█▇█████████████████████████████▇████</td></tr><tr><td>val_auc</td><td>▁▄▄▇▆▇▇▇▇███████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▇███████████████████████████████▇████</td></tr><tr><td>val_loss_epoch</td><td>███▇▂▂▂▂▂▂▁▂▂▁▂▂▁▁▂▂▁▁▁▁▁▁▁▁▁▁▂▁▁▂▂▂▁▁▁▁</td></tr><tr><td>val_loss_step</td><td>███▇▄▃▃▄▃▂▃▃▂▂▃▄▂▂▃▄▂▂▂▂▂▁▃▂▃▃▂▂▁▂▄▃▃▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.77745</td></tr><tr><td>train_auc</td><td>0.84135</td></tr><tr><td>train_f1</td><td>0.76829</td></tr><tr><td>train_loss_epoch</td><td>0.49891</td></tr><tr><td>train_loss_step</td><td>0.51991</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.78487</td></tr><tr><td>val_auc</td><td>0.85322</td></tr><tr><td>val_f1</td><td>0.80088</td></tr><tr><td>val_loss_epoch</td><td>0.46895</td></tr><tr><td>val_loss_step</td><td>0.44204</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_32_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/lc6xoyby' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/lc6xoyby</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240106_041601-lc6xoyby\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05e9c306347549f9a6713af04f8b0a5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240106_044423-8dta18pz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/8dta18pz' target=\"_blank\">MLP_4_32_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/8dta18pz' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/8dta18pz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 3.6 K \n",
      "2 | head        | Sequential       | 562   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "4.2 K     Trainable params\n",
      "0         Non-trainable params\n",
      "4.2 K     Total params\n",
      "0.017     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2ebbf6313754b12a92b8d5932043a9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▂▃▃▃▄▆▇▇▇▇▇▇▆▇▇▇▇▇▇▇▇▇▇█▇▇▇▇██▇▇▇▇█▇▇█</td></tr><tr><td>train_auc</td><td>▁▂▁▂▂▃▄▆▇▆▆▆▇▆▇▇▇▇▇▇▇▇▇▇▇███████████████</td></tr><tr><td>train_f1</td><td>▃▅▁▄▅▃▆▇▇▇▇▇▇▇▆▆▇▆▇▇▇▇▇▇▇█▇▇▇▇██▇▇▇▇▇▇▇▇</td></tr><tr><td>train_loss_epoch</td><td>█▇▇▇▇▇▆▄▄▄▄▃▄▄▄▃▂▃▃▂▂▃▂▃▃▂▁▁▂▁▂▁▂▂▂▂▁▂▁▂</td></tr><tr><td>train_loss_step</td><td>█████▇▇▅▄▄▃▄▃▅▃▅▃▄▃▄▃▄▄▁▁▂▂▂▃▂▂▃▄▁▃▅▃▂▁▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▅▅█▅▅▅▅████</td></tr><tr><td>val_auc</td><td>▁▁▆▇▇▇▇▇▆▇▇▇▇▇█▇█████▇▇█▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▄▄█▄▄▄▄████</td></tr><tr><td>val_loss_epoch</td><td>███████▅▆▅▅▄▄▄▅▄▄▅▅▅▄▅▆▄▄▄▂▃▄▅▂▂▄▅▃▁▃▃▄▁</td></tr><tr><td>val_loss_step</td><td>▇▇▇▇▇▇▆▆▆▆▅▅▆▅▆▆▆▆▅▆▅▆▇▆▆▆▄▅▆█▃▄▆▆▅▁▆▅▅▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.71901</td></tr><tr><td>train_auc</td><td>0.76669</td></tr><tr><td>train_f1</td><td>0.69682</td></tr><tr><td>train_loss_epoch</td><td>0.57999</td></tr><tr><td>train_loss_step</td><td>0.58093</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.74941</td></tr><tr><td>val_auc</td><td>0.74483</td></tr><tr><td>val_f1</td><td>0.8037</td></tr><tr><td>val_loss_epoch</td><td>0.62633</td></tr><tr><td>val_loss_step</td><td>0.60796</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_32_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/8dta18pz' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/8dta18pz</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240106_044423-8dta18pz\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9eb338b75b754d38b2f2ee96c819e0f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01691666666883975, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240106_051256-u2c41409</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/u2c41409' target=\"_blank\">MLP_4_32_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/u2c41409' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/u2c41409</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 3.6 K \n",
      "2 | head        | Sequential       | 562   \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "4.2 K     Trainable params\n",
      "0         Non-trainable params\n",
      "4.2 K     Total params\n",
      "0.017     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68f215e5601c40a583ce2536c4f01a2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▂▂▂▄▅▅▆▆▆▆▇▇▇█▇▇▇▇▇▇▇█▇█████▇███▇█████</td></tr><tr><td>train_auc</td><td>▁▂▂▂▂▃▄▅▅▆▅▆▆▇▆▇▇▇▇▆▇▇▆▇▇▇▇▇▆▇▇▇▇▇▇█▇███</td></tr><tr><td>train_f1</td><td>▁▂▃▄▄▃▄▆▆▅▆▆▇▇▇▇▇▇▇▇▇▇▇██▇▇█████████████</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▄▃▃▃▂▃▂▂▂▂▂▂▁▁▂▁▂▂▂▁▂▂▁▁▁▁▂▁▁▂▁▂▂▁▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▁▁▁▅▄▄▃▃▇▄▇▅▆▇▇▆█▇▇▆▆█▇▆▇█▇▇▇▆█▆▇▇▇▇█▇█</td></tr><tr><td>val_auc</td><td>▄▁▄▄▆▆▆▇▇▇█▇████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▁▁▆▅▅▄▄▇▅▇▆▇█▇▇█▇▇▆▇██▇█████▇█▇█▇█████</td></tr><tr><td>val_loss_epoch</td><td>██▇▇▆▆▅▆▆▃▅▃▄▃▂▂▂▂▂▂▃▂▁▂▂▁▂▂▂▂▃▁▂▂▂▁▁▁▁▁</td></tr><tr><td>val_loss_step</td><td>██▇▇▆▆▆▆▅▃▅▃▃▄▂▃▂▂▃▄▄▃▂▂▂▁▂▂▂▂▃▂▂▂▃▂▃▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.77686</td></tr><tr><td>train_auc</td><td>0.77465</td></tr><tr><td>train_f1</td><td>0.76838</td></tr><tr><td>train_loss_epoch</td><td>0.50272</td></tr><tr><td>train_loss_step</td><td>0.50384</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.80142</td></tr><tr><td>val_auc</td><td>0.86837</td></tr><tr><td>val_f1</td><td>0.81333</td></tr><tr><td>val_loss_epoch</td><td>0.45348</td></tr><tr><td>val_loss_step</td><td>0.42715</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_32_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/u2c41409' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/u2c41409</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240106_051256-u2c41409\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3630704564c466e9e787555c5463cde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01691666666883975, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240106_054134-wc8234o5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/wc8234o5' target=\"_blank\">MLP_4_32_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/wc8234o5' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/wc8234o5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 3.6 K \n",
      "2  | head        | Sequential       | 562   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 33    \n",
      "--------------------------------------------------\n",
      "4.2 K     Trainable params\n",
      "0         Non-trainable params\n",
      "4.2 K     Total params\n",
      "0.017     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "954c2e6923724350bf6ad24cf5a31bb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▂▅▅▇▇▇▇▇███▇▇█▇██▇██▇▇██████▇██████▇██</td></tr><tr><td>train_auc</td><td>▁▁▂▅▆▇▇▇████████████████████████████████</td></tr><tr><td>train_f1</td><td>▄▂▁▆▆▇▇▇████████▇███████████████████████</td></tr><tr><td>train_loss_epoch</td><td>███▆▅▃▂▃▂▂▂▂▁▁▂▁▂▂▁▂▁▁▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁</td></tr><tr><td>train_loss_step</td><td>███▆▆▄▄▄▃▄▂▄▃▃▂▃▅▄▃▂▃▄▁▆▃▃▂▅▁▂▂▂▂▄▃▃▃▁▂▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▁▁▃▆██████▇▇██████▇███▆███▇████████████</td></tr><tr><td>val_auc</td><td>▁▆▆▅▆▇██████████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▂▄▇██████▇▇██████▇███▆████████████████</td></tr><tr><td>val_loss_epoch</td><td>███▇▄▂▂▂▂▁▂▂▂▁▁▂▁▁▁▃▁▁▁▄▂▂▁▂▁▁▂▂▁▂▁▁▂▂▁▁</td></tr><tr><td>val_loss_step</td><td>████▅▄▂▂▃▂▃▃▄▂▂▃▂▁▂▃▂▃▁▇▃▃▃▃▂▁▂▂▂▃▂▁▄▄▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.75325</td></tr><tr><td>train_auc</td><td>0.83828</td></tr><tr><td>train_f1</td><td>0.75178</td></tr><tr><td>train_loss_epoch</td><td>0.49823</td></tr><tr><td>train_loss_step</td><td>0.51023</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.78487</td></tr><tr><td>val_auc</td><td>0.85029</td></tr><tr><td>val_f1</td><td>0.79912</td></tr><tr><td>val_loss_epoch</td><td>0.47934</td></tr><tr><td>val_loss_step</td><td>0.4751</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_32_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/wc8234o5' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/wc8234o5</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240106_054134-wc8234o5\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17b060e1e3004e578d2263d76fc27dcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016933333330477276, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240106_061303-u2pu57a7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/u2pu57a7' target=\"_blank\">MLP_4_32_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/u2pu57a7' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/u2pu57a7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 3.6 K \n",
      "2  | head        | Sequential       | 562   \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 1.1 K \n",
      "--------------------------------------------------\n",
      "5.3 K     Trainable params\n",
      "0         Non-trainable params\n",
      "5.3 K     Total params\n",
      "0.021     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0165f091e7624fca8bef26cc53c41d9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▂▅▇▇▇▇▇██▇█▇████▇█▇███████▇██▇███▇████</td></tr><tr><td>train_auc</td><td>▁▁▂▆▇▇█████▇████████▇███████▇███████████</td></tr><tr><td>train_f1</td><td>▆▁▅▇▇██▇████████████████████████████████</td></tr><tr><td>train_loss_epoch</td><td>███▅▃▂▂▂▂▂▂▃▂▂▂▁▁▁▂▂▂▂▁▁▁▁▂▁▂▁▁▂▁▁▁▂▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>███▆▄▁▃▃▂▁▂▄▃▃▄▃▃▂▂▃▂▂▃▃▄▁▂▁▂▂▃▃▂▃▃▂▃▃▃▄</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▁▂▆▇▇▇████████████████████████████▇████</td></tr><tr><td>val_auc</td><td>▁▃▄▄▆▇▇▇████████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▁▃▇████████████████████████████████████</td></tr><tr><td>val_loss_epoch</td><td>███▃▂▂▂▂▁▁▁▂▁▂▂▁▁▁▁▁▂▁▁▂▁▂▁▂▁▁▁▁▂▁▁▂▁▁▂▁</td></tr><tr><td>val_loss_step</td><td>███▄▃▄▃▅▃▂▂▃▂▄▃▁▃▂▃▃▄▂▂▄▃▂▃▄▃▂▃▂▂▂▂▃▃▂▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.76505</td></tr><tr><td>train_auc</td><td>0.84347</td></tr><tr><td>train_f1</td><td>0.76168</td></tr><tr><td>train_loss_epoch</td><td>0.49542</td></tr><tr><td>train_loss_step</td><td>0.54896</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.78251</td></tr><tr><td>val_auc</td><td>0.85115</td></tr><tr><td>val_f1</td><td>0.80172</td></tr><tr><td>val_loss_epoch</td><td>0.47671</td></tr><tr><td>val_loss_step</td><td>0.49053</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_32_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/u2pu57a7' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/u2pu57a7</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240106_061303-u2pu57a7\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e5026189956456da922b5978275087d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240106_064144-ilr6o1io</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/ilr6o1io' target=\"_blank\">MLP_4_64_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/ilr6o1io' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/ilr6o1io</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 13.4 K\n",
      "2 | head        | Sequential       | 2.1 K \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "15.5 K    Trainable params\n",
      "0         Non-trainable params\n",
      "15.5 K    Total params\n",
      "0.062     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2061ed7b70e0460881981af43c22c498",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▄▆████████████▇██▇▇████████████▇███████</td></tr><tr><td>train_auc</td><td>▁▃▆████████████▇██▇█████████████████████</td></tr><tr><td>train_f1</td><td>▁▄▆█▇▇█▇███████▇████████████████▇▇██████</td></tr><tr><td>train_loss_epoch</td><td>█▇▄▂▂▂▂▂▁▁▁▁▁▂▁▂▁▁▂▂▁▂▁▁▁▂▂▁▁▁▁▁▂▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▇▃▃▃▃▃▂▂▂▂▃▃▄▂▂▂▄▂▂▂▃▁▂▁▃▂▃▃▂▄▂▂▃▁▃▂▂▂▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▂▇▇███████████▇████████████████████████</td></tr><tr><td>val_auc</td><td>▁▄▅▅▆▇▇▇▇▇██████████████████████████████</td></tr><tr><td>val_f1</td><td>▁▂██████████████████████████████████████</td></tr><tr><td>val_loss_epoch</td><td>█▇▂▂▂▂▂▂▁▂▁▂▁▁▁▂▂▁▂▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss_step</td><td>█▇▃▄▃▄▂▂▃▃▁▃▂▁▂▂▃▁▃▁▂▁▁▂▂▁▂▃▁▃▂▂▂▃▃▃▂▃▄▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.76741</td></tr><tr><td>train_auc</td><td>0.84659</td></tr><tr><td>train_f1</td><td>0.76092</td></tr><tr><td>train_loss_epoch</td><td>0.4876</td></tr><tr><td>train_loss_step</td><td>0.44941</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.78251</td></tr><tr><td>val_auc</td><td>0.85389</td></tr><tr><td>val_f1</td><td>0.80172</td></tr><tr><td>val_loss_epoch</td><td>0.47371</td></tr><tr><td>val_loss_step</td><td>0.48919</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_64_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/ilr6o1io' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/ilr6o1io</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240106_064144-ilr6o1io\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55480dc3ae554ff8b7dbe365f611d349",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240106_071101-x6yfxsmn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/x6yfxsmn' target=\"_blank\">MLP_4_64_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/x6yfxsmn' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/x6yfxsmn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 13.4 K\n",
      "2 | head        | Sequential       | 2.1 K \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "15.5 K    Trainable params\n",
      "0         Non-trainable params\n",
      "15.5 K    Total params\n",
      "0.062     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cabdef2146074116a35e531853565b52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▃▃▆▆▇▇▇▇▇▇▇▇▇▇█▇▇▇▇██▇█▇██▇▇██▇███▇▇▇▇</td></tr><tr><td>train_auc</td><td>▁▁▂▃▆▆▆▇▇▇▇▇▇▇▇▇▇█▇▇████████▇▇██▇███▇▇▇█</td></tr><tr><td>train_f1</td><td>▁▃▂▂▅▆▆▇▇▇▇▇▇▇▇▇█▇▇▇▇█▇██▇▇█▇▇██▇███▇█▇▇</td></tr><tr><td>train_loss_epoch</td><td>██▇▇▅▄▄▃▃▃▃▃▃▂▃▃▂▂▃▂▂▂▂▂▂▂▂▁▂▂▂▂▃▂▂▁▂▂▂▂</td></tr><tr><td>train_loss_step</td><td>██▇▆▅▆▆▃▄▂▄▃▄▅▃▂▂▄▁▃▃▃▂▂▁▃▃▂▃▅▂▂▃▂▂▃▂▃▃▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█</td></tr><tr><td>val_auc</td><td>▁▆▅▇▆▇▇▇█▇████████▇█▇▇▇▇▇▆▆▆▆▆▇▇▇▆▇▇▇▆▆▆</td></tr><tr><td>val_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█</td></tr><tr><td>val_loss_epoch</td><td>▂▂▂▂▁▁▁▁▁▂▂▂▂▃▃▄▄▅▅▅▆▇▅▅▇▅█▄▄▄▅█▅▅▆▆▅▃▄▃</td></tr><tr><td>val_loss_step</td><td>▂▂▂▃▁▃▁▁▁▄▂▃▃▄▂▃▃▅▇▅▆▆▆▄▆▄▇▄▃▂▄█▃▆▆▆▆▃▄▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.72904</td></tr><tr><td>train_auc</td><td>0.80681</td></tr><tr><td>train_f1</td><td>0.71544</td></tr><tr><td>train_loss_epoch</td><td>0.52743</td></tr><tr><td>train_loss_step</td><td>0.49829</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.6383</td></tr><tr><td>val_auc</td><td>0.7495</td></tr><tr><td>val_f1</td><td>0.74711</td></tr><tr><td>val_loss_epoch</td><td>0.7406</td></tr><tr><td>val_loss_step</td><td>0.76533</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_64_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/x6yfxsmn' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/x6yfxsmn</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240106_071101-x6yfxsmn\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f387aa56362e4b36b4871c9bee362a6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240106_073941-n4fju5k4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/n4fju5k4' target=\"_blank\">MLP_4_64_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/n4fju5k4' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/n4fju5k4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | x_embedding | Identity         | 0     \n",
      "1 | model       | MLPModel         | 13.4 K\n",
      "2 | head        | Sequential       | 2.1 K \n",
      "3 | loss_module | CrossEntropyLoss | 0     \n",
      "4 | train_acc   | BinaryAccuracy   | 0     \n",
      "5 | train_auroc | BinaryAUROC      | 0     \n",
      "6 | train_f1    | BinaryF1Score    | 0     \n",
      "7 | valid_acc   | BinaryAccuracy   | 0     \n",
      "8 | valid_auroc | BinaryAUROC      | 0     \n",
      "9 | valid_f1    | BinaryF1Score    | 0     \n",
      "-------------------------------------------------\n",
      "15.5 K    Trainable params\n",
      "0         Non-trainable params\n",
      "15.5 K    Total params\n",
      "0.062     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▂▂▄▆▇▇▇▇▇▇▇▇▇▇▇█▇██▇█▇███▇█████▇██████</td></tr><tr><td>train_auc</td><td>▂▁▂▁▄▆▆▇▇▇▇██▇▇█▇█▇█████▇███▇██▇█▇▇▇▇███</td></tr><tr><td>train_f1</td><td>▁▁▁▃▄▆▆▇▇▇▇▇▇▇▇▇▇▇▇██▇███████████▇██████</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▄▃▃▃▂▂▂▂▂▂▂▂▂▁▁▂▂▁▂▁▂▁▁▁▂▁▂▂▁▂▁▂▂▁▂▂▂▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▁▁▄▆▇▆▅▅▇▇▇▇▆▇████▇▇██▇▇██▇▇█▇█▇▇██████</td></tr><tr><td>val_auc</td><td>▅▁▅▆▇▇██████████████████████████████████</td></tr><tr><td>val_f1</td><td>▆▆▁▅▆▇▆▅▅▇█▇▇▆█████▇▇██▇▇██▇▇███▇▇██████</td></tr><tr><td>val_loss_epoch</td><td>███▇▆▄▃▃▄▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▂▁▁▂▂▂▁▁▂▂▂▂▁▁▂▁</td></tr><tr><td>val_loss_step</td><td>██▇▇▆▅▃▄▅▃▂▃▂▂▁▂▂▂▂▁▂▂▂▃▂▁▁▃▂▃▂▂▄▃▃▃▁▂▄▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.77037</td></tr><tr><td>train_auc</td><td>0.80149</td></tr><tr><td>train_f1</td><td>0.76002</td></tr><tr><td>train_loss_epoch</td><td>0.48801</td></tr><tr><td>train_loss_step</td><td>0.43141</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.80615</td></tr><tr><td>val_auc</td><td>0.87856</td></tr><tr><td>val_f1</td><td>0.81938</td></tr><tr><td>val_loss_epoch</td><td>0.45916</td></tr><tr><td>val_loss_step</td><td>0.49251</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_64_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/n4fju5k4' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/n4fju5k4</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240106_073941-n4fju5k4\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11f54cce65ee424386619ebc763e1601",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01691666666495924, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240106_080828-fqfr9693</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/fqfr9693' target=\"_blank\">MLP_4_64_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/fqfr9693' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/fqfr9693</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 13.4 K\n",
      "2  | head        | Sequential       | 2.1 K \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | GlobalAttention  | 65    \n",
      "--------------------------------------------------\n",
      "15.6 K    Trainable params\n",
      "0         Non-trainable params\n",
      "15.6 K    Total params\n",
      "0.062     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6103563e30d4f4aa7c0739feeef5c02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▄▆▇▇█▇▇▇█▇▇▇███████████████████████████</td></tr><tr><td>train_auc</td><td>▁▄▆▇██████▇█▇███████████████████████████</td></tr><tr><td>train_f1</td><td>▁▂▇▇▇▇▇▇▇▇▇█▇█████████████▇█████████████</td></tr><tr><td>train_loss_epoch</td><td>█▇▄▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▂▂▁▂▁▂▁▁▂▂▁▁▂▂▂▁▂▁▁</td></tr><tr><td>train_loss_step</td><td>█▇▄▄▄▄▄▄▇▃▃▃▃▄▁▃▃▃▄▄▃▃▅▂▃▁▅▃▃▃▃▄▃▂▃▃▃▃▄▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▁▇▅▅▇█▆█▄█▇▅▇▇████▇█████▇▇█████████▇▇█▆█</td></tr><tr><td>val_auc</td><td>▁▃▄▅▆▇▇▇▇▇▇▇▇▇▇██████████▇▇█████████████</td></tr><tr><td>val_f1</td><td>▅█▄▃▆█▆█▁█▇▃█▆█▇▇█▇██████▇████▇███████▄█</td></tr><tr><td>val_loss_epoch</td><td>█▆▄▄▂▂▂▁▆▁▁▄▁▂▁▂▂▁▂▁▁▁▁▁▁▂▂▁▂▁▂▁▁▁▂▁▁▂▄▂</td></tr><tr><td>val_loss_step</td><td>█▆▅▄▂▃▂▁▄▂▂▅▁▂▃▃▃▂▅▂▃▂▃▂▃▄▃▁▄▂▃▃▄▂▄▃▂▂▅▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.76092</td></tr><tr><td>train_auc</td><td>0.84601</td></tr><tr><td>train_f1</td><td>0.75705</td></tr><tr><td>train_loss_epoch</td><td>0.48024</td></tr><tr><td>train_loss_step</td><td>0.42942</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.79669</td></tr><tr><td>val_auc</td><td>0.84991</td></tr><tr><td>val_f1</td><td>0.80543</td></tr><tr><td>val_loss_epoch</td><td>0.48636</td></tr><tr><td>val_loss_step</td><td>0.47635</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_64_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/fqfr9693' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/fqfr9693</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240106_080828-fqfr9693\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bb673428b3d4ae3b44f6717eac01981",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01693333333435779, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>y:\\coskun-lab\\Thomas\\23_PLA_revision\\notebooks\\wandb\\run-20240106_083713-387fu8b8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/387fu8b8' target=\"_blank\">MLP_4_64_onehot_4</a></strong> to <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/387fu8b8' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/387fu8b8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "   | Name        | Type             | Params\n",
      "--------------------------------------------------\n",
      "0  | x_embedding | Identity         | 0     \n",
      "1  | model       | MLPModel         | 13.4 K\n",
      "2  | head        | Sequential       | 2.1 K \n",
      "3  | loss_module | CrossEntropyLoss | 0     \n",
      "4  | train_acc   | BinaryAccuracy   | 0     \n",
      "5  | train_auroc | BinaryAUROC      | 0     \n",
      "6  | train_f1    | BinaryF1Score    | 0     \n",
      "7  | valid_acc   | BinaryAccuracy   | 0     \n",
      "8  | valid_auroc | BinaryAUROC      | 0     \n",
      "9  | valid_f1    | BinaryF1Score    | 0     \n",
      "10 | pool        | Attention_module | 4.2 K \n",
      "--------------------------------------------------\n",
      "19.7 K    Trainable params\n",
      "0         Non-trainable params\n",
      "19.7 K    Total params\n",
      "0.079     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f4dc7e61f054ae5a152f4284a607bf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr-Adam</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▇▇█▇▇▇▇▇▇██████████████▇█▇██▇██████▇▇█</td></tr><tr><td>train_auc</td><td>▁▁▆▇████████████████████████████████████</td></tr><tr><td>train_f1</td><td>▃▁▇▇████████████████████████████████████</td></tr><tr><td>train_loss_epoch</td><td>██▅▃▂▂▁▂▂▂▁▁▁▁▁▁▁▂▂▁▁▁▁▂▁▁▁▁▁▂▁▁▁▂▁▂▁▂▂▁</td></tr><tr><td>train_loss_step</td><td>██▆▃▃▃▃▃▄▃▃▂▃▁▃▃▃▃▂▃▂▂▃▃▂▂▃▂▁▂▂▂▃▃▃▃▃▂▂▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▃▁▁▂▃▃▄▄▄▄▄▅▅▅▅▅▅▂▂▂▂▆▇▇▃▃▃▇███</td></tr><tr><td>val_acc</td><td>▃▁▇█▇█▇████████████▇████████████████████</td></tr><tr><td>val_auc</td><td>▁▂▃▅▆▆▇▇▇▇▇▇▇██████▇▇▇▇█████████████████</td></tr><tr><td>val_f1</td><td>▇▁▇█▇███████████████████████████████████</td></tr><tr><td>val_loss_epoch</td><td>██▃▂▃▂▂▁▁▂▁▁▂▂▂▁▁▂▁▂▁▁▂▁▁▁▂▂▁▁▁▁▂▁▁▁▂▂▁▁</td></tr><tr><td>val_loss_step</td><td>██▄▄▃▃▃▂▂▃▂▂▃▃▄▁▂▄▁▃▁▃▄▂▁▁▃▄▃▂▂▃▄▁▂▂▃▅▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>lr-Adam</td><td>0.005</td></tr><tr><td>train_acc</td><td>0.76269</td></tr><tr><td>train_auc</td><td>0.84406</td></tr><tr><td>train_f1</td><td>0.75725</td></tr><tr><td>train_loss_epoch</td><td>0.48572</td></tr><tr><td>train_loss_step</td><td>0.46058</td></tr><tr><td>trainer/global_step</td><td>559</td></tr><tr><td>val_acc</td><td>0.7896</td></tr><tr><td>val_auc</td><td>0.85301</td></tr><tr><td>val_f1</td><td>0.8061</td></tr><tr><td>val_loss_epoch</td><td>0.46877</td></tr><tr><td>val_loss_step</td><td>0.45008</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_4_64_onehot_4</strong> at: <a href='https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/387fu8b8' target=\"_blank\">https://wandb.ai/thoomas/PLA_10152023_13PPI_Kfold/runs/387fu8b8</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240106_083713-387fu8b8\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "k_folds = 5\n",
    "kfold = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "for fold, (train_ids, valid_ids) in enumerate(kfold.split(dataset_filtered)):\n",
    "    train_subset = dataset_filtered.index_select(train_ids.tolist())\n",
    "    val_subset = dataset_filtered.index_select(valid_ids.tolist())\n",
    "    for NUM_LAYERS, HIDDEN_CHANNELS, pool, in list(itertools.product(*[num_layers, hiddens, pools])):\n",
    "            # Path to the folder where the pretrained models are saved\n",
    "        # CHECKPOINT_PATH = checkpoint_folder / f'GAT_{NUM_LAYERS}_{HIDDEN_CHANNELS}_onehot_{fold}' / pool\n",
    "        CHECKPOINT_PATH = checkpoint_folder / f'MLP_{NUM_LAYERS}_{HIDDEN_CHANNELS}_onehot_{fold}' / pool\n",
    "        CHECKPOINT_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Skip already trained kfold and pool\n",
    "        checkpoint = CHECKPOINT_PATH / f\"GraphLevel{model}\" / f\"GraphLevel{model}.ckpt\" \n",
    "        if checkpoint.exists():\n",
    "            print(checkpoint)\n",
    "            continue\n",
    "\n",
    "        # Run training\n",
    "        run = wandb.init(project=project_name, name=f'{model}_{NUM_LAYERS}_{HIDDEN_CHANNELS}_onehot_{fold}', \n",
    "                        group=f'{model}_{pool}', \n",
    "                        # mode=\"disabled\"\n",
    "                        )\n",
    "        PPIGraph.train_graph_classifier_kfold(model, \n",
    "                                                train_subset, \n",
    "                                                val_subset, \n",
    "                                                dataset, \n",
    "                                                CHECKPOINT_PATH, \n",
    "                                                AVAIL_GPUS, \n",
    "                                                in_channels=13,\n",
    "                                                hidden_channels=HIDDEN_CHANNELS, \n",
    "                                                out_channels = HIDDEN_CHANNELS,\n",
    "                                                num_layers=NUM_LAYERS, \n",
    "                                                epochs=epochs,\n",
    "                                                embedding=False,\n",
    "                                                batch_size=256,\n",
    "                                                graph_pooling=pool)\n",
    "        run.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snowflake",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
